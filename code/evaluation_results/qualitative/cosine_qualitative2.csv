model_name,query,language,function,url
tfidf_python_100_1.0,convert int to string,python,"def convert_number(string):
    """"""Convert a string to number
    If int convert to int otherwise float
    
    If not possible return None
    """"""
    res = None
    if isint(string):
        res = int(string)
    elif isfloat(string):
        res = float(string) 
    return res",https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/utils/convert.py#L24-L35
tfidf_python_100_1.0,convert int to string,python,"def _to_number(cls, string):
        """"""Convert string to int or float.""""""
        try:
            if float(string) - int(string) == 0:
                return int(string)
            return float(string)
        except ValueError:
            try:
                return float(string)
            except ValueError:
                return string",https://github.com/dgomes/pyipma/blob/cd808abeb70dca0e336afdf55bef3f73973eaa71/pyipma/api.py#L35-L45
tfidf_python_100_1.0,convert int to string,python,"def html_to_red_green_blue(string):
    string = _extract_html_hex(string)
    if string is None:
        return None, None, None
    return int(string[:2], 16), int(string[2:4], 16), int(string[4:], 16)",https://github.com/jalanb/pysyte/blob/4e278101943d1ceb1a6bcaf6ddc72052ecf13114/pysyte/colours/colour_numbers.py#L103-L107
tfidf_python_100_1.0,convert int to string,python,"def datetime_from_str(string):
    """"""

    Args:
        string: string of the form YYMMDD-HH_MM_SS, e.g 160930-18_43_01

    Returns: a datetime object

    """"""


    return datetime.datetime(year=2000+int(string[0:2]), month=int(string[2:4]), day=int(string[4:6]), hour=int(string[7:9]), minute=int(string[10:12]),second=int(string[13:15]))",https://github.com/LISE-B26/pylabcontrol/blob/67482e5157fcd1c40705e5c2cacfb93564703ed0/build/lib/pylabcontrol/src/core/helper_functions.py#L187-L198
tfidf_python_100_1.0,convert int to string,python,"def convert_string(string):
    """"""Convert string to int, float or bool.
    """"""
    if is_int(string):
        return int(string)
    elif is_float(string):
        return float(string)
    elif convert_bool(string)[0]:
        return convert_bool(string)[1]
    elif string == 'None':
        return None
    else:
        return string",https://github.com/theislab/scanpy/blob/9e4e5ee02e04cf618872d9b098e24f0542e8b227/scanpy/readwrite.py#L602-L614
tfidf_python_100_1.0,convert int to string,python,"def _isint(string):
    """"""
    >>> _isint(""123"")
    True
    >>> _isint(""123.45"")
    False
    """"""
    return type(string) is int or \
           (isinstance(string, _binary_type) or isinstance(string, _text_type)) and \
           _isconvertible(int, string)",https://github.com/pgmpy/pgmpy/blob/9381a66aba3c3871d3ccd00672b148d17d63239e/pgmpy/extern/tabulate.py#L254-L263
tfidf_python_100_1.0,convert int to string,python,"def int2base36(n):
    """"""
    Convert int base10 to base36.
    Back convert: int('<base36>', 36)
    """"""
    assert isinstance(n, (int, long))
    c = '0123456789abcdefghijklmnopqrstuvwxyz'
    if n < 0:
        return '-' + int2base36(-n)
    elif n < 36:
        return c[n]
    b36 = ''
    while n != 0:
        n, i = divmod(n, 36)
        b36 = c[i] + b36
    return b36",https://github.com/liminspace/dju-common/blob/c68860bb84d454a35e66275841c20f38375c2135/dju_common/tools.py#L23-L38
tfidf_python_100_1.0,convert int to string,python,"def _str_to_int(self, string, bits=8):
        i = 0
        if len(string) < 3:
            i = int(string, 10)
        elif string[0:2] == '0b' or string[0:3] == '-0b':
            i = int(string, 2)
        elif string[0:2] == '0o' or string[0:3] == '-0o':
            i = int(string, 8)
        elif string[0:2] == '0x' or string[0:3] == '-0x':
            i = int(string, 16)
        else:
            i = int(string)

        return self._get_complement(i, bits)",https://github.com/ymyzk/sasm/blob/3abf18affeb2f346b2a825018bfb6029c46d1512/sasm/instructions.py#L33-L46
tfidf_python_100_1.0,convert int to string,python,"def _isint(string):
    """"""
    >>> _isint(""123"")
    True
    >>> _isint(""123.45"")
    False
    """"""
    return type(string) is int or \
        (isinstance(string, _binary_type) or
         isinstance(string, string_types)) and \
        _isconvertible(int, string)",https://github.com/solvebio/solvebio-python/blob/b29614643043afd19c1d8074e8f25c6700d51a73/solvebio/utils/tabulate.py#L160-L170
tfidf_python_100_1.0,convert int to string,python,"def data_to_uuid(data):
    """"""Convert an array of binary data to the iBeacon uuid format.""""""
    string = data_to_hexstring(data)
    return string[0:8]+'-'+string[8:12]+'-'+string[12:16]+'-'+string[16:20]+'-'+string[20:32]",https://github.com/citruz/beacontools/blob/15a83e9750d0a4393f8a36868e07f6d9458253fe/beacontools/utils.py#L24-L27
tfidf_python_100_1.0,convert int to string,python,"def try_int_or_force_to_lower_case(input_str: str) -> Union[int, str]:
    """"""
    Tries to convert the passed-in string to an integer. If that fails, it converts it to lower case using norm_fold.
    :param input_str: string to convert
    :return: the string as an integer or a lower case version of the string
    """"""
    try:
        return int(input_str)
    except ValueError:
        return norm_fold(input_str)",https://github.com/python-cmd2/cmd2/blob/b22c0bd891ed08c8b09df56df9d91f48166a5e2a/cmd2/utils.py#L226-L235
tfidf_python_100_1.0,convert int to string,python,"def __init__(self, actfunc, strfunc, convert=lambda x: x):
        self.actfunc = actfunc
        self.strfunc = strfunc
        self.convert = convert",https://github.com/iotile/coretools/blob/2d794f5f1346b841b0dcd16c9d284e9bf2f3c6ec/iotilebuild/iotile/build/config/scons-local-3.0.1/SCons/Action.py#L1379-L1382
tfidf_python_100_1.0,convert int to string,python,"def c_str(string):
    """"""""Convert a python string to C string.""""""
    if not isinstance(string, str):
        string = string.decode('ascii')
    return ctypes.c_char_p(string.encode('utf-8'))",https://github.com/apache/incubator-mxnet/blob/1af29e9c060a4c7d60eeaacba32afdb9a7775ba7/amalgamation/python/mxnet_predict.py#L40-L44
tfidf_python_100_1.0,convert int to string,python,"def fromString(cls, string):
        """"""
        Convert a serialized Unicode string to a L{TaskLevel}.

        @param string: Output of L{TaskLevel.toString}.

        @return: L{TaskLevel} parsed from the string.
        """"""
        return cls(level=[int(i) for i in string.split(""/"") if i])",https://github.com/itamarst/eliot/blob/c03c96520c5492fadfc438b4b0f6336e2785ba2d/eliot/_action.py#L106-L114
tfidf_python_100_1.0,convert int to string,python,"def list(inner_type, sep="",""):
    def convert(string):
        return [inner_type(s.strip()) for s in string.split(sep)]

    return convert",https://github.com/GaretJax/coolfig/blob/6952aabc6ad2c9684d5d98dc470313f8f13fe636/coolfig/types.py#L49-L53
tfidf_python_100_1.0,convert int to string,python,"def pathcase(string):
    """"""Convert string into path case.
    Join punctuation with slash.

    Args:
        string: String to convert.

    Returns:
        string: Path cased string.

    """"""
    string = snakecase(string)
    if not string:
        return string
    return re.sub(r""_"", ""/"", string)",https://github.com/okunishinishi/python-stringcase/blob/700ad111be16b384aadaddcf8199f9390575c7b6/stringcase.py#L86-L100
tfidf_python_100_1.0,convert int to string,python,"def numerify(string):
    return int(''.join(str(_alphabet.index(c)) for c in string))",https://github.com/figo-connect/schwifty/blob/69376fade070dbfdf89c57a0060bc290f7a744bb/schwifty/iban.py#L29-L30
tfidf_python_100_1.0,convert int to string,python,"def value_to_bool(string):
    if isinstance(string, bool):
        return string

    if isinstance(string, int):
        string = str(string)

    if string.lower() in ['1', 'true']:
        return True

    return False",https://github.com/racker/rackspace-monitoring/blob/8a9929e5fd51826c0a392e21bc55acb2aefe54f7/rackspace_monitoring/utils.py#L29-L39
tfidf_python_100_1.0,convert int to string,python,"def _loadAnyChar(parentContext, xmlElement, attributeToFormatMap):
    string = _safeGetRequiredAttribute(xmlElement, 'String', '')
    abstractRuleParams = _loadAbstractRuleParams(parentContext, xmlElement, attributeToFormatMap)
    return _parserModule.AnyChar(abstractRuleParams, string)",https://github.com/andreikop/qutepart/blob/109d76b239751318bcef06f39b2fbbf18687a40b/qutepart/syntax/loader.py#L276-L279
tfidf_python_100_1.0,convert int to string,python,"def _make_identifier(string):
        """"""Attempt to convert string into a valid identifier by replacing invalid characters with ""_""s,
        and prefixing with ""a_"" if necessary.""""""
        string = re.sub(r""[ \-+/\\*%&$Â£#@.,;:'"" ""?<>]"", ""_"", string)
        if re.match(r""^\d"", string):
            string = ""a_{0}"".format(string)
        return string",https://github.com/brunns/brunns-row/blob/f7076a2d3072447719f728caadf08f33a6a26a7a/src/brunns/row/rowwrapper.py#L67-L73
tfidf_python_100_1.0,priority queue,python,"def pop_all(self, priority=None):
        """"""
        NON-BLOCKING POP ALL IN QUEUE, IF ANY
        """"""
        output = []
        with self.lock:
            if not priority:
                priority = self.highest_entry()
            if priority:
                output = list(self.queue[priority].queue)
                self.queue[priority].queue.clear()
        return output",https://github.com/klahnakoski/pyLibrary/blob/fa2dcbc48fda8d26999baef400e9a98149e0b982/mo_threads/queues.py#L353-L364
tfidf_python_100_1.0,priority queue,python,"def setPriority(self, queue, priority):
        '''
        Set priority of a sub-queue
        '''
        q = self.queueindex[queue]
        self.queues[q[0]].removeSubQueue(q[1])
        newPriority = self.queues.setdefault(priority, CBQueue.MultiQueue(self, priority))
        q[0] = priority
        newPriority.addSubQueue(q[1])",https://github.com/hubo1016/vlcp/blob/239055229ec93a99cc7e15208075724ccf543bd1/vlcp/event/pqueue.py#L1032-L1040
tfidf_python_100_1.0,priority queue,python,"def addQUnit(self, functionRef, fArgs, priority):
		self.queue.append(self.Unit(functionRef, fArgs, priority))",https://github.com/themichaelusa/AsyncPQ/blob/0165e807f7975bb709891d0ce57a72e551641f2c/AsyncPQ/ASPQ_Wrapper.py#L16-L17
tfidf_python_100_1.0,priority queue,python,"def pop_one(self, priority=None):
        """"""
        NON-BLOCKING POP IN QUEUE, IF ANY
        """"""
        with self.lock:
            if not priority:
                priority = self.highest_entry()
            if self.closed:
                return [THREAD_STOP]
            elif not self.queue:
                return None
            else:
                v =self.pop(priority=priority)
                if v is THREAD_STOP:  # SENDING A STOP INTO THE QUEUE IS ALSO AN OPTION
                    self.closed.go()
                return v",https://github.com/klahnakoski/pyLibrary/blob/fa2dcbc48fda8d26999baef400e9a98149e0b982/mo_threads/queues.py#L378-L393
tfidf_python_100_1.0,priority queue,python,"def put(self, queue, message, priority=0, **kwargs):
        self.client.use(queue)
        self.client.put(message, priority=priority)",https://github.com/ask/ghettoq/blob/22a0fcd865b618cbbbfd102efd88a7983507c24e/ghettoq/backends/beanstalk.py#L32-L34
tfidf_python_100_1.0,priority queue,python,"def addTask(self, priority, functionRef, *funcArgs):
		self.asyncQueue.addQUnit(functionRef, funcArgs, priority)",https://github.com/themichaelusa/AsyncPQ/blob/0165e807f7975bb709891d0ce57a72e551641f2c/AsyncPQ/AsyncQueue.py#L23-L24
tfidf_python_100_1.0,priority queue,python,"def push(self, item, priority=None):
        """"""Push the item in the priority queue.
        if priority is not given, priority is set to the value of item.
        """"""
        priority = item if priority is None else priority
        node = PriorityQueueNode(item, priority)
        for index, current in enumerate(self.priority_queue_list):
            if current.priority < node.priority:
                self.priority_queue_list.insert(index, node)
                return
        # when traversed complete queue
        self.priority_queue_list.append(node)",https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/queues/priority_queue.py#L38-L49
tfidf_python_100_1.0,priority queue,python,"def parse_priority(priority):
    if priority is None:
        priority = get_default_priority()
    # If priority is given as a string, returns the enum representation
    if isinstance(priority, string_types):
        priority = getattr(PRIORITY, priority, None)

        if priority is None:
            raise ValueError('Invalid priority, must be one of: %s' %
                             ', '.join(PRIORITY._fields))
    return priority",https://github.com/ui/django-post_office/blob/03e1ffb69829b475402f0f3ecd9f8a90af7da4bd/post_office/utils.py#L109-L119
tfidf_python_100_1.0,priority queue,python,"def get_priority(priority):
    """"""Get priority value.

    Args:
        priority (int or str or :obj:`Priority`): Priority.

    Returns:
        int: The priority value.
    """"""
    if isinstance(priority, int):
        if priority < 0 or priority > 100:
            raise ValueError('priority must be between 0 and 100')
        return priority
    elif isinstance(priority, Priority):
        return priority.value
    elif isinstance(priority, str):
        return Priority[priority.upper()].value
    else:
        raise TypeError('priority must be an integer or Priority enum value')",https://github.com/open-mmlab/mmcv/blob/0d77f61450aab4dde8b8585a577cc496acb95d7f/mmcv/runner/priority.py#L35-L53
tfidf_python_100_1.0,priority queue,python,"def _set_priority(self,priority):
        if not 0 <= priority <= 1:
            raise ValueError(""priority must be between 0 and 1"")
        self.__priority = priority
        if self.is_alive():
            self.priority = priority
        return priority",https://github.com/rfk/threading2/blob/7ec234ddd8c0d7e683b1a5c4a79a3d001143189f/threading2/t2_base.py#L372-L378
tfidf_python_100_1.0,priority queue,python,"def enqueue(self, data, priority=None):
        self.Task.create(queue=self.name, data=data, priority=priority or 0)",https://github.com/coleifer/huey/blob/416e8da1ca18442c08431a91bce373de7d2d200f/huey/contrib/sql_huey.py#L85-L86
tfidf_python_100_1.0,priority queue,python,"def notifyBlock(self, queue, blocked):
        '''
        Internal notify for sub-queues been blocked
        '''
        if blocked:
            if self.prioritySet[-1] == queue.priority:
                self.prioritySet.pop()
            else:
                pindex = bisect_left(self.prioritySet, queue.priority)
                if pindex < len(self.prioritySet) and self.prioritySet[pindex] == queue.priority:
                    del self.prioritySet[pindex]
        else:
            if queue.canPop():
                pindex = bisect_left(self.prioritySet, queue.priority)
                if pindex >= len(self.prioritySet) or self.prioritySet[pindex] != queue.priority:
                    self.prioritySet.insert(pindex, queue.priority)
        newblocked =  not self.canPop()
        if newblocked != self.blocked:
            self.blocked = newblocked
            if self.parent is not None:
                self.parent.notifyBlock(self, newblocked)",https://github.com/hubo1016/vlcp/blob/239055229ec93a99cc7e15208075724ccf543bd1/vlcp/event/pqueue.py#L903-L923
tfidf_python_100_1.0,priority queue,python,"def get_priority(priority):
    from mailer.models import PRIORITY_MAPPING, PRIORITY_MEDIUM
    if priority is None:
        priority = PRIORITY_MEDIUM

    if priority in PRIORITY_MAPPING:
        warnings.warn(""Please pass one of the PRIORITY_* constants to 'send_mail' ""
                      ""and 'send_html_mail', not '{0}'."".format(priority),
                      DeprecationWarning)
        priority = PRIORITY_MAPPING[priority]
    if priority not in PRIORITY_MAPPING.values():
        raise ValueError(""Invalid priority {0}"".format(repr(priority)))
    return priority",https://github.com/pinax/django-mailer/blob/129a848090d5de8a3e25067048ba6d3091c3b187/mailer/__init__.py#L23-L35
tfidf_python_100_1.0,priority queue,python,"def add_prioritized(self, command_obj, priority):
		"""""" Add command with the specified priority

		:param command_obj: command to add
		:param priority: command priority
		:return: None
		""""""
		if priority not in self.__priorities.keys():
			self.__priorities[priority] = []

		self.__priorities[priority].append(command_obj)",https://github.com/a1ezzz/wasp-general/blob/1029839d33eb663f8dec76c1c46754d53c1de4a9/wasp_general/command/command.py#L205-L215
tfidf_python_100_1.0,priority queue,python,"def add(self, func, *args, **kwargs):
        # this function adds other functions to the priority queue
        priority = kwargs['priority'] if 'priority' in kwargs else 1
        self.pqueue.put((priority, func, args, kwargs))
        self.startWorkers()",https://github.com/Arsh23/asynchronizer/blob/d90cfffcdd835ab0a37c3f07c0494cf69696668f/asynchronizer/__init__.py#L16-L20
tfidf_python_100_1.0,priority queue,python,"def priority(self, priority: int):
        if priority not in range(0, 201):
            raise TypeError(f'priority must be in range [0-200]! value was {priority}')
        self._priority = priority",https://github.com/Hundemeier/sacn/blob/f08bf3d7554a1ed2870f23a9e0e7b89a4a509231/sacn/messages/data_packet.py#L38-L41
tfidf_python_100_1.0,priority queue,python,"def _query_job_priorities(self, priority, excluded_build_system_type):
        job_priorities = JobPriority.objects.all()
        if priority:
            job_priorities = job_priorities.filter(priority=priority)
        if excluded_build_system_type:
            job_priorities = job_priorities.exclude(buildsystem=excluded_build_system_type)
        return job_priorities",https://github.com/mozilla/treeherder/blob/cc47bdec872e5c668d0f01df89517390a164cda3/treeherder/seta/job_priorities.py#L58-L64
tfidf_python_100_1.0,priority queue,python,"def __init__(self, priority, system_id_extension, mac_addr):
        super(BridgeId, self).__init__()
        self.priority = priority
        self.system_id_extension = system_id_extension
        self.mac_addr = mac_addr
        self.value = bpdu.ConfigurationBPDUs.encode_bridge_id(
            priority, system_id_extension, mac_addr)",https://github.com/osrg/ryu/blob/6f906e72c92e10bd0264c9b91a2f7bb85b97780c/ryu/lib/stplib.py#L1106-L1112
tfidf_python_100_1.0,priority queue,python,"def _enqueue_task_instances_with_queued_state(self, simple_dag_bag,
                                                  simple_task_instances):
        """"""
        Takes task_instances, which should have been set to queued, and enqueues them
        with the executor.

        :param simple_task_instances: TaskInstances to enqueue
        :type simple_task_instances: list[SimpleTaskInstance]
        :param simple_dag_bag: Should contains all of the task_instances' dags
        :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag
        """"""
        TI = models.TaskInstance
        # actually enqueue them
        for simple_task_instance in simple_task_instances:
            simple_dag = simple_dag_bag.get_dag(simple_task_instance.dag_id)
            command = TI.generate_command(
                simple_task_instance.dag_id,
                simple_task_instance.task_id,
                simple_task_instance.execution_date,
                local=True,
                mark_success=False,
                ignore_all_deps=False,
                ignore_depends_on_past=False,
                ignore_task_deps=False,
                ignore_ti_state=False,
                pool=simple_task_instance.pool,
                file_path=simple_dag.full_filepath,
                pickle_id=simple_dag.pickle_id)

            priority = simple_task_instance.priority_weight
            queue = simple_task_instance.queue
            self.log.info(
                ""Sending %s to executor with priority %s and queue %s"",
                simple_task_instance.key, priority, queue
            )

            self.executor.queue_command(
                simple_task_instance,
                command,
                priority=priority,
                queue=queue)",https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/jobs.py#L1282-L1322
tfidf_python_100_1.0,priority queue,python,"def priority(self, priority): # pylint: disable-msg=E0202,E0102,C0111
        priority = int(priority)
        if priority < -128 or priority > 127:
            raise ValueError(""Priority must be in the (-128, 128) range"")
        self._priority = priority
        self._dirty = True",https://github.com/Jajcus/pyxmpp2/blob/14a40a3950910a9cd008b55f0d8905aa0186ce18/pyxmpp2/presence.py#L212-L217
tfidf_python_100_1.0,string to date,python,"def _discover_publication_date(opf_xmldoc, date_html=None):
    date = __discover_dc(opf_xmldoc, 'date')

    if not date and date_html is not None:
        date = _find_publish_date_from_dom(date_html)

    return date",https://github.com/paulocheque/epub-meta/blob/3f0efb9f29a286b1a6896ad05422b23f10e10164/epub_meta/collector.py#L165-L171
tfidf_python_100_1.0,string to date,python,"def get_date(date):
    """"""
    Get the date from a value that could be a date object or a string.

    :param date: The date object or string.

    :returns: The date object.
    """"""
    if type(date) is str:
        return datetime.strptime(date, '%Y-%m-%d').date()
    else:
        return date",https://github.com/Ex-Mente/auxi.0/blob/2dcdae74154f136f8ca58289fe5b20772f215046/auxi/core/helpers.py#L23-L34
tfidf_python_100_1.0,string to date,python,"def _get_crime_categories(self, date=None):
        if date not in self.crime_categories:
            self._populate_crime_categories(date=date)
        return self.crime_categories[date]",https://github.com/rkhleics/police-api-client-python/blob/b5c1e493487eb2409e2c04ed9fbd304f73d89fdc/police_api/__init__.py#L166-L169
tfidf_python_100_1.0,string to date,python,"def to_date_string(date):
    if isinstance(date, numpy.int64) or isinstance(date, int):
        date = str(date)
        date = '%s-%s-%s' % (date[:4], date[4:6], date[6:8])
        return date",https://github.com/CxAalto/gtfspy/blob/bddba4b74faae6c1b91202f19184811e326547e5/gtfspy/util.py#L192-L196
tfidf_python_100_1.0,string to date,python,"def QA_util_date_int2str(int_date):
    """"""
    ç±»ådatetime.datatime
    :param date: int 8ä½æ´æ°
    :return: ç±»åstr
    """"""
    date = str(int_date)
    if len(date) == 8:
        return str(date[0:4] + '-' + date[4:6] + '-' + date[6:8])
    elif len(date) == 10:
        return date",https://github.com/QUANTAXIS/QUANTAXIS/blob/bb1fe424e4108b62a1f712b81a05cf829297a5c0/QUANTAXIS/QAUtil/QADate.py#L74-L84
tfidf_python_100_1.0,string to date,python,"def rollforward(self, date):
        if self.onOffset(date):
            return date
        else:
            return date + type(self)()",https://github.com/pydata/xarray/blob/6d93a95d05bdbfc33fff24064f67d29dd891ab58/xarray/coding/cftime_offsets.py#L141-L145
tfidf_python_100_1.0,string to date,python,"def check_date(date):
    if isinstance(date, six.string_types):
        return Date.parseISO(date)
    else:
        return Date.fromDateTime(date)",https://github.com/iLampard/x-utils/blob/291d92832ee0e0c89bc22e10ecf2f44445e0d300/xutils/date_utils/date.py#L254-L258
tfidf_python_100_1.0,string to date,python,"def format_date(self, date):
        if isinstance(date, datetime.datetime):
            date = date.date()

        if isinstance(date, datetime.date):
            date = date.strftime('%d%m%y')

        if not patterns.DATE.match(date):
            raise ValueError(""Invalid date: "" + date)

        return date",https://github.com/Turbo87/aerofiles/blob/d8b7b04a1fcea5c98f89500de1164619a4ec7ef4/aerofiles/igc/writer.py#L24-L34
tfidf_python_100_1.0,string to date,python,"def isEndOfMonth(date):
        m = date.month()
        y = date.year()
        return date.dayOfMonth() == _month_length(m, Date.isLeap(y))",https://github.com/iLampard/x-utils/blob/291d92832ee0e0c89bc22e10ecf2f44445e0d300/xutils/date_utils/date.py#L179-L182
tfidf_python_100_1.0,string to date,python,"def endOfMonth(date):
        m = date.month()
        y = date.year()
        return Date(y, m, _month_length(m, Date.isLeap(y)))",https://github.com/iLampard/x-utils/blob/291d92832ee0e0c89bc22e10ecf2f44445e0d300/xutils/date_utils/date.py#L173-L176
tfidf_python_100_1.0,string to date,python,"def _containsdate(self, date):
        date = Date(date)
        return ((self.firstdate <= date <= self.lastdate) and
                ((date-self.firstdate) // self.stepsize))",https://github.com/hydpy-dev/hydpy/blob/1bc6a82cf30786521d86b36e27900c6717d3348d/hydpy/core/timetools.py#L1534-L1537
tfidf_python_100_1.0,string to date,python,"def isworkday(self, date):
        """"""
        Check if a given date is a work date, ignoring holidays.

        Args:
            date (date, datetime or str): Date to be checked.

        Returns:
            bool: True if the date is a work date, False otherwise.
        """"""
        date = parsefun(date)
        return self.weekdaymap[date.weekday()].isworkday",https://github.com/antoniobotelho/py-business-calendar/blob/92365fbddd043e41e33b01f1ddd9dd6a5094c031/business_calendar/business_calendar.py#L188-L199
tfidf_python_100_1.0,string to date,python,"def convert_to_date_object(date):
    if isinstance(date, Constant):
        return convert_to_date_object(date.value)
    elif isinstance(date, Date) and date.start == date.end:
        return date.start
    else:
        try:
            return ensure_is_date_object(date)
        except TohuDateError:
            raise TohuTimestampError(f""Argument 'date' must represent some kind of constant date object. Got: {date}"")",https://github.com/maxalbert/tohu/blob/43380162fadec99cdd5c5c3152dd6b7d3a9d39a8/tohu/v6/derived_generators.py#L284-L293
tfidf_python_100_1.0,string to date,python,"def convertDate(date):
    """"""Convert DATE string into a decimal year.""""""

    d, t = date.split('T')
    return decimal_date(d, timeobs=t)",https://github.com/spacetelescope/stsci.tools/blob/9a022503ad24ca54ce83331482dfa3ff6de9f403/lib/stsci/tools/fileutil.py#L171-L175
tfidf_python_100_1.0,string to date,python,"def to_date(date):
    if isinstance(date, six.string_types):
        return parse(date).date()

    if isinstance(date, datetime.datetime):
        try:
            return date.date()
        except AttributeError:
            return date

    raise RQInvalidArgument(""unknown date value: {}"".format(date))",https://github.com/ricequant/rqalpha/blob/ac40a62d4e7eca9494b4d0a14f46facf5616820c/rqalpha/api/api_base.py#L913-L923
tfidf_python_100_1.0,string to date,python,"def __init__(self, date=None):
        """""" Parse the date string """"""
        if isinstance(date, datetime.date):
            self.date = date
        elif date is None or date.lower() == ""today"":
            self.date = TODAY
        elif date.lower() == ""yesterday"":
            self.date = TODAY - delta(days=1)
        else:
            try:
                self.date = datetime.date(*[int(i) for i in date.split(""-"")])
            except StandardError as error:
                log.debug(error)
                raise OptionError(
                    ""Invalid date format: '{0}', use YYYY-MM-DD."".format(date))
        self.datetime = datetime.datetime(
            self.date.year, self.date.month, self.date.day, 0, 0, 0)",https://github.com/psss/did/blob/04e4ee6f1aa14c0cae3ba9f9803871f3f98279cb/did/base.py#L189-L205
tfidf_python_100_1.0,string to date,python,"def _validate_and_parse_date(self, date):
        try:
            date = date_parser.parse(date)
            return self._localize_date(date)
        except TypeError:
            raise DateFormatException()",https://github.com/internap/almanach/blob/9986845af94cfffbf11b83d1825611bb31193299/almanach/core/controller.py#L295-L300
tfidf_python_100_1.0,string to date,python,"def _get_date(self):
        try:
            date = get_value_in_tag(self.document, 'pex-dc:date')
            if len(date) == 20:
                date = datetime.strptime(date, '%Y-%m-%dT%H:%M:%SZ')
                date = date.strftime(""%Y-%m-%d"")
                date = str(date)
            return date
        except Exception:
            print >> sys.stderr, ""Can't find date""
            return ''",https://github.com/inspirehep/harvesting-kit/blob/33a7f8aa9dade1d863110c6d8b27dfd955cb471f/harvestingkit/pos_package.py#L78-L88
tfidf_python_100_1.0,string to date,python,"def __init__(self, date=None, fmt=None):
        """""" Parse the date string """"""
        # REFERENCE - strftime
        # full iso: '%Y-%m-%d %H:%M:%S.%f %z'
        # full iso no micro: '%Y-%m-%d %H:%M:%S %z'
        # Unless asked for it explicitly, only show the date as str()
        self.fmt = fmt or ""%Y-%m-%d""
        date = date.lower() if isinstance(date, (str, unicode)) else date
        try:
            if isinstance(date, Date):
                # take the date from the Date() and let it recreate itself
                date = date.date
            elif isinstance(date, (datetime.datetime, datetime.date)):
                pass
            elif not date or date == ""today"":
                date = today()
            elif date == ""yesterday"":
                # produces datetime.date()
                date = today() - delta(days=1)
            elif isinstance(date, (unicode, str)):
                date = dt_parse(date)
            else:
                raise ValueError
        except ValueError:
            raise ValueError(
                ""Invalid date format: {0}('{1}'), use YYYY-MM-DD."".format(
                    type(date), date))

        # Make sure we're a datetime, not date
        if type(date) is datetime.date:
            date = datetime.datetime(
                date.year, date.month, date.day, 0, 0, 0, tzinfo=pytz.utc)

        # FIXME: Make sure we've converted to in UTC
        # if there is no tz info in the datetime object
        # assume it's UTC (WARNING: AMIGUOUS...)
        if date.tzinfo:
            log.debug('Timezone detected [{0}]; converting to UTC'.format(
                date.tzinfo))
            date = date.astimezone(pytz.utc)
        # Makesure the datetime is tz-aware (UTC)
        date = date.replace(tzinfo=pytz.utc)
        self.date = date",https://github.com/kejbaly2/idid/blob/0f19e9ca9c8fa4a81e95c490dfbbc1b452c7451c/idid/utils.py#L436-L478
tfidf_python_100_1.0,string to date,python,"def __call__(cls, *args, **kwargs):
        date = args[0] if len(args) == 1 else kwargs['date']

        if date is None:
            date = get_configuration().get('VERSIONS', 'defaultversion')

        if isinstance(date, DictionaryVersion):
            return date

        if isinstance(date, str):
            if date.startswith('dictionary_'):
                date = _str_to_date(date.split('.')[0].split('_', maxsplit=1)[1])
            else:
                date = _str_to_date(date)
        elif isinstance(date, datetime.date):
            date = _str_to_date(_date_to_str(date))
        else:
            raise ValueError(""Invalid date format for dictionary version %s."" % _date_to_str(date))

        if date not in cls._instances:
            cls._instances[date] = super(DictionaryVersionSingleton, cls).__call__(date)

        return cls._instances[date]",https://github.com/IEMLdev/ieml/blob/4c842ba7e6165e2f1b4a4e2e98759f9f33af5f25/ieml/dictionary/version.py#L56-L78
tfidf_python_100_1.0,sort string list,python,"def get_sort_string(self, sort=None):
        if not sort:
            sort = self.sort
        sort_string = ''
        if not sort == self.default_sort:
            sort_string = self.sort_parameter + '=' + sort
        return sort_string",https://github.com/aptivate/django-sortable-listview/blob/9d5fa5847f0c3e80893780c6540e5098635ace9f/sortable_listview/views.py#L127-L133
tfidf_python_100_1.0,sort string list,python,"def sort2sqlorderby(self, sort):
        sort = jx.normalize_sort_parameters(sort)
        return sql_list([quote_column(s.field) + (SQL_DESC if s.sort == -1 else SQL_ASC) for s in sort])",https://github.com/klahnakoski/pyLibrary/blob/fa2dcbc48fda8d26999baef400e9a98149e0b982/pyLibrary/sql/mysql.py#L464-L466
tfidf_python_100_1.0,sort string list,python,"def sort(self, sort=''):
        sort = sort or self.DEFAULT_SORT_ARG
        self.stats.sort_stats(sort)
        return self",https://github.com/ymichael/cprofilev/blob/dee710a2fa124a12ecf3afcb3d41b9cb0364cf57/cprofilev.py#L136-L139
tfidf_python_100_1.0,sort string list,python,"def list(self):
        l = list(self.subcmds.keys())
        l.sort()
        return l",https://github.com/rocky/python3-trepan/blob/14e91bc0acce090d67be145b1ac040cab92ac5f3/trepan/processor/subcmd.py#L104-L107
tfidf_python_100_1.0,sort string list,python,"def validate_sort(sort):
    if sort is None:
        return []
    elif isinstance(sort, string_types):
        sort = [sort]

    if isinstance(sort, list):
        ret = []
        for col, s in sort:
            if isinstance(s, Sort):
                s = s.value
            elif not isinstance(s, string_types) or s not in Sort.options():
                raise PSPException('Unrecognized Sort: %s', s)
            ret.append([col, s])
        return ret
    else:
        raise PSPException('Cannot parse sort type: %s', str(type(sort)))",https://github.com/timkpaine/perspective-python/blob/34cc59811909bc74d11e3de80010a3066ba9a8be/perspective/validate.py#L69-L85
tfidf_python_100_1.0,sort string list,python,"def get_sort(self):
        if not self.initial_sort:
            return None
        sort = '%s' % self.initial_sort
        if self.initial_sort_type == 'desc':
            sort = '-%s' % sort
        return sort",https://github.com/AndrewIngram/django-extra-views/blob/188e1bf1f15a44d9a599028d020083af9fb43ea7/extra_views/contrib/mixins.py#L133-L139
tfidf_python_100_1.0,sort string list,python,"def getPositiveCoordinateRangeOverlap(uStart1, uEnd1, uStart2, uEnd2):
    """"""@return: If the two coordinate ranges overlap on the same strand
    returns the overlap range. If no overlap it returns None.
    """"""
    if uEnd1 < uStart2 or uEnd2 < uStart1:
        return None
    l = [ uStart1, uEnd1, uStart2, uEnd2 ]
    l.sort()
    return l[1], l[2]",https://github.com/benedictpaten/sonLib/blob/1decb75bb439b70721ec776f685ce98e25217d26/misc.py#L44-L52
tfidf_python_100_1.0,sort string list,python,"def sort(self, sort):
        return ListContainer(""sorted ""+self.name, jx.sort(self.data, sort, already_normalized=True), self.schema)",https://github.com/klahnakoski/pyLibrary/blob/fa2dcbc48fda8d26999baef400e9a98149e0b982/jx_python/containers/list_usingPythonList.py#L152-L153
tfidf_python_100_1.0,sort string list,python,"def _get_single_indexer(join_key, index, sort=False):
    left_key, right_key, count = _factorize_keys(join_key, index, sort=sort)

    left_indexer, right_indexer = libjoin.left_outer_join(
        ensure_int64(left_key),
        ensure_int64(right_key),
        count, sort=sort)

    return left_indexer, right_indexer",https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/reshape/merge.py#L1613-L1621
tfidf_python_100_1.0,sort string list,python,"def finish(self):
        self._additions.sort()
        self._deletions.sort()
        self._modifications.sort()",https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/struct_diff.py#L11-L14
tfidf_python_100_1.0,sort string list,python,"def sort(self):
        """"""Sort list elements by ID.""""""
        super(JSSObjectList, self).sort(key=lambda k: k.id)",https://github.com/jssimporter/python-jss/blob/b95185d74e0c0531b0b563f280d4129e21d5fe5d/jss/jssobjectlist.py#L143-L145
tfidf_python_100_1.0,sort string list,python,"def sort(self, *sort):
        """""" Sort the results.

        Define how the results should be sorted. The arguments should be tuples
        of string defining the key and direction to sort by. For example
        `('name', 'asc')` and `('version', 'desc')`. The first sorte rule is
        considered first by the API. See also the API documentation on
        `sorting`_.

        Arguments:
            `*sort` (`tuple`)
                The rules to sort by

        .. _sorting: https://github.com/XereoNet/SpaceGDN/wiki/API#sorting

        """"""
        self.add_get_param('sort', FILTER_DELIMITER.join(
            [ELEMENT_DELIMITER.join(elements) for elements in sort]))
        return self",https://github.com/totokaka/pySpaceGDN/blob/55c8be8d751e24873e0a7f7e99d2b715442ec878/pyspacegdn/requests/find_request.py#L73-L91
tfidf_python_100_1.0,sort string list,python,"def time_merge_dataframe_integer_2key(self, sort):
        merge(self.df, self.df3, sort=sort)",https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/asv_bench/benchmarks/join_merge.py#L192-L193
tfidf_python_100_1.0,sort string list,python,"def double_linkage(L):
    if len(L) == 1:
        return L[0]
    L.sort()
    a, b = L[:2]
    return (a + b) / 2.",https://github.com/tanghaibao/jcvi/blob/d2e31a77b6ade7f41f3b321febc2b4744d1cdeca/jcvi/assembly/allmaps.py#L802-L807
tfidf_python_100_1.0,sort string list,python,"def sort(self, *args, **kwargs):
        """"""Sort the MultiMap.
        
        Takes the same arguments as list.sort, and operates on tuples of
        (key, value) pairs.
        
        >>> m = MutableMultiMap()
        >>> m['c'] = 1
        >>> m['b'] = 3
        >>> m['a'] = 2
        >>> m.sort()
        >>> m.keys()
        ['a', 'b', 'c']
        >>> m.sort(key=lambda x: x[1])
        >>> m.keys()
        ['c', 'a', 'b']
        
        """"""
        self._pairs.sort(*args, **kwargs)
        self._rebuild_key_ids()",https://github.com/mikeboers/MultiMap/blob/0251e5d5df693cc247b4ac5b95adfdd10e3bec04/multimap.py#L530-L549
tfidf_python_100_1.0,sort string list,python,"def list(self, base, filter=None, type=None, sort=None, limit=None, page=None, format=None): # pylint: disable=redefined-builtin
        if sort != None:
            if not isinstance(sort, list):
                sort = [sort]
            sort = ','.join(sort)
        return self.get(base, params={'filter': filter, 'type': type, 'sort': sort, 'limit': limit,
                                      'page': page, 'format': format})",https://github.com/qacafe/cdrouter.py/blob/aacf2c6ab0b987250f7b1892f4bba14bb2b7dbe5/cdrouter/cdrouter.py#L282-L288
tfidf_python_100_1.0,sort string list,python,"def _sort2sql(self, sort):
        """"""
        RETURN ORDER BY CLAUSE
        """"""
        if not sort:
            return """"
        return SQL_ORDERBY + sql_list([quote_column(o.field) + ("" DESC"" if o.sort == -1 else """") for o in sort])",https://github.com/klahnakoski/pyLibrary/blob/fa2dcbc48fda8d26999baef400e9a98149e0b982/jx_mysql/__init__.py#L314-L320
tfidf_python_100_1.0,sort string list,python,"def time_merge_2intkey(self, sort):
        merge(self.left, self.right, sort=sort)",https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/asv_bench/benchmarks/join_merge.py#L189-L190
tfidf_python_100_1.0,sort string list,python,"def finish(self):
        self._modifications.sort()
        self._deletions.sort(reverse=True)
        self._additions.sort()",https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/struct_diff.py#L250-L253
tfidf_python_100_1.0,sort string list,python,"def sort(self, key=None, reverse=False):
        """"""
        Sort contents using sort() method available to list()
        :return: None (because list().sort() doesn't return anything)
        """"""
        self.log('sort()')
        self.contents.sort(key=key, reverse=reverse)
        return None",https://github.com/jhazelwo/python-fileasobj/blob/4bdbb575e75da830b88d10d0c1020d787ceba44d/fileasobj/__init__.py#L286-L293
tfidf_python_100_1.0,save list to file,python,"def save(self, *args, **kwargs):
        save = super(RelatedFieldDefinition, self).save()
        if self.to_model_class_is_mutable:
            self.to_model_class.mark_as_obsolete()
        return save",https://github.com/charettes/django-mutant/blob/865a1b712ce30501901c4691ce2110ab03f0f93b/mutant/contrib/related/models.py#L90-L94
tfidf_python_100_1.0,save list to file,python,"def _save_yaml_file(self, file, val):
        """"""
        Save data to yaml file

        :param file: Writable object or path to file
        :type file: FileIO | str | unicode
        :param val: Value or struct to save
        :type val: None | int | float | str | unicode | list | dict
        :raises IOError: Failed to save
        """"""
        try:
            save_yaml_file(file, val)
        except:
            self.exception(""Failed to save to {}"".format(file))
            raise IOError(""Saving file failed"")",https://github.com/the01/python-flotils/blob/5954712776bb590107e5b2f4362d010bf74f77a1/flotils/loadable.py#L539-L553
tfidf_python_100_1.0,save list to file,python,"def main():
    from dirutility.gui import CompareTreesGUI
    params = CompareTreesGUI().sources
    src = params['source']
    dir1_u, dir2_u = compare_trees(src['dir1'], src['dir2'])

    if params['save']:
        from databasetools import CSVExport, DictTools
        save = params['save']
        if save['csv']:
            CSVExport(list(dir1_u), cols=['files'], file_path=save['directory'], file_name='dir1_unique')
            CSVExport(list(dir2_u), cols=['files'], file_path=save['directory'], file_name='dir2_unique')
        if save['json']:
            DictTools(save['directory'], 'dir1_unique').save(list(dir1_u))
            DictTools(save['directory'], 'dir2_unique').save(list(dir2_u))
    print('Done!')",https://github.com/mrstephenneal/dirutility/blob/339378659e2d7e09c53acfc51c5df745bb0cd517/dirutility/compare.py#L28-L43
tfidf_python_100_1.0,save list to file,python,"def save(self, filename):
        """"""
        Save document to a file.

        Parameters
        ----------
        filename : str
            The output string filename

        """"""
        self.doc.save(os.path.expanduser(filename))",https://github.com/shapiromatron/bmds/blob/395c6ce84ad82876fd9fa4a89a3497fb61616de0/bmds/reporter.py#L159-L169
tfidf_python_100_1.0,save list to file,python,"def save(self, file):
        util.io.object.save(file, self)
        logger.debug('Interpolator saved to {}.'.format(file))",https://github.com/jor-/util/blob/0eb0be84430f88885f4d48335596ca8881f85587/util/math/interpolate.py#L358-L360
tfidf_python_100_1.0,save list to file,python,"def save(self):

        self._kev.save()
        self._kev._create_error_dict = False
        return self._kev",https://github.com/capless/formy/blob/0c946e172a4ecbaf112db9b8f0d50088bd875987/formy/kev.py#L44-L48
tfidf_python_100_1.0,save list to file,python,"def save(self, banks_manager):
        """"""
        Save all data from a banks_manager

        :param BanksManager banks_manager: BanksManager that your banks data will be persisted
        """"""
        self.banks_files.delete_all_banks()
        self.banks_files.save(banks_manager)
        self.index_file.save(banks_manager)",https://github.com/PedalPi/PluginsManager/blob/2dcc9f6a79b48e9c9be82efffd855352fa15c5c7/pluginsmanager/observer/autosaver/autosaver.py#L104-L112
tfidf_python_100_1.0,save list to file,python,"def save(self, file, only_dict=True):
        logger.debug('Saving {} to {}.'.format(self, file))
        if only_dict:
            util.io.object.save(file, self.value_dict)
        else:
            util.io.object.save(file, self)",https://github.com/jor-/util/blob/0eb0be84430f88885f4d48335596ca8881f85587/util/multi_dict.py#L202-L207
tfidf_python_100_1.0,save list to file,python,"def save(self, *args):
        if self.edbox is None:
            Clock.schedule_once(self.save, 0)
            return
        self.edbox.save()",https://github.com/LogicalDash/LiSE/blob/fe6fd4f0a7c1780e065f4c9babb9bc443af6bb84/ELiDE/ELiDE/stores.py#L194-L198
tfidf_python_100_1.0,save list to file,python,"def attach(self, file):
        if not file.id:
            file.save()

        self.file_ids.append(file.id)",https://github.com/nylas/nylas-python/blob/c3e4dfc152b09bb8f886b1c64c6919b1f642cbc8/nylas/client/restful_models.py#L445-L449
tfidf_python_100_1.0,save list to file,python,"def Save(self):
        if not self.CurrentFile().filename == '':
            self.CurrentFile().Save()
            logging.info(""FileManager.%i.Save()"" % 
                    (self.index))",https://github.com/lamestation/lamestation-sdk-tools/blob/25d1fe8dc276cf4addbb13fd29ea3cf22b09f9ca/lspaint/FileManager.py#L173-L177
tfidf_python_100_1.0,save list to file,python,"def save(self, *args, **kwargs):
        save = super(FieldDefinitionChoice, self).save(*args, **kwargs)
        self.field_def.model_def.model_class(force_create=True)
        return save",https://github.com/charettes/django-mutant/blob/865a1b712ce30501901c4691ce2110ab03f0f93b/mutant/models/field/__init__.py#L341-L344
tfidf_python_100_1.0,save list to file,python,"def save(self, make_current=False):
        super(User, self).save()
        self._handle_save_result(make_current)",https://github.com/leancloud/python-sdk/blob/fea3240257ce65e6a32c7312a5cee1f94a51a587/leancloud/user.py#L110-L112
tfidf_python_100_1.0,save list to file,python,"def save(self, sortkey = True):
        """"""
        Save configurations to a list of strings
        """"""
        return [k + '=' + repr(v) for k,v in self.config_items(sortkey)]",https://github.com/hubo1016/vlcp/blob/239055229ec93a99cc7e15208075724ccf543bd1/vlcp/config/config.py#L271-L275
tfidf_python_100_1.0,save list to file,python,"def setLogTitle(self, title, save=True):
        self.logtitle = title
        if save:
            self.save()",https://github.com/ska-sa/purr/blob/4c848768d0485d0f88b30850d0d5372221b21b66/Purr/Purrer.py#L574-L577
tfidf_python_100_1.0,save list to file,python,"def save(file, name, value):
    with h5py.File(file, mode='r+') as file:
        file[name][()] = value",https://github.com/jor-/util/blob/0eb0be84430f88885f4d48335596ca8881f85587/util/io/hdf5.py#L7-L9
tfidf_python_100_1.0,save list to file,python,"def save(self):
        """"""
        Save the state to a file.
        """"""
        with open(self.path, 'w') as f:
            f.write(yaml.dump(dict(self.d)))",https://github.com/snare/scruffy/blob/0fedc08cfdb6db927ff93c09f25f24ce5a04c541/scruffy/state.py#L54-L59
tfidf_python_100_1.0,save list to file,python,"def remote_replica(sciobj_model, replica_info_model):
    remote_replica_model = RemoteReplica(sciobj=sciobj_model, info=replica_info_model)
    remote_replica_model.save()
    return remote_replica_model",https://github.com/DataONEorg/d1_python/blob/3ac4d4f3ca052d3e8641a6a329cab526c8ddcb0d/gmn/src/d1_gmn/app/models.py#L288-L291
tfidf_python_100_1.0,save list to file,python,"def save(self):
        '''Save current property list representation to the original file.'''
        with open(self.filename, 'w') as plist_file:
            plist_file.write(str(self.soup))",https://github.com/vedvyas/doxytag2zealdb/blob/8b07a88af6794248f8cfdabb0fda9dd61c777127/doxytag2zealdb/propertylist.py#L112-L115
tfidf_python_100_1.0,save list to file,python,"def save(self, name, content, save=True):
        content.seek(0)  # Ensure we upload the whole file
        super(MultiStorageFieldFile, self).save(name, content, save)",https://github.com/divio/django-filer/blob/946629087943d41eff290f07bfdf240b8853dd88/filer/fields/multistorage_file.py#L122-L124
tfidf_python_100_1.0,postgresql connection,python,"def __init__(self, postgresqls):
        super(Postgresql, self).__init__(postgresqls)
        self._meta_data['required_json_kind'] =\
            'tm:gtm:monitor:postgresql:postgresqlstate'",https://github.com/F5Networks/f5-common-python/blob/7e67d5acd757a60e3d5f8c88c534bd72208f5494/f5/bigip/tm/gtm/monitor.py#L362-L365
tfidf_python_100_1.0,postgresql connection,python,"def steady_connection(self):
        """"""Get a steady, unpooled PostgreSQL connection.""""""
        return SteadyPgConnection(self._maxusage, self._setsession, True,
                                  *self._args, **self._kwargs)",https://github.com/Cito/DBUtils/blob/90e8825e038f08c82044b8e50831480175fa026a/DBUtils/PooledPg.py#L200-L203
tfidf_python_100_1.0,postgresql connection,python,"def compile(self):
        self.emerge_env = dict(USE=""server"")
        super(Postgresql, self).compile()

        pg_socketdir = local.path(""/run/postgresql"")
        if not pg_socketdir.exists():
            pg_socketdir.mkdir()",https://github.com/PolyJIT/benchbuild/blob/9ad2ec54d96e97b642b1f06eddcbad9ba7aeaf58/benchbuild/projects/gentoo/postgresql.py#L21-L27
tfidf_python_100_1.0,postgresql connection,python,"def configure(self, *args, **kwargs):
        #TODO:set postgres user password?
        #https://help.ubuntu.com/community/PostgreSQL
        #set postgres ident in pg_hba.conf
        #sudo -u postgres psql postgres
        #sudo service postgresql restart
        #sudo -u postgres psql
        #\password postgres
        r = self.local_renderer

        #self.install_packages()

        if r.env.apt_repo_enabled:
            self.configure_apt_repository()

        r.env.pg_version = self.version()# or r.env.default_version

        self.write_pg_hba_conf()

#         r.pc('Backing up PostgreSQL configuration files...')
        r.sudo('cp /etc/postgresql/{pg_version}/main/postgresql.conf /etc/postgresql/{pg_version}/main/postgresql.conf.$(date +%Y%m%d%H%M).bak')
        r.pc('Enabling auto-vacuuming...')
        r.sed(
            filename='/etc/postgresql/{pg_version}/main/postgresql.conf',
            before='#autovacuum = on',
            after='autovacuum = on',
            backup='',
            use_sudo=True,
        )
        r.sed(
            filename='/etc/postgresql/{pg_version}/main/postgresql.conf',
            before='#track_counts = on',
            after='track_counts = on',
            backup='',
            use_sudo=True,
        )

        # Set UTF-8 as the default database encoding.
        #TODO:fix? throws error code?
#        sudo_or_dryrun('psql --user=postgres --no-password --command=""'
#            'UPDATE pg_database SET datistemplate = FALSE WHERE datname = \'template1\';'
#            'DROP DATABASE template1;'
#            'CREATE DATABASE template1 WITH TEMPLATE = template0 ENCODING = \'UNICODE\';'
#            'UPDATE pg_database SET datistemplate = TRUE WHERE datname = \'template1\';'
#            '\c template1\n'
#            'VACUUM FREEZE;'
#            'UPDATE pg_database SET datallowconn = FALSE WHERE datname = \'template1\';""')

        r.sudo('service postgresql restart')",https://github.com/chrisspen/burlap/blob/a92b0a8e5206850bb777c74af8421ea8b33779bd/burlap/postgresql.py#L534-L582
tfidf_python_100_1.0,postgresql connection,python,"def _json_column(**kwargs):
    """"""Return JSON column.""""""
    return db.Column(
        JSONType().with_variant(
            postgresql.JSON(none_as_null=True),
            'postgresql',
        ),
        nullable=True,
        **kwargs
    )",https://github.com/inveniosoftware/invenio-webhooks/blob/f407cb2245464543ee474a81189fb9d3978bdde5/invenio_webhooks/models.py#L197-L206
tfidf_python_100_1.0,postgresql connection,python,"def _make_patroni_test_config(self, name, custom_config):
        patroni_config_name = self.PATRONI_CONFIG.format(name)
        patroni_config_path = os.path.join(self._output_dir, patroni_config_name)

        with open(patroni_config_name) as f:
            config = yaml.safe_load(f)
            config.pop('etcd', None)

        host = config['postgresql']['listen'].split(':')[0]

        config['postgresql']['listen'] = config['postgresql']['connect_address'] = '{0}:{1}'.format(host, self.__PORT)

        config['name'] = name
        config['postgresql']['data_dir'] = self._data_dir
        config['postgresql']['use_unix_socket'] = True
        config['postgresql']['parameters'].update({
            'logging_collector': 'on', 'log_destination': 'csvlog', 'log_directory': self._output_dir,
            'log_filename': name + '.log', 'log_statement': 'all', 'log_min_messages': 'debug1',
            'unix_socket_directories': self._data_dir})

        if 'bootstrap' in config:
            config['bootstrap']['post_bootstrap'] = 'psql -w -c ""SELECT 1""'
            if 'initdb' in config['bootstrap']:
                config['bootstrap']['initdb'].extend([{'auth': 'md5'}, {'auth-host': 'md5'}])

        if custom_config is not None:
            self.recursive_update(config, custom_config)

        if config['postgresql'].get('callbacks', {}).get('on_role_change'):
            config['postgresql']['callbacks']['on_role_change'] += ' ' + str(self.__PORT)

        with open(patroni_config_path, 'w') as f:
            yaml.safe_dump(config, f, default_flow_style=False)

        user = config['postgresql'].get('authentication', config['postgresql']).get('superuser', {})
        self._connkwargs = {k: user[n] for n, k in [('username', 'user'), ('password', 'password')] if n in user}
        self._connkwargs.update({'host': host, 'port': self.__PORT, 'database': 'postgres'})

        self._replication = config['postgresql'].get('authentication', config['postgresql']).get('replication', {})
        self._replication.update({'host': host, 'port': self.__PORT, 'database': 'postgres'})

        return patroni_config_path",https://github.com/zalando/patroni/blob/f6d29081c90af52064b981cdd877a07338d86038/features/environment.py#L165-L206
tfidf_python_100_1.0,postgresql connection,python,"def syncdb():
    """"""
    Create tables if they don't exist
    """"""
    from flask_philo.db.postgresql.schema import Base
    from flask_philo.db.postgresql.orm import BaseModel  # noqa
    from flask_philo.db.postgresql.connection import get_pool

    for conn_name, conn in get_pool().connections.items():
        Base.metadata.create_all(conn.engine)",https://github.com/Riffstation/flask-philo/blob/76c9d562edb4a77010c8da6dfdb6489fa29cbc9e/flask_philo/db/postgresql/__init__.py#L1-L10
tfidf_python_100_1.0,postgresql connection,python,"def _connect(self):
        """"""Connect to PostgreSQL, either by reusing a connection from the pool
        if possible, or by creating the new connection.

        :rtype: psycopg2.extensions.connection
        :raises: pool.NoIdleConnectionsError

        """"""
        # Attempt to get a cached connection from the connection pool
        try:
            connection = self._pool_manager.get(self.pid, self)
            LOGGER.debug(""Re-using connection for %s"", self.pid)
        except pool.NoIdleConnectionsError:
            if self._pool_manager.is_full(self.pid):
                raise

            # Create a new PostgreSQL connection
            kwargs = utils.uri_to_kwargs(self._uri)
            LOGGER.debug(""Creating a new connection for %s"", self.pid)
            connection = self._psycopg2_connect(kwargs)

            self._pool_manager.add(self.pid, connection)
            self._pool_manager.lock(self.pid, connection, self)

            # Added in because psycopg2ct connects and leaves the connection in
            # a weird state: consts.STATUS_DATESTYLE, returning from
            # Connection._setup without setting the state as const.STATUS_OK
            if utils.PYPY:
                connection.reset()

            # Register the custom data types
            self._register_unicode(connection)
            self._register_uuid(connection)

        return connection",https://github.com/gmr/queries/blob/a68855013dc6aaf9ed7b6909a4701f8da8796a0a/queries/session.py#L273-L307
tfidf_python_100_1.0,postgresql connection,python,"def __init__(self, monitor):
        super(Postgresqls, self).__init__(monitor)
        self._meta_data['allowed_lazy_attributes'] = [Postgresql]
        self._meta_data['attribute_registry'] =\
            {'tm:gtm:monitor:postgresql:postgresqlstate': Postgresql}",https://github.com/F5Networks/f5-common-python/blob/7e67d5acd757a60e3d5f8c88c534bd72208f5494/f5/bigip/tm/gtm/monitor.py#L353-L357
tfidf_python_100_1.0,postgresql connection,python,"def migrate(self, connection):
        # use transactions
        if connection.engine.name == 'sqlite':
            self._migrate_sqlite(connection)
        elif connection.engine.name == 'postgresql':
            self._migrate_postgresql(connection)
        else:
            raise DatabaseMissingError(
                'Do not know how to migrate {} engine.'.format(self.connection))",https://github.com/CivicSpleen/ambry/blob/d7f2be4bf1f7ffd086f3fadd4fcae60c32473e42/ambry/orm/database.py#L785-L793
tfidf_python_100_1.0,postgresql connection,python,"def upgrade():
    op.alter_column('event', 'created',
               existing_type=postgresql.TIMESTAMP(),
               type_=postgresql.TIMESTAMP(timezone=True),
               existing_nullable=False,
               existing_server_default=sa.text('now()'))
    op.alter_column('instance', 'created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=postgresql.TIMESTAMP(timezone=True),
               existing_nullable=False,
               existing_server_default=sa.text('now()'))
    op.alter_column('instance', 'updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=postgresql.TIMESTAMP(timezone=True),
               existing_nullable=False,
               existing_server_default=sa.text('now()'))
    op.alter_column('invitation', 'invited_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=postgresql.TIMESTAMP(timezone=True),
               existing_nullable=False,
               existing_server_default=sa.text('now()'))
    op.alter_column('log', 'created',
               existing_type=postgresql.TIMESTAMP(),
               type_=postgresql.TIMESTAMP(timezone=True),
               existing_nullable=False,
               existing_server_default=sa.text('now()'))",https://github.com/quiltdata/quilt/blob/651853e7e89a8af86e0ff26167e752efa5878c12/registry/migrations/versions/6daddb340397_use_timestamptz_instead_of_timestamp_.py#L19-L44
tfidf_python_100_1.0,postgresql connection,python,"def _get_pg_query(self):
        from sqlalchemy.dialects import postgresql
        return str(self.get_query().compile(dialect=postgresql.dialect()))",https://github.com/redhat-cip/dci-control-server/blob/b416cf935ec93e4fdd5741f61a21cabecf8454d2/dci/api/v1/utils.py#L412-L414
tfidf_python_100_1.0,postgresql connection,python,"def make_connection():
    if not hasattr(make_connection, 'Connection'):
        from .connection import Connection
        make_connection.Connection = Connection

    return make_connection.Connection()",https://github.com/vishnevskiy/battlenet/blob/3f1561b012a535de7c29b5423f7537ea5130321b/battlenet/utils.py#L32-L37
tfidf_python_100_1.0,postgresql connection,python,"def load_dialect_impl(self, dialect):
        if dialect.name == 'postgresql':
            from sqlalchemy.dialects import postgresql
            return dialect.type_descriptor(postgresql.JSONB())
        else:
            return dialect.type_descriptor(String())",https://github.com/ONSdigital/ras-common-utils/blob/b78125a5dc5f30292f96aafa62f369c89e043def/ras_common_utils/ras_database/json_column.py#L15-L20
tfidf_python_100_1.0,postgresql connection,python,"def _create_tables(self, connection):
        self._create_element_table(connection)
        self._create_element_symbol_table(connection)
        self._create_element_name_table(connection)
        self._create_element_atomic_weight_table(connection)
        self._create_element_mass_density_table(connection)

        self._create_atomic_shell_table(connection)
        self._create_atomic_shell_notation_table(connection)

        self._create_atomic_subshell_table(connection)
        self._create_atomic_subshell_notation_table(connection)
        self._create_atomic_subshell_binding_energy_table(connection)
        self._create_atomic_subshell_radiative_width_table(connection)
        self._create_atomic_subshell_nonradiative_width_table(connection)
        self._create_atomic_subshell_occupancy_table(connection)

        self._create_xray_transition_table(connection)
        self._create_xray_transition_notation_table(connection)
        self._create_xray_transition_energy_table(connection)
        self._create_xray_transition_probability_table(connection)
        self._create_xray_transition_relative_weight_table(connection)

        self._create_xray_transitionset_table(connection)
        self._create_xray_transitionset_association_table(connection)
        self._create_xray_transitionset_notation_table(connection)
        self._create_xray_transitionset_energy_table(connection)
        self._create_xray_transitionset_relative_weight_table(connection)

        self._create_language_table(connection)
        self._create_notation_table(connection)
        self._create_reference_table(connection)",https://github.com/openmicroanalysis/pyxray/blob/cae89677f00ebcc0952f94d1ab70e6b35e1a51e9/pyxray/sql/build.py#L133-L164
tfidf_python_100_1.0,postgresql connection,python,"def _get_dbapi_connection(self, connection: engine.Connection) -> extensions.connection:
        return connection.connection.connection",https://github.com/socialwifi/sqlalchemy-postgres-autocommit/blob/53c6889598ef884357ab82e27d350f68d65f1253/sqlalchemy_postgres_autocommit/databases.py#L57-L58
tfidf_python_100_1.0,postgresql connection,python,"def pg_version(using=None):
    """"""
    Return tuple with PostgreSQL version of a specific connection
    :type using: str
    :param using: Connection name
    :rtype: tuple
    :return: PostgreSQL version
    """"""
    connection = get_connection(using)
    cursor = connection.cursor()

    cursor.execute('SHOW server_version')
    row = cursor.fetchone()

    return tuple([int(i) for i in row[0].split('.')])",https://github.com/tomi77/django-extra-tools/blob/fb6d226bc5cf3fc0eb8abe61a512c3f5c7dcc8a8/django_extra_tools/db/__init__.py#L12-L26
tfidf_python_100_1.0,postgresql connection,python,"def _raw_connection(self, connection):
        if isinstance(connection, Engine):
            return connection.raw_connection()
        return connection.connection",https://github.com/laughingman7743/PyAthena/blob/a6a30299ad82f1c4cebc2a11635798f3f9c9214d/pyathena/sqlalchemy_athena.py#L113-L116
tfidf_python_100_1.0,postgresql connection,python,"def __init__(self, ternya, connection):
        self.connection = connection
        self.ternya = ternya",https://github.com/ndrlslz/ternya/blob/c05aec10029e645d63ff04313dbcf2644743481f/ternya/ternya.py#L270-L272
tfidf_python_100_1.0,postgresql connection,python,"def connection(self):
        connection = connections[self.alias]
        if connection.connection is None:
            connection._cursor()
        if connection.vendor == 'sqlite':
            return SqliteWrapper(connection.connection)
        if self.wrap:
            return Wrapper(connection.connection)
        return connection.connection",https://github.com/Deepwalker/aldjemy/blob/d58359a3710e7f21e47a70765b9d75c61143ceb1/aldjemy/core.py#L114-L122
tfidf_python_100_1.0,confusion matrix,python,"def _getAccuracy(self):

    n = self.confusion.shape[0]
    assert n == self.confusion.shape[1], ""Confusion matrix is non-square.""
    return self.confusion[range(n), range(n)].sum(), self.confusion.sum()",https://github.com/numenta/nupic/blob/5922fafffdccc8812e72b3324965ad2f7d4bbdad/src/nupic/regions/knn_classifier_region.py#L684-L688
tfidf_python_100_1.0,confusion matrix,python,"def from_existing(cls, confusion, *args, **kwargs):
        """"""Creates a confusion matrix from a DataFrame that already contains confusion counts (but not meta stats)
        >>> df = pd.DataFrame(np.matrix([[0,1,2,0,1,2,1,2,2,1],[0,1,2,1,2,0,0,1,2,0]]).T, columns=['True', 'Pred'])
        >>> c = Confusion(df)
        >>> c2 = pd.DataFrame(c)
        >>> hasattr(c2, '_binary_sensitivity')
        False
        >>> c3 = Confusion.from_existing(c2)
        >>> hasattr(c3, '_binary_sensitivity')
        True
        >>> (c3 == c).all().all()
        True
        >>> c3
        Pred  0  1  2
        True
        0     1  1  0
        1     2  1  1
        2     1  1  2
        """"""
        # Extremely brute-force to recreate data from a confusion matrix!

        df = []
        for t, p in product(confusion.index.values, confusion.columns.values):
            df += [[t, p]] * confusion[p][t]
        if confusion.index.name is not None and confusion.columns.name is not None:
            return Confusion(pd.DataFrame(df, columns=[confusion.index.name, confusion.columns.name]))
        return Confusion(pd.DataFrame(df))",https://github.com/totalgood/pugnlp/blob/c43445b14afddfdeadc5f3076675c9e8fc1ee67c/src/pugnlp/stats.py#L536-L562
tfidf_python_100_1.0,confusion matrix,python,"def plot_confusion_reports(y, y_hat, class_names=None):
    if class_names is None:
        class_names = list(set(y).union(set(y_hat)))

    # Compute confusion matrix
    cnf_matrix = confusion_matrix(y, y_hat)
    np.set_printoptions(precision=2)

    # Plot non-normalized confusion matrix
    plt.figure()
    plot_confusion_matrix(cnf_matrix, classes=class_names,
                          title='Confusion matrix, without normalization')

    # Plot normalized confusion matrix
    plt.figure()
    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,
                          title='Normalized confusion matrix')

    plt.show()",https://github.com/openmednlp/bedrock/blob/15796bd7837ddfe7ad5235fd188e5d7af8c0be49/bedrock/viz.py#L47-L65
tfidf_python_100_1.0,confusion matrix,python,"def __init__(self, confusion=1., miss=1., false_alarm=1.,
                 collar=0., skip_overlap=False, **kwargs):

        super(IdentificationErrorRate, self).__init__(**kwargs)
        self.matcher_ = LabelMatcher()
        self.confusion = confusion
        self.miss = miss
        self.false_alarm = false_alarm
        self.collar = collar
        self.skip_overlap = skip_overlap",https://github.com/pyannote/pyannote-metrics/blob/b433fec3bd37ca36fe026a428cd72483d646871a/pyannote/metrics/identification.py#L84-L93
tfidf_python_100_1.0,confusion matrix,python,"def draw_confusion_matrix(matrix):
    '''Draw confusion matrix for MNIST.'''
    fig = tfmpl.create_figure(figsize=(7,7))
    ax = fig.add_subplot(111)
    ax.set_title('Confusion matrix for MNIST classification')
    
    tfmpl.plots.confusion_matrix.draw(
        ax, matrix,
        axis_labels=['Digit ' + str(x) for x in range(10)],
        normalize=True
    )

    return fig",https://github.com/cheind/tf-matplotlib/blob/c6904d3d2d306d9a479c24fbcb1f674a57dafd0e/tfmpl/samples/mnist.py#L24-L36
tfidf_python_100_1.0,confusion matrix,python,"def setMatrix(self, a, b, c, d, e, f):
        self.transform_dict[""matrix""] = 'matrix(%s %s %s %s %s %s)' % (a, b, c, d, e, f)",https://github.com/alorence/pysvg-py3/blob/ce217a4da3ada44a71d3e2f391d37c67d95c724e/pysvg/builders.py#L316-L317
tfidf_python_100_1.0,confusion matrix,python,"def init_Fx0(self):
        self.Gx0 = spmatrix([], [], [], (self.m, self.n), 'd')
        self.Fy0 = spmatrix([], [], [], (self.n, self.m), 'd')
        self.Fx0 = spmatrix([], [], [], (self.n, self.n), 'd')

        self._temp.update({
            'Fx0': {
                'I': matrix([]),
                'J': matrix([]),
                'V': matrix([])
            },
            'Fy0': {
                'I': matrix([]),
                'J': matrix([]),
                'V': matrix([])
            },
            'Gx0': {
                'I': matrix([]),
                'J': matrix([]),
                'V': matrix([])
            },
        })

        self._set.update({
            'Fx0': {
                'I': matrix([]),
                'J': matrix([]),
                'V': matrix([])
            },
            'Fy0': {
                'I': matrix([]),
                'J': matrix([]),
                'V': matrix([])
            },
            'Gx0': {
                'I': matrix([]),
                'J': matrix([]),
                'V': matrix([])
            },
        })",https://github.com/cuihantao/andes/blob/7067898d4f26ce7534e968b8486c4aa8fe3a511a/andes/variables/dae.py#L307-L346
tfidf_python_100_1.0,confusion matrix,python,"def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):
    """"""
    Generates a confusion matrix between rater_a and rater_b
    A confusion matrix shows how often 2 values agree and disagree
    See quadratic_weighted_kappa for argument descriptions
    """"""
    assert(len(rater_a) == len(rater_b))
    rater_a = [int(a) for a in rater_a]
    rater_b = [int(b) for b in rater_b]
    min_rating = int(min_rating)
    max_rating = int(max_rating)
    if min_rating is None:
        min_rating = min(rater_a)
    if max_rating is None:
        max_rating = max(rater_a)
    num_ratings = int(max_rating - min_rating + 1)
    conf_mat = [[0 for i in range(num_ratings)]
                for j in range(num_ratings)]
    for a, b in zip(rater_a, rater_b):
        conf_mat[int(a - min_rating)][int(b - min_rating)] += 1
    return conf_mat",https://github.com/edx/ease/blob/a7890ed403da94d03726b0639cd8ebda45af6bbb/ease/util_functions.py#L387-L407
tfidf_python_100_1.0,confusion matrix,python,"def __init__(self, matrix, explicit_hydrogens, kekulize):
        self.matrix = matrix
        self.explicit_hydrogens = explicit_hydrogens
        self.kekulize = kekulize",https://github.com/mordred-descriptor/mordred/blob/2848b088fd7b6735590242b5e22573babc724f10/mordred/_matrix_attributes.py#L29-L32
tfidf_python_100_1.0,confusion matrix,python,"def vector_from_matrix(matrix):
    N = matrix.shape[0]
    triu_idx = np.triu_indices(n=N, m=N, k=1)
    return matrix[triu_idx]",https://github.com/LCAV/pylocus/blob/c56a38c251d8a435caf4641a8ae6027ecba2c8c6/pylocus/basics.py#L166-L169
tfidf_python_100_1.0,confusion matrix,python,"def __init__(self, *matrixes):
        matrix = self.transformClass()
        if matrixes:
            if isinstance(matrixes[0], (int, float)):
                matrixes = [matrixes]
            for m in matrixes:
                matrix = matrix.transform(m)
        self.matrix = matrix",https://github.com/robotools/fontMath/blob/6abcb9d5a1ca19788fbde4418d7b5630c60990d8/Lib/fontMath/mathTransform.py#L100-L107
tfidf_python_100_1.0,confusion matrix,python,"def confusion_matrix(self):
        """"""
        Returns the normalised confusion matrix
        """"""
        confusion_matrix = self.pixel_classification_sum.astype(np.float)
        confusion_matrix = np.divide(confusion_matrix.T, self.pixel_truth_sum.T).T

        return confusion_matrix * 100.0",https://github.com/sentinel-hub/eo-learn/blob/b8c390b9f553c561612fe9eb64e720611633a035/ml_tools/eolearn/ml_tools/validator.py#L201-L208
tfidf_python_100_1.0,confusion matrix,python,"def __init__(self, m):
        if isinstance(m, ndarray):
            self.matrix = m
        elif isinstance(m, Matrix):
            self.matrix = np_array(m.matrix)
        else:
            self.matrix = np_array(m)
        if len(self.matrix.shape) < 2:
            self.matrix = self.matrix.reshape((self.matrix.shape[0], 1))
        if len(self.matrix.shape) > 2:
            raise ValueError()
        super().__init__(self.matrix)",https://github.com/mabuchilab/QNET/blob/cc20d26dad78691d34c67173e5cd67dcac94208a/src/qnet/algebra/core/matrix_algebra.py#L35-L46
tfidf_python_100_1.0,confusion matrix,python,"def spiralOrder(matrix):
    return matrix and list(matrix.pop(0)) + spiralOrder(list(zip(*matrix))[::-1])",https://github.com/ManiacalLabs/BiblioPixelAnimations/blob/fba81f6b94f5265272a53f462ef013df1ccdb426/BiblioPixelAnimations/cube/wave_spiral.py#L4-L5
tfidf_python_100_1.0,confusion matrix,python,"def data_to_rcv(self, matrix):
        r = [x[0][0] for x in matrix['data']]
        c = [x[0][1] for x in matrix['data']]
        v = [x[1] for x in matrix['data']]

        return r, c, v",https://github.com/pjamesjoyce/lcopt/blob/3f1caca31fece4a3068a384900707e6d21d04597/lcopt/bw2_utils.py#L198-L203
tfidf_python_100_1.0,confusion matrix,python,"def to_sympy_column_matrix(matrix):
    """"""
    Converts a sympy matrix to a column matrix (i.e. transposes it if it was row matrix)
    Raises ValueError if matrix provided is not a vector
    :param matrix: a vector to be converted to column
    :return:
    """"""
    matrix = to_sympy_matrix(matrix)
    if matrix.cols == 1:
        return matrix
    elif matrix.rows == 1:
        return matrix.T
    else:
        raise ValueError('Cannot convert {0!r} to a column matrix'.format(matrix))",https://github.com/theosysbio/means/blob/fe164916a1d84ab2a4fa039871d38ccdf638b1db/src/means/util/sympyhelpers.py#L92-L105
tfidf_python_100_1.0,confusion matrix,python,"def evaluate_mask(matrix, matrix_size):
    """"""\
    Evaluates the provided `matrix` of a QR code.

    ISO/IEC 18004:2015(E) -- 7.8.3 Evaluation of data masking results (page 53)

    :param matrix: The matrix to evaluate
    :param matrix_size: The width (or height) of the matrix.
    :return int: The penalty score of the matrix.
    """"""
    return score_n1(matrix, matrix_size) + score_n2(matrix, matrix_size) \
           + score_n3(matrix, matrix_size) + score_n4(matrix, matrix_size)",https://github.com/heuer/segno/blob/64d912a2bd17d0b5ff3e8b5d37098edfc663c2b3/segno/encoder.py#L736-L747
tfidf_python_100_1.0,confusion matrix,python,"def is_undirected(matrix):
    """"""
    Determine if the matrix reprensents a directed graph

    :param matrix: The matrix to tested
    :returns: boolean
    """"""
    if isspmatrix(matrix):
        return sparse_allclose(matrix, matrix.transpose())
    
    return np.allclose(matrix, matrix.T)",https://github.com/GuyAllard/markov_clustering/blob/28787cf64ef06bf024ff915246008c767ea830cf/markov_clustering/modularity.py#L12-L22
tfidf_python_100_1.0,confusion matrix,python,"def matrix_undirected_unweighted(user):
    """"""
    Returns an undirected, unweighted matrix where an edge exists if the
    relationship is reciprocated.
    """"""
    matrix = matrix_undirected_weighted(user, interaction=None)
    for a, b in combinations(range(len(matrix)), 2):
        if matrix[a][b] is None or matrix[b][a] is None:
            continue

        if matrix[a][b] > 0 and matrix[b][a] > 0:
            matrix[a][b], matrix[b][a] = 1, 1

    return matrix",https://github.com/yvesalexandre/bandicoot/blob/73a658f6f17331541cf0b1547028db9b70e8d58a/bandicoot/network.py#L158-L171
tfidf_python_100_1.0,confusion matrix,python,"def update(self, t, dt):
        matrix = pg.Matrix()
        matrix = self.wasd.get_matrix(matrix)
        matrix = matrix.perspective(65, self.aspect, 0.01, 100)
        self.context1.matrix = matrix
        self.context1.camera_position = self.wasd.position
        self.context1.object_color = (1.0, 0.2, 0.0)
        self.context2.matrix = matrix
        self.context2.camera_position = self.wasd.position",https://github.com/fogleman/pg/blob/124ea3803c788b2c98c4f3a428e5d26842a67b58/examples/context.py#L12-L20
tfidf_python_100_1.0,set working directory,python,"def set_current_client_working_directory(self, directory):
        """"""Set current client working directory.""""""
        shellwidget = self.get_current_shellwidget()
        if shellwidget is not None:
            shellwidget.set_cwd(directory)",https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/ipythonconsole/plugin.py#L571-L575
tfidf_python_100_1.0,set working directory,python,"def _create_working(self):
        working = deepcopy(self)
        self.schema.apply_defaults(working)
        return working",https://github.com/gamechanger/mongothon/blob/5305bdae8e38d09bfe7881f1edc99ac0a2e6b96b/mongothon/model.py#L57-L60
tfidf_python_100_1.0,set working directory,python,"def save(self, *args, **kwargs):
        # Create a working copy of ourselves and validate it
        working = self._create_working()
        self._do_validate(working)

        self._emit('will_save', working)

        # Attempt to save
        self.collection.save(working, *args, **kwargs)
        self._state = Model.PERSISTED

        self._emit('did_save', working)

        # On successful completion, update from the working copy
        self.populate(working)",https://github.com/gamechanger/mongothon/blob/5305bdae8e38d09bfe7881f1edc99ac0a2e6b96b/mongothon/model.py#L118-L132
tfidf_python_100_1.0,set working directory,python,"def regex_to_sql_like(regex_text: str,
                          single_wildcard: str = ""_"",
                          zero_or_more_wildcard: str = ""%"") -> List[str]:
        """"""
        Converts regular expression text to a reasonably close fragment
        for the SQL ``LIKE`` operator.

        NOT PERFECT, but works for current built-in regular expressions.

        Args:
            regex_text: regular expression text to work with
            single_wildcard: SQL single wildcard, typically an underscore
            zero_or_more_wildcard: SQL ""zero/one/many"" wildcard, probably always
                a percent symbol

        Returns:
            string for an SQL string literal

        Raises:
            :exc:`ValueError` for some regex text that it doesn't understand
            properly
        """"""
        def append_to_all(new_content: str) -> None:
            nonlocal results
            results = [r + new_content for r in results]

        def split_and_append(new_options: List[str]) -> None:
            nonlocal results
            newresults = []  # type: List[str]
            for option in new_options:
                newresults.extend([r + option for r in results])
            results = newresults

        def deduplicate_wildcards(text: str) -> str:
            while zero_or_more_wildcard + zero_or_more_wildcard in text:
                text = text.replace(
                    zero_or_more_wildcard + zero_or_more_wildcard,
                    zero_or_more_wildcard)
            return text

        # Basic processing
        working = regex_text  # strings are immutable
        results = [zero_or_more_wildcard]  # start with a wildcard

        while working:
            if working.startswith("".*""):
                # e.g. "".*ozapi""
                append_to_all(zero_or_more_wildcard)
                working = working[2:]
            elif working.startswith(""[""):
                # e.g. ""[io]peridol""
                close_bracket = working.index(""]"")  # may raise
                bracketed = working[1:close_bracket]
                option_groups = bracketed.split(""|"")
                options = [c for group in option_groups for c in group]
                split_and_append(options)
                working = working[close_bracket + 1:]
            elif len(working) > 1 and working[1] == ""?"":
                # e.g. ""r?azole""
                split_and_append(["""", working[0]])
                # ... regex ""optional character""
                # ... SQL: some results with a single wildcard, some without
                working = working[2:]
            elif working.startswith("".""):
                # single character wildcard
                append_to_all(single_wildcard)
                working = working[1:]
            else:
                append_to_all(working[0])
                working = working[1:]
        append_to_all(zero_or_more_wildcard)  # end with a wildcard

        # Remove any duplicate (consecutive) % wildcards:
        results = [deduplicate_wildcards(r) for r in results]

        # Done
        return results",https://github.com/RudolfCardinal/pythonlib/blob/0b84cb35f38bd7d8723958dae51b480a829b7227/cardinal_pythonlib/psychiatry/drugs.py#L529-L605
tfidf_python_100_1.0,set working directory,python,"def get_wd_data(self):
        """"""
        Show dialog to get user input for which directory
        to set as working directory.
        Called by self.get_dm_and_wd
        """"""
        wait = wx.BusyInfo('Reading in data from current working directory, please wait...')
        #wx.Yield()
        print('-I- Read in any available data from working directory')
        self.contribution = cb.Contribution(self.WD, dmodel=self.data_model)
        del wait",https://github.com/PmagPy/PmagPy/blob/c7984f8809bf40fe112e53dcc311a33293b62d0b/programs/pmag_gui.py#L130-L140
tfidf_python_100_1.0,set working directory,python,"def in_dir(directory, create=True):
    """"""Context manager to execute a code block in a directory.

    * The directory is created if it does not exist (unless
      create=False is set)
    * At the end or after an exception code always returns to
      the directory that was the current directory before entering
      the block.
    """"""
    startdir = os.getcwd()
    try:
        try:
            os.chdir(directory)
            logger.debug(""Working in {directory!r}..."".format(**vars()))
        except OSError as err:
            if create and err.errno == errno.ENOENT:
                os.makedirs(directory)
                os.chdir(directory)
                logger.info(""Working in {directory!r} (newly created)..."".format(**vars()))
            else:
                logger.exception(""Failed to start working in {directory!r}."".format(**vars()))
                raise
        yield os.getcwd()
    finally:
        os.chdir(startdir)",https://github.com/Becksteinlab/GromacsWrapper/blob/d4f9a8cb6f48292732cf7c7e4ef4a6d2ccbc51b9/gromacs/utilities.py#L411-L435
tfidf_python_100_1.0,set working directory,python,"def change_to_workdir(self):
        """"""Change working directory to working attribute

        :return: None
        """"""
        logger.info(""Changing working directory to: %s"", self.workdir)

        self.check_dir(self.workdir)
        try:
            os.chdir(self.workdir)
        except OSError as exp:
            self.exit_on_error(""Error changing to working directory: %s. Error: %s. ""
                               ""Check the existence of %s and the %s/%s account ""
                               ""permissions on this directory.""
                               % (self.workdir, str(exp), self.workdir, self.user, self.group),
                               exit_code=3)
        self.pre_log.append((""INFO"", ""Using working directory: %s"" % os.path.abspath(self.workdir)))",https://github.com/Alignak-monitoring/alignak/blob/f3c145207e83159b799d3714e4241399c7740a64/alignak/daemon.py#L1243-L1259
tfidf_python_100_1.0,set working directory,python,"def chdir(self, directory, browsing_history=False,
              refresh_explorer=True, refresh_console=True):
        """"""Set directory as working directory""""""
        if directory:
            directory = osp.abspath(to_text_string(directory))

        # Working directory history management
        if browsing_history:
            directory = self.history[self.histindex]
        elif directory in self.history:
            self.histindex = self.history.index(directory)
        else:
            if self.histindex is None:
                self.history = []
            else:
                self.history = self.history[:self.histindex+1]
            self.history.append(directory)
            self.histindex = len(self.history)-1
        
        # Changing working directory
        try:
            os.chdir(directory)
            if refresh_explorer:
                self.set_explorer_cwd.emit(directory)
            if refresh_console:
                self.set_current_console_wd.emit(directory)
            self.refresh_findinfiles.emit()
        except OSError:
            self.history.pop(self.histindex)
        self.refresh_plugin()",https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/workingdirectory/plugin.py#L211-L240
tfidf_python_100_1.0,set working directory,python,"def pull(directory: str) -> Commit:
    """"""
    Pulls the subrepo that has been cloned into the given directory.
    :param directory: the directory containing the subrepo
    :return: the commit the subrepo is on
    """"""
    if not os.path.exists(directory):
        raise ValueError(f""No subrepo found in \""{directory}\"""")
    try:
        result = run([GIT_COMMAND, _GIT_SUBREPO_COMMAND, _GIT_SUBREPO_PULL_COMMAND, _GIT_SUBREPO_VERBOSE_FLAG,
                      get_directory_relative_to_git_root(directory)],
                     execution_directory=get_git_root_directory(directory))
    except RunException as e:
        if ""Can't pull subrepo. Working tree has changes"" in e.stderr:
            raise UnstagedChangeException() from e
    return status(directory)[2]",https://github.com/wtsi-hgi/python-git-subrepo/blob/bb2eb2bd9a7e51b862298ddb4168cc5b8633dad0/gitsubrepo/subrepo.py#L144-L159
tfidf_python_100_1.0,set working directory,python,"def _configure(self):
        self.directory = self._require('directory')
        logging.debug('Creating directory: {directory}'.format(directory=self.directory))",https://github.com/abantos/bolt/blob/8b6a911d4a7b1a6e870748a523c9b2b91997c773/bolt/tasks/bolt_mkdir.py#L22-L24
tfidf_python_100_1.0,set working directory,python,"def get_env(working_directory=None):
    """"""get_env

This function grabs key/value pair items and assigns them for the
config.JSON.  This essentially checks the environment for key locations within
the filesystem.
    """"""
    working = working_directory if working_directory else os.getcwd()
    dist_dirs = glob.glob(working + ""/f5-*-dist"")
    print(dist_dirs)
    dist_dir_re = re.compile('/([^/]+)-dist/?')
    if dist_dirs:
        dist_dir = dist_dirs[0]
        match = dist_dir_re.search(dist_dir)
        if match:
            project_name = match.group(1)
        else:
            print(""Unrecognized run location:\n"" + working)
            exit_cleanly(errnum=errno.EIO)
    elif '-dist/scripts' in working:
        match = dist_dir_re.search(working)
        if match:
            project_name = match.group(1)
    else:
        print(""Unable to determine the *-dist directory from "" + working)
        exit_cleanly(errnum=errno.ENOSYS)
    stdeb_cfg = dist_dir + ""/deb_dist/stdeb.cfg""
    setup_cfg = working + ""/setup.cfg""
    stp_reqs = working + ""/setup_requirements.txt""
    scripts = dist_dir + ""/scripts""
    env = {'working': working, 'dist_dir': dist_dir, 'stdeb_cfg': stdeb_cfg,
           'setup_requirements': stp_reqs, 'setup_cfg': setup_cfg,
           'scripts': scripts, 'project': project_name}
    return env",https://github.com/F5Networks/f5-common-python/blob/7e67d5acd757a60e3d5f8c88c534bd72208f5494/f5-sdk-dist/scripts/configure.py#L86-L119
tfidf_python_100_1.0,set working directory,python,"def tmpWorking(tmp_name):

    # Directories
    working = tmp_name + uuid.uuid4()
    current = os.getcwd()
    
    # Enter
    fileable.mkdir(working)
    os.chdir(working)
    yield

    # Exit
    os.chdir(current)
    fileable.rm(working)",https://github.com/shotastage/mirage-django-lts/blob/4e32dd48fff4b191abb90813ce3cc5ef0654a2ab/mirage/system/tmp.py#L26-L39
tfidf_python_100_1.0,set working directory,python,"def add_package(pkg_type, pkg_name, working=None):
    """"""add_package

Adds a package to the existing config.JSON file.  This is a standalone function
to handle this functionality.  The existing config.JSON is read in and is left
unchanged.
    """"""
    kvp = {pkg_type: pkg_name}
    working = os.getcwd() if not working else os.path.abspath(working)
    if '-dist/scripts' in working:
        config = working + ""/config.JSON""
    elif '-dist' in working:
        config = working + ""/scripts/config.JSON""
    else:
        config = working + ""/*-dist/scripts/config.JSON""
        entropy = glob.glob(config)
        if entropy:
            config = entropy[0]  # we'll just assume it's the first one...
    print(""config"", config, 'working', working)
    config = load_config(config)
    config.update(kvp)
    export_to_json(config)",https://github.com/F5Networks/f5-common-python/blob/7e67d5acd757a60e3d5f8c88c534bd72208f5494/f5-sdk-dist/scripts/configure.py#L25-L46
tfidf_python_100_1.0,set working directory,python,"def runSwarm(self, workingDirPath):
    """"""
    Runs a swarm with data within a working directory. This assumes that the 
    user has already run prepareSwarm().
    :param workingDirPath: absolute or relative path to working directory
    """"""
    if not os.path.exists(workingDirPath):
      raise Exception(""Working directory %s does not exist!"" % workingDirPath)
    banner(""RUNNING SWARM"")
    self._modelParams = swarm(workingDirPath)",https://github.com/htm-community/menorah/blob/1991b01eda3f6361b22ed165b4a688ae3fb2deaf/menorah/menorah.py#L183-L192
tfidf_python_100_1.0,set working directory,python,"def _get_maybe_new_directory_id(self, db, directory):
        if directory is None:
            return None
        directory = normalize_directory(directory)
        return self._get_maybe_new_id(
            db, 'directory_list', {'directory': directory})",https://github.com/tkf/rash/blob/585da418ec37dd138f1a4277718b6f507e9536a2/rash/database.py#L316-L321
tfidf_python_100_1.0,set working directory,python,"def setLibraryDirectoriesWidget(self):
        for directory in self.currentDirectories:
            newItem = QListWidgetItem(directory, self.directoriesList)",https://github.com/gdankov/sonance-music-player/blob/49a0801029b6f3a87c6eff8ed4be4c3fd4de498f/gui/dialogs.py#L289-L291
tfidf_python_100_1.0,set working directory,python,"def _dispatch_container(self, textgroup, directory):
        super(ProtoNautilusCtsResolver, self)._dispatch_container(textgroup, directory)",https://github.com/Capitains/Nautilus/blob/6be453fe0cc0e2c1b89ff06e5af1409165fc1411/capitains_nautilus/cts/resolver/base.py#L145-L146
tfidf_python_100_1.0,set working directory,python,"def create_file_dialog(dialog_type, directory, allow_multiple, save_filename, file_types):
    return BrowserView.instance.create_file_dialog(dialog_type, directory, allow_multiple, save_filename)",https://github.com/r0x0r/pywebview/blob/fc44d84656e88f83ca496abb50ee75e95540996e/webview/win32.py#L305-L306
tfidf_python_100_1.0,set working directory,python,"def __list_files(self, directory):
        if directory and exists(directory):
            return [f for f in listdir(directory) if isfile(join(directory, f))]
        else:
            return []",https://github.com/galaxyproject/pulsar/blob/9ab6683802884324652da0a9f0808c7eb59d3ab4/pulsar/client/staging/up.py#L253-L257
tfidf_python_100_1.0,set working directory,python,"def clean(self, remove_working_directory=True):
        """"""
        Remove the temporary directory.
        If ``remove_working_directory`` is ``True``
        remove the working directory as well,
        otherwise just remove the temporary directory.

        :param bool remove_working_directory: if ``True``, remove
                                              the working directory as well
        """"""
        if remove_working_directory is not None:
            self.log(u""Removing working directory... "")
            gf.delete_directory(self.working_directory)
            self.working_directory = None
            self.log(u""Removing working directory... done"")
        self.log(u""Removing temporary directory... "")
        gf.delete_directory(self.tmp_directory)
        self.tmp_directory = None
        self.log(u""Removing temporary directory... done"")",https://github.com/readbeyond/aeneas/blob/9d95535ad63eef4a98530cfdff033b8c35315ee1/aeneas/executejob.py#L298-L316
tfidf_python_100_1.0,group by count,python,"def _get_suite_root_name(suite_names):
    top_names = [x.top_name() for group in suite_names for x in group]
    if top_names and top_names.count(top_names[0]) == len(top_names):
        return top_names[0]
    return ''",https://github.com/mkorpela/pabot/blob/b7d85546a58e398d579bb14fd9135858ec08a031/pabot/pabot.py#L1108-L1112
tfidf_python_100_1.0,group by count,python,"def get_last_archive(archive_dir):
    count = 0
    for filename in os.listdir(archive_dir):
        m = re.match(archive_file_pat, filename)
        if m and int(m.group(1)) > count:
            count = int(m.group(1))
    if count != 0:
        return os.path.join(archive_dir, archive_file_fmt % count)
    else:
        return None",https://github.com/DocNow/twarc/blob/47dd87d0c00592a4d583412c9d660ba574fc6f26/utils/twarc-archive.py#L145-L154
tfidf_python_100_1.0,group by count,python,"def get_next_archive(archive_dir):
    last_archive = get_last_archive(archive_dir)
    if last_archive:
        m = re.search(archive_file_pat, last_archive)
        count = int(m.group(1)) + 1
    else:
        count = 1
    return os.path.join(archive_dir, archive_file_fmt % count)",https://github.com/DocNow/twarc/blob/47dd87d0c00592a4d583412c9d660ba574fc6f26/utils/twarc-archive.py#L156-L163
tfidf_python_100_1.0,group by count,python,"def get_thread_count(self):
        count = 0
        for aProcess in self.iter_processes():
            count += aProcess.get_thread_count()
        return count",https://github.com/fabioz/PyDev.Debugger/blob/ed9c4307662a5593b8a7f1f3389ecd0e79b8c503/pydevd_attach_to_process/winappdbg/process.py#L4826-L4830
tfidf_python_100_1.0,group by count,python,"def get_module_count(self):
        count = 0
        for aProcess in self.iter_processes():
            count += aProcess.get_module_count()
        return count",https://github.com/fabioz/PyDev.Debugger/blob/ed9c4307662a5593b8a7f1f3389ecd0e79b8c503/pydevd_attach_to_process/winappdbg/process.py#L4841-L4845
tfidf_python_100_1.0,group by count,python,"def filing_8K(self, company_code, cik, priorto, count):
        self._fetch_report(company_code, cik, priorto, count, '8-K')",https://github.com/coyo8/sec-edgar/blob/79976a1e11f117d680b94a12ed34af3563043cc7/SECEdgar/crawler.py#L106-L107
tfidf_python_100_1.0,group by count,python,"def filing_4(self, company_code, cik, priorto, count):
        self._fetch_report(company_code, cik, priorto, count, '4')",https://github.com/coyo8/sec-edgar/blob/79976a1e11f117d680b94a12ed34af3563043cc7/SECEdgar/crawler.py#L115-L116
tfidf_python_100_1.0,group by count,python,"def compute_a(self, lst_ni):
        a = np.ones((self.datanum, 1))
        count = 0
        for N_i in lst_ni:
            if N_i == lst_ni[0]:
                a[count:count + N_i] = (float(1) / N_i) * a[count]
                count += N_i
            else:
                if N_i == lst_ni[1]:
                    a[count: count + N_i] = -(float(1) / N_i) * a[count]
                    count += N_i
        return a",https://github.com/SheffieldML/GPy/blob/54c32d79d289d622fb18b898aee65a2a431d90cf/GPy/core/parameterization/priors.py#L466-L477
tfidf_python_100_1.0,group by count,python,"def inc(self, count=1):
        c = Counter.from_db(db.inc_counter(Counter(
            group=self.group,
            name=self.name,
            title=self.title,
            description=self.description,
            count=count
        )))
        self.count = c.count",https://github.com/alerta/alerta/blob/6478d6addc217c96a4a6688fab841035bef134e1/alerta/models/metrics.py#L145-L153
tfidf_python_100_1.0,group by count,python,"def databaserequesttracer_set_round_trip_count(self, tracer_h, count):
        _livecheck(tracer_h, DbRequestHandle)
        _typecheck(count, int)
        assert count >= 0, 'Invalid count'
        tracer_h.round_trip_count = count",https://github.com/Dynatrace/OneAgent-SDK-for-Python/blob/f7b121b492f25b1c5b27316798e1a70b6be2bd01/src/oneagent/_impl/native/sdkmockiface.py#L462-L466
tfidf_python_100_1.0,group by count,python,"def databaserequesttracer_set_returned_row_count(self, tracer_h, count):
        _livecheck(tracer_h, DbRequestHandle)
        _typecheck(count, int)
        assert count >= 0, 'Invalid count'
        tracer_h.returned_row_count = count",https://github.com/Dynatrace/OneAgent-SDK-for-Python/blob/f7b121b492f25b1c5b27316798e1a70b6be2bd01/src/oneagent/_impl/native/sdkmockiface.py#L455-L459
tfidf_python_100_1.0,group by count,python,"def count(self):
        count = self.model.count
        assert count == self.color.count
        return count",https://github.com/AllTheWayDown/turgles/blob/1bb17abe9b3aa0953d9a8e9b05a23369c5bf8852/turgles/buffer.py#L130-L133
tfidf_python_100_1.0,group by count,python,"def update_tmg_group_permissions():
    group_name = TMG
    group = Group.objects.get(name=group_name)
    group.permissions.clear()

    # edc_action_item
    add_edc_action_permissions(group, allow_delete=True)
    make_view_only_model(group, ""edc_action_item.actiontype"")
    make_view_only_model(group, ""edc_action_item.reference"")

    # ambition_ae
    add_permissions_to_group_by_app_label(group, ""ambition_ae"")
    make_view_only_app_label(group, ""ambition_ae"")
    add_permissions_to_group_by_model(group, ""ambition_ae.aetmg"")
    add_permissions_to_group_by_model(group, ""ambition_ae.historicalaetmg"")

    # ambition_subject
    add_permissions_to_group_by_app_label(group, ""ambition_subject"")
    make_view_only_app_label(group, ""ambition_subject"")
    remove_permissions_by_model(group, ""ambition_subject.subjectconsent"")
    remove_permissions_by_model(group, ""ambition_subject.subjectreconsent"")

    # ambition_subject
    add_permissions_to_group_by_app_label(group, ""ambition_lists"")
    make_view_only_app_label(group, ""ambition_lists"")

    # ambition_prn
    add_permissions_to_group_by_model(group, ""ambition_prn.deathreporttmg"")
    add_permissions_to_group_by_codenames(
        group,
        codenames=[
            ""ambition_prn.view_amphotericinmisseddoses"",
            ""ambition_prn.view_fluconazolemisseddoses"",
            ""ambition_prn.view_flucytosinemisseddoses"",
            ""ambition_prn.view_significantdiagnoses"",
            ""ambition_prn.view_historicalamphotericinmisseddoses"",
            ""ambition_prn.view_historicalfluconazolemisseddoses"",
            ""ambition_prn.view_historicalflucytosinemisseddoses"",
            ""ambition_prn.view_historicalsignificantdiagnoses"",
            ""ambition_prn.view_deathreport"",
        ],
    )
    add_permissions_to_group_by_codenames(
        group,
        codenames=[
            ""edc_appointment.view_historicalappointment"",
            ""edc_appointment.view_appointment"",
        ],
    )

    # nav and dashboard
    add_permissions_to_group_by_codenames(
        group,
        codenames=[
            ""edc_navbar.nav_tmg_section"",
            ""edc_navbar.nav_subject_section"",
            ""edc_navbar.nav_screening_section"",
            ""edc_dashboard.view_subject_review_listboard"",
            ""edc_dashboard.view_screening_listboard"",
            ""edc_dashboard.view_subject_listboard"",
            ""edc_dashboard.view_tmg_listboard"",
        ],
    )

    remove_pii_permissions_from_group(group, extra_pii_models=pii_models)
    remove_historical_group_permissions(group)",https://github.com/ambition-trial/ambition-permissions/blob/4fcd642aa11e57c7aa8c433dbaf855d12c8a9de3/ambition_permissions/updaters/tmg.py#L19-L84
tfidf_python_100_1.0,group by count,python,"def resolve_pageInfo(self, graphene_info):
        count = len(self._logs)
        lastCursor = None
        if count > 0:
            lastCursor = str(count - 1)
        return graphene_info.schema.type_named('PageInfo')(
            lastCursor=lastCursor,
            hasNextPage=None,
            hasPreviousPage=None,
            count=count,
            totalCount=count,
        )",https://github.com/dagster-io/dagster/blob/4119f8c773089de64831b1dfb9e168e353d401dc/python_modules/dagster-graphql/dagster_graphql/schema/runs.py#L110-L121
tfidf_python_100_1.0,group by count,python,"def __init__(self, count, job_results):
        self.count = count
        self.job_results = job_results",https://github.com/samuelcolvin/arq/blob/1434646b48c45bd27e392f0162976404e4d8021d/arq/worker.py#L104-L106
tfidf_python_100_1.0,group by count,python,"def __init__(self, count, uniqueCount):
        self.count = count
        self.uniqueCount = uniqueCount",https://github.com/wwwiiilll/pyxlsb/blob/f77d99832edd337570f3e0c9c2135d0ce6b966b1/pyxlsb/records.py#L238-L240
tfidf_python_100_1.0,group by count,python,"def __init__(self, wmgMap, count = 1):
        self.wmgMap = wmgMap
        self.count = count",https://github.com/PrefPy/prefpy/blob/f395ba3782f05684fa5de0cece387a6da9391d02/prefpy/preference.py#L17-L19
tfidf_python_100_1.0,group by count,python,"def _count_k(self, k, count):
        try:
            count[k] += 1
        except KeyError:
            count[k] = 1
        return count",https://github.com/Phelimb/atlas/blob/02e85497bb5ac423d6452a10dca11964582ac4d7/mykatlas/cortex/server.py#L259-L264
tfidf_python_100_1.0,group by count,python,"def newTermRefs(count):
    a = PL_new_term_refs(count)
    return list(range(a, a + count))",https://github.com/yuce/pyswip/blob/f7c1f1e8c3a13b90bd775861d374788a8b5677d8/pyswip/easy.py#L504-L506
tfidf_python_100_1.0,group by count,python,"def output_link(self, m):
        return self._process_link(m, m.group(3), m.group(4))",https://github.com/tensorforce/tensorforce/blob/520a8d992230e382f08e315ede5fc477f5e26bfb/docs/mistune.py#L626-L627
tfidf_python_100_1.0,binomial distribution,python,"def __init__(self, gp_link=None):
        if gp_link is None:
            gp_link = link_functions.Probit()

        super(Binomial, self).__init__(gp_link, 'Binomial')",https://github.com/SheffieldML/GPy/blob/54c32d79d289d622fb18b898aee65a2a431d90cf/GPy/likelihoods/binomial.py#L24-L28
tfidf_python_100_1.0,binomial distribution,python,"def binomial(n,k):
    """"""
    Binomial coefficient
    >>> binomial(5,2)
    10
    >>> binomial(10,5)
    252
    """"""
    if n==k: return 1
    assert n>k, ""Attempting to call binomial(%d,%d)"" % (n,k)
    return factorial(n)//(factorial(k)*factorial(n-k))",https://github.com/chemlab/chemlab/blob/c8730966316d101e24f39ac3b96b51282aba0abe/chemlab/qc/utils.py#L30-L40
tfidf_python_100_1.0,binomial distribution,python,"def binomial(n, k):
    if 0 <= k <= n:
        ntok = 1
        ktok = 1
        for t in range(1, min(k, n - k) + 1):
            ntok *= n
            ktok *= t
            n -= 1
        return ntok // ktok
    else:
        return 0",https://github.com/msu-coinlab/pymop/blob/7b7e789e640126c6d254e86ede5d7f4baad7eaa5/pymop/util.py#L19-L29
tfidf_python_100_1.0,binomial distribution,python,"def __init__(self, p):
        super(Binomial, self).__init__()
        self.p = handle_continuous_param(p, ""p"")",https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/parameters.py#L644-L646
tfidf_python_100_1.0,binomial distribution,python,"def sample(self, rgen):
    x = rgen.binomial(self.n, self.p)
    return x, self.logProbability(x)",https://github.com/numenta/nupic/blob/5922fafffdccc8812e72b3324965ad2f7d4bbdad/src/nupic/math/dist.py#L48-L50
tfidf_python_100_1.0,binomial distribution,python,"def __init__(self, numSamples, distribution):
    self.numSamples = numSamples
    self.distribution = distribution
    self.possibleValues = distribution.possibleValues",https://github.com/numenta/htmresearch/blob/70c096b09a577ea0432c3f3bfff4442d4871b7aa/htmresearch/frameworks/location/ambiguity_index.py#L107-L110
tfidf_python_100_1.0,binomial distribution,python,"def get_distribution_time_index(self, distribution):
        if isinstance(distribution, dict):
            distribution = distribution
        else:
            distribution = self.get_distribution(distribution)

        return time_series.get_distribution_time_index(distribution)",https://github.com/datosgobar/pydatajson/blob/3141082ffbaa295e2deaf6ffbbc5a59f5859960e/pydatajson/core.py#L191-L197
tfidf_python_100_1.0,binomial distribution,python,"def get_distribution_time_index_frequency(self, distribution):
        if isinstance(distribution, dict):
            distribution = distribution
        else:
            distribution = self.get_distribution(distribution)

        return time_series.get_distribution_time_index_frequency(distribution)",https://github.com/datosgobar/pydatajson/blob/3141082ffbaa295e2deaf6ffbbc5a59f5859960e/pydatajson/core.py#L199-L205
tfidf_python_100_1.0,binomial distribution,python,"def Binomial(n, p, tag=None):
    """"""
    A Binomial random variate
    
    Parameters
    ----------
    n : int
        The number of trials
    p : scalar
        The probability of success
    """"""
    assert (
        int(n) == n and n > 0
    ), 'Binomial number of trials ""n"" must be an integer greater than zero'
    assert (
        0 < p < 1
    ), 'Binomial probability ""p"" must be between zero and one, non-inclusive'
    return uv(ss.binom(n, p), tag=tag)",https://github.com/tisimst/mcerp/blob/2bb8260c9ad2d58a806847f1b627b6451e407de1/mcerp/__init__.py#L1150-L1167
tfidf_python_100_1.0,binomial distribution,python,"def binomial(n):
    """"""
    Return all binomial coefficients for a given order.

    For n > 5, scipy.special.binom is used, below we hardcode
    to avoid the scipy.special dependency.

    Parameters
    --------------
    n : int
      Order

    Returns
    ---------------
    binom : (n + 1,) int
      Binomial coefficients of a given order
    """"""
    if n == 1:
        return [1, 1]
    elif n == 2:
        return [1, 2, 1]
    elif n == 3:
        return [1, 3, 3, 1]
    elif n == 4:
        return [1, 4, 6, 4, 1]
    elif n == 5:
        return [1, 5, 10, 10, 5, 1]
    else:
        from scipy.special import binom
        return binom(n, np.arange(n + 1))",https://github.com/mikedh/trimesh/blob/25e059bf6d4caa74f62ffd58ce4f61a90ee4e518/trimesh/path/curve.py#L100-L129
tfidf_python_100_1.0,binomial distribution,python,"def generateObjects(numObjects, featuresPerObject, objectWidth, numFeatures,
                    distribution=""AllFeaturesEqual_Replacement""):
  assert featuresPerObject <= (objectWidth ** 2)

  objectFeatures = generateObjectFeatures(numObjects,
                                          featuresPerObject,
                                          numFeatures,
                                          distribution)

  return arrangeFeatures(objectFeatures, objectWidth)",https://github.com/numenta/htmresearch/blob/70c096b09a577ea0432c3f3bfff4442d4871b7aa/htmresearch/frameworks/location/object_generation.py#L107-L116
tfidf_python_100_1.0,binomial distribution,python,"def setemission(self, state, distribution):
        self.nodes.add(state)
        if not isinstance(distribution, Distribution):
            distribution = Distribution(distribution)
        self.edges_toobservables[state] = distribution
        self.observablenodes.update(distribution.keys())",https://github.com/proycon/pynlpl/blob/7707f69a91caaa6cde037f0d0379f1d42500a68b/pynlpl/statistics.py#L454-L459
tfidf_python_100_1.0,binomial distribution,python,"def run(self):
        # Write the syspaths file
        if getattr(self.distribution, 'salt_syspaths_hardcoded_path', None) is None:
            print('This command is not meant to be called on it\'s own')
            exit(1)

        # Write the system paths file
        open(self.distribution.salt_syspaths_hardcoded_path, 'w').write(
            INSTALL_SYSPATHS_TEMPLATE.format(
                date=DATE,
                root_dir=self.distribution.salt_root_dir,
                share_dir=self.distribution.salt_share_dir,
                config_dir=self.distribution.salt_config_dir,
                cache_dir=self.distribution.salt_cache_dir,
                sock_dir=self.distribution.salt_sock_dir,
                srv_root_dir=self.distribution.salt_srv_root_dir,
                base_file_roots_dir=self.distribution.salt_base_file_roots_dir,
                base_pillar_roots_dir=self.distribution.salt_base_pillar_roots_dir,
                base_master_roots_dir=self.distribution.salt_base_master_roots_dir,
                base_thorium_roots_dir=self.distribution.salt_base_thorium_roots_dir,
                logs_dir=self.distribution.salt_logs_dir,
                pidfile_dir=self.distribution.salt_pidfile_dir,
                spm_parent_path=self.distribution.salt_spm_parent_dir,
                spm_formula_path=self.distribution.salt_spm_formula_dir,
                spm_pillar_path=self.distribution.salt_spm_pillar_dir,
                spm_reactor_path=self.distribution.salt_spm_reactor_dir,
                home_dir=self.distribution.salt_home_dir,
            )
        )",https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/setup.py#L286-L314
tfidf_python_100_1.0,binomial distribution,python,"def settransitions(self, state, distribution):
        self.nodes.add(state)
        if not isinstance(distribution, Distribution):
            distribution = Distribution(distribution)
        self.edges_out[state] = distribution
        self.nodes.update(distribution.keys())",https://github.com/proycon/pynlpl/blob/7707f69a91caaa6cde037f0d0379f1d42500a68b/pynlpl/statistics.py#L354-L359
tfidf_python_100_1.0,binomial distribution,python,"def add(self, distribution):
        key = '%s-%s' % (distribution.name, distribution.version)
        self[key] = distribution",https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/pkginfo/index.py#L12-L14
tfidf_python_100_1.0,binomial distribution,python,"def process_exclusion_distribution(self, last_candidate_aggregates):
        distribution, *self.exclusion_distributions_pending = self.exclusion_distributions_pending
        return self.process_exclusion(distribution, last_candidate_aggregates)",https://github.com/grahame/dividebatur/blob/adc1f6e8013943471f1679e3c94f9448a1e4a472/dividebatur/counter.py#L469-L471
tfidf_python_100_1.0,binomial distribution,python,"def process_election_distribution(self, last_candidate_aggregates):
        distribution, *self.election_distributions_pending = self.election_distributions_pending
        return self.process_election(distribution, last_candidate_aggregates)",https://github.com/grahame/dividebatur/blob/adc1f6e8013943471f1679e3c94f9448a1e4a472/dividebatur/counter.py#L476-L478
tfidf_python_100_1.0,binomial distribution,python,"def __init__(self, distribution):
        """"""
        Args:
            distribution (openturns.Distribution):
                1D distribution created in OpenTURNS.
        """"""
        Dist.__init__(self)
        if distribution.getDimension() != 1:
            raise Exception(""Only 1D OpenTURNS distribution are supported for now"")
        self.distribution = distribution",https://github.com/jonathf/chaospy/blob/25ecfa7bf5608dc10c0b31d142ded0e3755f5d74/chaospy/distributions/collection/openturns.py#L10-L19
tfidf_python_100_1.0,binomial distribution,python,"def install(self, install_prefix, distribution, configure=True, is_remote=False, in_project=False):
        if isinstance(distribution, string_types) or not distribution:
            distribution = MaltegoDistribution(distribution)
        if not isinstance(distribution, MtzDistribution):
            if distribution.version >= '3.4.0':
                raise ValueError(INCOMPATIBLE)
            print('Installing transform package %s...' % self.name, file=sys.stderr)

        install_prefix = self._init_install_prefix(install_prefix)

        self._install_transforms(install_prefix, distribution, in_project)
        self._install_entities(distribution)
        self._install_machines(distribution)

        if configure:
            self.configure(install_prefix, remote=is_remote)",https://github.com/redcanari/canari3/blob/322d2bae4b49ac728229f418b786b51fcc227352/src/canari/pkgutils/transform.py#L306-L321
tfidf_python_100_1.0,binomial distribution,python,"def dropout( X, p = 0. ):
    if p != 0:
        retain_p = 1 - p
        X = X * np.random.binomial(1,retain_p,size = X.shape)
        X /= retain_p
    return X",https://github.com/jorgenkg/python-neural-network/blob/617b9940fa157d54d7831c42c0f7ba6857239b9a/nimblenet/tools.py#L26-L31
tfidf_python_100_1.0,aes encryption,python,"def encrypt_email(email):
    """"""
    The default encryption function for storing emails in the database. This
    uses AES and the encryption key defined in the applications configuration.

    :param email:
        The email address.

    """"""
    aes = SimpleAES(flask.current_app.config[""AES_KEY""])
    return aes.encrypt(email)",https://github.com/dfm/ugly/blob/bc09834849184552619ee926d7563ed37630accb/ugly/models.py#L40-L50
tfidf_python_100_1.0,aes encryption,python,"def encryptedFileSize(self, originalFileSizeInBytes):
        return Encryption.FILE_KEYADDITION_LENGTH + ((originalFileSizeInBytes // Encryption.BLOCKSIZE_BYTES) + 1) * Encryption.BLOCKSIZE_BYTES",https://github.com/seiferma/deterministic_encryption_utils/blob/a747da3cd6daf39b0c26d4d497725e8863af1dd1/deterministic_encryption_utils/encryption/Encryption.py#L71-L72
tfidf_python_100_1.0,aes encryption,python,"def _extract_coil_coords(self, aes):
        groups_ix = groupby_ix(aes.secondary_id)
        coils_ix = groups_ix[aes.secondary_type[groups_ix[:, 0]] == 'C']
        
        # We remove id = 0 because they are heteroatoms
        coils_id = aes.secondary_id[coils_ix[:, 0]]
        coils_ix = coils_ix[coils_id != 0, :]
        
        coils_ix[:, 1] += 1
        coils_ix[:, 0] -= 1
        coils_ix[coils_ix > len(aes.secondary_type)] = len(aes.secondary_type)
        coils_ix[coils_ix < 0] = 0
        
        backbone_list = [aes.xyz[aes.types == 'CA'][i:j] for i, j in coils_ix]
        return backbone_list",https://github.com/gabrielelanaro/chemview/blob/2c9768dd23db99e59e27adff2a953bb8ee795fa3/chemview/gg.py#L322-L336
tfidf_python_100_1.0,aes encryption,python,"def __netjson_encryption_typecast(self, encryption):
        # type casting
        if 'port' in encryption:
            encryption['port'] = int(encryption['port'])
        if 'acct_port' in encryption:
            encryption['acct_port'] = int(encryption['acct_port'])
        if 'wps_label' in encryption:
            encryption['wps_label'] = encryption['wps_label'] == '1'
        if 'wps_pushbutton' in encryption:
            encryption['wps_pushbutton'] = encryption['wps_pushbutton'] == '1'
        return encryption",https://github.com/openwisp/netjsonconfig/blob/c23ce9732720856e2f6dc54060db71a8182c7d4b/netjsonconfig/backends/openwrt/converters/wireless.py#L232-L242
tfidf_python_100_1.0,aes encryption,python,"def main():
    runtest(""AES - CTR Mode"", AES.CTREnc, AES.CTRDec)
    runtest(""AES - GCM Mode"", AES.GCMEnc, AES.GCMDec)",https://github.com/ghackebeil/PyORAM/blob/b8832c1b753c0b2148ef7a143c5f5dd3bbbb61e7/examples/aesctr_performance.py#L113-L115
tfidf_python_100_1.0,aes encryption,python,"def decode_aes256(cipher, iv, data, encryption_key):
    """"""
    Decrypt AES-256 bytes.
    Allowed ciphers are: :ecb, :cbc.
    If for :ecb iv is not used and should be set to """".
    """"""
    if cipher == 'cbc':
        aes = AES.new(encryption_key, AES.MODE_CBC, iv)
    elif cipher == 'ecb':
        aes = AES.new(encryption_key, AES.MODE_ECB)
    else:
        raise ValueError('Unknown AES mode')
    d = aes.decrypt(data)
    # http://passingcuriosity.com/2009/aes-encryption-in-python-with-m2crypto/
    unpad = lambda s: s[0:-ord(d[-1:])]
    return unpad(d)",https://github.com/konomae/lastpass-python/blob/5063911b789868a1fd9db9922db82cdf156b938a/lastpass/parser.py#L269-L284
tfidf_python_100_1.0,aes encryption,python,"def __init__(self, aes=Aes()):
        self.aes = aes
        self.geometries = []
        self.scales = []",https://github.com/gabrielelanaro/chemview/blob/2c9768dd23db99e59e27adff2a953bb8ee795fa3/chemview/gg.py#L36-L39
tfidf_python_100_1.0,aes encryption,python,"def produce(self, aes=Aes()):
        # If an aes was passed, we override...
        aes = aes.updated(self.aes)

        # Return a dict of primitives produced from aes data
        return [{
                ""rep_id"" : uuid.uuid1().hex,
                'rep_type': ""points"",
                ""options"": { ""coordinates"": aes.xyz,
                             ""colors"": process_colors(len(aes.xyz), aes.get(""colors"", None)),
                             ""sizes"": process_sizes(len(aes.xyz), aes.get(""sizes"", 1)),
                             ""visible"": aes.get(""visible"", None) }
                }]",https://github.com/gabrielelanaro/chemview/blob/2c9768dd23db99e59e27adff2a953bb8ee795fa3/chemview/gg.py#L156-L168
tfidf_python_100_1.0,aes encryption,python,"def _make_frame_aes(aes, frame):
        frame_aes = Aes()
        
        # Make a copy
        for k in aes.keys():
            frame_aes[k] = aes[k]
        
        # Override the traj ones
        for k in aes.keys():
            if k.endswith(""_traj""):
                frame_aes[k[:-5]] = aes[k][frame]

        return frame_aes",https://github.com/gabrielelanaro/chemview/blob/2c9768dd23db99e59e27adff2a953bb8ee795fa3/chemview/gg.py#L116-L128
tfidf_python_100_1.0,aes encryption,python,"def _extract_sheet_coords_normals(self, aes):
        groups_ix = groupby_ix(aes.secondary_id)
        sheets_ix = groups_ix[aes.secondary_type[groups_ix[:, 0]] == 'E']
        
        ca_list = [aes.xyz[aes.types == 'CA'][i:j] for i, j in sheets_ix if j - i] 
        c_list = [aes.xyz[aes.types == 'C'][i:j] for i, j in sheets_ix if j - i] 
        o_list = [aes.xyz[aes.types == 'O'][i:j] for i, j in sheets_ix if j - i] 
        
        normals_list = [beta_sheet_normals(ca, c, o) for ca, c, o in zip(ca_list, c_list, o_list)]
        
        return ca_list, normals_list",https://github.com/gabrielelanaro/chemview/blob/2c9768dd23db99e59e27adff2a953bb8ee795fa3/chemview/gg.py#L310-L320
tfidf_python_100_1.0,aes encryption,python,"def _get_mode(mode = None):
        """"""
        Return the AES mode, or a list of valid AES modes, if mode == None
        """"""
        from Crypto.Cipher import AES

        AESModeMap = {
            'CCM': AES.MODE_CCM,
            'EAX': AES.MODE_EAX,
            'GCM': AES.MODE_GCM,
            'OCB': AES.MODE_OCB,
        }

        if mode is None:
            return AESModeMap.keys()
        return AESModeMap.get(mode)",https://github.com/frispete/keyrings.cryptfile/blob/cfa80d4848a5c3c0aeee41a954b2b120c80e69b2/keyrings/cryptfile/cryptfile.py#L61-L76
tfidf_python_100_1.0,aes encryption,python,"def apply(self, aes):
        aes = aes.copy()
        colors = process_colors(len(aes.xyz), aes.get(""colors"", None), self.limits, self.palette)
        aes.colors = colors
        return aes",https://github.com/gabrielelanaro/chemview/blob/2c9768dd23db99e59e27adff2a953bb8ee795fa3/chemview/gg.py#L398-L402
tfidf_python_100_1.0,aes encryption,python,"def validate_encryption(meta):
    encryption = meta.encryption
    if encryption is None:
        return

    if not isinstance(encryption, collections.abc.MutableMapping):
        raise InvalidModel(""Encryption must be None or a dict."")
    if ""enabled"" not in encryption:
        raise InvalidModel(""Encryption must specify whether it is enabled with the 'enabled' key."")",https://github.com/numberoverzero/bloop/blob/4c95f5a0ff0802443a1c258bfaccecd1758363e7/bloop/models.py#L671-L679
tfidf_python_100_1.0,aes encryption,python,"def decryptedFileSize(self, virtualFile):
        assert(isinstance(virtualFile, VirtualFile))
        absRootPathFileSize = virtualFile.size()
        if absRootPathFileSize % Encryption.BLOCKSIZE_BYTES != 0:
            raise MalformedInputException('The file ' + virtualFile.name() + ' is not properly encrypted.')
        lastBlock = virtualFile.read(absRootPathFileSize - Encryption.BLOCKSIZE_BYTES, Encryption.BLOCKSIZE_BYTES)
        
        keyAddition = self.__getKeyAdditionFromEncryptedVirtualFile(virtualFile)
        cipher = self.__createCipher(keyAddition)
        return absRootPathFileSize - Encryption.FILE_KEYADDITION_LENGTH - Encryption.BLOCKSIZE_BYTES + len(self.__decrypt(cipher, lastBlock, True))",https://github.com/seiferma/deterministic_encryption_utils/blob/a747da3cd6daf39b0c26d4d497725e8863af1dd1/deterministic_encryption_utils/encryption/Encryption.py#L74-L83
tfidf_python_100_1.0,aes encryption,python,"def _extract_helix_coords_normals(self, aes):
        # First, extract the helices from the secondary
        groups_ix = groupby_ix(aes.secondary_id)
        helices_ix = groups_ix[aes.secondary_type[groups_ix[:, 0]] == 'H']
        backbone_list = [aes.xyz[aes.types == 'CA'][i:j] for i, j in helices_ix if j - i] 
        normals_list = [alpha_helix_normals(backbone) for backbone in backbone_list]
        
        return backbone_list, normals_list",https://github.com/gabrielelanaro/chemview/blob/2c9768dd23db99e59e27adff2a953bb8ee795fa3/chemview/gg.py#L301-L308
tfidf_python_100_1.0,aes encryption,python,"def aes_encrypt(key, data, mode='ECB', iv=None):
    aes = AES()
    aes.mode = mode
    aes.iv = iv
    aes.key = key
    return aes.encrypt(data)",https://github.com/boldfield/s3-encryption/blob/d88549ba682745dc6b199934c5b5221de7f8d8bc/s3_encryption/crypto.py#L52-L57
tfidf_python_100_1.0,aes encryption,python,"def aes_decrypt(key, data, mode='ECB', iv=None):
    aes = AES()
    aes.mode = mode
    aes.iv = iv
    aes.key = key
    return aes.decrypt(data)",https://github.com/boldfield/s3-encryption/blob/d88549ba682745dc6b199934c5b5221de7f8d8bc/s3_encryption/crypto.py#L60-L65
tfidf_python_100_1.0,aes encryption,python,"def __init__(self, **kwargs):
        """"""Initialize a new PrettyPrinter class.

        Parameters
        ----------
        color: str or callable
            Layer that corresponds to color aesthetic.
        background: str or callable
            Layer that corresponds to background.
        ...

        color_value: str or list
            The alternative value for the color.
        background_value: str or list
            The background value for the color.
        """"""
        assert_legal_arguments(kwargs)
        self.__aesthetics, self.__values = parse_arguments(kwargs)
        self.__rules = dict((aes, create_rules(aes, self.values[aes])) for aes in self.aesthetics)",https://github.com/estnltk/estnltk/blob/28ae334a68a0673072febc318635f04da0dcc54a/estnltk/prettyprinter/prettyprinter.py#L74-L92
tfidf_python_100_1.0,aes encryption,python,"def get_cipher(self, master_key, encryption_iv):
        return AES.new(master_key, AES.MODE_CBC, encryption_iv)",https://github.com/pschmitt/pykeepass/blob/85da3630d6e410b2a10d3e711cd69308b51d401d/pykeepass/kdbx_parsing/common.py#L286-L287
tfidf_python_100_1.0,aes encryption,python,"def update(self, aes):
        # we return options
        return { ""coordinates"": aes.xyz,
                 ""colors"": process_colors(len(aes.xyz), aes.get(""colors"", None)),
                 ""sizes"": process_sizes(len(aes.xyz), aes.get(""sizes"", None)),
                 ""visible"": aes.get(""visible"", None) }",https://github.com/gabrielelanaro/chemview/blob/2c9768dd23db99e59e27adff2a953bb8ee795fa3/chemview/gg.py#L170-L175
tfidf_python_100_1.0,linear regression,python,"def regression_logprior(regression):
    if isinstance(regression, DiagonalRegression):
        return diag_regression_logprior(regression)
    elif isinstance(regression, Regression):
        return dense_regression_logprior(regression)",https://github.com/mattjj/pyslds/blob/c505c2bd05a5549d450b518f02493b68ed12e590/pyslds/util.py#L313-L317
tfidf_python_100_1.0,linear regression,python,"def expected_regression_log_prob(regression, stats):
    if isinstance(regression, DiagonalRegression):
        return expected_diag_regression_log_prob(
            regression.A, regression.sigmasq_flat, stats)
    elif isinstance(regression, Regression):
        return expected_dense_regression_log_prob(
            regression.A, regression.sigma, stats)
    else:
        raise Exception(""Unrecognized regression object! {}"".format(regression))",https://github.com/mattjj/pyslds/blob/c505c2bd05a5549d450b518f02493b68ed12e590/pyslds/util.py#L88-L96
tfidf_python_100_1.0,linear regression,python,"def save(self, regression, filename):
        self.create_figure()
        self.put_data_in_figure(regression)
        return self.save_figure(regression, filename)",https://github.com/limiear/soyprice/blob/b7f8847b1ab4ba7d9e654135321c5598c7d1aeb1/soyprice/bots/grapher.py#L61-L64
tfidf_python_100_1.0,linear regression,python,"def calculate_avg_error(self, X, Y, W):

        return super(Regression, self).calculate_avg_error(X, Y, W)",https://github.com/fukuball/fuku-ml/blob/0da15ad7af76adf344b5a6b3f3dbabbbab3446b0/FukuML/GradientBoostDecisionTree.py#L110-L112
tfidf_python_100_1.0,linear regression,python,"def put_data_in_figure(self, regression):
        x, y, fix, weights, rmse, next_x, next_y = regression.resume()
        self.draw_title(x, y, next_x, next_y, regression.description)
        self.draw_data(x, y, regression.reference, regression.x_label)
        if not regression.should_show_xticks:
            self.draw_xticks(x)
        self.draw_rmse(x, zip(fix, weights), rmse)
        self.draw_estimated(x, fix, next_x, next_y)",https://github.com/limiear/soyprice/blob/b7f8847b1ab4ba7d9e654135321c5598c7d1aeb1/soyprice/bots/grapher.py#L48-L55
tfidf_python_100_1.0,linear regression,python,"def init_W(ml_learner):
        W = []
        linear = BinaryClassifier()
        linear.status = 'init'
        linear.train_X = ml_learner.train_X
        linear.train_Y = ml_learner.train_Y
        linear.data_num = ml_learner.data_num
        linear.data_demension = ml_learner.data_demension
        linear.W = ml_learner.W
        W = linear.train()
        return W",https://github.com/fukuball/fuku-ml/blob/0da15ad7af76adf344b5a6b3f3dbabbbab3446b0/FukuML/LinearRegression.py#L429-L439
tfidf_python_100_1.0,linear regression,python,"def linear_regression(self):
        """""" Linear Regression.

        This function runs linear regression and stores the, 
        1. Model
        2. Model name 
        3. Mean score of cross validation
        4. Metrics

        """"""

        model = LinearRegression()
        scores = []

        kfold = KFold(n_splits=self.cv, shuffle=True, random_state=42)
        for i, (train, test) in enumerate(kfold.split(self.baseline_in, self.baseline_out)):
            model.fit(self.baseline_in.iloc[train], self.baseline_out.iloc[train])
            scores.append(model.score(self.baseline_in.iloc[test], self.baseline_out.iloc[test]))

        mean_score = sum(scores) / len(scores)
        
        self.models.append(model)
        self.model_names.append('Linear Regression')
        self.max_scores.append(mean_score)

        self.metrics['Linear Regression'] = {}
        self.metrics['Linear Regression']['R2'] = mean_score
        self.metrics['Linear Regression']['Adj R2'] = self.adj_r2(mean_score, self.baseline_in.shape[0], self.baseline_in.shape[1])",https://github.com/SoftwareDefinedBuildings/XBOS/blob/c12d4fb14518ea3ae98c471c28e0710fdf74dd25/apps/Data_quality_analysis/Model_Data.py#L176-L203
tfidf_python_100_1.0,linear regression,python,"def _lreg_bokeh(self, **kwargs):
        """"""
        Returns a Bokeh linear regression line
        """"""
        try:
            ds2 = self._duplicate_()
            ds2.timestamps(ds2.x)
            ds2.lreg(""Timestamps"", ds2.y)
            ds2.drop(ds2.y)
            ds2.df = ds2.df.rename(columns={'Regression': ds2.y})
            if ""date_format"" in self.chart_style:
                ds2.date(""Date"", format=self.chart_style[""date_format""])
            c = ds2.line_()
            return c
        except Exception as e:
            self.err(e, ""Can not draw linear regression chart"")",https://github.com/synw/dataswim/blob/4a4a53f80daa7cd8e8409d76a19ce07296269da2/dataswim/charts/bokeh.py#L90-L105
tfidf_python_100_1.0,linear regression,python,"def diag_regression_logprior(regression):
    from scipy.stats import multivariate_normal, gamma
    A = regression.A
    sigmasq = regression.sigmasq_flat
    J, h, alpha, beta = \
        regression.J_0, regression.h_0, regression.alpha_0, regression.beta_0
    Sigma = np.linalg.inv(J)
    mu = Sigma.dot(h)

    lp = 0
    for d in range(regression.D_out):
        lp += multivariate_normal(mu, Sigma).logpdf(A[d])
        lp += gamma(alpha, scale=1./beta).logpdf(1. / sigmasq[d])
    return lp",https://github.com/mattjj/pyslds/blob/c505c2bd05a5549d450b518f02493b68ed12e590/pyslds/util.py#L320-L333
tfidf_python_100_1.0,linear regression,python,"def __init__(self, n_in, n_latent, n_h):
        super(VAE, self).__init__()
        with self.init_scope():
            # encoder
            self.le1 = L.Linear(n_in, n_h)
            self.le2_mu = L.Linear(n_h, n_latent)
            self.le2_ln_var = L.Linear(n_h, n_latent)
            # decoder
            self.ld1 = L.Linear(n_latent, n_h)
            self.ld2 = L.Linear(n_h, n_in)",https://github.com/lanpa/tensorboardX/blob/0bf6c07d97b0745654fd9fab8ee3261ec707f253/examples/chainer/plain_logger/net.py#L12-L21
tfidf_python_100_1.0,linear regression,python,"def dense_regression_logprior(regression):
    A = regression.A
    Sigmainv = np.linalg.inv(regression.sigma)
    Sigmainv_A = Sigmainv.dot(A)
    AT_Sigmainv_A = A.T.dot(Sigmainv_A)
    logdetSigmainv = np.linalg.slogdet(Sigmainv)[1]

    A, B, C, d = regression.natural_hypparam
    bilinear_term = -1./2 * np.trace(A.dot(Sigmainv)) \
        + np.trace(B.T.dot(Sigmainv_A)) \
        - 1./2 * np.trace(C.dot(AT_Sigmainv_A)) \
        + 1./2 * d * logdetSigmainv

    # log normalizer term
    from pybasicbayes.util.stats import mniw_log_partitionfunction
    Z = mniw_log_partitionfunction(
        *regression._natural_to_standard(regression.natural_hypparam))

    return bilinear_term - Z",https://github.com/mattjj/pyslds/blob/c505c2bd05a5549d450b518f02493b68ed12e590/pyslds/util.py#L336-L354
tfidf_python_100_1.0,linear regression,python,"def tree_predict(x, root, proba=False, regression=False):
    """"""Predicts a probabilities/value/label for the sample x.
    """"""

    if isinstance(root, Leaf):
        if proba:
            return root.probabilities
        elif regression:
            return root.mean
        else:
            return root.most_frequent

    if root.question.match(x):
        return tree_predict(x, root.true_branch, proba=proba, regression=regression)
    else:
        return tree_predict(x, root.false_branch, proba=proba, regression=regression)",https://github.com/VIVelev/PyDojoML/blob/773fdce6866aa6decd306a5a85f94129fed816eb/dojo/tree/utils/functions.py#L174-L189
tfidf_python_100_1.0,linear regression,python,"def __init__(self, polarization_type='linear'):

        assert polarization_type in ['linear', 'stokes'], 'polarization must be linear or stokes'

        self._polarization_type = polarization_type


        Node.__init__(self, 'polarization')",https://github.com/threeML/astromodels/blob/9aac365a372f77603039533df9a6b694c1e360d5/astromodels/core/polarization.py#L9-L16
tfidf_python_100_1.0,linear regression,python,"def lreg(self, xcol, ycol, name=""Regression""):
        """"""
        Add a column to the main dataframe populted with
        the model's linear regression for a column
        """"""
        try:
            x = self.df[xcol].values.reshape(-1, 1)
            y = self.df[ycol]
            lm = linear_model.LinearRegression()
            lm.fit(x, y)
            predictions = lm.predict(x)
            self.df[name] = predictions
        except Exception as e:
            self.err(e, ""Can not calculate linear regression"")",https://github.com/synw/dataswim/blob/4a4a53f80daa7cd8e8409d76a19ce07296269da2/dataswim/data/stats.py#L8-L21
tfidf_python_100_1.0,linear regression,python,"def regression_map_estimation(stats, regression):
    D_out = regression.D_out

    # Add prior and likelihood statistics
    if isinstance(regression, DiagonalRegression):
        regression.max_likelihood(data=None, stats=stats)
    else:
        sum_tuples = lambda lst: list(map(sum, zip(*lst)))
        yyT, yxT, xxT, n = sum_tuples([stats, regression.natural_hypparam])

        A = np.linalg.solve(xxT, yxT.T).T
        sigma = (yyT - A.dot(yxT.T)) / n

        # Make sure sigma is symmetric
        symmetrize = lambda A: (A + A.T) / 2.
        sigma = 1e-10 * np.eye(D_out) + symmetrize(sigma)

        regression.A = A
        regression.sigma = sigma",https://github.com/mattjj/pyslds/blob/c505c2bd05a5549d450b518f02493b68ed12e590/pyslds/util.py#L269-L287
tfidf_python_100_1.0,linear regression,python,"def empirical_SVD(stream_list, linear=True):
    """"""
    Depreciated. Use empirical_svd.
    """"""
    warnings.warn('Depreciated, use empirical_svd instead.')
    return empirical_svd(stream_list=stream_list, linear=linear)",https://github.com/eqcorrscan/EQcorrscan/blob/3121b4aca801ee5d38f56ca297ce1c0f9515d9ff/eqcorrscan/utils/clustering.py#L416-L421
tfidf_python_100_1.0,linear regression,python,"def __str__(self):
        """"""Printable representation of a CrLinear instance.""""""

        output = ""<CrLinear instance>\n"" + \
                 ""crpix linear: "" + str(self.crpix) + ""\n"" + \
                 ""crval linear: "" + str(self.crval) + ""\n"" + \
                 ""cdelt linear: "" + str(self.cdelt) + ""\n"" + \
                 ""crmin linear: "" + str(self.crmin) + ""\n"" + \
                 ""crmax linear: "" + str(self.crmax)

        return output",https://github.com/guaix-ucm/numina/blob/6c829495df8937f77c2de9383c1038ffb3e713e3/numina/array/wavecalib/solutionarc.py#L59-L69
tfidf_python_100_1.0,linear regression,python,"def _linreg_model(fitmodel):
    output = ""Linear regression model\n\n""
    for k, v in result_iterator(fitmodel):
        if k == ""thetas"":
            output += ""Thetas\n""
            output += "", "".join(map(str, v)) + ""\n\n""
    return output",https://github.com/romanorac/discomll/blob/a4703daffb2ba3c9f614bc3dbe45ae55884aea00/discomll/utils/model_view.py#L87-L93
tfidf_python_100_1.0,linear regression,python,"def logx_linear(x, a, b):
    """"""logx linear

    Parameters
    ----------
    x: int
    a: float
    b: float

    Returns
    -------
    float
        a * np.log(x) + b
    """"""
    x = np.log(x)
    return a*x + b",https://github.com/Microsoft/nni/blob/c7cc8db32da8d2ec77a382a55089f4e17247ce41/src/sdk/pynni/nni/curvefitting_assessor/curvefunctions.py#L89-L104
tfidf_python_100_1.0,linear regression,python,"def time_regression(self, _, k, s):
        self.G.regression(self.y, self.mask, smoothness_penalty=s, kernel=k)",https://github.com/all-umass/graphs/blob/4fbeb025dfe33340335f34300f58dd3809228822/benchmarks/benchmarks/mixins.py#L55-L56
tfidf_python_100_1.0,socket recv timeout,python,"def recv(self):
        try:
            msg_bytes = self.c.recv()
        except socket.timeout:
            # print(""socket recv timeout"")
            msg_bytes = b""""
        return self.prot.input(msg_bytes)",https://github.com/AirtestProject/Poco/blob/2c559a586adf3fd11ee81cabc446d4d3f6f2d119/poco/utils/simplerpc/transport/tcp/main.py#L34-L40
tfidf_python_100_1.0,socket recv timeout,python,"def recv(self):
		try:
			d = self.s.recv(BUFSIZ)
		except socket.timeout:
			return """"
		except socket.error as e:
			logger.warn(""Socket error: %s"", e)
			raise Disconnect()

		if not d: raise Disconnect()
		return d",https://github.com/hpfeeds/hpfeeds/blob/f18712dd62255654c8f8d695761cce384fa3592f/lib/hpfeeds.py#L83-L93
tfidf_python_100_1.0,socket recv timeout,python,"def recv(self, timeout=None):
        """"""Overwrite standard recv for timeout calls to catch interrupt errors.
        """"""
        if timeout:
            try:
                testsock = self._zmq.select([self.socket], [], [], timeout)[0]
            except zmq.ZMQError as e:
                if e.errno == errno.EINTR:
                    testsock = None
                else:
                    raise
            if not testsock:
                return
            rv = self.socket.recv(self._zmq.NOBLOCK)
            return LogRecord.from_dict(json.loads(rv))
        else:
            return super(ZeroMQPullSubscriber, self).recv(timeout)",https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/log/logbook_zmqpush.py#L98-L114
tfidf_python_100_1.0,socket recv timeout,python,"def recv(self, timeout=None, *args, **kwargs):
        with self._lock_recv:
            return self.__wrapped__.recv(timeout=timeout, *args, **kwargs)",https://github.com/hardbyte/python-can/blob/cdc5254d96072df7739263623f3e920628a7d214/can/thread_safe_bus.py#L69-L71
tfidf_python_100_1.0,socket recv timeout,python,"def recv(self, buflen=1024, flags=0):
        if self._sslobj:
            if flags != 0:
                raise ValueError(""non-zero flags not allowed in calls to recv() on %s"" % self.__class__)
            # Shouldn't we wrap the SSL_WANT_READ errors as socket.timeout errors to match socket.recv's behavior?
            return self.read(buflen)
        else:
            return socket.recv(self, buflen, flags)",https://github.com/saghul/evergreen/blob/22f22f45892f397c23c3e09e6ea1ad4c00b3add8/evergreen/lib/ssl.py#L194-L201
tfidf_python_100_1.0,socket recv timeout,python,"def recv(self, timeout=None):
        try:
            return super(IOSafeMultiProcessingSubscriber, self).recv(timeout)
        except IOError as e:
            if ""Interrupted system call"" in str(e):
                return None
            else:
                raise",https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/log/__init__.py#L45-L52
tfidf_python_100_1.0,socket recv timeout,python,"def recv_response(socket, acceptable_length, timeout):
    if socket.poll(""recv"", timeout):
        snep_response = socket.recv()

        if len(snep_response) < 6:
            log.debug(""snep response initial fragment too short"")
            return None

        version, status, length = struct.unpack("">BBL"", snep_response[:6])

        if length > acceptable_length:
            log.debug(""snep response exceeds acceptable length"")
            return None

        if len(snep_response) - 6 < length:
            # request remaining fragments
            socket.send(b""\x10\x00\x00\x00\x00\x00"")
            while len(snep_response) - 6 < length:
                if socket.poll(""recv"", timeout):
                    snep_response += socket.recv()
                else:
                    return None

        return bytearray(snep_response)",https://github.com/nfcpy/nfcpy/blob/6649146d1afdd5e82b2b6b1ea00aa58d50785117/src/nfc/snep/client.py#L51-L74
tfidf_python_100_1.0,socket recv timeout,python,"def _read_response(self):
        try:
            raw_size = self.socket.recv(4)
            size = struct.unpack("">I"", raw_size)[0]
            message = bytearray(0)

            while len(message) < size:
                recv = self.socket.recv(size)
                message += recv

            return message
        except OSError as e:
            logger.debug(""CoreAgentSocket error on read response: %r"", e)
            return None",https://github.com/scoutapp/scout_apm_python/blob/e5539ee23b8129be9b75d5007c88b6158b51294f/src/scout_apm/core/socket.py#L183-L196
tfidf_python_100_1.0,socket recv timeout,python,"def _check_no_loop(self, recv):
        if recv is self:
            raise Error(self.loop_msg)

        for recv_ in self._receivers:
            if recv_ == recv:
                raise Error(self.loop_msg)
            if isinstance(recv_, Select):
                recv_._check_no_loop(recv)",https://github.com/dw/mitogen/blob/a7fdb55e1300a7e0a5e404b09eb730cf9a525da7/mitogen/select.py#L200-L208
tfidf_python_100_1.0,socket recv timeout,python,"def socket(self, socket_type):
        if socket_type == RAW_ACCESS_POINT:
            return tco.RawAccessPoint(recv_miu=self.cfg[""recv-miu""])
        if socket_type == LOGICAL_DATA_LINK:
            return tco.LogicalDataLink(recv_miu=self.cfg[""recv-miu""])
        if socket_type == DATA_LINK_CONNECTION:
            return tco.DataLinkConnection(recv_miu=128, recv_win=1)",https://github.com/nfcpy/nfcpy/blob/6649146d1afdd5e82b2b6b1ea00aa58d50785117/src/nfc/llcp/llc.py#L690-L696
tfidf_python_100_1.0,socket recv timeout,python,"def _recv_into(self, length):
        recv = bytearray(length)
        view = memoryview(recv)
        self.socket.recv_into(view)
        return recv",https://github.com/Swind/pure-python-adb/blob/8e076bc2b25ad33b6c5eb14293329a017d83455f/adb/connection.py#L58-L62
tfidf_python_100_1.0,socket recv timeout,python,"def recvfrom(self, socket):
        if not isinstance(socket, tco.TransmissionControlObject):
            raise err.Error(errno.ENOTSOCK)
        if not (socket.addr and self.sap[socket.addr]):
            raise err.Error(errno.EBADF)
        if isinstance(socket, tco.RawAccessPoint):
            return (socket.recv(), None)
        if isinstance(socket, tco.LogicalDataLink):
            return socket.recvfrom()
        if isinstance(socket, tco.DataLinkConnection):
            return (socket.recv(), socket.peer)",https://github.com/nfcpy/nfcpy/blob/6649146d1afdd5e82b2b6b1ea00aa58d50785117/src/nfc/llcp/llc.py#L845-L855
tfidf_python_100_1.0,socket recv timeout,python,"def create_fake_connection(address, timeout=socket._GLOBAL_DEFAULT_TIMEOUT, source_address=None):
    s = fakesock.socket(socket.AF_INET, socket.SOCK_STREAM, socket.IPPROTO_TCP)
    if timeout is not socket._GLOBAL_DEFAULT_TIMEOUT:
        s.settimeout(timeout)
    if source_address:
        s.bind(source_address)
    s.connect(address)
    return s",https://github.com/spulec/moto/blob/4a286c4bc288933bb023396e2784a6fdbb966bc9/moto/packages/httpretty/core.py#L492-L499
tfidf_python_100_1.0,socket recv timeout,python,"def _recv(self, socket):
        msg_id = []
        while True:
            recv = socket.recv()
            if recv == '__break__':
                raise RoutineStop()
            if socket.getsockopt(zmq.RCVMORE):
                msg_id.append(recv)
            else:
                msg = recv
                for extractor in self._message_extractors:
                    msg = extractor(msg)
                return msg_id, msg",https://github.com/pmdz/smite/blob/f967f644f8a51a6962ce5e1d4305daf7450d4585/smite/servant.py#L292-L304
tfidf_python_100_1.0,socket recv timeout,python,"def receive(self):
        raw_length = self.socket.recv(self.HEADER_LENGTH)
        length, = struct.unpack(""!I"", raw_length)
        payload = self.socket.recv(length)
        return payload",https://github.com/sbjorn/vici/blob/147135905b68892734b09ec8a569c71733648090/vici/protocol.py#L22-L26
tfidf_python_100_1.0,socket recv timeout,python,"def create_connection(
    address, timeout=socket._GLOBAL_DEFAULT_TIMEOUT, source_address=None
):
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM, socket.IPPROTO_TCP)
    if timeout is not socket._GLOBAL_DEFAULT_TIMEOUT:
        s.settimeout(timeout)
    # if source_address:
    #     s.bind(source_address)
    s.connect(address)
    return s",https://github.com/mindflayer/python-mocket/blob/d0ad06e614f6ce2d8cf88a29a16afb5b828d1a0f/mocket/mocket.py#L101-L110
tfidf_python_100_1.0,socket recv timeout,python,"def read_iterator(self, timeout=3):
        timeout_time = time.time() + timeout
        while time.time() < timeout_time:
            yield self.sock.recv(MAX_PACKET_SIZE)

        raise socket.timeout(""Read timeout"")",https://github.com/bacher09/xrcon/blob/6a883f780265cbca31af7a379dc7cb28fdd8b73f/xrcon/client.py#L68-L73
tfidf_python_100_1.0,socket recv timeout,python,"def recv(self, n=4096, timeout='default'):
        """"""
        Receive at most n bytes (default 4096) from the socket

        Aliases: read, get
        """"""

        self._print_recv_header(
            '======== Receiving {0}B{timeout_text} ========', timeout, n)

        return self._recv_predicate(lambda s: min(n, len(s)), timeout)",https://github.com/rhelmot/nclib/blob/6147779766557ee4fafcbae683bdd2f74157e825/nclib/netcat.py#L640-L650
tfidf_python_100_1.0,socket recv timeout,python,"def recv(self, amt, flags=0):
        if select.select([self.sock], [], [], self.timeout)[0]:
            return self.sock.recv(amt, flags)
        raise TimeoutError('socket recv() timeout.')",https://github.com/rameshg87/pyremotevbox/blob/123dffff27da57c8faa3ac1dd4c68b1cf4558b1a/pyremotevbox/ZSI/wstools/TimeoutSocket.py#L90-L93
tfidf_python_100_1.0,socket recv timeout,python,"def send_message(self, message, expect_reply=True):
        self.socket.send(message)

        if expect_reply:
            return self.socket.recv()",https://github.com/mbr/tinyrpc/blob/59ccf62452b3f37e8411ff0309a3a99857d05e19/tinyrpc/transports/zmq.py#L55-L59
tfidf_python_100_1.0,write csv,python,"def append_csv(self, csv):
        csv = CSV(csv, self)
#        if self.get_csv(csv.id):
#            raise Exception('csv already exists: {0}'.format(csv.id))
#        csv.get_import_table()
        self['csvs'].append(csv)
        return self",https://github.com/adamjaso/pyauto/blob/b11da69fb21a49241f5ad75dac48d9d369c6279b/csvdb/pyauto/csvdb/config.py#L31-L37
tfidf_python_100_1.0,write csv,python,"def get_csv(self, csv):
        for c in self.csvs:
            if csv == c.get_id():
                return c
        raise Exception('unknown csv: {0}'.format(csv))",https://github.com/adamjaso/pyauto/blob/b11da69fb21a49241f5ad75dac48d9d369c6279b/csvdb/pyauto/csvdb/config.py#L51-L55
tfidf_python_100_1.0,write csv,python,"def get_sla_csv(self):
    csv = os.path.join(self.resource_directory, self.label + '.sla.csv')
    return csv",https://github.com/linkedin/naarad/blob/261e2c0760fd6a6b0ee59064180bd8e3674311fe/src/naarad/metrics/metric.py#L174-L176
tfidf_python_100_1.0,write csv,python,"def get_important_sub_metrics_csv(self):
    csv = os.path.join(self.resource_directory, self.label + '.important_sub_metrics.csv')
    return csv",https://github.com/linkedin/naarad/blob/261e2c0760fd6a6b0ee59064180bd8e3674311fe/src/naarad/metrics/metric.py#L162-L164
tfidf_python_100_1.0,write csv,python,"def get_stats_csv(self):
    csv = os.path.join(self.resource_directory, self.label + '.stats.csv')
    return csv",https://github.com/linkedin/naarad/blob/261e2c0760fd6a6b0ee59064180bd8e3674311fe/src/naarad/metrics/metric.py#L166-L168
tfidf_python_100_1.0,write csv,python,"def __init__(self, fldr):
        """"""
        loads all the ref_*.csv files relating 
        to character traits
        """"""
        self.ref_folder = fldr
        #print('loading data files')
        self.races = RefFile(fldr, 'ref_races.csv')
        self.classes = RefFile(fldr, 'ref_classes.csv')
        self.stats = RefFile(fldr, 'ref_stats.csv')
        self.skills = RefFile(fldr, 'ref_skills.csv')
        self.stories = RefFile(fldr, 'ref_stories.csv')
        self.inventory = RefFile(fldr, 'ref_objects.csv')",https://github.com/acutesoftware/virtual-AI-simulator/blob/57de679a5b1a58c38fefe6aea58af1f3a7e79c58/vais/character.py#L70-L82
tfidf_python_100_1.0,write csv,python,"def test_csv():
    csv_filename = get_abs_filename_with_sub_path('csv', 'test_csv.csv')[1]
    print(csv_filename)
    csv_list = csv_file_to_list(csv_filename)
    print(csv_list)",https://github.com/chinapnr/fishbase/blob/23c5147a6bc0d8ed36409e55352ffb2c5b0edc82/demo/demo_csv.py#L7-L11
tfidf_python_100_1.0,write csv,python,"def csv(
            self,
            dirPath=None):
        """"""*Render the results in csv format*

        **Key Arguments:**
            - ``dirPath`` -- the path to the directory to save the rendered results to. Default *None*

        **Return:**
            - `csvSources` -- the top-level transient data
            - `csvPhot` -- all photometry associated with the transients
            - `csvSpec` -- all spectral data associated with the transients
            - `csvFiles`  -- all files associated with the matched transients found on the tns

        **Usage:**

            To render the results in csv format:

            .. code-block:: python

                csvSources, csvPhot, csvSpec, csvFiles  = tns.csv()
                print csvSources

            .. code-block:: text

                TNSId,TNSName,discoveryName,discSurvey,raSex,decSex,raDeg,decDeg,transRedshift,specType,discMag,discMagFilter,discDate,objectUrl,hostName,hostRedshift,separationArcsec,separationNorthArcsec,separationEastArcsec
                2016asf,SN2016asf,ASASSN-16cs,ASAS-SN,06:50:36.73,+31:06:45.36,102.6530,31.1126,0.021,SN Ia,17.1,V-Johnson,2016-03-06 08:09:36,http://wis-tns.weizmann.ac.il/object/2016asf,KUG 0647+311,,0.66,0.65,-0.13

            You can save the results to file by passing in a directory path within which to save the files to. The four flavours of data (sources, photometry, spectra and files) are saved to separate files but all data can be assoicated with its transient source using the transient's unique `TNSId`.

            .. code-block:: python

                tns.csv(""~/tns"")

            .. image:: https://i.imgur.com/BwwqMBg.png
                :width: 800px
                :alt: csv output
        """"""

        if dirPath:
            p = self._file_prefix()
            csvSources = self.sourceResults.csv(
                filepath=dirPath + ""/"" + p + ""sources.csv"")
            csvPhot = self.photResults.csv(
                filepath=dirPath + ""/"" + p + ""phot.csv"")
            csvSpec = self.specResults.csv(
                filepath=dirPath + ""/"" + p + ""spec.csv"")
            csvFiles = self.relatedFilesResults.csv(
                filepath=dirPath + ""/"" + p + ""relatedFiles.csv"")
        else:
            csvSources = self.sourceResults.csv()
            csvPhot = self.photResults.csv()
            csvSpec = self.specResults.csv()
            csvFiles = self.relatedFilesResults.csv()
        return csvSources, csvPhot, csvSpec, csvFiles",https://github.com/thespacedoctor/transientNamer/blob/39be410c84275ed4669632f5df67e728d66a318f/transientNamer/search.py#L226-L280
tfidf_python_100_1.0,write csv,python,"def CreateAllStaticRAPIDFiles(in_drainage_line,
                              river_id,
                              length_id,
                              slope_id,
                              next_down_id,
                              rapid_output_folder,
                              kfac_celerity=1000.0/3600.0,
                              kfac_formula_type=3,
                              kfac_length_units=""km"",
                              lambda_k=0.35,
                              x_value=0.3,
                              nhdplus=False,
                              taudem_network_connectivity_tree_file=None,
                              file_geodatabase=None):
    """"""
    To generate the static RAPID files (rapid_connect.csv, riv_bas_id.csv,
    kfac.csv, k.csv, x.csv, comid_lat_lon_z.csv) with default values.

    Parameters
    ----------
    in_drainage_line: str
        Path to the stream network (i.e. Drainage Line) shapefile.
    river_id: str
        The name of the field with the river ID
        (Ex. 'HydroID', 'COMID', or 'LINKNO').
    length_id: str
        The field name containging the length of the river segment
        (Ex. 'LENGTHKM' or 'Length').
    slope_id: str
        The field name containging the slope of the river segment
        (Ex. 'Avg_Slope' or 'Slope').
    next_down_id: str
        The name of the field with the river ID of the next downstream river
        segment (Ex. 'NextDownID' or 'DSLINKNO').
    rapid_output_folder: str
        The path to the folder where all of the RAPID output will be generated.
    kfac_celerity: float, optional
        The flow wave celerity for the watershed in meters per second.
        1 km/hr or 1000.0/3600.0 m/s is a reasonable value if unknown.
    kfac_formula_type: int, optional
        An integer representing the formula type to use when calculating kfac.
        Default is 3.
    kfac_length_units: str, optional
        The units for the length_id field. Supported types are ""m"" for meters
        and ""km"" for kilometers. Default is ""km"".
    lambda_k: float, optional
        The value for lambda given from RAPID after the calibration process.
        Default is 0.35.
    x_value: float, optional
        Value for the muskingum X parameter [0-0.5]. Default is 0.3.
    nhdplus: bool, optional
        If True, the drainage line is from the NHDPlus dataset with the VAA
        fields COMID, FROMNODE, TONODE, and DIVERGENCE. Default is False.
    taudem_network_connectivity_tree_file: str, optional
        If set, the connectivity file will be generated from the TauDEM
        connectivity tree file.
    file_geodatabase: str, optional
        Path to the file geodatabase. If you use this option,
        in_drainage_line is the name of the stream network feature class.
        (WARNING: Not always stable with GDAL.)


    Example::

        from RAPIDpy.gis.workflow import CreateAllStaticRAPIDFiles

        CreateAllStaticRAPIDFiles(
            in_drainage_line=""/path/to/drainage_line.shp"",
            river_id=""HydroID"",
            length_id=""LENGTHKM"",
            slope_id=""SLOPE"",
            next_down_river_id=""NextDownID"",
            rapid_output_folder=""/path/to/rapid/output"",
        )
    """"""
    # RAPID connect file
    rapid_connect_file = os.path.join(rapid_output_folder, 'rapid_connect.csv')
    if nhdplus:
        CreateNetworkConnectivityNHDPlus(in_drainage_line,
                                         rapid_connect_file,
                                         file_geodatabase)
    elif taudem_network_connectivity_tree_file:
        CreateNetworkConnectivityTauDEMTree(
            taudem_network_connectivity_tree_file,
            rapid_connect_file)
    else:
        CreateNetworkConnectivity(in_drainage_line,
                                  river_id,
                                  next_down_id,
                                  rapid_connect_file,
                                  file_geodatabase)

    # river basin id file
    riv_bas_id_file = os.path.join(rapid_output_folder, 'riv_bas_id.csv')
    CreateSubsetFile(in_drainage_line,
                     river_id,
                     riv_bas_id_file,
                     file_geodatabase)
    # kfac file
    kfac_file = os.path.join(rapid_output_folder, 'kfac.csv')
    CreateMuskingumKfacFile(in_drainage_line,
                            river_id,
                            length_id,
                            slope_id,
                            kfac_celerity,
                            kfac_formula_type,
                            rapid_connect_file,
                            kfac_file,
                            length_units=kfac_length_units,
                            file_geodatabase=file_geodatabase)
    # k file
    k_file = os.path.join(rapid_output_folder, 'k.csv')
    CreateMuskingumKFile(lambda_k,
                         kfac_file,
                         k_file)
    # x file
    x_file = os.path.join(rapid_output_folder, 'x.csv')
    CreateConstMuskingumXFile(x_value,
                              rapid_connect_file,
                              x_file)
    # comid lat lon z file
    comid_lat_lon_z_file = \
        os.path.join(rapid_output_folder, 'comid_lat_lon_z.csv')
    FlowlineToPoint(in_drainage_line,
                    river_id,
                    comid_lat_lon_z_file,
                    file_geodatabase)",https://github.com/erdc/RAPIDpy/blob/50e14e130554b254a00ff23b226cd7e4c6cfe91a/RAPIDpy/gis/workflow.py#L22-L148
tfidf_python_100_1.0,write csv,python,"def __init__(self, csv):
        InstrumentCSVResultsFileParser.__init__(self, csv)
        self._currentresultsheader = []
        self._currentanalysiskw = ''
        self._numline = 0",https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/exportimport/instruments/shimadzu/gcms/tq8030.py#L93-L97
tfidf_python_100_1.0,write csv,python,"def import_csv(config, csv_id):
    csv = config.csvdb.get_csv(csv_id)
    with csv.get_import_table() as it:
        it.import_table()
    return csv.database.filename",https://github.com/adamjaso/pyauto/blob/b11da69fb21a49241f5ad75dac48d9d369c6279b/csvdb/pyauto/csvdb/commands.py#L5-L9
tfidf_python_100_1.0,write csv,python,"def __openAndInitCSVFile(self, modelInfo):
    """"""
    - Backs up old report csv file;
    - opens the report csv file in append or overwrite mode (per
      self.__replaceReport);
    - emits column fields;
    - sets up self.__sortedVariableNames, self.__csvFileObj,
      self.__backupCSVPath, and self.__reportCSVPath

    Parameters:
    ----------------------------------------------------------------------
    modelInfo:      First _NupicModelInfo instance passed to emit()
    retval:         nothing
    """"""
    # Get the base path and figure out the path of the report file.
    basePath = self.__outputDirAbsPath

    # Form the name of the output csv file that will contain all the results
    reportCSVName = ""%s_Report.csv"" % (self.__outputLabel,)
    reportCSVPath = self.__reportCSVPath = os.path.join(basePath, reportCSVName)

    # If a report CSV file already exists, back it up
    backupCSVPath = None
    if os.path.exists(reportCSVPath):
      backupCSVPath = self.__backupCSVPath = _backupFile(reportCSVPath)


    # Open report file
    if self.__replaceReport:
      mode = ""w""
    else:
      mode = ""a""
    csv = self.__csvFileObj = open(reportCSVPath, mode)

    # If we are appending, add some blank line separators
    if not self.__replaceReport and backupCSVPath:
      print >> csv
      print >> csv

    # Print the column names
    print >> csv, ""jobID, "",
    print >> csv, ""modelID, "",
    print >> csv, ""status, "" ,
    print >> csv, ""completionReason, "",
    print >> csv, ""startTime, "",
    print >> csv, ""endTime, "",
    print >> csv, ""runtime(s), "" ,
    print >> csv, ""expDesc, "",
    print >> csv, ""numRecords, "",

    for key in self.__sortedVariableNames:
      print >> csv, ""%s, "" % key,
    for key in self.__sortedMetricsKeys:
      print >> csv, ""%s, "" % key,
    print >> csv",https://github.com/numenta/nupic/blob/5922fafffdccc8812e72b3324965ad2f7d4bbdad/src/nupic/swarming/permutations_runner.py#L1293-L1347
tfidf_python_100_1.0,write csv,python,"def __init__(self, csv):
        InstrumentCSVResultsFileParser.__init__(self, csv)
        self._end_header = False
        self._resultsheader = []
        self._numline = 0",https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/exportimport/instruments/lachat/quickchem.py#L49-L53
tfidf_python_100_1.0,write csv,python,"def __init__(self, csv):
        InstrumentCSVResultsFileParser.__init__(self, csv)
        self._end_header = False
        self._quantitationresultsheader = []
        self._numline = 0",https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/exportimport/instruments/shimadzu/gcms/qp2010se.py#L130-L134
tfidf_python_100_1.0,write csv,python,"def emit(self, modelInfo):
    """"""Emit model info to csv file

    Parameters:
    ----------------------------------------------------------------------
    modelInfo:      _NupicModelInfo instance
    retval:         nothing
    """"""
    # Open/init csv file, if needed
    if self.__csvFileObj is None:
      # sets up self.__sortedVariableNames and self.__csvFileObj
      self.__openAndInitCSVFile(modelInfo)

    csv = self.__csvFileObj

    # Emit model info row to report.csv
    print >> csv, ""%s, "" % (self.__searchJobID),
    print >> csv, ""%s, "" % (modelInfo.getModelID()),
    print >> csv, ""%s, "" % (modelInfo.statusAsString()),
    if modelInfo.isFinished():
      print >> csv, ""%s, "" % (modelInfo.getCompletionReason()),
    else:
      print >> csv, ""NA, "",
    if not modelInfo.isWaitingToStart():
      print >> csv, ""%s, "" % (modelInfo.getStartTime()),
    else:
      print >> csv, ""NA, "",
    if modelInfo.isFinished():
      dateFormat = ""%Y-%m-%d %H:%M:%S""
      startTime = modelInfo.getStartTime()
      endTime = modelInfo.getEndTime()
      print >> csv, ""%s, "" % endTime,
      st = datetime.strptime(startTime, dateFormat)
      et = datetime.strptime(endTime, dateFormat)
      print >> csv, ""%s, "" % (str((et - st).seconds)),
    else:
      print >> csv, ""NA, "",
      print >> csv, ""NA, "",
    print >> csv, ""%s, "" % str(modelInfo.getModelDescription()),
    print >> csv, ""%s, "" % str(modelInfo.getNumRecords()),
    paramLabelsDict = modelInfo.getParamLabels()
    for key in self.__sortedVariableNames:
      # Some values are complex structures,.. which need to be represented as
      # strings
      if key in paramLabelsDict:
        print >> csv, ""%s, "" % (paramLabelsDict[key]),
      else:
        print >> csv, ""None, "",
    metrics = modelInfo.getReportMetrics()
    for key in self.__sortedMetricsKeys:
      value = metrics.get(key, ""NA"")
      value = str(value)
      value = value.replace(""\n"", "" "")
      print >> csv, ""%s, "" % (value),

    print >> csv",https://github.com/numenta/nupic/blob/5922fafffdccc8812e72b3324965ad2f7d4bbdad/src/nupic/swarming/permutations_runner.py#L1212-L1267
tfidf_python_100_1.0,write csv,python,"def __init__(self, csv, analysiskey):
        InstrumentCSVResultsFileParser.__init__(self, csv)
        self.analysiskey = analysiskey
        self.header = None",https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/exportimport/instruments/lifetechnologies/qubit/__init__.py#L32-L35
tfidf_python_100_1.0,write csv,python,"def buildcsv():
    csv = docroot / 'generated_build_test.csv'
    support.build2csv(
        [Path('tests/min.pde')],
        csv,
        logdir=docroot / 'log',
        logger=None,
    )",https://github.com/ponty/pyavrutils/blob/7a396a25b3ac076ede07b5cd5cbd416ebb578a28/cogtask.py#L29-L36
tfidf_python_100_1.0,write csv,python,"def convert(csv, json, **kwargs):
    '''Convert csv to json.

    csv:  filename or file-like object
    json: filename  or file-like object


    if csv is '-' or None:
        stdin is used for input
    if json is '-' or None:
        stdout is used for output
    '''

    csv_local, json_local = None, None
    try:
        if csv == '-' or csv is None:
            csv = sys.stdin
        elif isinstance(csv, str):
            csv = csv_local = open(csv, 'r')

        if json == '-' or json is None:
            json = sys.stdout
        elif isinstance(json, str):
            json = json_local = open(json, 'w')

        data = load_csv(csv, **kwargs)
        save_json(data, json, **kwargs)
    finally:
        if csv_local is not None:
            csv_local.close()
        if json_local is not None:
            json_local.close()",https://github.com/oplatek/csv2json/blob/f2f95db71ba2ce683fd6d0d3e2f13c9d0a77ceb6/csv2json/__init__.py#L20-L51
tfidf_python_100_1.0,write csv,python,"def token(cls: Type[CSVType], time: int) -> CSVType:
        """"""
        Return CSV instance from time

        :param time: Timestamp
        :return:
        """"""
        csv = cls()
        csv.time = str(time)
        return csv",https://github.com/duniter/duniter-python-api/blob/3a1e5d61a2f72f5afaf29d010c6cf4dff3648165/duniterpy/grammars/output.py#L98-L107
tfidf_python_100_1.0,write csv,python,"def csv(self, output):
        """"""Output data as excel-compatible CSV""""""
        import csv
        csvwriter = csv.writer(self.outfile)
        csvwriter.writerows(output)",https://github.com/juju/charm-helpers/blob/aa785c40c3b7a8c69dbfbc7921d6b9f30142e171/charmhelpers/cli/__init__.py#L75-L79
tfidf_python_100_1.0,convert decimal to hex,python,"def dec2str(n):
    """"""
    decimal number to string.
    """"""
    s = hex(int(n))[2:].rstrip('L')
    if len(s) % 2 != 0:
        s = '0' + s
    return hex2str(s)",https://github.com/restran/mountains/blob/a97fee568b112f4e10d878f815d0db3dd0a98d74/mountains/encoding/converter.py#L163-L170
tfidf_python_100_1.0,convert decimal to hex,python,"def mbps2bps(mbps):
    _mbps = decimal.Decimal(mbps or 0)
    _kbps = _mbps * decimal.Decimal(1024*1024)
    return int(_kbps.to_integral_value())",https://github.com/talkincode/toughlib/blob/1c2f7dde3a7f101248f1b5f5d428cc85466995cf/toughlib/utils.py#L105-L108
tfidf_python_100_1.0,convert decimal to hex,python,"def yuan2fen(yuan=0):
    y = decimal.Decimal(yuan or 0)
    f = y * decimal.Decimal(100)
    return int(f.to_integral_value())",https://github.com/talkincode/toughlib/blob/1c2f7dde3a7f101248f1b5f5d428cc85466995cf/toughlib/utils.py#L156-L159
tfidf_python_100_1.0,convert decimal to hex,python,"def hour2sec(hor=0):
    _hor = decimal.Decimal(hor or 0)
    _sec = _hor * decimal.Decimal(3600)
    return int(_sec.to_integral_value())",https://github.com/talkincode/toughlib/blob/1c2f7dde3a7f101248f1b5f5d428cc85466995cf/toughlib/utils.py#L141-L144
tfidf_python_100_1.0,convert decimal to hex,python,"def _field_decimal(self):
        d = self._field_short_short_uint()
        n = self._field_long_int()
        return Decimal(n) / Decimal(10 ** d)",https://github.com/agoragames/haigha/blob/7b004e1c0316ec14b94fec1c54554654c38b1a25/haigha/reader.py#L335-L338
tfidf_python_100_1.0,convert decimal to hex,python,"def fen2yuan(fen=0):
    f = decimal.Decimal(fen or 0)
    y = f / decimal.Decimal(100)
    return str(y.quantize(decimal.Decimal('1.00')))",https://github.com/talkincode/toughlib/blob/1c2f7dde3a7f101248f1b5f5d428cc85466995cf/toughlib/utils.py#L151-L154
tfidf_python_100_1.0,convert decimal to hex,python,"def compute_unsigned_fixed_bounds(
    num_bits: int,
    frac_places: int,
) -> Tuple[decimal.Decimal, decimal.Decimal]:
    int_upper = compute_unsigned_integer_bounds(num_bits)[1]

    with decimal.localcontext(abi_decimal_context):
        upper = decimal.Decimal(int_upper) * TEN ** -frac_places

    return ZERO, upper",https://github.com/ethereum/eth-abi/blob/0a5cab0bdeae30b77efa667379427581784f1707/eth_abi/utils/numeric.py#L30-L39
tfidf_python_100_1.0,convert decimal to hex,python,"def __repr__(self):
        hex = '%032x' % self.int
        return '%s-%s-%s-%s-%s' % (
            hex[:8], hex[8:12], hex[12:16], hex[16:20], hex[20:])",https://github.com/markreidvfx/pyaaf2/blob/37de8c10d3c3495cc00c705eb6c5048bc4a7e51f/aaf2/auid.py#L111-L114
tfidf_python_100_1.0,convert decimal to hex,python,"def __str__(self):
        hex = '%032x' % self.int
        return '%s-%s-%s-%s-%s' % (
            hex[:8], hex[8:12], hex[12:16], hex[16:20], hex[20:])",https://github.com/fabioz/PyDev.Debugger/blob/ed9c4307662a5593b8a7f1f3389ecd0e79b8c503/_pydev_imps/_pydev_uuid_old.py#L197-L200
tfidf_python_100_1.0,convert decimal to hex,python,"def _hextorgb(self, hex):
        if hex.lower() in colors.lessColors:
            hex = colors.lessColors[hex.lower()]
        hex = hex.strip()
        if hex[0] == '#':
            hex = hex.strip('#').strip(';')
            if len(hex) == 3:
                hex = [c * 2 for c in hex]
            else:
                hex = [hex[i:i + 2] for i in range(0, len(hex), 2)]
            return tuple(int(c, 16) for c in hex)
        try:
            return [int(hex, 16)] * 3
        except:
            return [float(hex)] * 3",https://github.com/lesscpy/lesscpy/blob/51e392fb4a3cd4ccfb6175e0e42ce7d2f6b78126/lesscpy/lessc/color.py#L399-L413
tfidf_python_100_1.0,convert decimal to hex,python,"def hex_to_rgb(hex):
    """""" Returns RGB values for a hex color string.
    """"""
    hex = hex.lstrip(""#"")
    if len(hex) < 6:
        hex += hex[-1] * (6 - len(hex))
    if len(hex) == 6:
        r, g, b = hex[0:2], hex[2:4], hex[4:]
        r, g, b = [int(n, 16) / 255.0 for n in (r, g, b)]
        a = 1.0
    elif len(hex) == 8:
        r, g, b, a = hex[0:2], hex[2:4], hex[4:6], hex[6:]
        r, g, b, a = [int(n, 16) / 255.0 for n in (r, g, b, a)]
    return r, g, b, a",https://github.com/shoebot/shoebot/blob/d554c1765c1899fa25727c9fc6805d221585562b/shoebot/data/basecolor.py#L381-L394
tfidf_python_100_1.0,convert decimal to hex,python,"def compute_signed_fixed_bounds(
    num_bits: int,
    frac_places: int,
) -> Tuple[decimal.Decimal, decimal.Decimal]:
    int_lower, int_upper = compute_signed_integer_bounds(num_bits)

    with decimal.localcontext(abi_decimal_context):
        exp = TEN ** -frac_places
        lower = decimal.Decimal(int_lower) * exp
        upper = decimal.Decimal(int_upper) * exp

    return lower, upper",https://github.com/ethereum/eth-abi/blob/0a5cab0bdeae30b77efa667379427581784f1707/eth_abi/utils/numeric.py#L42-L53
tfidf_python_100_1.0,convert decimal to hex,python,"def __init__(self):
        # Depth / indentation level.
        self.depth = 0
        self.name = None
        
        self.set_allocation = Decimal(0)
        self.curr_allocation = Decimal(0)
        self.diff_allocation = Decimal(0)
        self.alloc_diff_perc = Decimal(0)
        
        self.set_value = Decimal(0)
        self.curr_value = Decimal(0)
        self.diff_value = Decimal(0)

        self.curr_value_own_currency = Decimal(0)
        self.own_currency = None",https://github.com/MisterY/asset-allocation/blob/72239aa20762cda67c091f27b86e65d61bf3b613/asset_allocation/view_model.py#L7-L22
tfidf_python_100_1.0,convert decimal to hex,python,"def convert_decimal(value, parameter):
    '''
    Converts to decimal.Decimal:
        '', '-', None convert to parameter default
        Anything else uses Decimal constructor
    '''
    value = _check_default(value, parameter, ( '', '-', None ))
    if value is None or isinstance(value, decimal.Decimal):
        return value
    try:
        return decimal.Decimal(value)
    except Exception as e:
        raise ValueError(str(e))",https://github.com/doconix/django-mako-plus/blob/a90f9b4af19e5fa9f83452989cdcaed21569a181/django_mako_plus/converter/converters.py#L77-L89
tfidf_python_100_1.0,convert decimal to hex,python,"def hex2web(hex):
    """"""Converts HEX representation to WEB

    :param rgb: 3 hex char or 6 hex char string representation
    :rtype: web string representation (human readable if possible)

    WEB representation uses X11 rgb.txt to define conversion
    between RGB and english color names.

    Usage
    =====

    >>> from colour import hex2web

    >>> hex2web('#ff0000')
    'red'

    >>> hex2web('#aaaaaa')
    '#aaa'

    >>> hex2web('#abc')
    '#abc'

    >>> hex2web('#acacac')
    '#acacac'

    """"""
    dec_rgb = tuple(int(v * 255) for v in hex2rgb(hex))
    if dec_rgb in RGB_TO_COLOR_NAMES:
        ## take the first one
        color_name = RGB_TO_COLOR_NAMES[dec_rgb][0]
        ## Enforce full lowercase for single worded color name.
        return color_name if len(re.sub(r""[^A-Z]"", """", color_name)) > 1 \
               else color_name.lower()

    # Hex format is verified by hex2rgb function. And should be 3 or 6 digit
    if len(hex) == 7:
        if hex[1] == hex[2] and \
           hex[3] == hex[4] and \
           hex[5] == hex[6]:
            return '#' + hex[1] + hex[3] + hex[5]
    return hex",https://github.com/vaab/colour/blob/11f138eb7841d2045160b378a2eec0c2321144c0/colour.py#L573-L614
tfidf_python_100_1.0,convert decimal to hex,python,"def sec2hour(sec=0):
    _sec = decimal.Decimal(sec or 0)
    _hor = _sec / decimal.Decimal(3600)
    return str(_hor.quantize(decimal.Decimal('1.00')))",https://github.com/talkincode/toughlib/blob/1c2f7dde3a7f101248f1b5f5d428cc85466995cf/toughlib/utils.py#L146-L149
tfidf_python_100_1.0,convert decimal to hex,python,"def round_half_up(number):
    return int(decimal.Decimal(number).quantize(decimal.Decimal('1'), rounding=decimal.ROUND_HALF_UP))",https://github.com/jameslyons/python_speech_features/blob/40c590269b57c64a8c1f1ddaaff2162008d1850c/python_speech_features/sigproc.py#L10-L11
tfidf_python_100_1.0,convert decimal to hex,python,"def decimal_format(value, TWOPLACES=Decimal(100) ** -2):
    'Format a decimal.Decimal like to 2 decimal places.'
    if not isinstance(value, Decimal):
        value = Decimal(str(value))
    return value.quantize(TWOPLACES)",https://github.com/openstates/billy/blob/5fc795347f12a949e410a8cfad0c911ea6bced67/billy/web/public/templatetags/customtags.py#L77-L81
tfidf_python_100_1.0,convert decimal to hex,python,"def satoshi_to_btc(satoshi_count):
    if satoshi_count == 0:
        return decimal.Decimal(0)
    r = satoshi_count * COIN_PER_SATOSHI
    return r.normalize()",https://github.com/LedgerHQ/btchip-python/blob/fe82d7f5638169f583a445b8e200fd1c9f3ea218/btchip/btchipHelpers.py#L27-L31
tfidf_python_100_1.0,convert decimal to hex,python,"def coerce_to_decimal(value):
    """"""Attempt to coerce the value to a Decimal, or raise an error if unable to do so.""""""
    if isinstance(value, decimal.Decimal):
        return value
    else:
        try:
            return decimal.Decimal(value)
        except decimal.InvalidOperation as e:
            raise GraphQLInvalidArgumentError(e)",https://github.com/kensho-technologies/graphql-compiler/blob/f6079c6d10f64932f6b3af309b79bcea2123ca8f/graphql_compiler/query_formatting/representations.py#L41-L49
tfidf_python_100_1.0,export to excel,python,"def export_translations(tasks_ids):
        qs = TransTask.objects.filter(pk__in=tasks_ids)
        export = ExportQueryset(
            qs,
            TransTask,
            ('id', 'object_name', 'object_pk', 'object_field_label', 'object_field_value', 'number_of_words',
             'object_field_value_translation', 'date_modification', 'done')
        )
        excel = export.get_excel()
        return excel",https://github.com/APSL/transmanager/blob/79157085840008e146b264521681913090197ed1/transmanager/export.py#L58-L67
tfidf_python_100_1.0,export to excel,python,"def to_export(export):
        """"""Serializes export to id string
        :param export: object to serialize
        :return: string id
        """"""
        from sevenbridges.models.storage_export import Export
        if not export:
            raise SbgError('Export is required!')
        elif isinstance(export, Export):
            return export.id
        elif isinstance(export, six.string_types):
            return export
        else:
            raise SbgError('Invalid export parameter!')",https://github.com/sbg/sevenbridges-python/blob/f62640d1018d959f0b686f2dbe5e183085336607/sevenbridges/meta/transformer.py#L211-L224
tfidf_python_100_1.0,export to excel,python,"def from_export(cls, export, connection):
        return cls(
            export['id'], export['name'], export['artist_id'],
            export['artist'], export['album_id'], export['album'],
            export['cover'], export['track'], export['duration'],
            export['popularity'], connection)",https://github.com/koehlma/pygrooveshark/blob/17673758ac12f54dc26ac879c30ea44f13b81057/src/grooveshark/classes/song.py#L84-L89
tfidf_python_100_1.0,export to excel,python,"def to_clipboard(self, excel=True, sep=None, **kwargs):  # pragma: no cover
        return self._default_to_pandas(""to_clipboard"", excel=excel, sep=sep, **kwargs)",https://github.com/modin-project/modin/blob/5b77d242596560c646b8405340c9ce64acb183cb/modin/pandas/base.py#L2719-L2720
tfidf_python_100_1.0,export to excel,python,"def from_export(cls, export, connection):
        return cls(export['artists'], export['radio'], connection,
                   export['recent_artists'], export['songs_already_seen'])",https://github.com/koehlma/pygrooveshark/blob/17673758ac12f54dc26ac879c30ea44f13b81057/src/grooveshark/classes/radio.py#L205-L207
tfidf_python_100_1.0,export to excel,python,"def to_clipboard(self, excel=True, sep=None, **kwargs):
        r""""""
        Copy object to the system clipboard.

        Write a text representation of object to the system clipboard.
        This can be pasted into Excel, for example.

        Parameters
        ----------
        excel : bool, default True
            - True, use the provided separator, writing in a csv format for
              allowing easy pasting into excel.
            - False, write a string representation of the object to the
              clipboard.

        sep : str, default ``'\t'``
            Field delimiter.
        **kwargs
            These parameters will be passed to DataFrame.to_csv.

        See Also
        --------
        DataFrame.to_csv : Write a DataFrame to a comma-separated values
            (csv) file.
        read_clipboard : Read text from clipboard and pass to read_table.

        Notes
        -----
        Requirements for your platform.

          - Linux : `xclip`, or `xsel` (with `gtk` or `PyQt4` modules)
          - Windows : none
          - OS X : none

        Examples
        --------
        Copy the contents of a DataFrame to the clipboard.

        >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=['A', 'B', 'C'])
        >>> df.to_clipboard(sep=',')
        ... # Wrote the following to the system clipboard:
        ... # ,A,B,C
        ... # 0,1,2,3
        ... # 1,4,5,6

        We can omit the the index by passing the keyword `index` and setting
        it to false.

        >>> df.to_clipboard(sep=',', index=False)
        ... # Wrote the following to the system clipboard:
        ... # A,B,C
        ... # 1,2,3
        ... # 4,5,6
        """"""
        from pandas.io import clipboards
        clipboards.to_clipboard(self, excel=excel, sep=sep, **kwargs)",https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/generic.py#L2621-L2676
tfidf_python_100_1.0,export to excel,python,"def main():

    parser = argparse.ArgumentParser()

    # parser.add_argument(""-e"", ""--export"", nargs='+', help =""exports default instrumets, probes, and scripts\n  \
    #                                                        call pylabcontrol target_folder source_folders(optional) class_type(optional) \
    #                                                        target_folder:\t target folder where to export .b26 files \
    #                                                        source_folder:\t source folder with location of .python files from which to create .b26 files \n \
    #                                                        \t\t can also be the name of a package that contains the python files (e.g. b26tool_kit) \n \
    #                                                        \t\t if empty export from pylabcontrol \n \
    #                                                        class_type:\t type of class to export as .b26 file (intrument, script, probe or all) \
    #                                                        "")
    parser.add_argument(""-g"", ""--gui"", nargs='?', const='', type=str, help=""loads the default gui"")
    args = parser.parse_args()

    # if args.export is not None:
    #
    #     if len(args.export) == 1:
    #         target_folder = args.export[0]
    #         export(target_folder)
    #     elif len(args.export) == 2:
    #         target_folder = args.export[0]
    #         source_folder = args.export[1]
    #         export(target_folder, source_folder)
    #     elif len(args.export) == 3:
    #         target_folder = args.export[0]
    #         source_folder = args.export[1]
    #         class_type = args.export[2]
    #
    #         export(target_folder, source_folder, class_type)
    #     else:
    #         parser.error('The -e or --export keyword must be given one to three arguments')
    #         parser.print_help()
    if args.gui is not None:

        fname = args.gui
        launch_gui(fname)
    else:
        parser.print_help()",https://github.com/LISE-B26/pylabcontrol/blob/67482e5157fcd1c40705e5c2cacfb93564703ed0/__main__.py#L5-L43
tfidf_python_100_1.0,export to excel,python,"def main():

    parser = argparse.ArgumentParser()

    parser.add_argument(""-e"", ""--export"", nargs='+', help =""exports default instrumets, probes, and scripts\n  \
                                                           call pylabcontrol target_folder source_folders(optional) class_type(optional) \
                                                           target_folder:\t target folder where to export .b26 files \
                                                           source_folder:\t source folder with location of .python files from which to create .b26 files \n \
                                                           \t\t can also be the name of a package that contains the python files (e.g. b26tool_kit) \n \
                                                           \t\t if empty export from pylabcontrol \n \
                                                           class_type:\t type of class to export as .b26 file (intrument, script, probe or all) \
                                                           "")
    parser.add_argument(""-g"", ""--gui"", nargs='?', const='', type=str, help=""loads the default gui"")
    args = parser.parse_args()

    if args.export is not None:

        if len(args.export) == 1:
            target_folder = args.export[0]
            export(target_folder)
        elif len(args.export) == 2:
            target_folder = args.export[0]
            source_folder = args.export[1]
            export(target_folder, source_folder)
        elif len(args.export) == 3:
            target_folder = args.export[0]
            source_folder = args.export[1]
            class_type = args.export[2]

            export(target_folder, source_folder, class_type)
        else:
            parser.error('The -e or --export keyword must be given one to three arguments')
            parser.print_help()
    elif args.gui is not None:

        fname = args.gui
        launch_gui(fname)
    else:
        parser.print_help()",https://github.com/LISE-B26/pylabcontrol/blob/67482e5157fcd1c40705e5c2cacfb93564703ed0/build/lib/pylabcontrol/__main__.py#L5-L43
tfidf_python_100_1.0,export to excel,python,"def importCommandRequest(self, commandHandler, export=False):
        from apitax.ah.commandtax.commands.Custom import Custom as CustomCommand

        if (commandHandler.getRequest().getResponseBody().strip() != ''):
            if (not isinstance(commandHandler.getRequest(), CustomCommand)):
                self.data.importScriptsExports(commandHandler.getRequest().parser.data, export=export)
            else:
                self.data.storeRequest(commandHandler.getRequest().getResponseBody(), export=export)",https://github.com/ShawnClake/Apitax/blob/2eb9c6990d4088b2503c7f13c2a76f8e59606e6d/apitax/grammar/AhVisitor.py#L42-L49
tfidf_python_100_1.0,export to excel,python,"def __init__(self, account, requests_options=()):
        from pykintone.user_api.export import Export
        self.for_exporting = Export(account, requests_options)",https://github.com/icoxfog417/pykintone/blob/756609fc956fc784325d58cc01473a67a640654c/pykintone/user_api/__init__.py#L3-L5
tfidf_python_100_1.0,export to excel,python,"def freeze_with_config(config, db=None):
    for export in config.exports:
        if db is not None:
            export.data['database'] = db
        if export.skip:
            log.info(""Skipping: %s"", export.name)
            continue
        log.info(""Running: %s"", export.name)
        freeze_export(export)",https://github.com/pudo/datafreeze/blob/adbb19ad71a9a6cec1fbec650ee1e784c33eae8e/datafreeze/app.py#L141-L149
tfidf_python_100_1.0,export to excel,python,"def handle(self, *args, **options):
        clean_dist_dir()

        export = ['python', 'setup.py', 'sdist']

        if options['nowheel']:
            call(export)
        else:
            export.append('bdist_wheel')
            call(export)

        if options['upload']:
            export.append('upload')
            call(export)",https://github.com/rhazdon/django-sonic-screwdriver/blob/89e885e8c1322fc5c3e0f79b03a55acdc6e63972/django_sonic_screwdriver/management/commands/pypi:export.py#L21-L34
tfidf_python_100_1.0,export to excel,python,"def get_render_method(self, format):
        if format == 'excel':
            return self.render_excel_response
        elif format == 'csv':
            return self.render_csv_response
        raise NotImplementedError(""Export format is not recognized."")",https://github.com/birdsarah/django-spreadsheetresponsemixin/blob/386abdacbe1c231e3a1d564e8d6974bc5a95b59d/spreadsheetresponsemixin/views.py#L213-L218
tfidf_python_100_1.0,export to excel,python,"def __init__(self, source, export):
        # type: (USourceNameArray, UExportNameArray) -> None
        self.source = ASourceNameArray(source)
        self.export = AExportNameArray(export)",https://github.com/dls-controls/pymalcolm/blob/80ea667e4da26365a6cebc0249f52fdc744bd983/malcolm/modules/builtin/util.py#L83-L86
tfidf_python_100_1.0,export to excel,python,"def export_dict(self):
        self.say('exporting:self:',
                 stuff=self, verbosity=100)
        export = {}
        for (k, v) in self.items():
            if k not in ['_doct_level', '_doct_as_key']:
                if isinstance(v, BubbleDoct):
                    export[k] = v.export_dict()
                else:
                    export[k] = v
        self.say('exporting:self:export', stuff=export, verbosity=100)
        return export",https://github.com/e7dal/bubble3/blob/59c735281a95b44f6263a25f4d6ce24fca520082/bubble3/util/cfg.py#L38-L49
tfidf_python_100_1.0,export to excel,python,"def get_outkey(dskey, export_types):
    """"""
    Extract the first pair (dskey, exptype) found in export
    """"""
    for exptype in export_types:
        if (dskey, exptype) in export:
            return (dskey, exptype)",https://github.com/gem/oq-engine/blob/8294553a0b8aba33fd96437a35065d03547d0040/openquake/engine/export/core.py#L106-L112
tfidf_python_100_1.0,export to excel,python,"def __init__(self, header=None, choices=None, list_validation=None, **kwargs):

        self.choices = tuple(choices) if choices else None
        self.list_validation = list_validation

        self.to_excel_map = {internal: excel for internal, excel in self.choices}
        self.from_excel_map = {excel: internal for internal, excel in self.choices}

        # Setup maps before super().__init__() to validation of default value.
        super(ChoiceColumn, self).__init__(header=header, **kwargs)

        if self.list_validation and not self.data_validation:
            self.data_validation = DataValidation(
                type=""list"",
                formula1=""\""%s\"""" % "","".join('%s' % str(excel) for internal, excel in self.choices)
            )",https://github.com/SverkerSbrg/openpyxl-templates/blob/59ff897c7667272e508faedef05a0b89847b2d2b/openpyxl_templates/table_sheet/columns.py#L377-L392
tfidf_python_100_1.0,export to excel,python,"def __init__(self, type, r_array, name=None, export=None):
        super(Atom, self).__init__()
        self.r_array = r_array
        self.type_array = type
        if name:
            self.atom_name = name
        self.export = export or {}",https://github.com/chemlab/chemlab/blob/c8730966316d101e24f39ac3b96b51282aba0abe/chemlab/core/atom.py#L14-L20
tfidf_python_100_1.0,export to excel,python,"def app_alias(self, alias_name):
        # It is VERY important to have the variables declared WITHIN the function
        return '''
            function {name} () {{
                TF_PYTHONIOENCODING=$PYTHONIOENCODING;
                export TF_SHELL=bash;
                export TF_ALIAS={name};
                export TF_SHELL_ALIASES=$(alias);
                export TF_HISTORY=$(fc -ln -10);
                export PYTHONIOENCODING=utf-8;
                TF_CMD=$(
                    thefuck {argument_placeholder} $@
                ) && eval $TF_CMD;
                unset TF_HISTORY;
                export PYTHONIOENCODING=$TF_PYTHONIOENCODING;
                {alter_history}
            }}
        '''.format(
            name=alias_name,
            argument_placeholder=ARGUMENT_PLACEHOLDER,
            alter_history=('history -s $TF_CMD;'
                           if settings.alter_history else ''))",https://github.com/nvbn/thefuck/blob/40ab4eb62db57627bff10cf029d29c94704086a2/thefuck/shells/bash.py#L12-L33
tfidf_python_100_1.0,export to excel,python,"def __init__(self, export, query):
        self._encoding = locale.getpreferredencoding()
        self.export = export
        self.query = query
        self._paths = []
        self._get_basepath()

        if export.get('filename') == '-':
            export.data['fileobj'] = sys.stdout
        self.fileobj = export.get('fileobj')",https://github.com/pudo/datafreeze/blob/adbb19ad71a9a6cec1fbec650ee1e784c33eae8e/datafreeze/format/common.py#L23-L32
tfidf_python_100_1.0,scatter plot,python,"def scatter(self):
        return self.state(self._actor, self.id, self._actor.scatter)",https://github.com/celery/cell/blob/c7f9b3a0c11ae3429eacb4114279cf2614e94a48/cell/actors.py#L700-L701
tfidf_python_100_1.0,scatter plot,python,"def scatter(self, x, y, s=None, c=None, **kwds):
        """"""
        Create a scatter plot with varying marker point size and color.

        The coordinates of each point are defined by two dataframe columns and
        filled circles are used to represent each point. This kind of plot is
        useful to see complex correlations between two variables. Points could
        be for instance natural 2D coordinates like longitude and latitude in
        a map or, in general, any pair of metrics that can be plotted against
        each other.

        Parameters
        ----------
        x : int or str
            The column name or column position to be used as horizontal
            coordinates for each point.
        y : int or str
            The column name or column position to be used as vertical
            coordinates for each point.
        s : scalar or array_like, optional
            The size of each point. Possible values are:

            - A single scalar so all points have the same size.

            - A sequence of scalars, which will be used for each point's size
              recursively. For instance, when passing [2,14] all points size
              will be either 2 or 14, alternatively.

        c : str, int or array_like, optional
            The color of each point. Possible values are:

            - A single color string referred to by name, RGB or RGBA code,
              for instance 'red' or '#a98d19'.

            - A sequence of color strings referred to by name, RGB or RGBA
              code, which will be used for each point's color recursively. For
              instance ['green','yellow'] all points will be filled in green or
              yellow, alternatively.

            - A column name or position whose values will be used to color the
              marker points according to a colormap.

        **kwds
            Keyword arguments to pass on to :meth:`DataFrame.plot`.

        Returns
        -------
        :class:`matplotlib.axes.Axes` or numpy.ndarray of them

        See Also
        --------
        matplotlib.pyplot.scatter : Scatter plot using multiple input data
            formats.

        Examples
        --------
        Let's see how to draw a scatter plot using coordinates from the values
        in a DataFrame's columns.

        .. plot::
            :context: close-figs

            >>> df = pd.DataFrame([[5.1, 3.5, 0], [4.9, 3.0, 0], [7.0, 3.2, 1],
            ...                    [6.4, 3.2, 1], [5.9, 3.0, 2]],
            ...                   columns=['length', 'width', 'species'])
            >>> ax1 = df.plot.scatter(x='length',
            ...                       y='width',
            ...                       c='DarkBlue')

        And now with the color determined by a column as well.

        .. plot::
            :context: close-figs

            >>> ax2 = df.plot.scatter(x='length',
            ...                       y='width',
            ...                       c='species',
            ...                       colormap='viridis')
        """"""
        return self(kind='scatter', x=x, y=y, c=c, s=s, **kwds)",https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/plotting/_core.py#L3463-L3542
tfidf_python_100_1.0,scatter plot,python,"def scatter(self, *args, **kwargs):
        """"""Add a scatter plot.""""""
        cls = _make_class(ScatterVisual,
                          _default_marker=kwargs.pop('marker', None),
                          )
        return self._add_item(cls, *args, **kwargs)",https://github.com/kwikteam/phy/blob/7e9313dc364304b7d2bd03b92938347343703003/phy/plot/plot.py#L153-L158
tfidf_python_100_1.0,scatter plot,python,"def _add_plots_to_output(out, data):
    """"""Add CNVkit plots summarizing called copy number values.
    """"""
    out[""plot""] = {}
    diagram_plot = _add_diagram_plot(out, data)
    if diagram_plot:
        out[""plot""][""diagram""] = diagram_plot
    scatter = _add_scatter_plot(out, data)
    if scatter:
        out[""plot""][""scatter""] = scatter
    scatter_global = _add_global_scatter_plot(out, data)
    if scatter_global:
        out[""plot""][""scatter_global""] = scatter_global
    return out",https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/structural/cnvkit.py#L633-L646
tfidf_python_100_1.0,scatter plot,python,"def scatter(x, y, **kwargs):
    """"""Draw a scatter in the current context figure.

    Parameters
    ----------

    x: numpy.ndarray, 1d
        The x-coordinates of the data points.
    y: numpy.ndarray, 1d
        The y-coordinates of the data points.
    options: dict (default: {})
        Options for the scales to be created. If a scale labeled 'x' is
        required for that mark, options['x'] contains optional keyword
        arguments for the constructor of the corresponding scale type.
    axes_options: dict (default: {})
        Options for the axes to be created. If an axis labeled 'x' is required
        for that mark, axes_options['x'] contains optional keyword arguments
        for the constructor of the corresponding axis type.
    """"""
    kwargs['x'] = x
    kwargs['y'] = y
    return _draw_mark(Scatter, **kwargs)",https://github.com/bloomberg/bqplot/blob/8eb8b163abe9ee6306f6918067e2f36c1caef2ef/bqplot/pyplot.py#L816-L837
tfidf_python_100_1.0,scatter plot,python,"def scatter_base() -> Scatter:
    c = (
        Scatter()
        .add_xaxis(Faker.choose())
        .add_yaxis(""åå®¶A"", Faker.values())
        .set_global_opts(title_opts=opts.TitleOpts(title=""Scatter-åºæ¬ç¤ºä¾""))
    )
    return c",https://github.com/pyecharts/pyecharts/blob/02050acb0e94bb9453b88a25028de7a0ce23f125/example/scatter_example.py#L10-L17
tfidf_python_100_1.0,scatter plot,python,"def scatter(self,  ra_deg, dec_deg, *args, **kwargs):
        x,y = self.skyToPix(ra_deg, dec_deg)
        mp.scatter(x,y, *args, **kwargs)",https://github.com/KeplerGO/K2fov/blob/fb122b35687340e0357cba9e0dd47b3be0760693/K2fov/projection.py#L119-L121
tfidf_python_100_1.0,scatter plot,python,"def scatter2d(data, **kwargs):
    """"""Create a 2D scatter plot

    Builds upon `matplotlib.pyplot.scatter` with nice defaults
    and handles categorical colors / legends better.

    Parameters
    ----------
    data : array-like, shape=[n_samples, n_features]
        Input data. Only the first two components will be used.
    c : list-like or None, optional (default: None)
        Color vector. Can be a single color value (RGB, RGBA, or named
        matplotlib colors), an array of these of length n_samples, or a list of
        discrete or continuous values of any data type. If `c` is not a single
        or list of matplotlib colors, the values in `c` will be used to
        populate the legend / colorbar with colors from `cmap`
    cmap : `matplotlib` colormap, str, dict or None, optional (default: None)
        matplotlib colormap. If None, uses `tab20` for discrete data and
        `inferno` for continuous data. If a dictionary, expects one key
        for every unique value in `c`, where values are valid matplotlib colors
        (hsv, rbg, rgba, or named colors)
    s : float, optional (default: 1)
        Point size.
    discrete : bool or None, optional (default: None)
        If True, the legend is categorical. If False, the legend is a colorbar.
        If None, discreteness is detected automatically. Data containing
        non-numeric `c` is always discrete, and numeric data with 20 or less
        unique values is discrete.
    ax : `matplotlib.Axes` or None, optional (default: None)
        axis on which to plot. If None, an axis is created
    legend : bool, optional (default: True)
        States whether or not to create a legend. If data is continuous,
        the legend is a colorbar.
    figsize : tuple, optional (default: None)
        Tuple of floats for creation of new `matplotlib` figure. Only used if
        `ax` is None.
    xticks : True, False, or list-like (default: False)
        If True, keeps default x ticks. If False, removes x ticks.
        If a list, sets custom x ticks
    yticks : True, False, or list-like (default: False)
        If True, keeps default y ticks. If False, removes y ticks.
        If a list, sets custom y ticks
    zticks : True, False, or list-like (default: False)
        If True, keeps default z ticks. If False, removes z ticks.
        If a list, sets custom z ticks.  Only used for 3D plots.
    xticklabels : True, False, or list-like (default: True)
        If True, keeps default x tick labels. If False, removes x tick labels.
        If a list, sets custom x tick labels
    yticklabels : True, False, or list-like (default: True)
        If True, keeps default y tick labels. If False, removes y tick labels.
        If a list, sets custom y tick labels
    zticklabels : True, False, or list-like (default: True)
        If True, keeps default z tick labels. If False, removes z tick labels.
        If a list, sets custom z tick labels. Only used for 3D plots.
    label_prefix : str or None (default: ""PHATE"")
        Prefix for all axis labels. Axes will be labelled `label_prefix`1,
        `label_prefix`2, etc. Can be overriden by setting `xlabel`,
        `ylabel`, and `zlabel`.
    xlabel : str or None (default : None)
        Label for the x axis. Overrides the automatic label given by
        label_prefix. If None and label_prefix is None, no label is set.
    ylabel : str or None (default : None)
        Label for the y axis. Overrides the automatic label given by
        label_prefix. If None and label_prefix is None, no label is set.
    zlabel : str or None (default : None)
        Label for the z axis. Overrides the automatic label given by
        label_prefix. If None and label_prefix is None, no label is set.
        Only used for 3D plots.
    title : str or None (default: None)
        axis title. If None, no title is set.
    legend_title : str (default: """")
        title for the colorbar of legend
    legend_loc : int or string or pair of floats, default: 'best'
        Matplotlib legend location. Only used for discrete data.
        See <https://matplotlib.org/api/_as_gen/matplotlib.pyplot.legend.html>
        for details.
    filename : str or None (default: None)
        file to which the output is saved
    dpi : int or None, optional (default: None)
        The resolution in dots per inch. If None it will default to the value
        savefig.dpi in the matplotlibrc file. If 'figure' it will set the dpi
        to be the value of the figure. Only used if filename is not None.
    **plot_kwargs : keyword arguments
        Extra arguments passed to `matplotlib.pyplot.scatter`.

    Returns
    -------
    ax : `matplotlib.Axes`
        axis on which plot was drawn

    Examples
    --------
    >>> import phate
    >>> import matplotlib.pyplot as plt
    >>> ###
    >>> # Running PHATE
    >>> ###
    >>> tree_data, tree_clusters = phate.tree.gen_dla(n_dim=100, n_branch=20,
    ...                                               branch_length=100)
    >>> tree_data.shape
    (2000, 100)
    >>> phate_operator = phate.PHATE(k=5, a=20, t=150)
    >>> tree_phate = phate_operator.fit_transform(tree_data)
    >>> tree_phate.shape
    (2000, 2)
    >>> ###
    >>> # Plotting using phate.plot
    >>> ###
    >>> phate.plot.scatter2d(tree_phate, c=tree_clusters)
    >>> # You can also pass the PHATE operator instead of data
    >>> phate.plot.scatter2d(phate_operator, c=tree_clusters)
    >>> phate.plot.scatter3d(phate_operator, c=tree_clusters)
    >>> ###
    >>> # Using a cmap dictionary
    >>> ###
    >>> import numpy as np
    >>> X = np.random.normal(0,1,[1000,2])
    >>> c = np.random.choice(['a','b'], 1000, replace=True)
    >>> X[c=='a'] += 10
    >>> phate.plot.scatter2d(X, c=c, cmap={'a' : [1,0,0,1], 'b' : 'xkcd:sky blue'})
    """"""
    warnings.warn(""`phate.plot.scatter2d` is deprecated. ""
                  ""Use `scprep.plot.scatter2d` instead."",
                  FutureWarning)
    data = _get_plot_data(data, ndim=2)
    return scprep.plot.scatter2d(data, **kwargs)",https://github.com/KrishnaswamyLab/PHATE/blob/346a4597dcfc523f8bef99bce482e677282b6719/Python/phate/plot.py#L220-L345
tfidf_python_100_1.0,scatter plot,python,"def _register_scatter():
    """"""
    Patch `PathCollection` and `scatter` to register their return values.

    This registration allows us to distinguish `PathCollection`s created by
    `Axes.scatter`, which should use point-like picking, from others, which
    should use path-like picking.  The former is more common, so we store the
    latter instead; this also lets us guess the type better if this module is
    imported late.
    """"""

    @functools.wraps(PathCollection.__init__)
    def __init__(self, *args, **kwargs):
        _nonscatter_pathcollections.add(self)
        return __init__.__wrapped__(self, *args, **kwargs)
    PathCollection.__init__ = __init__

    @functools.wraps(Axes.scatter)
    def scatter(*args, **kwargs):
        paths = scatter.__wrapped__(*args, **kwargs)
        with suppress(KeyError):
            _nonscatter_pathcollections.remove(paths)
        return paths
    Axes.scatter = scatter",https://github.com/anntzer/mplcursors/blob/a4bce17a978162b5a1837cc419114c910e7992f9/lib/mplcursors/_pick_info.py#L37-L60
tfidf_python_100_1.0,scatter plot,python,"def __init__(self, X, init_K=2, plot=False):
        self.X = X
        self.init_K = init_K
        self.plot = plot",https://github.com/urinieto/msaf/blob/9dbb57d77a1310465a65cc40f1641d083ca74385/msaf/algorithms/fmc2d/xmeans.py#L13-L16
tfidf_python_100_1.0,scatter plot,python,"def run(self, file_store):
        cwljob = resolve_indirect(self.cwljob)

        if isinstance(self.step.tool[""scatter""], string_types):
            scatter = [self.step.tool[""scatter""]]
        else:
            scatter = self.step.tool[""scatter""]

        scatterMethod = self.step.tool.get(""scatterMethod"", None)
        if len(scatter) == 1:
            scatterMethod = ""dotproduct""
        outputs = []

        valueFrom = {shortname(i[""id""]): i[""valueFrom""]
                     for i in self.step.tool[""inputs""] if ""valueFrom"" in i}

        def postScatterEval(io):
            shortio = {shortname(k): v for k, v in iteritems(io)}
            for k in valueFrom:
                io.setdefault(k, None)

            def valueFromFunc(k, v):
                if k in valueFrom:
                    return cwltool.expression.do_eval(
                            valueFrom[k], shortio, self.step.requirements,
                            None, None, {}, context=v)
                else:
                    return v
            return {k: valueFromFunc(k, v) for k, v in list(io.items())}

        if scatterMethod == ""dotproduct"":
            for i in range(0, len(cwljob[shortname(scatter[0])])):
                copyjob = copy.copy(cwljob)
                for sc in [shortname(x) for x in scatter]:
                    copyjob[sc] = cwljob[sc][i]
                copyjob = postScatterEval(copyjob)
                (subjob, follow_on) = makeJob(
                    self.step.embedded_tool, copyjob, None,
                    self.runtime_context)
                self.addChild(subjob)
                outputs.append(follow_on.rv())
        elif scatterMethod == ""nested_crossproduct"":
            outputs = self.nested_crossproduct_scatter(
                cwljob, scatter, postScatterEval)
        elif scatterMethod == ""flat_crossproduct"":
            self.flat_crossproduct_scatter(
                cwljob, scatter, outputs, postScatterEval)
        else:
            if scatterMethod:
                raise validate.ValidationException(
                    ""Unsupported complex scatter type '%s'"" % scatterMethod)
            else:
                raise validate.ValidationException(
                    ""Must provide scatterMethod to scatter over multiple""
                    "" inputs."")

        return outputs",https://github.com/DataBiosphere/toil/blob/a8252277ff814e7bee0971139c2344f88e44b644/src/toil/cwl/cwltoil.py#L692-L748
tfidf_python_100_1.0,scatter plot,python,"def onPlot(self):
        logger.debug(""received plot command for plot=%r, axis_index %r"" % (self.plot, self.axis_index))
        self.plot.plot()",https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-ui/vaex/ui/layers.py#L120-L122
tfidf_python_100_1.0,scatter plot,python,"def scatter_visualmap_color() -> Scatter:
    c = (
        Scatter()
        .add_xaxis(Faker.choose())
        .add_yaxis(""åå®¶A"", Faker.values())
        .set_global_opts(
            title_opts=opts.TitleOpts(title=""Scatter-VisualMap(Color)""),
            visualmap_opts=opts.VisualMapOpts(max_=150),
        )
    )
    return c",https://github.com/pyecharts/pyecharts/blob/02050acb0e94bb9453b88a25028de7a0ce23f125/example/scatter_example.py#L36-L46
tfidf_python_100_1.0,scatter plot,python,"def main():
    """"""Event display for an event of station 503

    Date        Time      Timestamp   Nanoseconds
    2012-03-29  10:51:36  1333018296  870008589

    Number of MIPs
    35.0  51.9  35.8  78.9

    Arrival time
    15.0  17.5  20.0  27.5

    """"""
    # Detector positions in ENU relative to the station GPS
    x = [-6.34, -2.23, -3.6, 3.46]
    y = [6.34, 2.23, -3.6, 3.46]

    # Scale mips to fit the graph
    n = [35.0, 51.9, 35.8, 78.9]

    # Make times relative to first detection
    t = [15., 17.5, 20., 27.5]
    dt = [ti - min(t) for ti in t]

    plot = Plot()
    plot.scatter([0], [0], mark='triangle')
    plot.add_pin_at_xy(0, 0, 'Station 503', use_arrow=False, location='below')
    plot.scatter_table(x, y, dt, n)

    plot.set_scalebar(location=""lower right"")
    plot.set_colorbar('$\Delta$t [ns]')
    plot.set_axis_equal()
    plot.set_mlimits(max=16.)
    plot.set_slimits(min=10., max=100.)

    plot.set_xlabel('x [m]')
    plot.set_ylabel('y [m]')

    plot.save('event_display')


    # Add event by Station 508
    # Detector positions in ENU relative to the station GPS
    x508 = [6.12, 0.00, -3.54, 3.54]
    y508 = [-6.12, -13.23, -3.54, 3.54]

    # Event GPS timestamp: 1371498167.016412100
    # MIPS
    n508 = [5.6, 16.7, 36.6, 9.0]
    # Arrival Times
    t508 = [15., 22.5, 22.5, 30.]
    dt508 = [ti - min(t508) for ti in t508]

    plot = MultiPlot(1, 2, width=r'.33\linewidth')
    plot.set_xlimits_for_all(min=-10, max=15)
    plot.set_ylimits_for_all(min=-15, max=10)
    plot.set_mlimits_for_all(min=0., max=16.)
    plot.set_colorbar('$\Delta$t [ns]', False)
    plot.set_colormap('blackwhite')
    plot.set_scalebar_for_all(location=""upper right"")

    p = plot.get_subplot_at(0, 0)
    p.scatter([0], [0], mark='triangle')
    p.add_pin_at_xy(0, 0, 'Station 503', use_arrow=False, location='below')
    p.scatter_table(x, y, dt, n)
    p.set_axis_equal()

    p = plot.get_subplot_at(0, 1)
    p.scatter([0], [0], mark='triangle')
    p.add_pin_at_xy(0, 0, 'Station 508', use_arrow=False, location='below')
    p.scatter_table(x508, y508, dt508, n508)
    p.set_axis_equal()

    plot.show_yticklabels_for_all([(0, 0)])
    plot.show_xticklabels_for_all([(0, 0), (0, 1)])

    plot.set_xlabel('x [m]')
    plot.set_ylabel('y [m]')

    plot.save('multi_event_display')",https://github.com/davidfokkema/artist/blob/26ae7987522622710f2910980770c50012fda47d/demo/demo_event_display.py#L4-L83
tfidf_python_100_1.0,scatter plot,python,"def scatter_nb(self,xdata,ydata=[],trendline=False):
        '''Graphs a scatter plot and embeds it in a Jupyter notebook. See 'help(figure.scatter)' for more info.'''
        self.scatter(xdata,ydata,trendline)",https://github.com/Dfenestrator/GooPyCharts/blob/57117f213111dfe0401b1dc9720cdba8a23c3028/gpcharts.py#L561-L563
tfidf_python_100_1.0,scatter plot,python,"def addPlot(self, plot_name):
        plot = self.plot(name=plot_name)
        plot.setPen(self.n_plots)
        self.n_plots += 1
        self.plot_dict[plot_name] = plot",https://github.com/shreyaspotnis/rampage/blob/e2565aef7ee16ee06523de975e8aa41aca14e3b2/rampage/widgets/RampViewer.py#L29-L33
tfidf_python_100_1.0,scatter plot,python,"def plot_rebit_modelparams(modelparams, rebit_axes=REBIT_AXES, **kwargs):
    """"""
    Given model parameters representing rebits, plots the
    rebit states as a scatter plot. Additional keyword arguments
    are passed to :ref:`plt.scatter`.

    :param np.ndarray modelparams: Model parameters representing
        rebits.
    :param list rebit_axes: List containing indices for the :math:`x`
        and :math:`z` axes.
    """"""
    mps = modelparams[:, rebit_axes] * np.sqrt(2)
    plt.scatter(mps[:, 0], mps[:, 1], **kwargs)",https://github.com/QInfer/python-qinfer/blob/8170c84a0be1723f8c6b09e0d3c7a40a886f1fe3/src/qinfer/tomography/plotting_tools.py#L80-L92
tfidf_python_100_1.0,scatter plot,python,"def mult(self, context, x, y):
        logger.debug('Multiplying petsc matrix with vector without explicit matrix.')

        ## copy x to local vec
        scatter, x_local = petsc.Scatter.toAll(x)
        scatter.scatterBegin(x, x_local)
        scatter.scatterEnd(x, x_local)
        scatter.destroy()


        ## set y values
        y_ownership_range = y.getOwnershipRange()
        y_size_local = y_ownership_range[1] - y_ownership_range[0]
        y_size_global = y.getSize()

        for i_local in range(y_size_local):
            i_global = y_ownership_range[0] + i_local

            ## compute value
            value = 0
            for j_global in range(y_size_global):
                value += self.entry_function(i_global, j_global) * x_local.getValue(j_global)

            y.setValue(i_global, value)
        y.assemblyBegin()
        y.assemblyEnd()

        ## destroy local copy
        x_local.destroy()",https://github.com/jor-/util/blob/0eb0be84430f88885f4d48335596ca8881f85587/util/petsc/with_petsc4py.py#L230-L258
tfidf_python_100_1.0,scatter plot,python,"def scatter(chart, value, y):
		return 'Ã' if y == chart.y(value) else ' '",https://github.com/gduverger/plainchart/blob/11c3e9348bb4b0cbd8d0bdc3d2cdcb4c6e6640f0/plainchart/plainchart.py#L45-L46
tfidf_python_100_1.0,scatter plot,python,"def plot(self, method='plot', **kwargs):
        """"""Plot the data for this series

        Returns
        -------
        figure : `~matplotlib.figure.Figure`
            the newly created figure, with populated Axes.

        See Also
        --------
        matplotlib.pyplot.figure
            for documentation of keyword arguments used to create the
            figure
        matplotlib.figure.Figure.add_subplot
            for documentation of keyword arguments used to create the
            axes
        matplotlib.axes.Axes.plot
            for documentation of keyword arguments used in rendering the data
        """"""
        from ..plot import Plot
        from ..plot.text import default_unit_label

        # correct for log scales and zeros
        if kwargs.get('xscale') == 'log' and self.x0.value == 0:
            kwargs.setdefault('xlim', (self.dx.value, self.xspan[1]))

        # make plot
        plot = Plot(self, method=method, **kwargs)

        # set default y-axis label (xlabel is set by Plot())
        default_unit_label(plot.gca().yaxis, self.unit)

        return plot",https://github.com/gwpy/gwpy/blob/7a92b917e7dd2d99b15895293a1fa1d66cdb210a/gwpy/types/series.py#L422-L454
tfidf_python_100_1.0,convert json to csv,python,"def convert(csv, json, **kwargs):
    '''Convert csv to json.

    csv:  filename or file-like object
    json: filename  or file-like object


    if csv is '-' or None:
        stdin is used for input
    if json is '-' or None:
        stdout is used for output
    '''

    csv_local, json_local = None, None
    try:
        if csv == '-' or csv is None:
            csv = sys.stdin
        elif isinstance(csv, str):
            csv = csv_local = open(csv, 'r')

        if json == '-' or json is None:
            json = sys.stdout
        elif isinstance(json, str):
            json = json_local = open(json, 'w')

        data = load_csv(csv, **kwargs)
        save_json(data, json, **kwargs)
    finally:
        if csv_local is not None:
            csv_local.close()
        if json_local is not None:
            json_local.close()",https://github.com/oplatek/csv2json/blob/f2f95db71ba2ce683fd6d0d3e2f13c9d0a77ceb6/csv2json/__init__.py#L20-L51
tfidf_python_100_1.0,convert json to csv,python,"def append_csv(self, csv):
        csv = CSV(csv, self)
#        if self.get_csv(csv.id):
#            raise Exception('csv already exists: {0}'.format(csv.id))
#        csv.get_import_table()
        self['csvs'].append(csv)
        return self",https://github.com/adamjaso/pyauto/blob/b11da69fb21a49241f5ad75dac48d9d369c6279b/csvdb/pyauto/csvdb/config.py#L31-L37
tfidf_python_100_1.0,convert json to csv,python,"def get_csv(self, csv):
        for c in self.csvs:
            if csv == c.get_id():
                return c
        raise Exception('unknown csv: {0}'.format(csv))",https://github.com/adamjaso/pyauto/blob/b11da69fb21a49241f5ad75dac48d9d369c6279b/csvdb/pyauto/csvdb/config.py#L51-L55
tfidf_python_100_1.0,convert json to csv,python,"def get_sla_csv(self):
    csv = os.path.join(self.resource_directory, self.label + '.sla.csv')
    return csv",https://github.com/linkedin/naarad/blob/261e2c0760fd6a6b0ee59064180bd8e3674311fe/src/naarad/metrics/metric.py#L174-L176
tfidf_python_100_1.0,convert json to csv,python,"def get_important_sub_metrics_csv(self):
    csv = os.path.join(self.resource_directory, self.label + '.important_sub_metrics.csv')
    return csv",https://github.com/linkedin/naarad/blob/261e2c0760fd6a6b0ee59064180bd8e3674311fe/src/naarad/metrics/metric.py#L162-L164
tfidf_python_100_1.0,convert json to csv,python,"def __init__(self, fldr):
        """"""
        loads all the ref_*.csv files relating 
        to character traits
        """"""
        self.ref_folder = fldr
        #print('loading data files')
        self.races = RefFile(fldr, 'ref_races.csv')
        self.classes = RefFile(fldr, 'ref_classes.csv')
        self.stats = RefFile(fldr, 'ref_stats.csv')
        self.skills = RefFile(fldr, 'ref_skills.csv')
        self.stories = RefFile(fldr, 'ref_stories.csv')
        self.inventory = RefFile(fldr, 'ref_objects.csv')",https://github.com/acutesoftware/virtual-AI-simulator/blob/57de679a5b1a58c38fefe6aea58af1f3a7e79c58/vais/character.py#L70-L82
tfidf_python_100_1.0,convert json to csv,python,"def get_stats_csv(self):
    csv = os.path.join(self.resource_directory, self.label + '.stats.csv')
    return csv",https://github.com/linkedin/naarad/blob/261e2c0760fd6a6b0ee59064180bd8e3674311fe/src/naarad/metrics/metric.py#L166-L168
tfidf_python_100_1.0,convert json to csv,python,"def test_csv():
    csv_filename = get_abs_filename_with_sub_path('csv', 'test_csv.csv')[1]
    print(csv_filename)
    csv_list = csv_file_to_list(csv_filename)
    print(csv_list)",https://github.com/chinapnr/fishbase/blob/23c5147a6bc0d8ed36409e55352ffb2c5b0edc82/demo/demo_csv.py#L7-L11
tfidf_python_100_1.0,convert json to csv,python,"def csv(
            self,
            dirPath=None):
        """"""*Render the results in csv format*

        **Key Arguments:**
            - ``dirPath`` -- the path to the directory to save the rendered results to. Default *None*

        **Return:**
            - `csvSources` -- the top-level transient data
            - `csvPhot` -- all photometry associated with the transients
            - `csvSpec` -- all spectral data associated with the transients
            - `csvFiles`  -- all files associated with the matched transients found on the tns

        **Usage:**

            To render the results in csv format:

            .. code-block:: python

                csvSources, csvPhot, csvSpec, csvFiles  = tns.csv()
                print csvSources

            .. code-block:: text

                TNSId,TNSName,discoveryName,discSurvey,raSex,decSex,raDeg,decDeg,transRedshift,specType,discMag,discMagFilter,discDate,objectUrl,hostName,hostRedshift,separationArcsec,separationNorthArcsec,separationEastArcsec
                2016asf,SN2016asf,ASASSN-16cs,ASAS-SN,06:50:36.73,+31:06:45.36,102.6530,31.1126,0.021,SN Ia,17.1,V-Johnson,2016-03-06 08:09:36,http://wis-tns.weizmann.ac.il/object/2016asf,KUG 0647+311,,0.66,0.65,-0.13

            You can save the results to file by passing in a directory path within which to save the files to. The four flavours of data (sources, photometry, spectra and files) are saved to separate files but all data can be assoicated with its transient source using the transient's unique `TNSId`.

            .. code-block:: python

                tns.csv(""~/tns"")

            .. image:: https://i.imgur.com/BwwqMBg.png
                :width: 800px
                :alt: csv output
        """"""

        if dirPath:
            p = self._file_prefix()
            csvSources = self.sourceResults.csv(
                filepath=dirPath + ""/"" + p + ""sources.csv"")
            csvPhot = self.photResults.csv(
                filepath=dirPath + ""/"" + p + ""phot.csv"")
            csvSpec = self.specResults.csv(
                filepath=dirPath + ""/"" + p + ""spec.csv"")
            csvFiles = self.relatedFilesResults.csv(
                filepath=dirPath + ""/"" + p + ""relatedFiles.csv"")
        else:
            csvSources = self.sourceResults.csv()
            csvPhot = self.photResults.csv()
            csvSpec = self.specResults.csv()
            csvFiles = self.relatedFilesResults.csv()
        return csvSources, csvPhot, csvSpec, csvFiles",https://github.com/thespacedoctor/transientNamer/blob/39be410c84275ed4669632f5df67e728d66a318f/transientNamer/search.py#L226-L280
tfidf_python_100_1.0,convert json to csv,python,"def CreateAllStaticRAPIDFiles(in_drainage_line,
                              river_id,
                              length_id,
                              slope_id,
                              next_down_id,
                              rapid_output_folder,
                              kfac_celerity=1000.0/3600.0,
                              kfac_formula_type=3,
                              kfac_length_units=""km"",
                              lambda_k=0.35,
                              x_value=0.3,
                              nhdplus=False,
                              taudem_network_connectivity_tree_file=None,
                              file_geodatabase=None):
    """"""
    To generate the static RAPID files (rapid_connect.csv, riv_bas_id.csv,
    kfac.csv, k.csv, x.csv, comid_lat_lon_z.csv) with default values.

    Parameters
    ----------
    in_drainage_line: str
        Path to the stream network (i.e. Drainage Line) shapefile.
    river_id: str
        The name of the field with the river ID
        (Ex. 'HydroID', 'COMID', or 'LINKNO').
    length_id: str
        The field name containging the length of the river segment
        (Ex. 'LENGTHKM' or 'Length').
    slope_id: str
        The field name containging the slope of the river segment
        (Ex. 'Avg_Slope' or 'Slope').
    next_down_id: str
        The name of the field with the river ID of the next downstream river
        segment (Ex. 'NextDownID' or 'DSLINKNO').
    rapid_output_folder: str
        The path to the folder where all of the RAPID output will be generated.
    kfac_celerity: float, optional
        The flow wave celerity for the watershed in meters per second.
        1 km/hr or 1000.0/3600.0 m/s is a reasonable value if unknown.
    kfac_formula_type: int, optional
        An integer representing the formula type to use when calculating kfac.
        Default is 3.
    kfac_length_units: str, optional
        The units for the length_id field. Supported types are ""m"" for meters
        and ""km"" for kilometers. Default is ""km"".
    lambda_k: float, optional
        The value for lambda given from RAPID after the calibration process.
        Default is 0.35.
    x_value: float, optional
        Value for the muskingum X parameter [0-0.5]. Default is 0.3.
    nhdplus: bool, optional
        If True, the drainage line is from the NHDPlus dataset with the VAA
        fields COMID, FROMNODE, TONODE, and DIVERGENCE. Default is False.
    taudem_network_connectivity_tree_file: str, optional
        If set, the connectivity file will be generated from the TauDEM
        connectivity tree file.
    file_geodatabase: str, optional
        Path to the file geodatabase. If you use this option,
        in_drainage_line is the name of the stream network feature class.
        (WARNING: Not always stable with GDAL.)


    Example::

        from RAPIDpy.gis.workflow import CreateAllStaticRAPIDFiles

        CreateAllStaticRAPIDFiles(
            in_drainage_line=""/path/to/drainage_line.shp"",
            river_id=""HydroID"",
            length_id=""LENGTHKM"",
            slope_id=""SLOPE"",
            next_down_river_id=""NextDownID"",
            rapid_output_folder=""/path/to/rapid/output"",
        )
    """"""
    # RAPID connect file
    rapid_connect_file = os.path.join(rapid_output_folder, 'rapid_connect.csv')
    if nhdplus:
        CreateNetworkConnectivityNHDPlus(in_drainage_line,
                                         rapid_connect_file,
                                         file_geodatabase)
    elif taudem_network_connectivity_tree_file:
        CreateNetworkConnectivityTauDEMTree(
            taudem_network_connectivity_tree_file,
            rapid_connect_file)
    else:
        CreateNetworkConnectivity(in_drainage_line,
                                  river_id,
                                  next_down_id,
                                  rapid_connect_file,
                                  file_geodatabase)

    # river basin id file
    riv_bas_id_file = os.path.join(rapid_output_folder, 'riv_bas_id.csv')
    CreateSubsetFile(in_drainage_line,
                     river_id,
                     riv_bas_id_file,
                     file_geodatabase)
    # kfac file
    kfac_file = os.path.join(rapid_output_folder, 'kfac.csv')
    CreateMuskingumKfacFile(in_drainage_line,
                            river_id,
                            length_id,
                            slope_id,
                            kfac_celerity,
                            kfac_formula_type,
                            rapid_connect_file,
                            kfac_file,
                            length_units=kfac_length_units,
                            file_geodatabase=file_geodatabase)
    # k file
    k_file = os.path.join(rapid_output_folder, 'k.csv')
    CreateMuskingumKFile(lambda_k,
                         kfac_file,
                         k_file)
    # x file
    x_file = os.path.join(rapid_output_folder, 'x.csv')
    CreateConstMuskingumXFile(x_value,
                              rapid_connect_file,
                              x_file)
    # comid lat lon z file
    comid_lat_lon_z_file = \
        os.path.join(rapid_output_folder, 'comid_lat_lon_z.csv')
    FlowlineToPoint(in_drainage_line,
                    river_id,
                    comid_lat_lon_z_file,
                    file_geodatabase)",https://github.com/erdc/RAPIDpy/blob/50e14e130554b254a00ff23b226cd7e4c6cfe91a/RAPIDpy/gis/workflow.py#L22-L148
tfidf_python_100_1.0,convert json to csv,python,"def __init__(self, csv):
        InstrumentCSVResultsFileParser.__init__(self, csv)
        self._currentresultsheader = []
        self._currentanalysiskw = ''
        self._numline = 0",https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/exportimport/instruments/shimadzu/gcms/tq8030.py#L93-L97
tfidf_python_100_1.0,convert json to csv,python,"def import_csv(config, csv_id):
    csv = config.csvdb.get_csv(csv_id)
    with csv.get_import_table() as it:
        it.import_table()
    return csv.database.filename",https://github.com/adamjaso/pyauto/blob/b11da69fb21a49241f5ad75dac48d9d369c6279b/csvdb/pyauto/csvdb/commands.py#L5-L9
tfidf_python_100_1.0,convert json to csv,python,"def emit(self, modelInfo):
    """"""Emit model info to csv file

    Parameters:
    ----------------------------------------------------------------------
    modelInfo:      _NupicModelInfo instance
    retval:         nothing
    """"""
    # Open/init csv file, if needed
    if self.__csvFileObj is None:
      # sets up self.__sortedVariableNames and self.__csvFileObj
      self.__openAndInitCSVFile(modelInfo)

    csv = self.__csvFileObj

    # Emit model info row to report.csv
    print >> csv, ""%s, "" % (self.__searchJobID),
    print >> csv, ""%s, "" % (modelInfo.getModelID()),
    print >> csv, ""%s, "" % (modelInfo.statusAsString()),
    if modelInfo.isFinished():
      print >> csv, ""%s, "" % (modelInfo.getCompletionReason()),
    else:
      print >> csv, ""NA, "",
    if not modelInfo.isWaitingToStart():
      print >> csv, ""%s, "" % (modelInfo.getStartTime()),
    else:
      print >> csv, ""NA, "",
    if modelInfo.isFinished():
      dateFormat = ""%Y-%m-%d %H:%M:%S""
      startTime = modelInfo.getStartTime()
      endTime = modelInfo.getEndTime()
      print >> csv, ""%s, "" % endTime,
      st = datetime.strptime(startTime, dateFormat)
      et = datetime.strptime(endTime, dateFormat)
      print >> csv, ""%s, "" % (str((et - st).seconds)),
    else:
      print >> csv, ""NA, "",
      print >> csv, ""NA, "",
    print >> csv, ""%s, "" % str(modelInfo.getModelDescription()),
    print >> csv, ""%s, "" % str(modelInfo.getNumRecords()),
    paramLabelsDict = modelInfo.getParamLabels()
    for key in self.__sortedVariableNames:
      # Some values are complex structures,.. which need to be represented as
      # strings
      if key in paramLabelsDict:
        print >> csv, ""%s, "" % (paramLabelsDict[key]),
      else:
        print >> csv, ""None, "",
    metrics = modelInfo.getReportMetrics()
    for key in self.__sortedMetricsKeys:
      value = metrics.get(key, ""NA"")
      value = str(value)
      value = value.replace(""\n"", "" "")
      print >> csv, ""%s, "" % (value),

    print >> csv",https://github.com/numenta/nupic/blob/5922fafffdccc8812e72b3324965ad2f7d4bbdad/src/nupic/swarming/permutations_runner.py#L1212-L1267
tfidf_python_100_1.0,convert json to csv,python,"def __openAndInitCSVFile(self, modelInfo):
    """"""
    - Backs up old report csv file;
    - opens the report csv file in append or overwrite mode (per
      self.__replaceReport);
    - emits column fields;
    - sets up self.__sortedVariableNames, self.__csvFileObj,
      self.__backupCSVPath, and self.__reportCSVPath

    Parameters:
    ----------------------------------------------------------------------
    modelInfo:      First _NupicModelInfo instance passed to emit()
    retval:         nothing
    """"""
    # Get the base path and figure out the path of the report file.
    basePath = self.__outputDirAbsPath

    # Form the name of the output csv file that will contain all the results
    reportCSVName = ""%s_Report.csv"" % (self.__outputLabel,)
    reportCSVPath = self.__reportCSVPath = os.path.join(basePath, reportCSVName)

    # If a report CSV file already exists, back it up
    backupCSVPath = None
    if os.path.exists(reportCSVPath):
      backupCSVPath = self.__backupCSVPath = _backupFile(reportCSVPath)


    # Open report file
    if self.__replaceReport:
      mode = ""w""
    else:
      mode = ""a""
    csv = self.__csvFileObj = open(reportCSVPath, mode)

    # If we are appending, add some blank line separators
    if not self.__replaceReport and backupCSVPath:
      print >> csv
      print >> csv

    # Print the column names
    print >> csv, ""jobID, "",
    print >> csv, ""modelID, "",
    print >> csv, ""status, "" ,
    print >> csv, ""completionReason, "",
    print >> csv, ""startTime, "",
    print >> csv, ""endTime, "",
    print >> csv, ""runtime(s), "" ,
    print >> csv, ""expDesc, "",
    print >> csv, ""numRecords, "",

    for key in self.__sortedVariableNames:
      print >> csv, ""%s, "" % key,
    for key in self.__sortedMetricsKeys:
      print >> csv, ""%s, "" % key,
    print >> csv",https://github.com/numenta/nupic/blob/5922fafffdccc8812e72b3324965ad2f7d4bbdad/src/nupic/swarming/permutations_runner.py#L1293-L1347
tfidf_python_100_1.0,convert json to csv,python,"def __init__(self, csv):
        InstrumentCSVResultsFileParser.__init__(self, csv)
        self._end_header = False
        self._resultsheader = []
        self._numline = 0",https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/exportimport/instruments/lachat/quickchem.py#L49-L53
tfidf_python_100_1.0,convert json to csv,python,"def __init__(self, csv):
        InstrumentCSVResultsFileParser.__init__(self, csv)
        self._end_header = False
        self._quantitationresultsheader = []
        self._numline = 0",https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/exportimport/instruments/shimadzu/gcms/qp2010se.py#L130-L134
tfidf_python_100_1.0,convert json to csv,python,"def __init__(self, csv, analysiskey):
        InstrumentCSVResultsFileParser.__init__(self, csv)
        self.analysiskey = analysiskey
        self.header = None",https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/exportimport/instruments/lifetechnologies/qubit/__init__.py#L32-L35
tfidf_python_100_1.0,convert json to csv,python,"def import_str(self, csv, params={}):
        """"""
        Imports a CSV string.

        https://canvas.instructure.com/doc/api/sis_imports.html#method.sis_imports_api.create
        """"""
        if not self._canvas_account_id:
            raise MissingAccountID()

        params[""import_type""] = SISImportModel.CSV_IMPORT_TYPE
        url = SIS_IMPORTS_API.format(
            self._canvas_account_id) + "".json{}"".format(self._params(params))
        headers = {""Content-Type"": ""text/csv""}

        return SISImportModel(data=self._post_resource(url, headers, csv))",https://github.com/uw-it-aca/uw-restclients-canvas/blob/9845faf33d49a8f06908efc22640c001116d6ea2/uw_canvas/sis_import.py#L18-L32
tfidf_python_100_1.0,convert json to csv,python,"def merge_csv_metadata(d, csvs):
    """"""
    Using the given metadata dictionary, retrieve CSV data from CSV files, and insert the CSV
    values into their respective metadata columns. Checks for both paleoData and chronData tables.

    :param dict d: Metadata
    :return dict: Modified metadata dictionary
    """"""
    logger_csvs.info(""enter merge_csv_metadata"")

    # Add CSV to paleoData
    if ""paleoData"" in d:
        d[""paleoData""] = _merge_csv_section(d[""paleoData""], ""paleo"", csvs)

    # Add CSV to chronData
    if ""chronData"" in d:
        d[""chronData""] = _merge_csv_section(d[""chronData""], ""chron"", csvs)

    logger_csvs.info(""exit merge_csv_metadata"")
    return d",https://github.com/nickmckay/LiPD-utilities/blob/5dab6bbeffc5effd68e3a6beaca6b76aa928e860/Python/lipd/csvs.py#L18-L37
tfidf_python_100_1.0,convert json to csv,python,"def convert(json_input, build_direction=""LEFT_TO_RIGHT"", table_attributes=None):
    """"""
    Converts JSON to HTML Table format.

    Parameters
    ----------
    json_input : dict
        JSON object to convert into HTML.
    build_direction : {""TOP_TO_BOTTOM"", ""LEFT_TO_RIGHT""}
        String denoting the build direction of the table. If ``""TOP_TO_BOTTOM""`` child
        objects will be appended below parents, i.e. in the subsequent row. If ``""LEFT_TO_RIGHT""``
        child objects will be appended to the right of parents, i.e. in the subsequent column.
        Default is ``""LEFT_TO_RIGHT""``.
    table_attributes : dict, optional
        Dictionary of ``(key, value)`` pairs describing attributes to add to the table. 
        Each attribute is added according to the template ``key=""value"". For example, 
        the table ``{ ""border"" : 1 }`` modifies the generated table tags to include 
        ``border=""1""`` as an attribute. The generated opening tag would look like 
        ``<table border=""1"">``. Default is ``None``.

    Returns
    -------
    str
        String of converted HTML.

    An example usage is shown below:

    >>> json_object = {""key"" : ""value""}
    >>> build_direction = ""TOP_TO_BOTTOM""
    >>> table_attributes = {""border"" : 1}
    >>> html = convert(json_object, build_direction=build_direction, table_attributes=table_attributes)
    >>> print(html)
    ""<table border=""1""><tr><th>key</th><td>value</td></tr></table>""

    """"""
    json_converter = JsonConverter(build_direction=build_direction, table_attributes=table_attributes)
    return json_converter.convert(json_input)",https://github.com/latture/json2table/blob/8bd1363f54ee4fd608ffb7677761526184a9da83/json2table/json2table.py#L12-L48
tfidf_python_100_1.0,pretty print json,python,"def to_hex_words(self, pretty=False):
        hex_words = (
            _convert_int_to_hex_word(self.mode, pretty=pretty),
            _convert_int_to_hex_word(self.pid, pretty=pretty),
            )
        return hex_words",https://github.com/franciscoruiz/python-elm/blob/cdcecfc363b1eb25d21659bc14cf68a4a19970b6/elm327/obd.py#L106-L111
tfidf_python_100_1.0,pretty print json,python,"def json(value, pretty=True):
    """"""
    convert value to JSON
    :param value:
    :param pretty:
    :return:
    """"""
    if not _Duration:
        _late_import()
    return _json_encoder(value, pretty=pretty)",https://github.com/klahnakoski/mo-logs/blob/0971277ac9caf28a755b766b70621916957d4fea/mo_logs/strings.py#L171-L180
tfidf_python_100_1.0,pretty print json,python,"def pp_update(path):
    pretty = os.path.splitext(path)[0]
    if pretty.startswith('updates/'):
        pretty = pretty[8:]
    return pretty",https://github.com/yac/rdoupdate/blob/63e83f52e97474123cc9ed08077a044496db55e4/rdoupdate/core.py#L16-L20
tfidf_python_100_1.0,pretty print json,python,"def git_pretty():
    """"""returns a pretty summary of the commit or unkown if not in git repo""""""
    if git_repo() is None:
        return ""unknown""
    pretty = subprocess.check_output(
        [""git"", ""log"", ""--pretty=format:%h %s"", ""-n"", ""1""])
    pretty = pretty.decode(""utf-8"")
    pretty = pretty.strip()
    return pretty",https://github.com/richardliaw/track/blob/7ac42ea34e5c1d7bb92fd813e938835a06a63fc7/track/autodetect.py#L57-L65
tfidf_python_100_1.0,pretty print json,python,"def pretty_tree(self):
        pretty = deepcopy(self.tree)
        for field in pretty:
            self._purge_empty_dicts(pretty[field])
        return pretty",https://github.com/pypa/pipenv/blob/cae8d76c210b9777e90aab76e9c4b0e53bb19cde/pipenv/vendor/cerberus/errors.py#L495-L499
tfidf_python_100_1.0,pretty print json,python,"def dump_string(self, html_string, filename, pretty=False):
        tempy_trees = self.from_string(html_string)
        self.dump(tempy_trees, filename, pretty=pretty)",https://github.com/Hrabal/TemPy/blob/7d229b73e2ce3ccbb8254deae05c1f758f626ed6/tempy/t.py#L109-L111
tfidf_python_100_1.0,pretty print json,python,"def print_element(element):
    """"""
    Pretty- print an lxml.etree element.

    Parameters
    ------------
    element : etree element
    """"""
    pretty = etree.tostring(
        element, pretty_print=True).decode('utf-8')
    print(pretty)
    return pretty",https://github.com/mikedh/trimesh/blob/25e059bf6d4caa74f62ffd58ce4f61a90ee4e518/trimesh/exchange/xml_based.py#L425-L436
tfidf_python_100_1.0,pretty print json,python,"def render(self, *args, **kwargs):
        pretty = kwargs.pop(""pretty"", False)
        return ""<!DOCTYPE %s>%s"" % (DOCTYPES[self.type_code], ""\n"" if pretty else """")",https://github.com/Hrabal/TemPy/blob/7d229b73e2ce3ccbb8254deae05c1f758f626ed6/tempy/tags.py#L50-L52
tfidf_python_100_1.0,pretty print json,python,"def to_json(data, pretty):
    """"""
    Converts object to JSON formatted string with typeToken adapter
    :param data: A dictionary to convert to JSON string
    :param pretty: A boolean deciding whether or not to pretty format the JSON string
    :return: The JSON string
    """"""
    if pretty:
        return json.dumps(data, sort_keys=True, indent=4, separators=(',', ': '))
    return json.dumps(data)",https://github.com/draperunner/fjlc/blob/d2cc8cf1244984e7caf0bf95b11ed1677a94c994/fjlc/utils/json_utils.py#L6-L15
tfidf_python_100_1.0,pretty print json,python,"def to_xml(self, pretty=False):
		self._update_document()
		return toxml(self.root, pretty=pretty)",https://github.com/HearthSim/python-hsreplay/blob/ec03a18a75ae4c1e0facc583a7213a4c5b7f99ff/hsreplay/document.py#L74-L76
tfidf_python_100_1.0,pretty print json,python,"def serialize_to_xml_str(obj_pyxb, pretty=True, strip_prolog=False, xslt_url=None):
    """"""Serialize PyXB object to pretty printed XML ``str`` for display.

    Args:
      obj_pyxb: PyXB object
        PyXB object to serialize.

      pretty: bool
        False: Disable pretty print formatting. XML will not have line breaks.

      strip_prolog:
        True: remove any XML prolog (e.g., ``<?xml version=""1.0"" encoding=""utf-8""?>``),
        from the resulting XML doc.

      xslt_url: str
        If specified, add a processing instruction to the XML doc that specifies the
        download location for an XSLT stylesheet.

    Returns:
      str: Pretty printed XML document

    """"""
    return serialize_gen(obj_pyxb, None, pretty, strip_prolog, xslt_url)",https://github.com/DataONEorg/d1_python/blob/3ac4d4f3ca052d3e8641a6a329cab526c8ddcb0d/lib_common/src/d1_common/xml.py#L172-L194
tfidf_python_100_1.0,pretty print json,python,"def json_pretty_print(s):
    '''pretty print JSON'''
    s = json.loads(s)
    return json.dumps(s,
                      sort_keys=True,
                      indent=4,
                      separators=(',', ': '))",https://github.com/mosesschwartz/scrypture/blob/d51eb0c9835a5122a655078268185ce8ab9ec86a/scrypture/demo_scripts/Utils/json_pretty_print.py#L8-L14
tfidf_python_100_1.0,pretty print json,python,"def _to_string(self):
        bytes_value = self._get_calculated_value(self.value)
        return _bytes_to_hex(bytes_value, pretty=True, hex_per_line=0)",https://github.com/jborean93/smbprotocol/blob/d8eb00fbc824f97d0f4946e3f768c5e6c723499a/smbprotocol/structure.py#L399-L401
tfidf_python_100_1.0,pretty print json,python,"def value2json(obj, pretty=False, sort_keys=False, keep_whitespace=True):
    """"""
    :param obj:  THE VALUE TO TURN INTO JSON
    :param pretty: True TO MAKE A MULTI-LINE PRETTY VERSION
    :param sort_keys: True TO SORT KEYS
    :param keep_whitespace: False TO strip() THE WHITESPACE IN THE VALUES
    :return:
    """"""
    if FIND_LOOPS:
        obj = scrub(obj, scrub_text=_keep_whitespace if keep_whitespace else _trim_whitespace())
    try:
        json = json_encoder(obj, pretty=pretty)
        if json == None:
            Log.note(str(type(obj)) + "" is not valid{{type}}JSON"", type="" (pretty) "" if pretty else "" "")
            Log.error(""Not valid JSON: "" + str(obj) + "" of type "" + str(type(obj)))
        return json
    except Exception as e:
        e = Except.wrap(e)
        try:
            json = pypy_json_encode(obj)
            return json
        except Exception:
            pass
        Log.error(""Can not encode into JSON: {{value}}"", value=text_type(repr(obj)), cause=e)",https://github.com/klahnakoski/mo-json/blob/0d44d6a7e37f0ea50e583c30c2cbc42488d5de7f/mo_json/__init__.py#L231-L254
tfidf_python_100_1.0,pretty print json,python,"def __init__(self, seq, width, pretty):
        self.obj = seq
        self.width = width
        self.pretty = pretty
        self.indentation = pretty.indentation
        self.group = pretty.group_stack[-1]
        self.group.breakables.append(self)",https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/lib/pretty.py#L408-L414
tfidf_python_100_1.0,pretty print json,python,"def to_json(self, *, include_keys=None, exclude_keys=None, use_default_excludes=True,
              pretty=False):
    """"""Converts the response from to_dict to a JSON string. If pretty is True then newlines,
    indentation and key sorting are used.
    """"""
    return to_json(
      self.to_dict(
        include_keys=include_keys,
        exclude_keys=exclude_keys,
        use_default_excludes=use_default_excludes),
      pretty=pretty)",https://github.com/treycucco/bidon/blob/d9f24596841d0e69e8ac70a1d1a1deecea95e340/bidon/db/model/model_base.py#L138-L148
tfidf_python_100_1.0,pretty print json,python,"def pretty_str(self, indent=0):
        """"""Return a human-readable string representation of this object.

        Kwargs:
            indent (int): The amount of spaces to use as indentation.
        """"""
        spaces = ' ' * indent
        pretty = spaces + 'class ' + self.name
        if self.superclasses:
            superclasses = ', '.join(self.superclasses)
            pretty += '(' + superclasses + ')'
        pretty += ':\n'
        if self.members:
            pretty += '\n\n'.join(
                    c.pretty_str(indent + 2)
                    for c in self.members
            )
        else:
            pretty += spaces + '  [declaration]'
        return pretty",https://github.com/git-afsantos/bonsai/blob/aa5af3f535b3b506bfc95c107c501fc9c4bcd072/bonsai/model.py#L409-L428
tfidf_python_100_1.0,pretty print json,python,"def provider_action(args):

    client = authenticate()

    if args.action == ""list-providers"":
        pretty = args.pretty
        list_providers(client,pretty)",https://github.com/mistio/mist.client/blob/bc190af2cba358fa556a69b205c12a77a34eb2a8/src/mistcommand/helpers/providers.py#L30-L36
tfidf_python_100_1.0,pretty print json,python,"def data_json(self, pretty=False):
        """"""Returns the data as a valid JSON string.""""""
        if pretty:
            return json.dumps(self.data, sort_keys=True, indent=4, separators=(',', ': '))
        else:
            return json.dumps(self.data)",https://github.com/aparsons/threadfix_api/blob/76fd1bd26e9ac863636112cd30d733543807ff7d/threadfix_api/threadfix.py#L409-L414
tfidf_python_100_1.0,pretty print json,python,"def output(s):
    """"""
    Parse, transform, and pretty print
    the result
    """"""
    p = Parser()
    t = ExpressionsTransformer()

    ast = p.parse(s)
    logging.debug(ast.pretty())
    print(ast.pretty())
    d = t.transform(ast)
    print(json.dumps(d, indent=4))
    return d",https://github.com/geographika/mappyfile/blob/aecbc5e66ec06896bc4c5db41313503468829d00/docs/examples/parsing.py#L117-L130
tfidf_python_100_1.0,replace in file,python,"def write_row(file, data):
    file.write(
        "","".join(
            str(v).replace(""\"""", ""\""\"""").replace(""\n"", """").replace(""\r"", """")
            for v in data
        ),
    )
    file.write(""\n"")",https://github.com/mordred-descriptor/mordred/blob/2848b088fd7b6735590242b5e22573babc724f10/mordred/__main__.py#L184-L191
tfidf_python_100_1.0,replace in file,python,"def windows_clean(a):
        def clean_str(s):
            s = s.replace('%', '')
            s = s.replace(' ', '\ ')
            s = s.replace('/', '\/')
            return s
        return [clean_str(s) for s in a]",https://github.com/egh/ledger-autosync/blob/7a303f3a693261d10f677c01fb08f35c105a1e1b/ledgerautosync/ledgerwrap.py#L54-L60
tfidf_python_100_1.0,replace in file,python,"def dot_escape(s):
    return s.replace('\n', r'\n')\
            .replace('\\', '\\\\')\
            .replace('""', r'\""')\
            .replace('|', r'\|')\
            .replace('{', r'\{')\
            .replace('}', r'\}')\
            .replace('>', r'\>')\
            .replace('<', r'\<')\
            .replace('?', r'\?')",https://github.com/textX/textX/blob/5796ac38116ad86584392dbecdbf923ede746361/textx/export.py#L90-L99
tfidf_python_100_1.0,replace in file,python,"def _unescape(s):
    s = s.replace('\\\n', '')
    return s.replace('\\""', '""').replace(r'\n', '\n').replace(r'\r', '\r')",https://github.com/chrisballinger/python-localizable/blob/15d3bf2466d0de1a826d3f0ff1f365b0c1910f56/localizable.py#L24-L26
tfidf_python_100_1.0,replace in file,python,"def convert_monomial_to_string(monomial):
    monomial_str = ('%s' % monomial)
    monomial_str = monomial_str.replace('Dagger(', '')
    monomial_str = monomial_str.replace(')', 'T')
    monomial_str = monomial_str.replace('**', '^')
    return monomial_str",https://github.com/peterwittek/ncpol2sdpa/blob/bce75d524d0b9d0093f32e3a0a5611f8589351a7/ncpol2sdpa/nc_utils.py#L554-L559
tfidf_python_100_1.0,replace in file,python,"def escape_special_chars(s):
    for a, b in [('\\', '\\\\'), ('\t', '\\t'), ('\n', '\\n'), ('\r', '\\r')]:
        s = s.replace(a, b)
    return s",https://github.com/solidsnack/tsv/blob/2d3c0f45477c8ffbed5cd61050855c4e4617e6be/tsv.py#L146-L149
tfidf_python_100_1.0,replace in file,python,"def backslashEscape(s):
    s = s.replace(""\\"", ""\\\\"").replace("";"", ""\;"").replace("","", ""\,"")
    return s.replace(""\r\n"", ""\\n"").replace(""\n"", ""\\n"").replace(""\r"", ""\\n"")",https://github.com/eventable/vobject/blob/498555a553155ea9b26aace93332ae79365ecb31/vobject/base.py#L1218-L1220
tfidf_python_100_1.0,replace in file,python,"def escape_str(s):
    s = s.replace('\\', r'\\')
    s = s.replace('""', r'\""')
    s = s.replace('\t', r'\t')
    return s",https://github.com/mbj4668/pyang/blob/f2a5cc3142162e5b9ee4e18d154568d939ff63dd/pyang/translators/yang.py#L317-L321
tfidf_python_100_1.0,replace in file,python,"def _normalize_string(self, s):
        s = s.replace('â', '""')
        s = s.replace('â', '""')
        return s",https://github.com/svenkreiss/html5validator/blob/6c09f0e230dc9ba6d76cd314b69294495949fdfc/html5validator/validator.py#L81-L84
tfidf_python_100_1.0,replace in file,python,"def _remove_newline(self, x):
        x = x.replace('\n', ' ')
        x = x.replace('\n\n', ' ')
        return x",https://github.com/neptune-ml/steppy-toolkit/blob/bf3f48cfcc65dffc46e65ddd5d6cfec6bb9f9132/toolkit/preprocessing/text.py#L120-L123
tfidf_python_100_1.0,replace in file,python,"def __cyrillic_to_roman(self, word):
        """"""
        Transliterate a Russian word into the Roman alphabet.

        A Russian word whose letters consist of the Cyrillic
        alphabet are transliterated into the Roman alphabet
        in order to ease the forthcoming stemming process.

        :param word: The word that is transliterated.
        :type word: unicode
        :return: the transliterated word.
        :rtype: unicode
        :note: This helper method is invoked by the stem method of the subclass
               RussianStemmer. It is not to be invoked directly!

        """"""
        word = (word.replace(""\u0410"", ""a"").replace(""\u0430"", ""a"")
                    .replace(""\u0411"", ""b"").replace(""\u0431"", ""b"")
                    .replace(""\u0412"", ""v"").replace(""\u0432"", ""v"")
                    .replace(""\u0413"", ""g"").replace(""\u0433"", ""g"")
                    .replace(""\u0414"", ""d"").replace(""\u0434"", ""d"")
                    .replace(""\u0415"", ""e"").replace(""\u0435"", ""e"")
                    .replace(""\u0401"", ""e"").replace(""\u0451"", ""e"")
                    .replace(""\u0416"", ""zh"").replace(""\u0436"", ""zh"")
                    .replace(""\u0417"", ""z"").replace(""\u0437"", ""z"")
                    .replace(""\u0418"", ""i"").replace(""\u0438"", ""i"")
                    .replace(""\u0419"", ""i`"").replace(""\u0439"", ""i`"")
                    .replace(""\u041A"", ""k"").replace(""\u043A"", ""k"")
                    .replace(""\u041B"", ""l"").replace(""\u043B"", ""l"")
                    .replace(""\u041C"", ""m"").replace(""\u043C"", ""m"")
                    .replace(""\u041D"", ""n"").replace(""\u043D"", ""n"")
                    .replace(""\u041E"", ""o"").replace(""\u043E"", ""o"")
                    .replace(""\u041F"", ""p"").replace(""\u043F"", ""p"")
                    .replace(""\u0420"", ""r"").replace(""\u0440"", ""r"")
                    .replace(""\u0421"", ""s"").replace(""\u0441"", ""s"")
                    .replace(""\u0422"", ""t"").replace(""\u0442"", ""t"")
                    .replace(""\u0423"", ""u"").replace(""\u0443"", ""u"")
                    .replace(""\u0424"", ""f"").replace(""\u0444"", ""f"")
                    .replace(""\u0425"", ""kh"").replace(""\u0445"", ""kh"")
                    .replace(""\u0426"", ""t^s"").replace(""\u0446"", ""t^s"")
                    .replace(""\u0427"", ""ch"").replace(""\u0447"", ""ch"")
                    .replace(""\u0428"", ""sh"").replace(""\u0448"", ""sh"")
                    .replace(""\u0429"", ""shch"").replace(""\u0449"", ""shch"")
                    .replace(""\u042A"", ""''"").replace(""\u044A"", ""''"")
                    .replace(""\u042B"", ""y"").replace(""\u044B"", ""y"")
                    .replace(""\u042C"", ""'"").replace(""\u044C"", ""'"")
                    .replace(""\u042D"", ""e`"").replace(""\u044D"", ""e`"")
                    .replace(""\u042E"", ""i^u"").replace(""\u044E"", ""i^u"")
                    .replace(""\u042F"", ""i^a"").replace(""\u044F"", ""i^a""))


        return word",https://github.com/summanlp/textrank/blob/6844bbe8c4b2b468020ae0dfd6574a743f9ad442/summa/preprocessing/snowball.py#L3183-L3234
tfidf_python_100_1.0,replace in file,python,"def _process_json_data(jdata):
    fina = Finance()
    fina.tuition_accbalance = jdata[""AccountBalance""].replace(""$"", """")
    if ""PCEAccountBalance"" in jdata:
        fina.pce_accbalance = jdata[""PCEAccountBalance""].replace(""$"", """")
    return fina",https://github.com/uw-it-aca/uw-restclients-sws/blob/4d36776dcca36855fc15c1b8fe7650ae045194cf/uw_sws/financial.py#L19-L24
tfidf_python_100_1.0,replace in file,python,"def clean_id(id):
        return id.replace('/', '_').\
            replace('$', '_').\
            replace(' ', '_').\
            replace('@', '_').\
            replace('*', '_').\
            replace('+', '_').\
            replace('[', '_').\
            replace(']', '_')",https://github.com/egh/ledger-autosync/blob/7a303f3a693261d10f677c01fb08f35c105a1e1b/ledgerautosync/converter.py#L218-L226
tfidf_python_100_1.0,replace in file,python,"def fix_text(self, btext, bhref):
        if not btext:
            return bhref

        return btext.replace(b'\n', b' ').replace(b'\r', b' ')",https://github.com/tgbugs/pyontutils/blob/3d913db29c177db39151592909a4f56170ef8b35/nifstd/nifstd_tools/docs.py#L118-L122
tfidf_python_100_1.0,replace in file,python,"def _normalise_zpl(zpl):
        zpl = zpl.replace('\n', '').replace('\r', '')
        zpl = zpl.replace('^', '\n^').replace('~', '\n~')
        return zpl.split('\n')",https://github.com/kylemacfarlane/zplgrf/blob/aacad3e69c7abe04dbfe9ce3c5cf6ac2a1ab67b3/src/zplgrf/__init__.py#L175-L178
tfidf_python_100_1.0,replace in file,python,"def unescapeshelloperators(s):
    if sys.version < '3':
        s = s.replace(b'%PIPE%',b'|')
        s = s.replace(b'%OUT%',b'>')
        s = s.replace(b'%AMP%',b'&')
        s = s.replace(b'%EXCL%',b'!')
        s = s.replace(b'%IN%',b'<')
    else:
        s = s.replace('%PIPE%','|')
        s = s.replace('%OUT%','>')
        s = s.replace('%AMP%','&')
        s = s.replace('%EXCL%','!')
        s = s.replace('%IN%','<')
    return s",https://github.com/proycon/clam/blob/09d15cfc26d7cbe0f5976cdd5424dc446d10dbf3/clam/common/data.py#L2464-L2477
tfidf_python_100_1.0,replace in file,python,"def configuration_to_faacets(A_configuration, B_configuration):
    a = str(A_configuration).replace('[', '(').replace(']', ')').replace(',',
                                                                         '')
    b = str(B_configuration).replace('[', '(').replace(']', ')').replace(',',
                                                                         '')
    return '[' + a + ' ' + b + ']'",https://github.com/peterwittek/ncpol2sdpa/blob/bce75d524d0b9d0093f32e3a0a5611f8589351a7/ncpol2sdpa/faacets_relaxation.py#L24-L29
tfidf_python_100_1.0,replace in file,python,"def __roman_to_cyrillic(self, word):
        """"""
        Transliterate a Russian word back into the Cyrillic alphabet.

        A Russian word formerly transliterated into the Roman alphabet
        in order to ease the stemming process, is transliterated back
        into the Cyrillic alphabet, its original form.

        :param word: The word that is transliterated.
        :type word: str or unicode
        :return: word, the transliterated word.
        :rtype: unicode
        :note: This helper method is invoked by the stem method of the subclass
               RussianStemmer. It is not to be invoked directly!

        """"""
        word = (word.replace(""i^u"", ""\u044E"").replace(""i^a"", ""\u044F"")
                    .replace(""shch"", ""\u0449"").replace(""kh"", ""\u0445"")
                    .replace(""t^s"", ""\u0446"").replace(""ch"", ""\u0447"")
                    .replace(""e`"", ""\u044D"").replace(""i`"", ""\u0439"")
                    .replace(""sh"", ""\u0448"").replace(""k"", ""\u043A"")
                    .replace(""e"", ""\u0435"").replace(""zh"", ""\u0436"")
                    .replace(""a"", ""\u0430"").replace(""b"", ""\u0431"")
                    .replace(""v"", ""\u0432"").replace(""g"", ""\u0433"")
                    .replace(""d"", ""\u0434"").replace(""e"", ""\u0435"")
                    .replace(""z"", ""\u0437"").replace(""i"", ""\u0438"")
                    .replace(""l"", ""\u043B"").replace(""m"", ""\u043C"")
                    .replace(""n"", ""\u043D"").replace(""o"", ""\u043E"")
                    .replace(""p"", ""\u043F"").replace(""r"", ""\u0440"")
                    .replace(""s"", ""\u0441"").replace(""t"", ""\u0442"")
                    .replace(""u"", ""\u0443"").replace(""f"", ""\u0444"")
                    .replace(""''"", ""\u044A"").replace(""y"", ""\u044B"")
                    .replace(""'"", ""\u044C""))


        return word",https://github.com/summanlp/textrank/blob/6844bbe8c4b2b468020ae0dfd6574a743f9ad442/summa/preprocessing/snowball.py#L3238-L3273
tfidf_python_100_1.0,replace in file,python,"def sluggify(astr):
    return (astr.replace('.', '-').replace(""["", ""__"")
                .replace(""]"", ""__"").replace("" "", ""-"")
                .replace(""#"", ""_"").replace(""/"", ""_"")
                .replace(""+"", ""_"")
                .replace(""'"", ""_"").replace('""', ""_"")
                .replace(""?"", ""_"").replace('(', ""_"").replace(')', ""_""))",https://github.com/RealTimeWeb/datasets/blob/2fe5befd251c783744d000bd4763e277616a152f/builder/languages/build_python.py#L175-L181
tfidf_python_100_1.0,replace in file,python,"def _remove_cmd_chars(s):
    if isinstance(s, str):
        return s.replace(""'"", '_').replace('""', '_').replace('\r\n', ' ').replace('\n', ' ')
    return s",https://github.com/Azure/azure-cli-extensions/blob/3d4854205b0f0d882f688cfa12383d14506c2e35/src/alias/azext_alias/telemetry.py#L164-L167
tfidf_python_100_1.0,k means clustering,python,"def _keep_existing_clusters(self, cluster_ids):
        return [c for c in cluster_ids
                if c in self.clustering.cluster_ids]",https://github.com/kwikteam/phy/blob/7e9313dc364304b7d2bd03b92938347343703003/phy/cluster/supervisor.py#L332-L334
tfidf_python_100_1.0,k means clustering,python,"def __init__(self, original_network, clustered_network, clustering,
                 skip=()):
        """"""
        :param original_network: Initial (unclustered) network structure
        :param clustered_network: Clustered network used for the optimization
        :param clustering: The clustering object as returned by
        `pypsa.networkclustering.get_clustering_from_busmap`
        """"""
        self.original_network = original_network
        self.clustered_network = clustered_network
        self.clustering = clustering

        self.buses = pd.merge(original_network.buses,
                              clustering.busmap.to_frame(name='cluster'),
                              left_index=True, right_index=True)

        self.skip = skip

        self.idx_prefix = '_'",https://github.com/openego/eTraGo/blob/2a8fc6d4368d0e9abe6fe0d0c39baf66ea0126b9/etrago/cluster/disaggregation.py#L15-L33
tfidf_python_100_1.0,k means clustering,python,"def __init__(self,
                 keywordCount = 30,
                 maxEventsToCluster = 10000,
                 returnInfo = ReturnInfo()):
        """"""
        return hierarchical clustering of events into smaller clusters. 2-means clustering is applied on each node in the tree
        @param keywordCount: number of keywords to report in each of the clusters (at most 100)
        @param maxEventsToCluster: try to cluster at most this number of events (at most 10000)
        @param returnInfo: what details about the concepts should be included in the returned information
        """"""
        assert keywordCount <= 100
        assert maxEventsToCluster <= 10000
        self.resultType = ""eventClusters""
        self.eventClustersKeywordCount = keywordCount
        self.eventClustersMaxEventsToCluster = maxEventsToCluster
        self.__dict__.update(returnInfo.getParams(""eventClusters""))",https://github.com/EventRegistry/event-registry-python/blob/534d20b616de02f5e1cd73665a02d189645dbeb6/eventregistry/QueryEvents.py#L564-L579
tfidf_python_100_1.0,k means clustering,python,"def reset(self):
        """"""Reset the clustering to the original clustering.

        All changes are lost.

        """"""
        self._undo_stack.clear()
        self._spike_clusters = self._spike_clusters_base
        self._new_cluster_id = self._new_cluster_id_0",https://github.com/kwikteam/phy/blob/7e9313dc364304b7d2bd03b92938347343703003/phy/cluster/clustering.py#L178-L186
tfidf_python_100_1.0,k means clustering,python,"def clustering_fields(self, value):
        """"""Union[List[str], None]: Fields defining clustering for the table

        (Defaults to :data:`None`).
        """"""
        if value is not None:
            prop = self._properties.setdefault(""clustering"", {})
            prop[""fields""] = value
        else:
            if ""clustering"" in self._properties:
                del self._properties[""clustering""]",https://github.com/googleapis/google-cloud-python/blob/85e80125a59cb10f8cb105f25ecc099e4b940b50/bigquery/google/cloud/bigquery/table.py#L664-L674
tfidf_python_100_1.0,k means clustering,python,"def _cluster_by_taxa(self, linkage=""average""):
        dist_matrix = euclidean_distances(self._results.T).round(6)
        clustering = hierarchy.linkage(squareform(dist_matrix), method=linkage)
        scipy_tree = hierarchy.dendrogram(clustering, no_plot=True)
        ids_in_order = [self._results.T.index[int(x)] for x in scipy_tree[""ivl""]]
        labels_in_order = [""{} ({})"".format(self.taxonomy[""name""][t], t) for t in ids_in_order]

        return {
            ""dist_matrix"": dist_matrix,
            ""clustering"": clustering,
            ""scipy_tree"": scipy_tree,
            ""ids_in_order"": ids_in_order,
            ""labels_in_order"": labels_in_order,
        }",https://github.com/onecodex/onecodex/blob/326a0a1af140e3a57ccf31c3c9c5e17a5775c13d/onecodex/viz/_distance.py#L61-L74
tfidf_python_100_1.0,k means clustering,python,"def optimal_clustering(df, patch, method='kmeans', statistic='gap', max_K=5):
    if len(patch) == 1:
        return [patch]

    if statistic == 'db':
        if method == 'kmeans':
            if len(patch) <= 5:
                K_max = 2
            else:
                K_max = min(len(patch) / 2, max_K)
            clustering = {}
            db_index = []
            X = df.ix[patch, :]
            for k in range(2, K_max + 1):
                kmeans = cluster.KMeans(n_clusters=k).fit(X)
                clustering[k] = pd.DataFrame(kmeans.predict(X), index=patch)
                dist_mu = squareform(pdist(kmeans.cluster_centers_))
                sigma = []
                for i in range(k):
                    points_in_cluster = clustering[k][clustering[k][0] == i].index
                    sigma.append(sqrt(X.ix[points_in_cluster, :].var(axis=0).sum()))
                db_index.append(davies_bouldin(dist_mu, np.array(sigma)))
            db_index = np.array(db_index)
            k_optimal = np.argmin(db_index) + 2
            return [list(clustering[k_optimal][clustering[k_optimal][0] == i].index) for i in range(k_optimal)]

        elif method == 'agglomerative':
            if len(patch) <= 5:
                K_max = 2
            else:
                K_max = min(len(patch) / 2, max_K)
            clustering = {}
            db_index = []
            X = df.ix[patch, :]
            for k in range(2, K_max + 1):
                agglomerative = cluster.AgglomerativeClustering(n_clusters=k, linkage='average').fit(X)
                clustering[k] = pd.DataFrame(agglomerative.fit_predict(X), index=patch)
                tmp = [list(clustering[k][clustering[k][0] == i].index) for i in range(k)]
                centers = np.array([np.mean(X.ix[c, :], axis=0) for c in tmp])
                dist_mu = squareform(pdist(centers))
                sigma = []
                for i in range(k):
                    points_in_cluster = clustering[k][clustering[k][0] == i].index
                    sigma.append(sqrt(X.ix[points_in_cluster, :].var(axis=0).sum()))
                db_index.append(davies_bouldin(dist_mu, np.array(sigma)))
            db_index = np.array(db_index)
            k_optimal = np.argmin(db_index) + 2
            return [list(clustering[k_optimal][clustering[k_optimal][0] == i].index) for i in range(k_optimal)]

    elif statistic == 'gap':
        X = np.array(df.ix[patch, :])
        if method == 'kmeans':
            f = cluster.KMeans
        gaps = gap(X, ks=range(1, min(max_K, len(patch))), method=f)
        k_optimal = list(gaps).index(max(gaps))+1
        clustering = pd.DataFrame(f(n_clusters=k_optimal).fit_predict(X), index=patch)
        return [list(clustering[clustering[0] == i].index) for i in range(k_optimal)]

    else:
        raise 'error: only db and gat statistics are supported'",https://github.com/szairis/sakmapper/blob/ac462fd2674e6aa1aa3b209222d8ac4e9268a790/sakmapper/network.py#L111-L170
tfidf_python_100_1.0,k means clustering,python,"def var(self, means=None):
        # 'pack' means, and check if it makes sence
        if means is not None:
            means = self._pack(means)

        def var(means):
            return self.subspace.var(means=means)
        if self.delay:
            # if means is None:
            # return self.subspace.mean().then(var).then(self._unpack)
            # else:
            return var(means).then(self._unpack)
        else:
            # if means is None:
            # means = self.subspace.mean()
            # logger.debug(""means: %r"", means)
            return self._unpack(var(means=means))",https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-core/vaex/legacy.py#L355-L371
tfidf_python_100_1.0,k means clustering,python,"def means_(self):
        two_N = 2 * (self.n_observations_ - self.lag_time * self.n_sequences_)
        means = (self._sum_0_to_TminusTau + self._sum_tau_to_T) / float(two_N)
        return means",https://github.com/msmbuilder/msmbuilder/blob/556a93a170782f47be53f4a1e9d740fb1c8272b3/msmbuilder/decomposition/tica.py#L229-L232
tfidf_python_100_1.0,k means clustering,python,"def search(self, word):
        means = self._search_from_db(word)
        if not means:
            means = self._search_from_youdao(word)
        return means",https://github.com/ubear/cha/blob/f4f077a4d5fc8452a4d7d1cf3cbbf3360fe59254/cc/c.py#L34-L38
tfidf_python_100_1.0,k means clustering,python,"def _mean_image_subtraction(image, means, num_channels):
  """"""Subtracts the given means from each image channel.

  For example:
    means = [123.68, 116.779, 103.939]
    image = _mean_image_subtraction(image, means)

  Note that the rank of `image` must be known.

  Args:
    image: a tensor of size [height, width, C].
    means: a C-vector of values to subtract from each channel.
    num_channels: number of color channels in the image that will be distorted.

  Returns:
    the centered image.

  Raises:
    ValueError: If the rank of `image` is unknown, if `image` has a rank other
      than three or if the number of channels in `image` doesn't match the
      number of values in `means`.
  """"""
  if image.get_shape().ndims != 3:
    raise ValueError('Input must be of size [height, width, C>0]')

  if len(means) != num_channels:
    raise ValueError('len(means) must match the number of channels')

  mlperf_log.resnet_print(key=mlperf_log.INPUT_MEAN_SUBTRACTION,
                          value=means)

  # We have a 1-D tensor of means; convert to 3-D.
  means = tf.expand_dims(tf.expand_dims(means, 0), 0)

  return image - means",https://github.com/mlperf/training/blob/1c6ae725a81d15437a2b2df05cac0673fde5c3a4/image_classification/tensorflow/official/resnet/imagenet_preprocessing.py#L144-L178
tfidf_python_100_1.0,k means clustering,python,"def var(self, means=None):
        return self.dataset.server._call_subspace(""var"", self, means=means)",https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-core/vaex/remote.py#L474-L475
tfidf_python_100_1.0,k means clustering,python,"def __init__(self,
                 host=DEFAULT_HOST,
                 port=DEFAULT_PORT,
                 db=DEFAULT_DB,
                 on_close=None,
                 clustering=False,
                 auto_connect=True):
        super(RedisClient, self).__init__(
            [{
                'host': host,
                'port': port,
                'db': db
            }],
            on_close,
            clustering=clustering,
            auto_connect=auto_connect)",https://github.com/gmr/tredis/blob/2e91c6a58a35460be0525c51ac6a98fde3b506ad/tredis/client.py#L705-L720
tfidf_python_100_1.0,k means clustering,python,"def extensions():
    from Cython.Build import cythonize
    from numpy import get_include
    np_inc = get_include()
    extensions = [
          Extension('bhmm.hidden.impl_c.hidden',
                    sources = ['./bhmm/hidden/impl_c/hidden.pyx',
                               './bhmm/hidden/impl_c/_hidden.c'],
                    include_dirs = ['/bhmm/hidden/impl_c/', np_inc]),
          Extension('bhmm.output_models.impl_c.discrete',
                    sources = ['./bhmm/output_models/impl_c/discrete.pyx',
                               './bhmm/output_models/impl_c/_discrete.c'],
                    include_dirs = ['/bhmm/output_models/impl_c/', np_inc]),
          Extension('bhmm.output_models.impl_c.gaussian',
                    sources = ['./bhmm/output_models/impl_c/gaussian.pyx',
                               './bhmm/output_models/impl_c/_gaussian.c'],
                    include_dirs = ['/bhmm/output_models/impl_c/', np_inc]),
          Extension('bhmm._external.clustering.kmeans_clustering_64',
                    sources=['./bhmm/_external/clustering/src/clustering.c',
                             './bhmm/_external/clustering/src/kmeans.c'],
                    include_dirs=['./bhmm/_external/clustering/include',
                                  np_inc],
                    extra_compile_args=['-std=c99','-O3', '-DCLUSTERING_64']),
        Extension('bhmm._external.clustering.kmeans_clustering_32',
                    sources=['./bhmm/_external/clustering/src/clustering.c',
                             './bhmm/_external/clustering/src/kmeans.c'],
                    include_dirs=['./bhmm/_external/clustering/include',
                                  np_inc],
                    extra_compile_args=['-std=c99','-O3']),
          ]

    return cythonize(extensions)",https://github.com/bhmm/bhmm/blob/9804d18c2ddb684fb4d90b544cc209617a89ca9a/setup.py#L53-L84
tfidf_python_100_1.0,k means clustering,python,"def hyperplanes(means, stds, n_planes):
    if len(means) != len(stds):
        raise ValueError('means and stds must have the same length')

    n_features = len(means)
    a = np.random.normal(means, stds, (n_planes, n_features))
    b = np.random.normal(means, stds, (n_planes, n_features))
    plane_vectors = a - b
    return plane_vectors",https://github.com/JohnVinyard/zounds/blob/337b3f98753d09eaab1c72dcd37bb852a3fa5ac6/zounds/learn/functional.py#L4-L12
tfidf_python_100_1.0,k means clustering,python,"def get_labels(obj):
        """"""
        Retrieve the labels of a clustering.rst object

        :param obj: the clustering.rst object
        :return: the resulting labels
        """"""
        if Clustering.is_pyclustering_instance(obj.model):
            return obj._labels_from_pyclusters
        else:
            return obj.model.labels_",https://github.com/DEIB-GECO/PyGMQL/blob/e58b2f9402a86056dcda484a32e3de0bb06ed991/gmql/ml/algorithms/clustering.py#L266-L276
tfidf_python_100_1.0,k means clustering,python,"def limits_sigma(self, sigmas=3, square=False):
        if self.delay:
            means_wrapper = [None]

            def do_vars(means):
                means_wrapper[0] = means
                return self.var(means)

            def do_limits(vars):
                stds = vars**0.5
                means = means_wrapper[0]
                if square:
                    stds = np.repeat(stds.mean(), len(stds))
                return np.array(list(zip(means - sigmas * stds, means + sigmas * stds)))
            return self.mean().then(do_vars).then(do_limits)
        else:
            means = self.mean()
            stds = self.var(means=means)**0.5
            if square:
                stds = np.repeat(stds.mean(), len(stds))
            return np.array(list(zip(means - sigmas * stds, means + sigmas * stds)))",https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-core/vaex/legacy.py#L1306-L1326
tfidf_python_100_1.0,k means clustering,python,"def _cluster_by_sample(self, rank=""auto"", metric=""braycurtis"", linkage=""average""):
        if metric == ""euclidean"":
            dist_matrix = euclidean_distances(self._results).round(6)
        else:
            dist_matrix = self._compute_distance(rank=rank, metric=metric).to_data_frame().round(6)
        clustering = hierarchy.linkage(squareform(dist_matrix), method=linkage)
        scipy_tree = hierarchy.dendrogram(clustering, no_plot=True)
        ids_in_order = [self._results.index[int(x)] for x in scipy_tree[""ivl""]]

        return {
            ""dist_matrix"": dist_matrix,
            ""clustering"": clustering,
            ""scipy_tree"": scipy_tree,
            ""ids_in_order"": ids_in_order,
        }",https://github.com/onecodex/onecodex/blob/326a0a1af140e3a57ccf31c3c9c5e17a5775c13d/onecodex/viz/_distance.py#L45-L59
tfidf_python_100_1.0,k means clustering,python,"def plot_feature_means(features, **kwargs):
    means = np.vstack(d.mean for d in features)
    return plot(means[:, 0], means[:, 1], **kwargs)",https://github.com/rjw57/starman/blob/1f9475e2354c9630a61f4898ad871de1d2fdbc71/doc/plotutils.py#L51-L53
tfidf_python_100_1.0,k means clustering,python,"def merge(self, cluster_ids=None, to=None):
        """"""Merge the selected clusters.""""""
        if cluster_ids is None:
            cluster_ids = self.selected
        if len(cluster_ids or []) <= 1:
            return
        self.clustering.merge(cluster_ids, to=to)
        self._global_history.action(self.clustering)",https://github.com/kwikteam/phy/blob/7e9313dc364304b7d2bd03b92938347343703003/phy/cluster/supervisor.py#L575-L582
tfidf_python_100_1.0,connect to sql,python,"def databaserequesttracer_create_sql(
            self, dbh, sql):
        _livecheck(dbh, DbInfoHandle)
        _strcheck(sql)
        return DbRequestHandle(self, dbh, sql)",https://github.com/Dynatrace/OneAgent-SDK-for-Python/blob/f7b121b492f25b1c5b27316798e1a70b6be2bd01/src/oneagent/_impl/native/sdkmockiface.py#L449-L453
tfidf_python_100_1.0,connect to sql,python,"def get_query_str_extended(self, query_obj):
        sqlaq = self.get_sqla_query(**query_obj)
        sql = self.database.compile_sqla_query(sqlaq.sqla_query)
        logging.info(sql)
        sql = sqlparse.format(sql, reindent=True)
        if query_obj['is_prequery']:
            query_obj['prequeries'].append(sql)
        sql = self.mutate_query_from_config(sql)
        return QueryStringExtended(labels_expected=sqlaq.labels_expected, sql=sql)",https://github.com/apache/incubator-superset/blob/ca2996c78f679260eb79c6008e276733df5fb653/superset/connectors/sqla/models.py#L480-L488
tfidf_python_100_1.0,connect to sql,python,"def ensure_sql_unicode(sql, script_encoding):
        if not sql or not script_encoding:
            return """"

        if (sys.version_info > (3, 0)):
            if isinstance(sql, bytes):
                sql = str(sql, script_encoding)
            else:
                sql
        else:
            try:
                sql = unicode(sql.decode(script_encoding))
            except UnicodeEncodeError:
                sql = unicode(sql)

        return sql",https://github.com/guilhermechapiewski/simple-db-migrate/blob/7ea6ffd0c58f70079cc344eae348430c7bdaaab3/simple_db_migrate/core/__init__.py#L78-L93
tfidf_python_100_1.0,connect to sql,python,"def pretty_sql(sql):
    try:
        sql = sqlparse.format(sql, reindent=True, keyword_case='upper')
        return sql
    except Exception:
        return sql",https://github.com/telminov/sw-django-utils/blob/43b8491c87a5dd8fce145834c00198f4de14ceb9/djutils/templatetags/djutils.py#L17-L22
tfidf_python_100_1.0,connect to sql,python,"def get_sql(self, debug=False, use_cache=True):
        """"""
        Generates the sql for this query window and returns the sql as a string.

        :type debug: bool
        :param debug: If True, the sql will be returned in a format that is easier to read and debug.
            Defaults to False

        :type use_cache: bool
        :param use_cache: If True, the query will returned the cached sql if it exists rather
            then generating the sql again. If False, the sql will be generated again. Defaults to True.

        :rtype: str
        :return: The generated sql for this query window
        """"""
        # TODO: implement caching and debug
        sql = ''
        sql += self.build_partition_by_fields()
        sql += self.build_order_by(use_alias=False)
        sql += self.build_limit()
        sql = sql.strip()
        sql = 'OVER ({0})'.format(sql)
        self.sql = sql

        return self.sql",https://github.com/ambitioninc/django-query-builder/blob/113a7d845d3ddc6a45621b9880308e756f87c5bf/querybuilder/query.py#L1941-L1965
tfidf_python_100_1.0,connect to sql,python,"def build(self):
        sql = self.build_sql()
        if self.multiple_statements:
            return CompoundSqlStatement.from_sql(sql)
        return SingleSqlStatement(sql)",https://github.com/alvarogzp/python-sqlite-framework/blob/29db97a64f95cfe13eb7bae1d00b624b5a37b152/sqlite_framework/sql/statement/builder/base.py#L14-L18
tfidf_python_100_1.0,connect to sql,python,"def build_insert_query(self):
        table_name = self.get_table_name()
        insert_fields = self.get_insert_fields()
        sql = ""INSERT INTO "" + table_name + "" ( ""
        sql +=  ','.join(insert_fields) + "" ) VALUES ( ""
        sql += len(insert_fields) * '?,'
        if sql.endswith(','):
            sql = sql.rstrip(',')
        sql += "" )""
        return sql",https://github.com/tonyrein/pogo/blob/af1c04bc3028340ea55362d911d487ab8bdbcb77/pogo/dao/record_dao_local.py#L24-L33
tfidf_python_100_1.0,connect to sql,python,"def __init__(
            self, sql,
            presto_conn_id='presto_default',
            *args, **kwargs):
        super().__init__(sql=sql, *args, **kwargs)

        self.presto_conn_id = presto_conn_id
        self.sql = sql",https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/operators/presto_check_operator.py#L61-L68
tfidf_python_100_1.0,connect to sql,python,"def __init__(
            self, sql,
            druid_broker_conn_id='druid_broker_default',
            *args, **kwargs):
        super().__init__(sql=sql, *args, **kwargs)
        self.druid_broker_conn_id = druid_broker_conn_id
        self.sql = sql",https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/operators/druid_check_operator.py#L60-L66
tfidf_python_100_1.0,connect to sql,python,"def __init__(self, sql):
        if isinstance(sql, six.string_types):
            sql = [sql]

        assert isinstance(sql, list), ""'sql' parameter must be string or list of strings""
        self._sql = sql",https://github.com/Infinidat/infi.clickhouse_orm/blob/595f2023e334e3925a5c3fbfdd6083a5992a7169/src/infi/clickhouse_orm/migrations.py#L158-L163
tfidf_python_100_1.0,connect to sql,python,"def create_audit_table(self, target, bind, **kwargs):
        sql = ''
        if (
            self.use_statement_level_triggers and
            bind.dialect.server_version_info >= (10, 0)
        ):
            sql += self.render_tmpl('create_activity_stmt_level.sql')
            sql += self.render_tmpl('audit_table_stmt_level.sql')
        else:
            sql += self.render_tmpl('create_activity_row_level.sql')
            sql += self.render_tmpl('audit_table_row_level.sql')
        StatementExecutor(sql)(target, bind, **kwargs)",https://github.com/kvesteri/postgresql-audit/blob/91b497ced2e04dd44bb757b02983d2a64a2b1514/postgresql_audit/base.py#L227-L238
tfidf_python_100_1.0,connect to sql,python,"def _getSQL(self, sql_name):
        try:
            return self._statement_cache[sql_name]
        except KeyError:
            sql = getattr(self, sql_name)
            sql %= self._table_names
            self._statement_cache[sql_name] = sql
            return sql",https://github.com/openid/python-openid/blob/f7e13536f0d1828d3cef5ae7a7b55cabadff37fc/openid/store/sqlstore.py#L126-L133
tfidf_python_100_1.0,connect to sql,python,"def to_mongo(self):
        return {
            'update': {
                '$set': {
                    sql.lhs_column: self.query.params[sql.rhs_indexes]
                    if sql.rhs_indexes is not None else None
                    for sql in self.sql_tokens}
            }
        }",https://github.com/nesdis/djongo/blob/7f9d79455cf030cb5eee0b822502c50a0d9d3abb/djongo/sql2mongo/converters.py#L345-L353
tfidf_python_100_1.0,connect to sql,python,"def __init__(self,
                 sql,
                 bigquery_conn_id='google_cloud_default',
                 use_legacy_sql=True,
                 *args, **kwargs):
        super().__init__(sql=sql, *args, **kwargs)
        self.bigquery_conn_id = bigquery_conn_id
        self.sql = sql
        self.use_legacy_sql = use_legacy_sql",https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/bigquery_check_operator.py#L67-L75
tfidf_python_100_1.0,connect to sql,python,"def execute(self, sql, args=()):

        if isinstance(sql, (list, tuple)):
            sql = ' '.join(sql)

        with sqlite3.connect(self.path) as con:
            return con.execute(sql, args)",https://github.com/posativ/isso/blob/78997f491044b7d694ac7170edc32030544095b7/isso/db/__init__.py#L56-L62
tfidf_python_100_1.0,connect to sql,python,"def executeSchemaSQL(self, sql, args=()):
        sql = sql.replace(""*DATABASE*"", self.databaseName)
        return self.executeSQL(sql, args)",https://github.com/twisted/axiom/blob/7de70bc8fe1bb81f9c2339fba8daec9eb2e92b68/axiom/store.py#L2383-L2385
tfidf_python_100_1.0,connect to sql,python,"def querySchemaSQL(self, sql, args=()):
        sql = sql.replace(""*DATABASE*"", self.databaseName)
        return self.querySQL(sql, args)",https://github.com/twisted/axiom/blob/7de70bc8fe1bb81f9c2339fba8daec9eb2e92b68/axiom/store.py#L2328-L2330
tfidf_python_100_1.0,connect to sql,python,"def get_sql(self, debug=False, use_cache=True):
        """"""
        Generates the sql for this query and returns the sql as a string.

        :type debug: bool
        :param debug: If True, the sql will be returned in a format that is easier to read and debug.
            Defaults to False

        :type use_cache: bool
        :param use_cache: If True, the query will returned the cached sql if it exists rather
            then generating the sql again. If False, the sql will be generated again. Defaults to True.

        :rtype: str
        :return: The generated sql for this query
        """"""
        # TODO: enable caching
        # if self.sql and use_cache and not debug:
        #     return self.sql

        # auto alias any naming collisions
        self.check_name_collisions()

        # if debugging, return the debug formatted sql
        if debug:
            return self.format_sql()

        # build each part of the query
        sql = ''
        sql += self.build_withs()
        sql += self.build_select_fields()
        sql += self.build_from_table()
        sql += self.build_joins()
        sql += self.build_where()
        sql += self.build_groups()
        sql += self.build_order_by()
        sql += self.build_limit()

        # remove any whitespace from the beginning and end of the sql
        self.sql = sql.strip()

        return self.sql",https://github.com/ambitioninc/django-query-builder/blob/113a7d845d3ddc6a45621b9880308e756f87c5bf/querybuilder/query.py#L1064-L1104
tfidf_python_100_1.0,connect to sql,python,"def sql(self, sql, *params):
        """"""Execure raw SQL.""""""
        self.ops.append(self.migrator.sql(sql, *params))",https://github.com/klen/peewee_migrate/blob/b77895ab1c9be3121bc127e0c2dfb047eed8b24c/peewee_migrate/migrator.py#L138-L140
tfidf_python_100_1.0,connect to sql,python,"def pformat_sql_html(sql):
    """"""
    Highlight common SQL words in a string.
    """"""
    sql = escape(sql)
    sql = RE_SQL_NL.sub(u'<br>\n\\1', sql)
    sql = RE_SQL.sub(u'<strong>\\1</strong>', sql)
    return sql",https://github.com/edoburu/django-debugtools/blob/5c609c00fa9954330cd135fc62a1e18b8e7fea8a/debugtools/formatter.py#L52-L59
tfidf_python_100_1.0,html encode string,python,"def html(self, html: str) -> None:
        self._html = html.encode(self.encoding)",https://github.com/kennethreitz/requests-html/blob/b59a9f2fb9333d7d467154a0fd82978efdb9d23b/requests_html.py#L110-L111
tfidf_python_100_1.0,html encode string,python,"def encode(self, string):
        """"""Encode a_string as per the canonicalisation encoding rules.

        See the AWS dev reference page 186 (2009-11-30 version).
        @return: a_string encoded.
        """"""
        if isinstance(string, unicode):
            string = string.encode(""utf-8"")
        return quote(string, safe=""~"")",https://github.com/twisted/txaws/blob/5c3317376cd47e536625027e38c3b37840175ce0/txaws/ec2/client.py#L1122-L1130
tfidf_python_100_1.0,html encode string,python,"def encode(self, string):
        if self.encoding == ""cp437"":
            return string
        return string.encode(self.encoding)",https://github.com/Tenchi2xh/Almonds/blob/6b27024729f055f2cb5e14ae3ca3cb428ae054bc/almonds/cursebox/symbols.py#L70-L73
tfidf_python_100_1.0,html encode string,python,"def plain(html):
	
	try: html = str(html)
	except:
		pass
	
	if html == ""None"": html = """"
	html = strip_javascript(html)
	html = strip_inline_css(html)
	html = strip_comments(html)
	html = strip_forms(html)
	html = strip_tags(html, columns="""")
	html = replace_entities(html)
	html = collapse_tabs(html)
	html = collapse_spaces(html)
	html = collapse_linebreaks(html)	
	
	return html",https://github.com/shoebot/shoebot/blob/d554c1765c1899fa25727c9fc6805d221585562b/lib/web/html.py#L228-L245
tfidf_python_100_1.0,html encode string,python,"def clean_attributes(html):
    while htmlstrip.search(html):
        html = htmlstrip.sub('<\\1\\2>', html)
    return html",https://github.com/usc-isi-i2/etk/blob/aab077c984ea20f5e8ae33af622fe11d3c4df866/etk/extractors/readability/cleaners.py#L17-L20
tfidf_python_100_1.0,html encode string,python,"def mydecoder(string):
    try:
        string.encode('ascii')
        return string
    except:
        ustring = string.encode('utf-8')
        string = re.sub(b""\xe2\x80\x90"", b""-"", ustring)
        return string.decode('utf-8')",https://github.com/tgbugs/pyontutils/blob/3d913db29c177db39151592909a4f56170ef8b35/ilxutils/ilxutils/tools.py#L114-L121
tfidf_python_100_1.0,html encode string,python,"def encode(self, string):
        assert(isinstance(string, text_type))
        if self.encoding:
            return string.encode(self.encoding, unicode_encode_errors)
        else:
            return string",https://github.com/AkihikoITOH/capybara/blob/e86c2173ea386654f4ae061148e8fbe3f25e715c/capybara/virtualenv/lib/python2.7/site-packages/pip/_vendor/html5lib/serializer/htmlserializer.py#L157-L162
tfidf_python_100_1.0,html encode string,python,"def html_to_rgb(html):
  """"""Convert the HTML color to (r, g, b).

  Parameters:
    :html:
      the HTML definition of the color (#RRGGBB or #RGB or a color name).

  Returns:
    The color as an (r, g, b) tuple in the range:
    r[0...1],
    g[0...1],
    b[0...1]

  Throws:
    :ValueError:
      If html is neither a known color name or a hexadecimal RGB
      representation.

  >>> '(%g, %g, %g)' % html_to_rgb('#ff8000')
  '(1, 0.501961, 0)'
  >>> '(%g, %g, %g)' % html_to_rgb('ff8000')
  '(1, 0.501961, 0)'
  >>> '(%g, %g, %g)' % html_to_rgb('#f60')
  '(1, 0.4, 0)'
  >>> '(%g, %g, %g)' % html_to_rgb('f60')
  '(1, 0.4, 0)'
  >>> '(%g, %g, %g)' % html_to_rgb('lemonchiffon')
  '(1, 0.980392, 0.803922)'

  """"""
  html = html.strip().lower()
  if html[0]=='#':
    html = html[1:]
  elif html in NAMED_COLOR:
    html = NAMED_COLOR[html][1:]

  if len(html)==6:
    rgb = html[:2], html[2:4], html[4:]
  elif len(html)==3:
    rgb = ['%c%c' % (v,v) for v in html]
  else:
    raise ValueError(""input #%s is not in #RRGGBB format"" % html)

  return tuple(((int(n, 16) / 255.0) for n in rgb))",https://github.com/xav/Grapefruit/blob/b3d88375be727a3a1ec5839fbc462e0e8e0836e4/grapefruit.py#L869-L912
tfidf_python_100_1.0,html encode string,python,"def encode(self):
        if self.html:
            return self.html
        elif self.fig:
            return self.encode2()
        else:
            return self.encode1()",https://github.com/wdecoster/nanoplotter/blob/80908dd1be585f450da5a66989de9de4d544ec85/nanoplotter/plot.py#L18-L24
tfidf_python_100_1.0,html encode string,python,"def convert_html_to_plain_text(html):
    if not html:
        return html

    if six.PY2:
        html = html.decode('utf-8')

    html = decode_html_entities(html)
    # Replace HTML break rules with new lines
    html = html.replace('<br>', '\n')
    # Remove multiple spaces
    html = re.sub(' +', ' ', html)

    return html",https://github.com/markfinger/django-node/blob/a2f56bf027fd3c4cbc6a0213881922a50acae1d6/django_node/utils.py#L196-L209
tfidf_python_100_1.0,html encode string,python,"def body_io(string, encoding='utf-8'):
    if hasattr(string, 'encode'):
        string = string.encode(encoding)
    return io.BytesIO(string)",https://github.com/h2non/pook/blob/e64094e41e4d89d98d2d29af7608ef27dc50cf19/pook/interceptors/urllib3.py#L45-L48
tfidf_python_100_1.0,html encode string,python,"def encode_string(string):
    data = string.encode('utf-8')
    datalen = '{0:06x}'.format(len(data) + 1).encode()
    return _JOIN_BYTES([datalen, data, _NEWLINE_BYTE])",https://github.com/tkf/python-epc/blob/f3673ae5c35f20a0f71546ab34c28e3dde3595c1/epc/handler.py#L69-L72
tfidf_python_100_1.0,html encode string,python,"def data_append_string(self, string):
        string = bytes(string.encode(self.encoding) + b'\0')
        self.data_append_auto(string)",https://github.com/mon/kbinxml/blob/ca4a6e309ec458dd359f1bf25f91a4443758365a/kbinxml/kbinxml.py#L143-L145
tfidf_python_100_1.0,html encode string,python,"def HtmlToRgb(html):
    '''Convert the HTML color to (r, g, b).

    Parameters:
      :html:
        the HTML definition of the color (#RRGGBB or #RGB or a color name).

    Returns:
      The color as an (r, g, b) tuple in the range:
      r[0...1],
      g[0...1],
      b[0...1]

    Throws:
      :ValueError:
        If html is neither a known color name or a hexadecimal RGB
        representation.

    >>> '(%g, %g, %g)' % Color.HtmlToRgb('#ff8000')
    '(1, 0.501961, 0)'
    >>> '(%g, %g, %g)' % Color.HtmlToRgb('ff8000')
    '(1, 0.501961, 0)'
    >>> '(%g, %g, %g)' % Color.HtmlToRgb('#f60')
    '(1, 0.4, 0)'
    >>> '(%g, %g, %g)' % Color.HtmlToRgb('f60')
    '(1, 0.4, 0)'
    >>> '(%g, %g, %g)' % Color.HtmlToRgb('lemonchiffon')
    '(1, 0.980392, 0.803922)'

    '''
    html = html.strip().lower()
    if html[0]=='#':
      html = html[1:]
    elif html in Color.NAMED_COLOR:
      html = Color.NAMED_COLOR[html][1:]

    if len(html)==6:
      rgb = html[:2], html[2:4], html[4:]
    elif len(html)==3:
      rgb = ['%c%c' % (v,v) for v in html]
    else:
      raise ValueError('input #%s is not in #RRGGBB format' % html)

    return tuple(((int(n, 16) / 255.0) for n in rgb))",https://github.com/jsvine/spectra/blob/2269a0ae9b5923154b15bd661fb81179608f7ec2/spectra/grapefruit.py#L955-L998
tfidf_python_100_1.0,html encode string,python,"def __init__(self, html):
        if isinstance(html, (str, bytes)):
            self.html = fromstring(html)

        elif isinstance(html, HtmlWrapper):
            self.html = html.html

        elif isinstance(html, HtmlElement):
            self.html = html

        elif isinstance(html, BS4_TYPES):
            self.html = fromstring(str(html))

        else:
            msg = ""Object of type %s not compatible with HtmlWrapper"" % str(type(html))

            raise TypeError(msg)",https://github.com/thismachinechills/html_wrapper/blob/bef8c93f99bdbb4646d96845fed4e2ddf9213947/html_wrapper/wrapper.py#L17-L33
tfidf_python_100_1.0,html encode string,python,"def encode_string(self, string):
        if isinstance(string, unicode):
            string = string.encode('utf-8')
        return string",https://github.com/LandRegistry/lr-utils/blob/811c9e5c11678a04ee203fa55a7c75080f4f9d89/lrutils/password/password_utils.py#L39-L42
tfidf_python_100_1.0,html encode string,python,"def get_start_widget(appbase, jupbase, notebase):
    html = template.format(appbase=appbase, jupbase=jupbase, notebase=notebase)
    return ipw.HTML(html)",https://github.com/aiidalab/aiidalab-widgets-base/blob/291a9b159eac902aee655862322670ec1b0cd5b1/start.py#L26-L28
tfidf_python_100_1.0,html encode string,python,"def html_to_text(html):
    html = html.replace('\n', ' ')
    html = ' '.join(html.split())
    s = HTMLTextExtractor()
    s.feed(html)
    return s.get_text()",https://github.com/asciimoo/searx/blob/a84caa22cf947e973c10aa968d35fb2bdda6d048/searx/utils.py#L136-L141
tfidf_python_100_1.0,html encode string,python,"def encode(string):
		""""""
		Encode the given string as an OID.

		>>> import snmp_passpersist as snmp
		>>> snmp.PassPersist.encode(""hello"")
		'5.104.101.108.108.111'
		>>>
		""""""

		result=""."".join([ str(ord(s)) for s in string ])
		return  ""%s."" % (len(string)) + result",https://github.com/nagius/snmp_passpersist/blob/8cc584d2e90c920ae98a318164a55bde209a18f7/snmp_passpersist.py#L106-L117
tfidf_python_100_1.0,html encode string,python,"def encode(self, f):
        num_bytes_written = 0
        num_bytes_written += MqttFixedHeader.encode(self, f)
        num_bytes_written += self.encode_body(f)

        return num_bytes_written",https://github.com/kcallin/mqtt-codec/blob/0f754250cc3f44f4376777e7e8b3676c5a4d413a/mqtt_codec/packet.py#L359-L364
tfidf_python_100_1.0,finding time elapsed using a timer,python,"def __init__(self, timer, elapsed):
        self.timer = timer
        self.elapsed = elapsed",https://github.com/buildbot/buildbot/blob/5df3cfae6d760557d99156633c32b1822a1e130c/master/buildbot/process/metrics.py#L74-L76
tfidf_python_100_1.0,finding time elapsed using a timer,python,"def _calc_benchmark_stat(self, f):
        timer = Timer()
        i = 0
        while True:
            f()
            i += 1
            if i >= self.min_run:
                _, elapsed = timer.lap()
                if elapsed > self.min_time:
                    break
        return BenchmarkStat(elapsed / i, i)",https://github.com/sony/nnabla/blob/aaf3d33b7cbb38f2a03aa754178ba8f7c8481320/python/benchmark/function/function_benchmark.py#L224-L234
tfidf_python_100_1.0,finding time elapsed using a timer,python,"def elapsed(self, total=True):
        """"""Return the elapsed time for the timer.

        Parameters
        ----------
        total : bool, optional (default True)
          If ``True`` return the total elapsed time since the first
          call of :meth:`start` for the selected timer, otherwise
          return the elapsed time since the most recent call of
          :meth:`start` for which there has not been a corresponding
          call to :meth:`stop`.

        Returns
        -------
        dlt : float
          Elapsed time
        """"""

        return self.timer.elapsed(self.label, total=total)",https://github.com/bwohlberg/sporco/blob/8946a04331106f4e39904fbdf2dc7351900baa04/sporco/util.py#L1438-L1456
tfidf_python_100_1.0,finding time elapsed using a timer,python,"def get_mean_rate(self):
        elapsed = time.time() - self.start_time
        if elapsed == 0:
            return 0.0
        return self.count / elapsed",https://github.com/Yelp/uwsgi_metrics/blob/534966fd461ff711aecd1e3d4caaafdc23ac33f0/uwsgi_metrics/meter.py#L66-L70
tfidf_python_100_1.0,finding time elapsed using a timer,python,"def __exit__(self, *args, **kwargs):
        elapsed = time.time() - self.t
        self.elapsed_total += elapsed",https://github.com/mete0r/hypua2jamo/blob/caceb33a26c27645703d659a82bb1152deef1469/benchmark.py#L78-L80
tfidf_python_100_1.0,finding time elapsed using a timer,python,"def __repr__(self):
        if self.elapsed is not None:
            return ""Timer(title=%s, elapsed=%.6f)"" % (self.title, self.elapsed)
        else:
            return ""Timer(title=%s)"" % self.title",https://github.com/MacHu-GWU/single_file_module-project/blob/01f7a6b250853bebfd73de275895bf274325cfc1/sfm/timer.py#L53-L57
tfidf_python_100_1.0,finding time elapsed using a timer,python,"def reset_timer(self, timer, time_sec):
        with self._timer_lock:
            # self.logger.debug(""setting timer..."")
            timer.timer = time.time() + time_sec",https://github.com/ejeschke/ginga/blob/a78c893ec6f37a837de851947e9bb4625c597915/ginga/web/pgw/Widgets.py#L2438-L2441
tfidf_python_100_1.0,finding time elapsed using a timer,python,"def reset_timer(self, timer, time_sec):
        with self.timer_lock:
            if timer not in self.timer:
                self.timer[timer.name] = timer
            self.logger.debug(""setting timer..."")
            timer.deadline = time.time() + time_sec",https://github.com/ejeschke/ginga/blob/a78c893ec6f37a837de851947e9bb4625c597915/ginga/web/jupyterw/JpHelp.py#L88-L93
tfidf_python_100_1.0,finding time elapsed using a timer,python,"def elapsed(self):
        """"""
        Get elapsed time is seconds (float)
        """"""

        # Clock stops running when total is reached
        if self.count == self.total:
            elapsed = self.last_update - self.start
        else:
            elapsed = time.time() - self.start

        return elapsed",https://github.com/Rockhopper-Technologies/enlighten/blob/857855f940e6c1bb84d0be849b999a18fff5bf5a/enlighten/_counter.py#L488-L499
tfidf_python_100_1.0,finding time elapsed using a timer,python,"def __call__(self):
        tp1 = timer()
        self.workload()
        self.time = timer() - tp1",https://github.com/enkore/i3pystatus/blob/14cfde967cecf79b40e223e35a04600f4c875af7/i3pystatus/core/threading.py#L125-L128
tfidf_python_100_1.0,finding time elapsed using a timer,python,"def __repr__(self):
        stats = self._stats_copy()

        elapsed = int(time.time() - self._start_time)
        elapsed_str = ''
        if elapsed < 60:
            elapsed_str = str(elapsed) + 'sec'
        elif 60 <= elapsed < 3600:
            elapsed_str = '%dmn%dsec' % (elapsed / 60, elapsed % 60)
        elif 3600 <= elapsed < 86400:
            elapsed_str = '%dh%dmn' % (elapsed / 3600, (elapsed % 3600) / 60)
        elif elapsed >= 86400:
            elapsed_str = '%dd%dh' % (elapsed / 86400, (elapsed % 86400) / 3600)
        stats['ElapsedTime'] = elapsed_str

        l = []
        for ev, value in sorted(stats.items(), key=lambda x: x[0]):
            l.append(' %s=%s' % (output_format.field_name(ev),
                                 output_format.field_value(value)))
        s = '<%s%s >' % (output_format.class_name(self.__class__.__name__),
                         ''.join(l))
        return s",https://github.com/seb-m/pyinotify/blob/0f3f8950d12e4a6534320153eed1a90a778da4ae/python3/pyinotify.py#L1018-L1039
tfidf_python_100_1.0,finding time elapsed using a timer,python,"def elapsed(self):
        logger.debug('Fetching MPD elapsed time')
        time = self.status.get('time')
        if time:
            return self._parse_time(time.split(':')[0])
        else:
            return None",https://github.com/rbarrois/mpdlcd/blob/85f16c8cc0883f8abb4c2cc7f69729c3e2f857da/mpdlcd/mpdwrapper.py#L78-L84
tfidf_python_100_1.0,finding time elapsed using a timer,python,"def update(self, i):
        elapsed = time.time() - self.start
        i = i + 1

        if elapsed - self.last > self.animation_interval:
            self.animate(i + 1, elapsed)
            self.last = elapsed
        elif i == self.iterations:
            self.animate(i, elapsed)",https://github.com/pymc-devs/pymc/blob/c6e530210bff4c0d7189b35b2c971bc53f93f7cd/pymc/progressbar.py#L29-L37
tfidf_python_100_1.0,finding time elapsed using a timer,python,"def elapsed_and_total(self):
        logger.debug('Fetching MPD elapsed and total time')
        time = self.status.get('time')
        if time and ':' in time:
            elapsed, total = time.split(':', 1)
            return self._parse_time(elapsed), self._parse_time(total)
        else:
            return (None, None)",https://github.com/rbarrois/mpdlcd/blob/85f16c8cc0883f8abb4c2cc7f69729c3e2f857da/mpdlcd/mpdwrapper.py#L96-L103
tfidf_python_100_1.0,finding time elapsed using a timer,python,"def set_timer(self, timer=""wall""):
        """"""Set the timer function

        Parameters
        ----------
        timer : {'wall', 'cpu', or callable}
                Timer function used to measure task running times.
                'wall' uses `time.time`, 'cpu' uses `time.process_time`

        Returns
        -------
        self
        """"""
        if timer == ""wall"":
            timer = time.time
        elif timer == ""cpu"":
            try:
                timer = time.process_time
            except AttributeError:
                raise RuntimeError(
                    ""Python2.7 on Windows does not offer a CPU time function. ""
                    ""Please upgrade to Python >= 3.5."")
        self.timer = timer
        return self",https://github.com/scottgigante/tasklogger/blob/06a263715d2db0653615c17b2df14b8272967b8d/tasklogger/logger.py#L98-L121
tfidf_python_100_1.0,finding time elapsed using a timer,python,"def checkpoint(self, name=''):
        print('{timer} {checkpoint} took {elapsed} seconds'.format(timer=self.name,
                                                                   checkpoint=name,
                                                                   elapsed=self.elapsed,
                                                                   ).strip())",https://github.com/pbrod/numdifftools/blob/2c88878df732c9c6629febea56e7a91fd898398d/src/numdifftools/profiletools.py#L130-L134
tfidf_python_100_1.0,finding time elapsed using a timer,python,"def elapsed(self):
        elapsed = self._end - self._start
        # start with Î¼s
        elapsed = elapsed * 1000000
        unit = ""Î¼s""
        if elapsed > 1000:
            # switch to ms
            elapsed = elapsed / 1000
            unit = ""ms""
        return ""{:.3f} {}"".format(float(elapsed), unit)",https://github.com/intelsdi-x/snap-plugin-lib-py/blob/8da5d00ac5f9d2b48a7239563ac7788209891ca4/snap_plugin/v1/plugin.py#L51-L60
tfidf_python_100_1.0,finding time elapsed using a timer,python,"def timeinto(l, f, *a, **k):
    then = time.time()
    try:
        return f(*a, **k)
    finally:
        now = time.time()
        elapsed = now - then
        l.append(elapsed)",https://github.com/twisted/axiom/blob/7de70bc8fe1bb81f9c2339fba8daec9eb2e92b68/axiom/store.py#L2420-L2427
tfidf_python_100_1.0,finding time elapsed using a timer,python,"def _add_timer(self, timer):
        if timer not in self._timers:
            self._timers.append(timer)",https://github.com/glue-viz/glue-vispy-viewers/blob/54a4351d98c1f90dfb1a557d1b447c1f57470eea/glue_vispy_viewers/extern/vispy/app/backends/_osmesa.py#L56-L58
tfidf_python_100_1.0,finding time elapsed using a timer,python,"def stop(self, key):
        elapsed = time.time() - self.time_points[key]
        self.timers[key] += elapsed
        del self.time_points[key]
        return elapsed",https://github.com/lorien/grab/blob/8b301db2a08c830245b61c589e58af6234f4db79/grab/stat.py#L112-L116
tfidf_python_100_1.0,parse binary file to custom class,python,"def set_binary(self, binary):
        if binary is not None:
            log.info('update naviseccli binary location to: {}'.format(binary))
            self._heart_beat.set_binary(binary)",https://github.com/emc-openstack/storops/blob/24b4b13bf065c0ef0538dd0b5ebb8f25d24176bd/storops/vnx/block_cli.py#L128-L131
tfidf_python_100_1.0,parse binary file to custom class,python,"def binary(self):
        """""":class:`.BinaryQuadraticModel`: An instance of the QUBO model subclass of
        the :class:`.BinaryQuadraticModel` superclass, corresponding to a binary quadratic
        model with binary variables.

        Enables access to biases for the binary-valued binary quadratic model
        regardless of the :class:`vartype` set when the model was created. If the model
        was created with the :attr:`.spin` vartype, the QUBO model subclass is instantiated
        upon the first use of the :attr:`.binary` property and used in any subsequent reads.

        Examples:
           This example creates an Ising model and uses the :attr:`.binary` property
           to instantiate the corresponding QUBO model.

           >>> import dimod
           ...
           >>> bqm_spin = dimod.BinaryQuadraticModel({0: 0.0, 1: 0.0}, {(0, 1): 0.5}, -0.5, dimod.SPIN)
           >>> bqm_qubo = bqm_spin.binary
           >>> bqm_qubo  # doctest: +SKIP
           BinaryQuadraticModel({0: -1.0, 1: -1.0}, {(0, 1): 2.0}, 0.0, Vartype.BINARY)
           >>> bqm_qubo.binary is bqm_qubo
           True

        Note:
            Methods like :meth:`.add_variable`, :meth:`.add_variables_from`,
            :meth:`.add_interaction`, etc. should only be used on the base model.

        """"""
        # NB: The existence of the _binary property implies that it is up to date, methods that
        # invalidate it will erase the property
        try:
            binary = self._binary
            if binary is not None:
                return binary
        except AttributeError:
            pass

        if self.vartype is Vartype.BINARY:
            self._binary = binary = self
        else:
            self._counterpart = self._binary = binary = self.change_vartype(Vartype.BINARY, inplace=False)

            # we also want to go ahead and set binary.spin to refer back to self
            binary._spin = self

        return binary",https://github.com/dwavesystems/dimod/blob/beff1b7f86b559d923ac653c1de6d593876d6d38/dimod/binary_quadratic_model.py#L342-L387
tfidf_python_100_1.0,parse binary file to custom class,python,"def _binary(self):
        if self._customized_cli is None:
            binary = 'naviseccli'
            for c in self._cli_binary_candidates:
                if os.path.exists(c):
                    binary = c
                    break
        else:
            binary = self._customized_cli
        self._init_security_level(binary)
        return binary",https://github.com/emc-openstack/storops/blob/24b4b13bf065c0ef0538dd0b5ebb8f25d24176bd/storops/vnx/navi_command.py#L109-L119
tfidf_python_100_1.0,parse binary file to custom class,python,"def to_user(self, binary):
        i = 0
        for _ in range(self.size):
            c = binary[:1]
            i = i * 256 + ord(c)
            binary = binary[1:]
        return i",https://github.com/osrg/ryu/blob/6f906e72c92e10bd0264c9b91a2f7bb85b97780c/ryu/lib/type_desc.py#L32-L38
tfidf_python_100_1.0,parse binary file to custom class,python,"def read_element_resolution(binary):
    (offset_width, val_w) = read_element_property(binary, RES_W_SIGNATURE, 'I')
    (offset_height, val_h) = read_element_property(binary, RES_H_SIGNATURE, 'I')
    return (offset_width, offset_height, {'width': val_w[0], 'height':val_h[0] } )",https://github.com/tnajdek/lol-clarity/blob/44bebd511227cce5b1f82433834e24a751f0886f/clarity/clarity.py#L44-L47
tfidf_python_100_1.0,parse binary file to custom class,python,"def __init__(self, binary, element_head):
        self.binary = binary
        self.head = element_head
        (self._name_offset, self._name) = read_element_name(binary)
        (self._anchor_offset, self._anchor) = read_element_anchor(binary)
        (self._position_offset, self._position) = read_element_position(binary)
        (self._res_w_offset, self._res_h_offset, self._resolution) = read_element_resolution(binary)
        (self._properties_count_offset, self._properties_count) = read_properties_count(binary)",https://github.com/tnajdek/lol-clarity/blob/44bebd511227cce5b1f82433834e24a751f0886f/clarity/clarity.py#L106-L113
tfidf_python_100_1.0,parse binary file to custom class,python,"def get_let_vars(binary, klass):
  if not isinstance(binary.y, _VarSymbol): raise DinpySyntaxError()
  if isinstance(binary.x, _VarSymbol): return (binary.x, binary.y)
  if isinstance(binary.x, klass): 
    return get_let_vars(binary.x, klass)+(binary.y,)
  raise DinpySyntaxError()",https://github.com/chaosim/dao/blob/d7ba65c98ee063aefd1ff4eabb192d1536fdbaaa/dao/dinpy/dinpy.py#L192-L197
tfidf_python_100_1.0,parse binary file to custom class,python,"def export_stl(surface, file_name, **kwargs):
    """""" Exports surface(s) as a .stl file in plain text or binary format.

    Keyword Arguments:
        * ``binary``: flag to generate a binary STL file. *Default: True*
        * ``vertex_spacing``: size of the triangle edge in terms of points sampled on the surface. *Default: 1*
        * ``update_delta``: use multi-surface evaluation delta for all surfaces. *Default: True*

    :param surface: surface or surfaces to be saved
    :type surface: abstract.Surface or multi.SurfaceContainer
    :param file_name: name of the output file
    :type file_name: str
    :raises GeomdlException: an error occurred writing the file
    """"""
    binary = kwargs.get('binary', True)
    if 'binary' in kwargs:
        kwargs.pop('binary')
    content = export_stl_str(surface, binary=binary, **kwargs)
    return exch.write_file(file_name, content, binary=binary)",https://github.com/orbingol/NURBS-Python/blob/b1c6a8b51cf143ff58761438e93ba6baef470627/geomdl/exchange.py#L575-L593
tfidf_python_100_1.0,parse binary file to custom class,python,"def _cmd(binary, *args):
    '''
    Wrapper to run at(1) or return None.
    '''
    binary = salt.utils.path.which(binary)
    if not binary:
        raise CommandNotFoundError('{0}: command not found'.format(binary))
    cmd = [binary] + list(args)
    return __salt__['cmd.run_stdout']([binary] + list(args),
                                      python_shell=False)",https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/modules/at.py#L51-L60
tfidf_python_100_1.0,parse binary file to custom class,python,"def to_text(binary, encoding='utf-8'):
    if binary is None:
        return binary
    if isinstance(binary, (six.binary_type, bytearray)):
        return binary.decode(encoding)
    elif isinstance(binary, six.text_type):
        return binary
    else:
        return str(binary) if six.PY3 else str(binary).decode(encoding)",https://github.com/aliyun/aliyun-odps-python-sdk/blob/4b0de18f5864386df6068f26f026e62f932c41e4/odps/utils.py#L399-L407
tfidf_python_100_1.0,parse binary file to custom class,python,"def __init__(self, config, binary='', verbose=0, debug=False):
        """"""Initialize.""""""

        super().__init__(config, binary, verbose, debug)
        self.binary = binary if binary else 'hunspell'",https://github.com/facelessuser/pyspelling/blob/c25d5292cc2687ad65891a12ead43f7182ca8bb3/pyspelling/__init__.py#L460-L464
tfidf_python_100_1.0,parse binary file to custom class,python,"def _get_binary_info(binary: bytes):
    hex_data = """"
    for i in range(0, 10):
        if i > 0:
            hex_data += "" ""
        hex_data += hex(binary[i])
    hex_data += "" ...""
    return ""Binary data\nLength: {}\nFirst 10 bytes: {}"".format(len(binary), hex_data)",https://github.com/chovanecm/sacredboard/blob/47e1c99e3be3c1b099d3772bc077f5666020eb0b/sacredboard/app/webapi/files.py#L26-L33
tfidf_python_100_1.0,parse binary file to custom class,python,"def __init__(self, packet_type=NOOP, data=None, binary=None,
                 encoded_packet=None):
        self.packet_type = packet_type
        self.data = data
        if binary is not None:
            self.binary = binary
        elif isinstance(data, six.text_type):
            self.binary = False
        elif isinstance(data, binary_types):
            self.binary = True
        else:
            self.binary = False
        if encoded_packet:
            self.decode(encoded_packet)",https://github.com/miguelgrinberg/python-engineio/blob/261fd67103cb5d9a44369415748e66fdf62de6fb/engineio/packet.py#L17-L30
tfidf_python_100_1.0,parse binary file to custom class,python,"def from_user(self, i):
        binary = b''
        for _ in range(self.size):
            binary = six.int2byte(i & 255) + binary
            i //= 256
        return binary",https://github.com/osrg/ryu/blob/6f906e72c92e10bd0264c9b91a2f7bb85b97780c/ryu/lib/type_desc.py#L40-L45
tfidf_python_100_1.0,parse binary file to custom class,python,"def encode_ulid_base32(binary):
    """"""
    Encode 16 binary bytes into a 26-character long base32 string.
    :param binary: Bytestring or list of bytes
    :return: ASCII string of 26 characters
    :rtype: str
    """"""
    assert len(binary) == 16

    if not py3 and isinstance(binary, str):
        binary = [ord(b) for b in binary]

    symbols = _symbols
    return ''.join([
        symbols[(binary[0] & 224) >> 5],
        symbols[binary[0] & 31],
        symbols[(binary[1] & 248) >> 3],
        symbols[((binary[1] & 7) << 2) | ((binary[2] & 192) >> 6)],
        symbols[(binary[2] & 62) >> 1],
        symbols[((binary[2] & 1) << 4) | ((binary[3] & 240) >> 4)],
        symbols[((binary[3] & 15) << 1) | ((binary[4] & 128) >> 7)],
        symbols[(binary[4] & 124) >> 2],
        symbols[((binary[4] & 3) << 3) | ((binary[5] & 224) >> 5)],
        symbols[binary[5] & 31],
        symbols[(binary[6] & 248) >> 3],
        symbols[((binary[6] & 7) << 2) | ((binary[7] & 192) >> 6)],
        symbols[(binary[7] & 62) >> 1],
        symbols[((binary[7] & 1) << 4) | ((binary[8] & 240) >> 4)],
        symbols[((binary[8] & 15) << 1) | ((binary[9] & 128) >> 7)],
        symbols[(binary[9] & 124) >> 2],
        symbols[((binary[9] & 3) << 3) | ((binary[10] & 224) >> 5)],
        symbols[binary[10] & 31],
        symbols[(binary[11] & 248) >> 3],
        symbols[((binary[11] & 7) << 2) | ((binary[12] & 192) >> 6)],
        symbols[(binary[12] & 62) >> 1],
        symbols[((binary[12] & 1) << 4) | ((binary[13] & 240) >> 4)],
        symbols[((binary[13] & 15) << 1) | ((binary[14] & 128) >> 7)],
        symbols[(binary[14] & 124) >> 2],
        symbols[((binary[14] & 3) << 3) | ((binary[15] & 224) >> 5)],
        symbols[binary[15] & 31],
    ])",https://github.com/valohai/ulid2/blob/cebc523ac70c5d5ca055c0c3de6318de617b07d7/ulid2.py#L60-L100
tfidf_python_100_1.0,parse binary file to custom class,python,"def _get_ghostscript_binary():
    binary = 'gs'

    if sys.platform.startswith('win'):
        binary = EpsImagePlugin.gs_windows_binary
        if not binary:
            raise TreepoemError(
                'Cannot determine path to ghostscript, is it installed?',
            )

    return binary",https://github.com/adamchainz/treepoem/blob/1fe2e9c82b54d01005a42067778d45cef7799ef7/treepoem/__init__.py#L102-L112
tfidf_python_100_1.0,parse binary file to custom class,python,"def _loadBinarys(self, parentXML, parent):

        binarysXML = parentXML.findall(""binary"")

        for binaryXML in binarysXML:
            binaryParams = BinaryParameters()

            for value in binaryXML:

                tag = value.tag
                text = value.text
                attrib = value.attrib

                binaryParams.addParam(tag, text, attrib)

            binary = Binary(binaryParams.params)
            binary.parent = parent

            parent._addChild(binary)  # Add star to the system

            self._loadBinarys(binaryXML, binary)
            self._loadStars(binaryXML, binary)
            self._loadPlanets(binaryXML, binary)  # Load planets

            self.binaries.append(binary)",https://github.com/ryanvarley/ExoData/blob/e0d3652117214d2377a707d6778f93b7eb201a41/exodata/database.py#L156-L180
tfidf_python_100_1.0,parse binary file to custom class,python,"def set_custom_value(custom_name, custom_val):
    """"""
    Set a custom value to be given back in the feedback
    :param custom_name: name/key of the entry to be placed in the custom dict
    :param custom_val: content of the entry to be placed in the custom dict
    """"""
    rdict = load_feedback()
    if not ""custom"" in rdict:
        rdict[""custom""] = {}
    rdict[""custom""][custom_name] = custom_val
    save_feedback(rdict)",https://github.com/UCL-INGI/INGInious/blob/cbda9a9c7f2b8e8eb1e6d7d51f0d18092086300c/base-containers/base/inginious/feedback.py#L118-L128
tfidf_python_100_1.0,parse binary file to custom class,python,"def _resolve_versioned_paths(self) -> None:
        version_hash = self.version_hash
        if not version_hash:
            return

        binary = self._binary
        if binary:
            self._binary = binary.replace(""%V"", version_hash)
        typeshed = self._typeshed
        if typeshed:
            self._typeshed = typeshed.replace(""%V"", version_hash)",https://github.com/facebook/pyre-check/blob/4a9604d943d28ef20238505a51acfb1f666328d7/client/configuration.py#L466-L476
tfidf_python_100_1.0,parse binary file to custom class,python,"def write(self, ostream, kmip_version=enums.KMIPVersion.KMIP_1_0):
        """"""
        Write the encoding of the BigInteger to the output stream.

        Args:
            ostream (Stream): A buffer to contain the encoded bytes of a
                BigInteger object. Usually a BytearrayStream object.
                Required.
            kmip_version (KMIPVersion): An enumeration defining the KMIP
                version with which the object will be encoded. Optional,
                defaults to KMIP 1.0.
        """"""
        # Convert the value to binary and pad it as needed.
        binary = ""{0:b}"".format(abs(self.value))
        binary = (""0"" * (64 - (len(binary) % 64))) + binary

        # If the value is negative, convert via two's complement.
        if self.value < 0:
            binary = binary.replace('1', 'i')
            binary = binary.replace('0', '1')
            binary = binary.replace('i', '0')

            pivot = binary.rfind('0')
            binary = binary[0:pivot] + '1' + ('0' * len(binary[pivot + 1:]))

        # Convert each byte to hex and build the hex string for the value.
        hexadecimal = b''
        for i in range(0, len(binary), 8):
            byte = binary[i:i + 8]
            byte = int(byte, 2)
            hexadecimal += struct.pack('!B', byte)

        self.length = len(hexadecimal)
        super(BigInteger, self).write(ostream, kmip_version=kmip_version)
        ostream.write(hexadecimal)",https://github.com/OpenKMIP/PyKMIP/blob/b51c5b044bd05f8c85a1d65d13a583a4d8fc1b0e/kmip/core/primitives.py#L479-L513
tfidf_python_100_1.0,get current ip address,python,"def get_data(self, addr_id=None, ip=None):
        for address in self.values():
            if addr_id is not None:
                if addr_id == address.address_id:
                    return address
            else:
                assert ip is not None
                if ipv4_apply_mask(ip, address.netmask) == address.nw_addr:
                    return address
        return None",https://github.com/osrg/ryu/blob/6f906e72c92e10bd0264c9b91a2f7bb85b97780c/ryu/app/rest_router.py#L1296-L1305
tfidf_python_100_1.0,get current ip address,python,"def split_address(address):
    if "":"" in address:
        ip, _, port = address.partition("":"")
    else:
        if address.isdigit():
            ip, port = """", address
        else:
            ip, port = address, """"
    return ip, port",https://github.com/tjguk/networkzero/blob/0e3e81d2e9200b25a83ac07741612283599486d7/networkzero/core.py#L91-L99
tfidf_python_100_1.0,get current ip address,python,"def address(self, ip, **kwargs):
        """"""Add Address data to Batch object.

        Args:
            ip (str): The value for this Indicator.
            confidence (str, kwargs): The threat confidence for this Indicator.
            date_added (str, kwargs): The date timestamp the Indicator was created.
            last_modified (str, kwargs): The date timestamp the Indicator was last modified.
            rating (str, kwargs): The threat rating for this Indicator.
            xid (str, kwargs): The external id for this Indicator.

        Returns:
            obj: An instance of Address.
        """"""
        indicator_obj = Address(ip, **kwargs)
        return self._indicator(indicator_obj)",https://github.com/ThreatConnect-Inc/tcex/blob/dd4d7a1ef723af1561687120191886b9a2fd4b47/tcex/tcex_ti_batch.py#L359-L374
tfidf_python_100_1.0,get current ip address,python,"def get_sds_by_ip(self,ip):
        """"""
        Get ScaleIO SDS object by its ip address
        :param name: IP address of SDS
        :return: ScaleIO SDS object
        :raise KeyError: No SDS with specified ip found
        :rtype: SDS object
        """"""
        if self.conn.is_ip_addr(ip):
            for sds in self.sds:
                for sdsIp in sds.ipList:
                    if sdsIp == ip:
                        return sds
            raise KeyError(""SDS of that name not found"")
        else:
            raise ValueError(""Malformed IP address - get_sds_by_ip()"")",https://github.com/swevm/scaleio-py/blob/d043a0137cb925987fd5c895a3210968ce1d9028/scaleiopy/api/scaleio/cluster/sds.py#L104-L119
tfidf_python_100_1.0,get current ip address,python,"def is_valid_address(address, port_range=range(65536)):
    ip, port = split_address(address)
    return is_valid_ip_pattern(ip) and is_valid_port(port, port_range)",https://github.com/tjguk/networkzero/blob/0e3e81d2e9200b25a83ac07741612283599486d7/networkzero/core.py#L118-L120
tfidf_python_100_1.0,get current ip address,python,"def validate_rpc_host(ip):
    """"""
    Validates the given ip for use as RPC server address.
    """"""
    if not is_valid_ipv4(ip) and not is_valid_ipv6(ip):
        raise ApplicationException(
            desc='Invalid RPC ip address: %s' % ip)
    return ip",https://github.com/osrg/ryu/blob/6f906e72c92e10bd0264c9b91a2f7bb85b97780c/ryu/services/protocols/bgp/application.py#L153-L160
tfidf_python_100_1.0,get current ip address,python,"def _check_ip(ip: str) -> bool:
    """"""
    Check IP in range

    :param ip:
    :return:
    """"""
    address = ipaddress.IPv4Address(ip)
    return address in allowed_ips",https://github.com/aiogram/aiogram/blob/2af930149ce2482547721e2c8755c10307295e48/aiogram/dispatcher/webhook.py#L39-L47
tfidf_python_100_1.0,get current ip address,python,"def ip(ip_address, return_format=None):
    """"""Returns a summary of the information our database holds for a
    particular IP address (similar to /ipinfo.html).

    In the returned data:

    Count: (also reports or records) total number of packets blocked from
    this IP.
    Attacks: (also targets) number of unique destination IP addresses for
    these packets.

    :param ip_address: a valid IP address
    """"""
    response = _get('ip/{address}'.format(address=ip_address), return_format)
    if 'bad IP address' in str(response):
        raise Error('Bad IP address, {address}'.format(address=ip_address))
    else:
        return response",https://github.com/rshipp/python-dshield/blob/1b003d0dfac0bc2ee8b86ca5f1a44b765b8cc6e0/dshield.py#L57-L74
tfidf_python_100_1.0,get current ip address,python,"def lookup_mac(self, ip):
		""""""Look up a lease object with given ip address and return the
		associated mac address.

		@type ip: str
		@rtype: str or None
		@raises ValueError:
		@raises OmapiError:
		@raises OmapiErrorNotFound: if no lease object with the given ip could be found
		@raises OmapiErrorAttributeNotFound: if lease could be found, but objects lacks a mac
		@raises socket.error:
		""""""
		res = self.lookup_by_lease(ip=ip)
		try:
			return res[""hardware-address""]
		except KeyError:
			raise OmapiErrorAttributeNotFound()",https://github.com/CygnusNetworks/pypureomapi/blob/ff4459678ec023fd56e64ce518a86860efec26bf/pypureomapi.py#L1111-L1127
tfidf_python_100_1.0,get current ip address,python,"def get_sdc_by_ip(self, ip):
        """"""
        Get ScaleIO SDC object by its ip
        :param name: IP address of SDC
        :return: ScaleIO SDC object
        :raise KeyError: No SDC with specified IP found
        :rtype: SDC object
        """"""
        if self.conn.is_ip_addr(ip):
            for sdc in self.sdc:
                if sdc.sdcIp == ip:
                    return sdc
            raise KeyError(""SDS of that name not found"")
        else:
            raise ValueError(""Malformed IP address - get_sdc_by_ip()"")",https://github.com/swevm/scaleio-py/blob/d043a0137cb925987fd5c895a3210968ce1d9028/scaleiopy/api/scaleio/cluster/sdc.py#L88-L102
tfidf_python_100_1.0,get current ip address,python,"async def ban(ip, timeout=0):
    """"""
    """"""
    ipset = Ipset()
    address, ipset_name = ipset.chose_blacklist(ip)
    print(""Adding {0} to {1}"".format(address, ipset_name))

    return await ipset.add(ipset_name, address, timeout)",https://github.com/Frzk/Ellis/blob/39ce8987cbc503354cf1f45927344186a8b18363/ellis_actions/ipset.py#L119-L126
tfidf_python_100_1.0,get current ip address,python,"def set_ip(self, ip):
        """"""Change the current IP.""""""
        self.set(ip=ip, netmask=self._nm)",https://github.com/alberanid/python-iplib/blob/488b56fe57ad836b27feec9e76f51883db28faa6/iplib.py#L764-L766
tfidf_python_100_1.0,get current ip address,python,"def __init__(self, ip, **kwargs):
        """"""Initialize Class Properties.

        Args:
            ip (str): The value for this Indicator.
            active (bool, kwargs): If False the indicator is marked ""inactive"" in TC.
            confidence (str, kwargs): The threat confidence for this Indicator.
            date_added (str, kwargs): The date timestamp the Indicator was created.
            last_modified (str, kwargs): The date timestamp the Indicator was last modified.
            private_flag (bool, kwargs): If True the indicator is marked as private in TC.
            rating (str, kwargs): The threat rating for this Indicator.
            xid (str, kwargs): The external id for this Indicator.
        """"""
        super(Address, self).__init__('Address', ip, **kwargs)",https://github.com/ThreatConnect-Inc/tcex/blob/dd4d7a1ef723af1561687120191886b9a2fd4b47/tcex/tcex_ti_indicator.py#L382-L395
tfidf_python_100_1.0,get current ip address,python,"def is_valid_ipv4 (ip):
    """"""
    Return True if given ip is a valid IPv4 address.
    """"""
    if not _ipv4_re.match(ip):
        return False
    a, b, c, d = [int(i) for i in ip.split(""."")]
    return a <= 255 and b <= 255 and c <= 255 and d <= 255",https://github.com/wummel/linkchecker/blob/c2ce810c3fb00b895a841a7be6b2e78c64e7b042/linkcheck/network/iputil.py#L104-L111
tfidf_python_100_1.0,get current ip address,python,"def get_address_reachability(self, address: Address) -> AddressReachability:
        """""" Return the current reachability state for ``address``. """"""
        return self._address_to_reachability.get(address, AddressReachability.UNKNOWN)",https://github.com/raiden-network/raiden/blob/407ba15c72074e9de88771d6b9661ff4dc36bef5/raiden/network/transport/matrix/utils.py#L146-L148
tfidf_python_100_1.0,get current ip address,python,"def vt_ip_check(ip, vt_api):
    """"""Checks VirusTotal for occurrences of an IP address""""""
    if not is_IPv4Address(ip):
        return None

    url = 'https://www.virustotal.com/vtapi/v2/ip-address/report'
    parameters = {'ip': ip, 'apikey': vt_api}
    response = requests.get(url, params=parameters)
    try:
        return response.json()
    except ValueError:
        return None",https://github.com/yolothreat/utilitybelt/blob/55ac6c31f87963d5e97be0402a4343c84846d118/utilitybelt/utilitybelt.py#L284-L295
tfidf_python_100_1.0,get current ip address,python,"def get_local_current_sample(ip):
    """"""Gets current sample from *local* Neurio device IP address.

    This is a static method. It doesn't require a token to authenticate.

    Note, call get_user_information to determine local Neurio IP addresses.

    Args:
      ip (string): address of local Neurio device

    Returns:
      dictionary object containing current sample information
    """"""
    valid_ip_pat = re.compile(
      ""^((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$""
    )
    if not valid_ip_pat.match(ip):
      raise ValueError(""ip address invalid"")

    url = ""http://%s/current-sample"" % (ip)
    headers = { ""Content-Type"": ""application/json"" }

    r = requests.get(url, headers=headers)
    return r.json()",https://github.com/jordanh/neurio-python/blob/3a1bcadadb3bb3ad48f2df41c039d8b828ffd9c8/neurio/__init__.py#L390-L413
tfidf_python_100_1.0,get current ip address,python,"def enumerate_interfaces_of_adapter(nice_name, address):
    
    # Iterate through linked list and fill list
    addresses = []
    while True:
        addresses.append(address)
        if not address.Next:
            break
        address = address.Next[0]
        
    for address in addresses:
        ip = shared.sockaddr_to_ip(address.Address.lpSockaddr)
        network_prefix = address.OnLinkPrefixLength
        yield shared.IP(ip, network_prefix, nice_name)",https://github.com/pydron/ifaddr/blob/9fae414dfab97e109bf089773a075df83b7ae91a/ifaddr/_win32.py#L75-L88
tfidf_python_100_1.0,get current ip address,python,"def _get_next_available_fixed_ip(port):
    floating_ips = [ip for ip in port.ip_addresses
                    if ip.get('address_type') in
                    (ip_types.FLOATING, ip_types.SCALING)]
    fixed_ips = [ip for ip in port.ip_addresses
                 if ip.get('address_type') == ip_types.FIXED]

    if not fixed_ips or len(fixed_ips) == 0:
        return None

    used = [ip.fixed_ip.address for ip in floating_ips
            if ip and ip.fixed_ip]

    return next((ip for ip in sorted(fixed_ips,
                                     key=lambda ip: ip.get('allocated_at'))
                if ip.address not in used), None)",https://github.com/openstack/quark/blob/1112e6a66917d3e98e44cb7b33b107fd5a74bb2e/quark/plugin_modules/floating_ips.py#L113-L128
tfidf_python_100_1.0,get current ip address,python,"def resolve_peer(self, info, **kwargs):
        ip = kwargs.get('ip')

        if ip is not None:
            return ark_blockchain.Peers.objects.get(ip=ip)
        return None",https://github.com/BlockHub/django-ark/blob/424c3b4f258ba756aa63b2da185d0c0ef946f75f/ark/schema.py#L156-L161
tfidf_python_100_1.0,convert int to bool,python,"def uint16_gt(a: int, b: int) -> bool:
    """"""
    Return a > b.
    """"""
    half_mod = 0x8000
    return (((a < b) and ((b - a) > half_mod)) or
            ((a > b) and ((a - b) < half_mod)))",https://github.com/aiortc/aiortc/blob/60ed036abf4575bd63985724b4493d569e6da29b/aiortc/utils.py#L20-L26
tfidf_python_100_1.0,convert int to bool,python,"def uint32_gt(a: int, b: int) -> bool:
    """"""
    Return a > b.
    """"""
    half_mod = 0x80000000
    return (((a < b) and ((b - a) > half_mod)) or
            ((a > b) and ((a - b) < half_mod)))",https://github.com/aiortc/aiortc/blob/60ed036abf4575bd63985724b4493d569e6da29b/aiortc/utils.py#L43-L49
tfidf_python_100_1.0,convert int to bool,python,"def uint16_gte(a: int, b: int) -> bool:
    """"""
    Return a >= b.
    """"""
    return (a == b) or uint16_gt(a, b)",https://github.com/aiortc/aiortc/blob/60ed036abf4575bd63985724b4493d569e6da29b/aiortc/utils.py#L29-L33
tfidf_python_100_1.0,convert int to bool,python,"def uint32_gte(a: int, b: int) -> bool:
    """"""
    Return a >= b.
    """"""
    return (a == b) or uint32_gt(a, b)",https://github.com/aiortc/aiortc/blob/60ed036abf4575bd63985724b4493d569e6da29b/aiortc/utils.py#L52-L56
tfidf_python_100_1.0,convert int to bool,python,"def __init__(
            self,
            is_winner: bool,
            is_crashed: bool,
            building_score: int,
            kill_score: int,
            razing_score: int,
            unit_score: int,
    ) -> None:
        self.is_winner = is_winner
        self.is_crashed = is_crashed
        self.building_score = building_score
        self.kill_score = kill_score
        self.razing_score = razing_score
        self.unit_score = unit_score",https://github.com/Games-and-Simulations/sc-docker/blob/1d7adb9b5839783655564afc4bbcd204a0055dcb/scbw/result.py#L12-L26
tfidf_python_100_1.0,convert int to bool,python,"def int2base36(n):
    """"""
    Convert int base10 to base36.
    Back convert: int('<base36>', 36)
    """"""
    assert isinstance(n, (int, long))
    c = '0123456789abcdefghijklmnopqrstuvwxyz'
    if n < 0:
        return '-' + int2base36(-n)
    elif n < 36:
        return c[n]
    b36 = ''
    while n != 0:
        n, i = divmod(n, 36)
        b36 = c[i] + b36
    return b36",https://github.com/liminspace/dju-common/blob/c68860bb84d454a35e66275841c20f38375c2135/dju_common/tools.py#L23-L38
tfidf_python_100_1.0,convert int to bool,python,"def collided_done(py_measurements):
    m = py_measurements
    collided = (m[""collision_vehicles""] > 0 or m[""collision_pedestrians""] > 0
                or m[""collision_other""] > 0)
    return bool(collided or m[""total_reward""] < -100)",https://github.com/ray-project/ray/blob/4eade036a0505e244c976f36aaa2d64386b5129b/python/ray/rllib/examples/carla/env.py#L660-L664
tfidf_python_100_1.0,convert int to bool,python,"def set_double_width(self, on_off):
        self._write_immediately(h2b(b'1b57') + bchr(int(bool(on_off == DoubleWidth.on))))",https://github.com/lukegb/ticketml/blob/0da6eabead31c43bc8cd7301b5f20abd735e569d/ticketml/ticketml.py#L179-L180
tfidf_python_100_1.0,convert int to bool,python,"def set_double_height(self, on_off):
        self._write_immediately(h2b(b'1b68') + bchr(int(bool(on_off == DoubleHeight.on))))",https://github.com/lukegb/ticketml/blob/0da6eabead31c43bc8cd7301b5f20abd735e569d/ticketml/ticketml.py#L176-L177
tfidf_python_100_1.0,convert int to bool,python,"def integer_value_convert(dictin, dropfailedvalues=False):
    # type: (DictUpperBound, bool) -> Dict
    """"""Convert values of dictionary to integers

    Args:
        dictin (DictUpperBound): Input dictionary
        dropfailedvalues (bool): Whether to drop dictionary entries where key conversion fails. Defaults to False.

    Returns:
        Dict: Dictionary with values converted to integers

    """"""
    return key_value_convert(dictin, valuefn=int, dropfailedvalues=dropfailedvalues)",https://github.com/OCHA-DAP/hdx-python-utilities/blob/9c89e0aa5afac2c002b02a2d8f0e5b91eeb3d2a3/src/hdx/utilities/dictandlist.py#L283-L295
tfidf_python_100_1.0,convert int to bool,python,"def integer_key_convert(dictin, dropfailedkeys=False):
    # type: (DictUpperBound, bool) -> Dict
    """"""Convert keys of dictionary to integers

    Args:
        dictin (DictUpperBound): Input dictionary
        dropfailedkeys (bool): Whether to drop dictionary entries where key conversion fails. Defaults to False.

    Returns:
        Dict: Dictionary with keys converted to integers

    """"""
    return key_value_convert(dictin, keyfn=int, dropfailedkeys=dropfailedkeys)",https://github.com/OCHA-DAP/hdx-python-utilities/blob/9c89e0aa5afac2c002b02a2d8f0e5b91eeb3d2a3/src/hdx/utilities/dictandlist.py#L268-L280
tfidf_python_100_1.0,convert int to bool,python,"def __init__(self, *, need_technical_acknowledgement: bool, batch_technical_acknowledgements: bool, need_functional_acknowledgement: bool, batch_functional_acknowledgements: bool, need_loop_for_valid_messages: bool, send_synchronous_acknowledgement: bool, acknowledgement_control_number_lower_bound: int, acknowledgement_control_number_upper_bound: int, rollover_acknowledgement_control_number: bool, acknowledgement_control_number_prefix: str=None, acknowledgement_control_number_suffix: str=None, **kwargs) -> None:
        super(EdifactAcknowledgementSettings, self).__init__(**kwargs)
        self.need_technical_acknowledgement = need_technical_acknowledgement
        self.batch_technical_acknowledgements = batch_technical_acknowledgements
        self.need_functional_acknowledgement = need_functional_acknowledgement
        self.batch_functional_acknowledgements = batch_functional_acknowledgements
        self.need_loop_for_valid_messages = need_loop_for_valid_messages
        self.send_synchronous_acknowledgement = send_synchronous_acknowledgement
        self.acknowledgement_control_number_prefix = acknowledgement_control_number_prefix
        self.acknowledgement_control_number_suffix = acknowledgement_control_number_suffix
        self.acknowledgement_control_number_lower_bound = acknowledgement_control_number_lower_bound
        self.acknowledgement_control_number_upper_bound = acknowledgement_control_number_upper_bound
        self.rollover_acknowledgement_control_number = rollover_acknowledgement_control_number",https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-mgmt-logic/azure/mgmt/logic/models/edifact_acknowledgement_settings_py3.py#L81-L93
tfidf_python_100_1.0,convert int to bool,python,"def list(self, include_disabled=None, include_count=None,
             include_users=None):
        if isinstance(include_disabled, bool):
            include_disabled = int(include_disabled)

        if isinstance(include_count, bool):
            include_count = int(include_count)

        if isinstance(include_users, bool):
            include_users = int(include_users)

        return self.get('usergroups.list', params={
            'include_disabled': include_disabled,
            'include_count': include_count,
            'include_users': include_users,
        })",https://github.com/os/slacker/blob/133596971b44a7b3bd07d11afb37745a67f0ec46/slacker/__init__.py#L981-L996
tfidf_python_100_1.0,convert int to bool,python,"def get_context_size(self):
        if self.is_assignment:
            return 1
        return int((len(self._updates or []) * 2) + int(bool(self._removals)))",https://github.com/datastax/python-driver/blob/30a80d0b798b1f45f8cb77163b1fa791f3e3ca29/cassandra/cqlengine/statements.py#L394-L397
tfidf_python_100_1.0,convert int to bool,python,"def __init__(self, *, validate_character_set: bool, check_duplicate_interchange_control_number: bool, interchange_control_number_validity_days: int, check_duplicate_group_control_number: bool, check_duplicate_transaction_set_control_number: bool, validate_edi_types: bool, validate_xsd_types: bool, allow_leading_and_trailing_spaces_and_zeroes: bool, trim_leading_and_trailing_spaces_and_zeroes: bool, trailing_separator_policy, **kwargs) -> None:
        super(X12ValidationSettings, self).__init__(**kwargs)
        self.validate_character_set = validate_character_set
        self.check_duplicate_interchange_control_number = check_duplicate_interchange_control_number
        self.interchange_control_number_validity_days = interchange_control_number_validity_days
        self.check_duplicate_group_control_number = check_duplicate_group_control_number
        self.check_duplicate_transaction_set_control_number = check_duplicate_transaction_set_control_number
        self.validate_edi_types = validate_edi_types
        self.validate_xsd_types = validate_xsd_types
        self.allow_leading_and_trailing_spaces_and_zeroes = allow_leading_and_trailing_spaces_and_zeroes
        self.trim_leading_and_trailing_spaces_and_zeroes = trim_leading_and_trailing_spaces_and_zeroes
        self.trailing_separator_policy = trailing_separator_policy",https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-mgmt-logic/azure/mgmt/logic/models/x12_validation_settings_py3.py#L80-L91
tfidf_python_100_1.0,convert int to bool,python,"def pick_charset(use_unicode: bool, emphasize: bool) -> BoxDrawCharacterSet:
    if not use_unicode:
        return ASCII_BOX_CHARS
    if emphasize:
        return BOLD_BOX_CHARS
    return NORMAL_BOX_CHARS",https://github.com/quantumlib/Cirq/blob/0827da80dd7880e5b923eb69407e980ed9bc0bd2/cirq/circuits/text_diagram_drawer.py#L50-L55
tfidf_python_100_1.0,convert int to bool,python,"def __init__(self, actfunc, strfunc, convert=lambda x: x):
        self.actfunc = actfunc
        self.strfunc = strfunc
        self.convert = convert",https://github.com/iotile/coretools/blob/2d794f5f1346b841b0dcd16c9d284e9bf2f3c6ec/iotilebuild/iotile/build/config/scons-local-3.0.1/SCons/Action.py#L1379-L1382
tfidf_python_100_1.0,convert int to bool,python,"def __init__(self, multiple_errors_tb_limit: int = 3, full_paths_in_logs: bool = False, 
                 dict_to_object_subclass_limit: int = 50):
        self.multiple_errors_tb_limit = multiple_errors_tb_limit
        self.full_paths_in_logs = full_paths_in_logs
        self.dict_to_object_subclass_limit = dict_to_object_subclass_limit",https://github.com/smarie/python-parsyfiles/blob/344b37e1151e8d4e7c2ee49ae09d6568715ae64e/parsyfiles/global_config.py#L6-L10
tfidf_python_100_1.0,convert int to bool,python,"def __init__(self, *, override_message_properties: bool, encrypt_message: bool, sign_message: bool, compress_message: bool, check_duplicate_message: bool, interchange_duplicates_validity_days: int, check_certificate_revocation_list_on_send: bool, check_certificate_revocation_list_on_receive: bool, encryption_algorithm, signing_algorithm=None, **kwargs) -> None:
        super(AS2ValidationSettings, self).__init__(**kwargs)
        self.override_message_properties = override_message_properties
        self.encrypt_message = encrypt_message
        self.sign_message = sign_message
        self.compress_message = compress_message
        self.check_duplicate_message = check_duplicate_message
        self.interchange_duplicates_validity_days = interchange_duplicates_validity_days
        self.check_certificate_revocation_list_on_send = check_certificate_revocation_list_on_send
        self.check_certificate_revocation_list_on_receive = check_certificate_revocation_list_on_receive
        self.encryption_algorithm = encryption_algorithm
        self.signing_algorithm = signing_algorithm",https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-mgmt-logic/azure/mgmt/logic/models/as2_validation_settings_py3.py#L79-L90
tfidf_python_100_1.0,convert int to bool,python,"def console_credits_render(x: int, y: int, alpha: bool) -> bool:
    return bool(lib.TCOD_console_credits_render(x, y, alpha))",https://github.com/libtcod/python-tcod/blob/8ba10c5cfb813eaf3e834de971ba2d6acb7838e4/tcod/libtcodpy.py#L1127-L1128
tfidf_python_100_1.0,read text file line by line,python,"def _same_frame(frameA, frameB):
    return (frameA.line, frameA.file) == (frameB.line, frameB.file)",https://github.com/klahnakoski/mo-logs/blob/0971277ac9caf28a755b766b70621916957d4fea/mo_logs/__init__.py#L399-L400
tfidf_python_100_1.0,read text file line by line,python,"def __init__(self, text, line):
        self.line = line
        self.text = text",https://github.com/SergeySatskiy/cdm-pythonparser/blob/7e933aca899b1853d744082313ffc3a8b1154505/cdmpyparser.py#L237-L239
tfidf_python_100_1.0,read text file line by line,python,"def __init__(self, text, file=None, line=None):
        Exception.__init__(self, text)
        self.file = file
        self.line = line",https://github.com/UAVCAN/pyuavcan/blob/a06a9975c1c0de4f1d469f05b29b374332968e2b/uavcan/dsdl/common.py#L24-L27
tfidf_python_100_1.0,read text file line by line,python,"def block_quote(self, text):
    r = ''
    for line in text.splitlines():
      r += (line and '> ' or '') + line + '\n'
    return r",https://github.com/lepture/mistune-contrib/blob/3180edfc6b4477ead5ef7754a57907ae94080c24/mistune_contrib/mdrenderer.py#L59-L63
tfidf_python_100_1.0,read text file line by line,python,"def ints_of(line=0):
    x_coords, y_coords = all_coords[line]
    return [coord2int(x) for x in x_coords], [coord2int(x) for x in y_coords]",https://github.com/MrMinimal64/timezonefinder/blob/96cc43afb3bae57ffd002ab4cf104fe15eda2257/timezonefinder/file_converter.py#L188-L190
tfidf_python_100_1.0,read text file line by line,python,"def process_line(line):
    if not line:
        return """"

    line = reindent_line(line)
    line = clean_line(line)

    return line",https://github.com/9seconds/concierge/blob/5fc7582b0bab86731b4eaefe85637c4960932e59/concierge/core/lexer.py#L51-L58
tfidf_python_100_1.0,read text file line by line,python,"def read_vcf(vcf_file):
    """"""
    Read a vcf file to a dict of lists.

    :param str vcf_file: Path to a vcf file.
    :return: dict of lists of vcf records
    :rtype: dict
    """"""
    vcf_dict = []
    with open(vcf_file, 'r') as invcf:
        for line in invcf:
            if line.startswith('#'):
                continue
            line = line.strip().split()
            vcf_dict.append((line[0], line[1], line[3], line[4]))
    return vcf_dict",https://github.com/BD2KGenomics/protect/blob/06310682c50dcf8917b912c8e551299ff7ee41ce/src/protect/mutation_calling/common.py#L146-L161
tfidf_python_100_1.0,read text file line by line,python,"def get_psm_id(line):
    return '{0}_{1}_{2}'.format(line[mzidtsvdata.HEADER_SPECFILE],
                                line[mzidtsvdata.HEADER_SCANNR],
                                line[mzidtsvdata.HEADER_PEPTIDE])",https://github.com/glormph/msstitch/blob/ded7e5cbd813d7797dc9d42805778266e59ff042/src/app/readers/tsv.py#L68-L71
tfidf_python_100_1.0,read text file line by line,python,"def _removeQuotes(line):
    origLine = line
    line = line.replace(r'\\', 'X')
    line = re.sub(r'\\\""|\\\'', 'X', line)
    line = _removeQuoteSet(line, '""""""', ""'''"")
    if line is None:
        return None
    if line != _removeQuoteSet(line, '""""', ""''""):
        return origLine
    line = _removeQuoteSet(line, '""', ""'"")
    if line is None:
        return origLine
    return line",https://github.com/sassoftware/epdb/blob/5a8375aa59862d787e6496810a508297a5522967/epdb/__init__.py#L1135-L1147
tfidf_python_100_1.0,read text file line by line,python,"def TabsToSpaces(line):
	i = 0
	while i < len(line) and line[i] == '\t':
		i += 1
	return ' '*(8*i) + line[i:]",https://github.com/kpdyer/regex2dfa/blob/109f877e60ef0dfcb430f11516d215930b7b9936/third_party/re2/lib/codereview/codereview.py#L631-L635
tfidf_python_100_1.0,read text file line by line,python,"def stripcomments(self, line):
		for commentstart in self.commentstarts:
			if(commentstart in line):
				line = line[:line.index(commentstart)]
		return line",https://github.com/daknuett/py_register_machine2/blob/599c53cd7576297d0d7a53344ed5d9aa98acc751/tools/assembler/assembler.py#L280-L284
tfidf_python_100_1.0,read text file line by line,python,"def argument_run(self, sp_r):
		""""""
		.. _argument_run:

		Converts Arguments according to ``to_int``
		""""""
		arg_run = []

		for line in sp_r:
			logging.debug(""argument run: handling: "" + str(line))
			if(line[1] == ""data""):
				arg_run.append( (line[0], line[1], line[2], line[2].get_words(line[3])))
				continue
			if(line[1] == ""command""):
				self.checkargs(line[0], line[2], line[3])
				arg_run.append( (line[0], line[1], line[2], [a for a in self.convert_args(line[2], line[3])]))
		return arg_run",https://github.com/daknuett/py_register_machine2/blob/599c53cd7576297d0d7a53344ed5d9aa98acc751/tools/assembler/assembler.py#L162-L178
tfidf_python_100_1.0,read text file line by line,python,"def match(self, file, line):
        line = line.replace(""\r"", """")
        return line.rstrip() != line",https://github.com/ansible/ansible-lint/blob/b4a8743794e592698c32e760bc774a85f8eebeb5/lib/ansiblelint/rules/TrailingWhitespaceRule.py#L32-L34
tfidf_python_100_1.0,read text file line by line,python,"def maybe_strip_comment(line):
    if '#' in line:
        line = line[:line.index('#')]
        line = line.rstrip()
    return line",https://github.com/natasha/natasha/blob/08e9f1809111c1300db3ca45cc621ff5279a1bef/natasha/data/__init__.py#L25-L29
tfidf_python_100_1.0,read text file line by line,python,"def handle_line(self, line):
        """"""Read one line.""""""
        if line.kind == ConfigLine.KIND_HEADER:
            self.enter_block(line.header)
        else:
            self.insert_line(line)",https://github.com/rbarrois/confutils/blob/26bbb3f31c09a99ee2104263a9e97d6d3fc8e4f4/confutils/configfile.py#L397-L402
tfidf_python_100_1.0,read text file line by line,python,"def GetInitialSpaces(line):
    initial_spaces = 0
    while initial_spaces < len(line) and line[initial_spaces] == ' ':
        initial_spaces += 1
    return initial_spaces",https://github.com/richq/cmake-lint/blob/058c6c0ed2536abd3e79a51c38ee6e686568e3b3/cmakelint/main.py#L315-L319
tfidf_python_100_1.0,read text file line by line,python,"def write_inband(self, line):
        if line.startswith('#$#'):
            line = '#$""' + line
        self.write_cb(line)",https://github.com/revarbat/mudclientprotocol/blob/cc7c9b9286a0ababd437685a28982d66e88bca5c/mudclientprotocol/__init__.py#L305-L308
tfidf_python_100_1.0,read text file line by line,python,"def decode_waypoint(self, line):
        line = line.strip()

        if not line or line.startswith('$'):
            return

        # Check valid line length
        if len(line) != 64:
            raise ParserError('Line length does not match 64')

        return {
            'shortform': self.decode_shortform(line),
            'is_airfield': self.decode_is_airfield(line),
            'is_unclear': self.decode_is_unclear(line),
            'is_outlanding': self.decode_is_outlanding(line),
            'shortform_zander': self.decode_shortform_zander(line),
            'text': self.decode_text(line),
            'icao': self.decode_icao(line),
            'is_ulm': self.decode_is_ulm(line),
            'field_number': self.decode_field_number(line),
            'is_glidersite': self.decode_is_glidersite(line),
            'runway_surface': self.decode_runway_surface(line),
            'runway_length': self.decode_runway_length(line),
            'runway_directions': self.decode_runway_directions(line),
            'frequency': self.decode_frequency(line),
            'elevation': self.decode_elevation(line),
            'elevation_proved': self.decode_elevation_proved(line),
            'latitude': self.decode_latitude(line),
            'longitude': self.decode_longitude(line),
            'ground_check_necessary': self.decode_ground_check_necessary(line),
            'better_coordinates': self.decode_better_coordinates(line),
            'country': self.decode_country(line),
            'year_code': self.decode_year_code(line),
            'source_code': self.decode_source_code(line),
        }",https://github.com/Turbo87/aerofiles/blob/d8b7b04a1fcea5c98f89500de1164619a4ec7ef4/aerofiles/welt2000/reader.py#L47-L81
tfidf_python_100_1.0,read text file line by line,python,"def fix_line_from_coverage_file(line):
    line = line.rstrip()
    if line.startswith('!'):
        line = line[1:]
    return line",https://github.com/quantumlib/Cirq/blob/0827da80dd7880e5b923eb69407e980ed9bc0bd2/dev_tools/incremental_coverage.py#L108-L112
tfidf_python_100_1.0,read text file line by line,python,"def __init__(self, line):
        if not line:
            line = repr(line)
        self.args = line,
        self.line = line",https://github.com/PythonCharmers/python-future/blob/c423752879acc05eebc29b0bb9909327bd5c7308/src/future/backports/http/client.py#L1320-L1324
tfidf_python_100_1.0,get executable path,python,"def _get_executable(process_dict):
    try:
        executable = process_dict.get('executable')
    except (AttributeError, TypeError):
        return None
    if isinstance(executable, string_types):
        executable = executable.lower().rsplit('.', 1)[0]
    return executable",https://github.com/pypa/pipenv/blob/cae8d76c210b9777e90aab76e9c4b0e53bb19cde/pipenv/vendor/shellingham/nt.py#L96-L103
tfidf_python_100_1.0,get executable path,python,"def add_to_executables_found(executables_found, executable):
    if is_valid_mongo_exe(executable):
        if executable not in executables_found:
            executables_found.append(executable)
    else:
        log_verbose(""Not a valid executable '%s'. Skipping..."" % executable)",https://github.com/mongolab/mongoctl/blob/fab15216127ad4bf8ea9aa8a95d75504c0ef01a2/mongoctl/commands/command_utils.py#L128-L133
tfidf_python_100_1.0,get executable path,python,"def __init__(self, executable, input_file_grps, output_file_grps, parameter_path=None):
        self.executable = executable
        self.input_file_grps = input_file_grps
        self.output_file_grps = output_file_grps
        self.parameter_path = parameter_path",https://github.com/OCR-D/core/blob/57e68c578526cb955fd2e368207f5386c459d91d/ocrd/ocrd/task_sequence.py#L34-L38
tfidf_python_100_1.0,get executable path,python,"def __init__(self, executable=None, **params):
        if executable is None:
            executable = sys.executable
        self._pprint_args = ([],[],None,{})
        super(Command,self).__init__(executable=executable, **params)
        self.pprint_args([],[])",https://github.com/ioam/lancet/blob/1fbbf88fa0e8974ff9ed462e3cb11722ddebdd6e/lancet/launch.py#L52-L57
tfidf_python_100_1.0,get executable path,python,"def run(self):
        executable = find_executable(""configure"", path=SOURCE)
        if executable:
            import subprocess
            executable = os.path.abspath(executable)
            subprocess.check_call(executable, cwd=os.path.dirname(executable))
        build.run(self)",https://github.com/pbanaszkiewicz/python-rrdtool/blob/fc39e4d06bd279fb1780a240a1f18989ba57e9e0/setup.py#L37-L43
tfidf_python_100_1.0,get executable path,python,"def run(self):
        if not os.path.exists(os.path.join(SOURCE, ""config.status"")):
            executable = find_executable(""configure"", path=SOURCE)
            if executable:
                import subprocess
                executable = os.path.abspath(executable)
                os.chmod(executable, 0777)
                subprocess.check_call(executable, cwd=os.path.dirname(executable))",https://github.com/pbanaszkiewicz/python-rrdtool/blob/fc39e4d06bd279fb1780a240a1f18989ba57e9e0/setup.py#L55-L62
tfidf_python_100_1.0,get executable path,python,"def append_executable(self, executable):
        """"""Append san executable os command to the list to be called.

        Argument:
          executable (str): os callable executable.

        """"""
        if isinstance(executable, str) and not isinstance(executable, unicode):
            executable = unicode(executable)
        if not isinstance(executable, unicode):
            raise TypeError(""expected executable name as str, not {}"".
                            format(executable.__class__.__name__))
        self._executables.append(executable)",https://github.com/elmotec/massedit/blob/57e22787354896d63a8850312314b19aa0308906/massedit.py#L296-L308
tfidf_python_100_1.0,get executable path,python,"def __init__(self, executable, **params):
        super(ShellCommand,self).__init__(executable = executable,
                                          do_format=False,
                                          **params)
        self.pprint_args(['executable','posargs'],['long_prefix'])",https://github.com/ioam/lancet/blob/1fbbf88fa0e8974ff9ed462e3cb11722ddebdd6e/lancet/launch.py#L155-L159
tfidf_python_100_1.0,get executable path,python,"def find_executable(executable):
    '''
    Finds executable in PATH

    Returns:
        string or None
    '''
    logger = logging.getLogger(__name__)
    logger.debug(""Checking executable '%s'..."", executable)
    executable_path = _find_executable(executable)
    found = executable_path is not None
    if found:
        logger.debug(""Executable '%s' found: '%s'"", executable, executable_path)
    else:
        logger.debug(""Executable '%s' not found"", executable)
    return executable_path",https://github.com/grigi/talkey/blob/5d2d4a1f7001744c4fd9a79a883a3f2001522329/talkey/utils.py#L12-L27
tfidf_python_100_1.0,get executable path,python,"def __init__(self, executable):
        super(Node, self).__init__(executable)
        self.executed = False
        self.set_category(executable.name)

        if executable.universe == 'vanilla' and executable.installed:
            self.add_profile('condor', 'getenv', 'True')

        if hasattr(executable, 'execution_site'):
            self.add_profile('hints', 'execution.site', executable.execution_site)

        self._options += self.executable.common_options
        self._raw_options += self.executable.common_raw_options
        for inp in self.executable.common_input_files:
            self._add_input(inp)",https://github.com/gwastro/pycbc/blob/7a64cdd104d263f1b6ea0b01e6841837d05a4cb3/pycbc/workflow/core.py#L828-L842
tfidf_python_100_1.0,get executable path,python,"def program_files(self, executable):
        paths = self.REQUIRED_PATHS_SVCOMP17 if self._is_svcomp17_version(executable) else self.REQUIRED_PATHS
        return [executable] + self._program_files_from_executable(executable, paths)",https://github.com/sosy-lab/benchexec/blob/44428f67f41384c03aea13e7e25f884764653617/benchexec/tools/ultimate.py#L253-L255
tfidf_python_100_1.0,get executable path,python,"def clean_any_prior_installation(self):
        for executable in self.executables:
            self._remove_executable(executable)
            self._assert_command_removed(executable)",https://github.com/andreafrancia/trash-cli/blob/5abecd53e1d84f2a5fd3fc60d2f5d71e518826c5/check_release_installation.py#L43-L46
tfidf_python_100_1.0,get executable path,python,"def find_executable(executable, path=None):
  """"""Tries to find 'executable' in the directories listed in 'path'.

  A string listing directories separated by 'os.pathsep'; defaults to
  os.environ['PATH'].  Returns the complete filename or None if not found.
  """"""
  if path is None:
    path = os.environ['PATH']
  paths = path.split(os.pathsep)
  base, ext = os.path.splitext(executable)

  if (sys.platform == 'win32' or os.name == 'os2') and (ext != '.exe'):
    executable = executable + '.exe'

  if not os.path.isfile(executable):
    for p in paths:
      f = os.path.join(p, executable)
      if os.path.isfile(f):
        # the file exists, we have a shot at spawn working
        return f
    return None
  else:
    return executable",https://github.com/nikcub/paths/blob/2200b85273d07d7a3c8b15ceb3b03cbb5c11439a/paths/__init__.py#L85-L107
tfidf_python_100_1.0,get executable path,python,"def version(self, executable):
        return self._version_from_tool(os.path.join(os.path.dirname(executable), self.blast_exe()))[6:11]",https://github.com/sosy-lab/benchexec/blob/44428f67f41384c03aea13e7e25f884764653617/benchexec/tools/blast.py#L46-L47
tfidf_python_100_1.0,get executable path,python,"def _popen(command, args):
    # type: (str, str) -> str
    for directory in PATH:
        executable = os.path.join(directory, command)
        if (os.path.exists(executable)
            and os.access(executable, os.F_OK | os.X_OK)
                and not os.path.isdir(executable)):
            break
    else:
        executable = command
    if DEBUG >= 3:
        log.debug(""Running: '%s %s'"", executable, args)
    return _call_proc(executable, args)",https://github.com/GhostofGoes/getmac/blob/553846d6b8b8873ea874ea8cfb35a07ae523569a/getmac/getmac.py#L237-L249
tfidf_python_100_1.0,get executable path,python,"def get_binary_path(executable):
    """"""Gets the software name and returns the path of the binary.""""""
    if sys.platform == 'win32':
        if executable == 'start':
            return executable
        executable = executable + '.exe'
        if executable in os.listdir('.'):
            binary = os.path.join(os.getcwd(), executable)
        else:
            binary = next((os.path.join(path, executable)
                           for path in os.environ['PATH'].split(os.pathsep)
                           if os.path.isfile(os.path.join(path, executable))), None)
    else:
        binary = Popen(['which', executable], stdout=PIPE).stdout.read().strip().decode('utf-8')
    return binary if binary else None",https://github.com/schubergphilis/terraformtestinglib/blob/fa9112f562b74448007bdaabecbdb76ae531d29f/_CI/library/library.py#L187-L201
tfidf_python_100_1.0,get executable path,python,"def can_execute(self):
        executable = super().can_execute
        executable = executable and self.value is not None
        self._collection = self.owner.eGet(self.feature)
        return executable",https://github.com/pyecore/pyecore/blob/22b67ad8799594f8f44fd8bee497583d4f12ed63/pyecore/commands.py#L120-L124
tfidf_python_100_1.0,get executable path,python,"def find_executable(executable: str, *paths: str) -> typing.Optional[Path]:
    """"""
    Based on: https://gist.github.com/4368898

    Public domain code by anatoly techtonik <techtonik@gmail.com>

    Programmatic equivalent to Linux `which` and Windows `where`

    Find if Â´executableÂ´ can be run. Looks for it in 'path'
    (string that lists directories separated by 'os.pathsep';
    defaults to os.environ['PATH']). Checks for all executable
    extensions. Returns full path or None if no command is found.

    Args:
        executable: executable name to look for
        paths: root paths to examine (defaults to system PATH)

    Returns: executable path as string or None

    """"""

    if not executable.endswith('.exe'):
        executable = f'{executable}.exe'

    if executable in _KNOWN_EXECUTABLES:
        return _KNOWN_EXECUTABLES[executable]

    output = f'{executable}'

    if not paths:
        path = os.environ['PATH']
        paths = tuple([str(Path(sys.exec_prefix, 'Scripts').absolute())] + path.split(os.pathsep))
    executable_path = Path(executable).absolute()
    if not executable_path.is_file():
        for path_ in paths:
            executable_path = Path(path_, executable).absolute()
            if executable_path.is_file():
                break
        else:
            _LOGGER.error('%s -> not found', output)
            return None

    _KNOWN_EXECUTABLES[executable] = executable_path
    _LOGGER.info('%s -> %s', output, str(executable_path))
    return executable_path",https://github.com/etcher-be/elib_run/blob/c9d8ba9f067ab90c5baa27375a92b23f1b97cdde/elib_run/_find_exe.py#L18-L62
tfidf_python_100_1.0,get executable path,python,"def set_executable_path(self):
        if is_windows and not self.is_relative_paths_option_set():
            python_executable = self.buildout.get('buildout').get('executable')
            self.options['executable'] = python_executable",https://github.com/Infinidat/infi.recipe.console_scripts/blob/7beab59537654ee475527dbbd59b0aa49348ebd3/src/infi/recipe/console_scripts/__init__.py#L16-L19
tfidf_python_100_1.0,get executable path,python,"def apply_settings(self, options):
        if not self.def_exec_radio.isChecked():
            executable = self.pyexec_edit.text()
            executable = osp.normpath(executable)
            if executable.endswith('pythonw.exe'):
                executable = executable.replace(""pythonw.exe"", ""python.exe"")
            change = self.python_executable_changed(executable)
            if change:
                self.set_custom_interpreters_list(executable)
                self.set_option('executable', executable)
                self.set_option('custom_interpreter', executable)
        if not self.pyexec_edit.text():
            self.set_option('custom_interpreter', '')
        self.main.apply_settings()",https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/preferences/maininterpreter.py#L251-L264
tfidf_python_100_1.0,httpclient post json,python,"def sync_fetch(request, method, default_headers=None,
               httpclient=None, **kwargs):
    """"""
    fetch resource using the synchronous HTTPClient
    :param request: HTTPRequest object or a url
    :param method: HTTP method in string format, e.g. GET, POST
    :param kwargs: query string entities or POST data
    """"""
    updated_request = make_request(request, method, default_headers, **kwargs)
    if not httpclient:
        httpclient = HTTPClient()
    rsp = httpclient.fetch(updated_request)
    return parse_response(rsp)",https://github.com/openpermissions/chub/blob/00762aa17015f4b3010673d1570c708eab3c34ed/chub/handlers.py#L104-L116
tfidf_python_100_1.0,httpclient post json,python,"def listen_to_http_client(self, client: HTTPClient):
        client.event_dispatcher.add_listener(HTTPClient.ClientEvent.new_session,
                                             self._http_session_callback)",https://github.com/ArchiveTeam/wpull/blob/ddf051aa3322479325ba20aa778cb2cb97606bf5/wpull/warc/recorder.py#L213-L215
tfidf_python_100_1.0,httpclient post json,python,"def __init__(self, httpclient, cfg, serverURL=None):
        self.httpclient = httpclient
        self.cfg = cfg
        self.serverURL = serverURL",https://github.com/3ll3d00d/vibe/blob/124b029f13ac746723e92cb47e9cb56edd2e54b5/backend/src/recorder/common/heartbeater.py#L11-L14
tfidf_python_100_1.0,httpclient post json,python,"def __call__(self, *args, **kwargs):
        httpclient = self.http.new_client('')
        return Selector(httpclient, self, *args, **kwargs)",https://github.com/openatx/facebook-wda/blob/aa644204620c6d5c7705a9c7452d8c0cc39330d5/wda/__init__.py#L552-L554
tfidf_python_100_1.0,httpclient post json,python,"def sign_request(self, request, httpclient):
        request.headers.append(
            ('Authorization', self._get_authorization(request, httpclient)))",https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicebus/azure/servicebus/control_client/servicebusservice.py#L1263-L1265
tfidf_python_100_1.0,httpclient post json,python,"def __init__(self, *args, **kwargs):
        super(HTTPClient, self).__init__(*args, **kwargs)
        self.client = httpclient.AsyncHTTPClient()",https://github.com/cablehead/python-consul/blob/53eb41c4760b983aec878ef73e72c11e0af501bb/consul/tornado.py#L13-L15
tfidf_python_100_1.0,httpclient post json,python,"def httpclient(options, c):
    k = 'HTTPClient'

    R = options['R']
    try:
        x = HTTPClient(credentials=c,
                       **R['R0_obj'][k])
    except Exception as e:
        print_exception(k, e)
        sys.exit(1)

    setters(options, x)
    methods(options, x)

    return x",https://github.com/PaloAltoNetworks/pancloud/blob/c51e4c8aca3c988c60f062291007534edcb55285/bin/summit.py#L212-L226
tfidf_python_100_1.0,httpclient post json,python,"def _patch_tornado_client(tracer=None, start_span_cb=None):
    if getattr(tornado, '__opentracing_client_patch', False) is True:
        return

    setattr(tornado, '__opentracing_client_patch', True)
    httpclient._set_tracing_enabled(True)
    httpclient._set_tracing_info(tracer, start_span_cb)

    wrap_function('tornado.httpclient', 'AsyncHTTPClient.fetch',
                  httpclient.fetch_async)",https://github.com/opentracing-contrib/python-tornado/blob/2c87f423c316805c6140d7f0613c800dd05b47dc/tornado_opentracing/initialization.py#L54-L63
tfidf_python_100_1.0,httpclient post json,python,"def request(apiurl, data=None, **kwargs):
    headers = {""Content-Type"": [""application/json""]}
    return httpclient.post(apiurl, data=data, **kwargs)",https://github.com/talkincode/toughlib/blob/1c2f7dde3a7f101248f1b5f5d428cc85466995cf/toughlib/apiutils.py#L109-L111
tfidf_python_100_1.0,httpclient post json,python,"def __init__(self, sotrg_host=None):
        self.history_host = sotrg_host if sotrg_host else storageurl
        self.client = HTTPClient(settings.storageurl)",https://github.com/Robin8Put/pmes/blob/338bec94162098f05b75bad035417317e1252fd2/coin/StorgCli.py#L7-L9
tfidf_python_100_1.0,httpclient post json,python,"def __init__(self, API_key, httpclient):
        self._API_key = API_key
        self._client = httpclient",https://github.com/csparpa/pyowm/blob/cdd59eb72f32f7238624ceef9b2e2329a5ebd472/pyowm/pollutionapi30/airpollution_client.py#L20-L22
tfidf_python_100_1.0,httpclient post json,python,"def _get_authorization(self, request, httpclient):
        ''' return the signed string with token. '''
        return 'WRAP access_token=""' + \
                self._get_token(request.host, request.path, httpclient) + '""'",https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicebus/azure/servicebus/control_client/servicebusservice.py#L1267-L1270
tfidf_python_100_1.0,httpclient post json,python,"def __init__(self, httpclient, id):
        """"""
        base_url eg: http://localhost:8100/session/$SESSION_ID
        """"""
        self.http = httpclient
        self._id = id",https://github.com/openatx/facebook-wda/blob/aa644204620c6d5c7705a9c7452d8c0cc39330d5/wda/__init__.py#L916-L921
tfidf_python_100_1.0,httpclient post json,python,"def _attach_event_listeners(self):
        http_client = cast(HTTPClient, self.app_session.factory['HTTPClient'])
        http_client.event_dispatcher.add_listener(
            HTTPClient.ClientEvent.new_session,
            self._http_session_callback
        )

        ftp_client = cast(FTPClient, self.app_session.factory['FTPClient'])
        ftp_client.event_dispatcher.add_listener(
            ftp_client.ClientEvent.new_session,
            self._ftp_session_callback
        )",https://github.com/ArchiveTeam/wpull/blob/ddf051aa3322479325ba20aa778cb2cb97606bf5/wpull/application/plugins/download_progress.plugin.py#L38-L49
tfidf_python_100_1.0,httpclient post json,python,"def request(endpoint: str, *args: list, **kwargs: dict) -> Response:
    from .clients.http_client import HTTPClient

    return HTTPClient(endpoint).request(*args, **kwargs)",https://github.com/bcb/jsonrpcclient/blob/5b5abc28d1466d694c80b80c427a5dcb275382bb/jsonrpcclient/__init__.py#L5-L8
tfidf_python_100_1.0,httpclient post json,python,"def __init__(self, targetStateController, dataDir, httpclient, maxAgeSeconds=30):
        self.httpclient = httpclient
        self.devices = {}
        self.targetStateController = targetStateController
        self.dataDir = dataDir
        if dataDir is None or httpclient is None or targetStateController is None:
            raise ValueError(""Mandatory args missing"")
        self.maxAgeSeconds = maxAgeSeconds
        self.running = True
        self.worker = threading.Thread(name='DeviceCaretaker', target=self._evictStaleDevices, daemon=True)
        self.worker.start()",https://github.com/3ll3d00d/vibe/blob/124b029f13ac746723e92cb47e9cb56edd2e54b5/backend/src/analyser/common/devicecontroller.py#L47-L57
tfidf_python_100_1.0,httpclient post json,python,"def async_fetch(request, method, default_headers=None,
                callback=None, httpclient=None, **kwargs):
    """"""
    fetch resource using the asynchronous AsyncHTTPClient
    :param request: HTTPRequest object or a url
    :param method: HTTP method in string format, e.g. GET, POST
    :param callback: callback function on the result. it is used
    by the coroutine decorator.
    :param kwargs: query string entities or POST data
    """"""
    updated_request = make_request(request, method, default_headers, **kwargs)
    if not httpclient:
        httpclient = AsyncHTTPClient()
    rsp = yield httpclient.fetch(updated_request)
    raise Return(parse_response(rsp))",https://github.com/openpermissions/chub/blob/00762aa17015f4b3010673d1570c708eab3c34ed/chub/handlers.py#L120-L134
tfidf_python_100_1.0,httpclient post json,python,"def post(self, json=None):
        """"""Send a POST request and return the JSON decoded result.

        Args:
            json (dict, optional): Object to encode and send in request.

        Returns:
            mixed: JSON decoded response data.
        """"""
        return self._call('post', url=self.endpoint, json=json)",https://github.com/LasLabs/python-helpscout/blob/84bf669417d72ca19641a02c9a660e1ae4271de4/helpscout/request_paginator/__init__.py#L114-L123
tfidf_python_100_1.0,httpclient post json,python,"def _validate_request(self, email, httpclient, callback):
        if not httpclient:
            check = requests.get(""https://api.mailgun.net/v2/address/validate"",
                                 params={'address': email},
                                 auth=('api', self._key))
            callback(check.json())

        elif isinstance(httpclient, TornadoAsyncHTTPClient):
            httpclient.fetch(""https://api.mailgun.net/v2/address/validate?"" + urlencode({'address': email}),
                             method=""GET"",
                             callback=callback,
                             auth_username=""api"",
                             auth_password=self._key)",https://github.com/stevepeak/stuffed/blob/cc18d5d34b36225035d618d666275c913f5f66de/stuffed/email.py#L66-L78
tfidf_python_100_1.0,httpclient post json,python,"def __init__(self):
        http_error = httpclient.HTTPError(404)
        couch.NotFound.__init__(self, http_error)",https://github.com/openpermissions/perch/blob/36d78994133918f3c52c187f19e50132960a0156/perch/exceptions.py#L23-L25
tfidf_python_100_1.0,get inner html,python,"def __init__(self, simple_engine: SimpleEngine, inner: AnalysisResult) -> None:
        self.simple_engine = simple_engine
        self.inner = inner",https://github.com/niklasf/python-chess/blob/d91f986ca3e046b300a0d7d9ee2a13b07610fe1a/chess/engine.py#L2376-L2378
tfidf_python_100_1.0,get inner html,python,"def _convert_string_literal(x, quote_initial, quote_replace, quote_search):
    if len(x) >= 2 and x[0] == quote_initial and x[-1] == quote_initial:
        inner = x[1:-1]
        s = quote_replace
        while inner:
            m = re.search(quote_search, inner)
            if m is None:
                s += inner
                break
            s += m.group(1)
            s += u'\\'
            s += quote_replace
            inner = inner[m.end():]
        s += quote_replace
        return s
    return x",https://github.com/google/neuroglancer/blob/9efd12741013f464286f0bf3fa0b667f75a66658/python/neuroglancer/url_state.py#L37-L52
tfidf_python_100_1.0,get inner html,python,"def pre_filter(self):
        """""" Return rTorrent condition to speed up data transfer.
        """"""
        inner = self._inner.pre_filter()
        if inner:
            if inner.startswith('""not=$') and inner.endswith('""') and '\\' not in inner:
                return inner[6:-1]  # double negation, return inner command
            elif inner.startswith('""'):
                inner = '""$' + inner[1:]
            else:
                inner = '$' + inner
            return 'not=' + inner
        else:
            return ''",https://github.com/pyroscope/pyrocore/blob/89ad01346a570943d20311a0b488440975876612/src/pyrocore/util/matching.py#L170-L183
tfidf_python_100_1.0,get inner html,python,"def eval(self):
        if isinstance(self.inner, true) or self.inner is Boolean3.Top: return false()
        elif isinstance(self.inner, false) or self.inner is Boolean3.Bottom: return true()
        elif isinstance(self.inner, Neg): return self.inner.inner
        else: return self",https://github.com/hkff/FodtlMon/blob/0c9015a1a1f0a4a64d52945c86b45441d5871c56/fodtlmon/ltl/ltl.py#L439-L443
tfidf_python_100_1.0,get inner html,python,"def __init__(self, inner, n=None):
        self.inner = inner
        self.n = n
        self.cache = list()
        self.cachecomplete = False",https://github.com/petl-developers/petl/blob/1d33ca055f7e04e0d28a772041c9fd30c8d415d6/petl/util/materialise.py#L137-L141
tfidf_python_100_1.0,get inner html,python,"def _reversed_directories(outer, inner):
    while outer != inner:
        yield inner
        inner = os.path.dirname(inner)",https://github.com/tlevine/vlermv/blob/0b332ea1c20e4065b30f5e3ec0c1d0fffbce6b20/vlermv/_fs.py#L44-L47
tfidf_python_100_1.0,get inner html,python,"def render_table_cell(self, token, in_header=False):
        if in_header:
            template = '||{inner}'
        else:
            template = '|{inner}'
        
        inner = self.render_inner(token)
        return template.format(inner=inner)",https://github.com/miyuchina/mistletoe/blob/846a419bcb83afab02f3f19d151ab0166fab68f6/contrib/jira_renderer.py#L173-L180
tfidf_python_100_1.0,get inner html,python,"def __init__(self, message, inner=None):
    super(EvaluationError, self).__init__(message)
    self.inner = inner",https://github.com/rix0rrr/gcl/blob/4e3bccc978a9c60aaaffd20f6f291c4d23775cdf/gcl/exceptions.py#L16-L18
tfidf_python_100_1.0,get inner html,python,"def get_object(self, g=None):
        cell_contents = uncan(self.cell_contents, g)

        def inner():
            return cell_contents
        return py3compat.get_closure(inner)[0]",https://github.com/Parsl/parsl/blob/d7afb3bc37f50dcf224ae78637944172edb35dac/parsl/executors/serialize/canning.py#L168-L173
tfidf_python_100_1.0,get inner html,python,"def render(self):
        indent = self.doc.indent
        inner = self.content or ''
        if not self.safe:
            inner = escape(inner, quote=False)
        inner += ''.join([n.render() for n in self.child_nodes])
        html = self.doc.render_tag(self.tag_name, inner, self.attrs)

        if indent:
            pretty_html = '\n' + (indent * self.level) + html
            if self._is_last():
                pretty_html += '\n' + indent * (self.level - 1)
            html = pretty_html

        return html",https://github.com/zenwalker/python-xmltag/blob/5ba900753d939b0f3811c88b0f95ebbbdecd1727/xmltag/nodes.py#L45-L59
tfidf_python_100_1.0,get inner html,python,"def __getattr__(self, item):
        inner = getattr(self._wrapped, item)
        if callable(inner):
            return self.wrap(inner)
        return inner",https://github.com/hazelcast/hazelcast-python-client/blob/3f6639443c23d6d036aa343f8e094f052250d2c1/hazelcast/future.py#L264-L268
tfidf_python_100_1.0,get inner html,python,"def __init__(self, inner, refScope):
        # type: (_Loader, Union[int, None]) -> None
        self.inner = inner
        self.refScope = refScope",https://github.com/common-workflow-language/schema_salad/blob/608ba207b9058fe0a9c3db161058ab3782eef015/schema_salad/metaschema.py#L463-L466
tfidf_python_100_1.0,get inner html,python,"def _and(f, g):
    def inner(data):
        return f(data) and g(data)
    return inner",https://github.com/RedHatInsights/insights-core/blob/b57cbf8ed7c089672426ede0441e0a4f789ef4a1/insights/configtree/__init__.py#L599-L602
tfidf_python_100_1.0,get inner html,python,"def _or(f, g):
    def inner(data):
        return f(data) or g(data)
    return inner",https://github.com/RedHatInsights/insights-core/blob/b57cbf8ed7c089672426ede0441e0a4f789ef4a1/insights/configtree/__init__.py#L593-L596
tfidf_python_100_1.0,get inner html,python,"def render_table_row(self, token, is_header=False):
        if is_header:
            template = '{inner}||\n'
        else:
            template = '{inner}|\n'
            
        inner = ''.join([self.render_table_cell(child, is_header)
                         for child in token.children])

        return template.format(inner=inner)",https://github.com/miyuchina/mistletoe/blob/846a419bcb83afab02f3f19d151ab0166fab68f6/contrib/jira_renderer.py#L162-L171
tfidf_python_100_1.0,get inner html,python,"def __init__(self, inner):
        self.inner = inner
        self.lock = threading.Lock()",https://github.com/rbarrois/throttle/blob/cc00e6b446f3938c81826ee258975ebdc12511a2/throttle/storage/locking.py#L36-L38
tfidf_python_100_1.0,get inner html,python,"def __init__(self, inner):
        """"""Initialize a new instance.

        Parameters
        ----------
        inner : callable
            The inner product implementation. It must accept two
            `ProductSpaceElement` arguments, return a element from
            the field of the space (real or complex number) and
            satisfy the following conditions for all space elements
            ``x, y, z`` and scalars ``s``:

            - ``<x, y> = conj(<y, x>)``
            - ``<s*x + y, z> = s * <x, z> + <y, z>``
            - ``<x, x> = 0``  if and only if  ``x = 0``
        """"""
        super(ProductSpaceCustomInner, self).__init__(
            impl='numpy', inner=inner)",https://github.com/odlgroup/odl/blob/b8443f6aca90e191ba36c91d32253c5a36249a6c/odl/space/pspace.py#L1784-L1801
tfidf_python_100_1.0,get inner html,python,"def context_factory():
    context_store = []
    def inner():
        if not context_store:
            context_store.append(zmq.Context())
        return context_store[0]

    return inner",https://github.com/m0n5t3r/gstats/blob/ae600d309ae8a159079fe1d6e6fa1c9097125f5b/gstats/wsgi.py#L63-L70
tfidf_python_100_1.0,get inner html,python,"def _check_pattern(self, inner, outer):
        if isinstance(inner, _String):
            self._add_pattern(inner.kind, outer)
        elif isinstance(inner, Sequence):
            self._check_pattern(inner.patterns[0], outer)
        elif isinstance(inner, (Tag, Forward)):
            self._check_pattern(inner.pattern, outer)
        elif isinstance(inner, ChoiceDict):
            for pattern in inner.patterns_map.values():
                self._check_pattern(pattern, outer)
        else:
            raise Error(
                'Unsupported pattern type {}.'.format(type(inner)))",https://github.com/eerimoq/textparser/blob/5f158210cfd3a20b19775191e92f22a13054a852/textparser.py#L309-L321
tfidf_python_100_1.0,get inner html,python,"def blocks2numList(blocks, n):
    """"""inverse function of numList2blocks.""""""
    toProcess = copy.copy(blocks)
    returnList = []
    for numBlock in toProcess:
        inner = []
        for i in range(0, n):
            inner.append(numBlock % 256)
            numBlock >>= 8
        inner.reverse()
        returnList.extend(inner)
    return returnList",https://github.com/lazygunner/xunleipy/blob/cded7598a7bf04495156bae2d747883d1eacb3f4/xunleipy/rsa_lib.py#L239-L250
tfidf_python_100_1.0,convert string to number,python,"def _string_to_int(self, string):
        """"""
        Convert a string to a number, using the given alphabet..
        """"""
        number = 0
        for char in string[::-1]:
            number = number * self._alpha_len + self._alphabet.index(char)
        return number",https://github.com/rootpy/rootpy/blob/3926935e1f2100d8ba68070c2ab44055d4800f73/rootpy/extern/shortuuid/__init__.py#L30-L37
tfidf_python_100_1.0,convert string to number,python,"def nepali_number(number):
    """"""
    Convert a number to nepali
    """"""
    nepnum = """"
    for n in str(number):
        nepnum += values.NEPDIGITS[int(n)]
    return nepnum",https://github.com/nepalicalendar/nepalicalendar-py/blob/a589c28b8e085049f30a7287753476b59eca6f50/nepalicalendar/functions.py#L31-L38
tfidf_python_100_1.0,convert string to number,python,"def to_cardinal(self, number):
        if number < 0:
            string = Num2Word_IT.MINUS_PREFIX_WORD + self.to_cardinal(-number)
        elif isinstance(number, float):
            string = self.float_to_words(number)
        elif number < 20:
            string = CARDINAL_WORDS[number]
        elif number < 100:
            string = self.tens_to_cardinal(number)
        elif number < 1000:
            string = self.hundreds_to_cardinal(number)
        elif number < 1000000:
            string = self.thousands_to_cardinal(number)
        else:
            string = self.big_number_to_cardinal(number)
        return accentuate(string)",https://github.com/savoirfairelinux/num2words/blob/f4b2bac098ae8e4850cf2f185f6ff52a5979641f/num2words/lang_IT.py#L171-L186
tfidf_python_100_1.0,convert string to number,python,"def number(string):
    try:
        number = int(string)
    except ValueError:
        number = float(string)
    return number",https://github.com/brechtm/rinohtype/blob/40a63c4e5ad7550f62b6860f1812cb67cafb9dc7/src/rinoh/font/type1.py#L29-L34
tfidf_python_100_1.0,convert string to number,python,"def prompt_text(string, name):
    number = colour_numbers.name_to_int(name)
    return ansi_escapes.prompt_string(string, number)",https://github.com/jalanb/pysyte/blob/4e278101943d1ceb1a6bcaf6ddc72052ecf13114/pysyte/colours/texts.py#L74-L76
tfidf_python_100_1.0,convert string to number,python,"def _rstrip_newlines(self, string, number=1):
        for _ in range(number):
            if string and string[-1] == ""\n"":
                string = string[:-1]
        return string",https://github.com/googlefonts/glyphsLib/blob/9c12dc70c8d13f08d92b824e6710f6e3bb5037bb/Lib/glyphsLib/builder/features.py#L616-L620
tfidf_python_100_1.0,convert string to number,python,"def number(v):
    """"""Convert a value to a number.""""""
    if nodesetp(v):
        v = string(v)
    try:
        return float(v)
    except ValueError:
        return float('NaN')",https://github.com/gabrielfalcao/dominic/blob/a42f418fc288f3b70cb95847b405eaf7b83bb3a0/dominic/xpath/expr.py#L133-L140
tfidf_python_100_1.0,convert string to number,python,"def base(number, input_base=10, output_base=10, max_depth=10,
         string=False, recurring=True):
    """"""
    Converts a number from any base to any another.

    Args:
        number(tuple|str|int): The number to convert.
        input_base(int): The base to convert from (defualt 10).
        output_base(int): The base to convert to (default 10).
        max_depth(int): The maximum number of fractional digits (defult 10).
        string(bool): If True output will be in string representation,
            if False output will be in tuple representation (defult False).
        recurring(bool): Attempt to find repeating digits in the fractional
            part of a number. Repeated digits will be enclosed with ""["" and ""]""
            (default True).
    Returns:
        A tuple of digits in the specified base:
        (int, int, int, ... , '.' , int, int, int)
        If the string flag is set to True,
        a string representation will be used instead.

    Raises:
        ValueError if a digit value is too high for the input_base.

    Example:
        >>> base((1,9,6,'.',5,1,6), 17, 20)
        (1, 2, 8, '.', 5, 19, 10, 7, 17, 2, 13, 13, 1, 8)
    """"""
    # Convert number to tuple representation.
    if type(number) == int or type(number) == float:
        number = str(number)
    if type(number) == str:
        number = represent_as_tuple(number)
    # Check that the number is valid for the input base.
    if not check_valid(number, input_base):
        raise ValueError
    # Deal with base-1 special case
    if input_base == 1:
        number = (1,) * number.count(1)
    # Expand any recurring digits.
    number = expand_recurring(number, repeat=5)
    # Convert a fractional number.
    if ""."" in number:
        radix_point = number.index(""."")
        integer_part = number[:radix_point]
        fractional_part = number[radix_point:]
        integer_part = integer_base(integer_part, input_base, output_base)
        fractional_part = fractional_base(fractional_part, input_base,
                                          output_base, max_depth)
        number = integer_part + fractional_part
        number = truncate(number)
    # Convert an integer number.
    else:
        number = integer_base(number, input_base, output_base)
    if recurring:
        number = find_recurring(number, min_repeat=2)
    # Return the converted number as a srring or tuple.
    return represent_as_string(number) if string else number",https://github.com/squdle/baseconvert/blob/26c9a2c07c2ffcde7d078fb812419ca6d388900b/baseconvert/baseconvert.py#L625-L682
tfidf_python_100_1.0,convert string to number,python,"def to_ordinal(self, number):
        tens = number % 100
        # Italian grammar is poorly defined here Â¯\_(ã)_/Â¯:
        #   centodecimo VS centodieciesimo VS centesimo decimo?
        is_outside_teens = not 10 < tens < 20
        if number < 0:
            return Num2Word_IT.MINUS_PREFIX_WORD + self.to_ordinal(-number)
        elif number % 1 != 0:
            return self.float_to_words(number, ordinal=True)
        elif number < 20:
            return ORDINAL_WORDS[number]
        elif is_outside_teens and tens % 10 == 3:
            # Gets ride of the accent      ~~~~~~~~~~
            return self.to_cardinal(number)[:-1] + ""eesimo""
        elif is_outside_teens and tens % 10 == 6:
            return self.to_cardinal(number) + ""esimo""
        else:
            string = self.to_cardinal(number)[:-1]
            if string[-3:] == ""mil"":
                string += ""l""
            return string + ""esimo""",https://github.com/savoirfairelinux/num2words/blob/f4b2bac098ae8e4850cf2f185f6ff52a5979641f/num2words/lang_IT.py#L188-L208
tfidf_python_100_1.0,convert string to number,python,"def convert_number(string):
    """"""Convert a string to number
    If int convert to int otherwise float
    
    If not possible return None
    """"""
    res = None
    if isint(string):
        res = int(string)
    elif isfloat(string):
        res = float(string) 
    return res",https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/utils/convert.py#L24-L35
tfidf_python_100_1.0,convert string to number,python,"def num(string):
    """"""convert a string to float""""""
    if not isinstance(string, type('')):
        raise ValueError(type(''))
    try:
        string = re.sub('[^a-zA-Z0-9\.\-]', '', string)
        number = re.findall(r""[-+]?\d*\.\d+|[-+]?\d+"", string)
        return float(number[0])
    except Exception as e:
        logger = logging.getLogger('tradingAPI.utils.num')
        logger.debug(""number not found in %s"" % string)
        logger.debug(e)
        return None",https://github.com/federico123579/Trading212-API/blob/0fab20b71a2348e72bbe76071b81f3692128851f/tradingAPI/utils.py#L33-L45
tfidf_python_100_1.0,convert string to number,python,"def filter(self, p_todo_str, p_todo):
        """""" Prepends the number to the todo string. """"""
        return ""|{:>3}| {}"".format(self.todolist.number(p_todo), p_todo_str)",https://github.com/bram85/topydo/blob/b59fcfca5361869a6b78d4c9808c7c6cd0a18b58/topydo/lib/prettyprinters/Numbers.py#L29-L31
tfidf_python_100_1.0,convert string to number,python,"def data_to_uuid(data):
    """"""Convert an array of binary data to the iBeacon uuid format.""""""
    string = data_to_hexstring(data)
    return string[0:8]+'-'+string[8:12]+'-'+string[12:16]+'-'+string[16:20]+'-'+string[20:32]",https://github.com/citruz/beacontools/blob/15a83e9750d0a4393f8a36868e07f6d9458253fe/beacontools/utils.py#L24-L27
tfidf_python_100_1.0,convert string to number,python,"def __call__(self, number):
        """"""Convert a number.""""""
        return base(number, self.input_base, self.output_base,
                    self.max_depth, self.string, self.recurring)",https://github.com/squdle/baseconvert/blob/26c9a2c07c2ffcde7d078fb812419ca6d388900b/baseconvert/baseconvert.py#L175-L178
tfidf_python_100_1.0,convert string to number,python,"def tas2mach(Vtas, H):
    """"""True Airspeed to Mach number""""""
    a = vsound(H)
    Mach = Vtas/a
    return Mach",https://github.com/junzis/pyModeS/blob/8cd5655a04b08171a9ad5f1ffd232b7e0178ea53/pyModeS/extra/aero.py#L129-L133
tfidf_python_100_1.0,convert string to number,python,"def mach2tas(Mach, H):
    """"""Mach number to True Airspeed""""""
    a = vsound(H)
    Vtas = Mach*a
    return Vtas",https://github.com/junzis/pyModeS/blob/8cd5655a04b08171a9ad5f1ffd232b7e0178ea53/pyModeS/extra/aero.py#L136-L140
tfidf_python_100_1.0,convert string to number,python,"def pack_unsigned_int(number, size, le):
  if not isinstance(number, int):
    raise StructError(""argument for i,I,l,L,q,Q,h,H must be integer"")
  if number < 0:
    raise TypeError(""can't convert negative long to unsigned"")
  if number > (1 << (8 * size)) - 1:
    raise OverflowError(""Number:%i too large to convert"" % number)
  return pack_int(number, size, le)",https://github.com/google/grumpy/blob/3ec87959189cfcdeae82eb68a47648ac25ceb10b/third_party/pypy/_struct.py#L101-L108
tfidf_python_100_1.0,convert string to number,python,"def __init__(self, actfunc, strfunc, convert=lambda x: x):
        self.actfunc = actfunc
        self.strfunc = strfunc
        self.convert = convert",https://github.com/iotile/coretools/blob/2d794f5f1346b841b0dcd16c9d284e9bf2f3c6ec/iotilebuild/iotile/build/config/scons-local-3.0.1/SCons/Action.py#L1379-L1382
tfidf_python_100_1.0,convert string to number,python,"def _highest_bit(number):
    if number == 0:
        return 0
    number -= 1
    number |= number >> 1
    number |= number >> 2
    number |= number >> 4
    number |= number >> 8
    number |= number >> 16
    number += 1
    return math.sqrt(number)",https://github.com/knipknap/exscript/blob/72718eee3e87b345d5a5255be9824e867e42927b/Exscript/util/ipv4.py#L41-L51
tfidf_python_100_1.0,convert string to number,python,"def c_str(string):
    """"""""Convert a python string to C string.""""""
    if not isinstance(string, str):
        string = string.decode('ascii')
    return ctypes.c_char_p(string.encode('utf-8'))",https://github.com/apache/incubator-mxnet/blob/1af29e9c060a4c7d60eeaacba32afdb9a7775ba7/amalgamation/python/mxnet_predict.py#L40-L44
tfidf_python_100_1.0,format date,python,"def format_date(date, timestamp_format):
    try:
        date = DATE_ADD.format(int(date))
    except ValueError:
        date = timestamp_format.format(date)
    return date",https://github.com/ofek/pypinfo/blob/48d56e690d7667ae5854752c3a2dc07e321d5637/pypinfo/core.py#L65-L70
tfidf_python_100_1.0,format date,python,"def std_tsymbol(tsymbol):
    s, date = tsymbol
    if date == 0:
        return '_{}_'.format(s)
    elif date <= 0:
        return '_{}_m{}_'.format(s, str(-date))
    elif date >= 0:
        return '_{}__{}_'.format(s, str(date))",https://github.com/EconForge/dolo/blob/d91ddf148b009bf79852d9aec70f3a1877e0f79a/dolo/compiler/symbolic.py#L10-L17
tfidf_python_100_1.0,format date,python,"def _discover_publication_date(opf_xmldoc, date_html=None):
    date = __discover_dc(opf_xmldoc, 'date')

    if not date and date_html is not None:
        date = _find_publish_date_from_dom(date_html)

    return date",https://github.com/paulocheque/epub-meta/blob/3f0efb9f29a286b1a6896ad05422b23f10e10164/epub_meta/collector.py#L165-L171
tfidf_python_100_1.0,format date,python,"def _get_crime_categories(self, date=None):
        if date not in self.crime_categories:
            self._populate_crime_categories(date=date)
        return self.crime_categories[date]",https://github.com/rkhleics/police-api-client-python/blob/b5c1e493487eb2409e2c04ed9fbd304f73d89fdc/police_api/__init__.py#L166-L169
tfidf_python_100_1.0,format date,python,"def datetime_from_iso8601(date):
    """"""Small helper that parses ISO-8601 date dates.

        >>> datetime_from_iso8601(""2013-04-10T12:52:39"")
        datetime.datetime(2013, 4, 10, 12, 52, 39)
        >>> datetime_from_iso8601(""2013-01-07T12:55:19.257"")
        datetime.datetime(2013, 1, 7, 12, 55, 19, 257000)
    """"""
    format = ISO8610_FORMAT
    if date.endswith(""Z""):
        date = date[:-1]  # Date date is UTC
    if re.match("".*\.\d+"", date):
        # Date includes microseconds
        format = ISO8610_FORMAT_MICROSECONDS
    return datetime.datetime.strptime(date, format)",https://github.com/saschpe/rapport/blob/ccceb8f84bd7e8add88ab5e137cdab6424aa4683/rapport/util.py#L55-L69
tfidf_python_100_1.0,format date,python,"def to_date_string(date):
    if isinstance(date, numpy.int64) or isinstance(date, int):
        date = str(date)
        date = '%s-%s-%s' % (date[:4], date[4:6], date[6:8])
        return date",https://github.com/CxAalto/gtfspy/blob/bddba4b74faae6c1b91202f19184811e326547e5/gtfspy/util.py#L192-L196
tfidf_python_100_1.0,format date,python,"def QA_util_date_int2str(int_date):
    """"""
    ç±»ådatetime.datatime
    :param date: int 8ä½æ´æ°
    :return: ç±»åstr
    """"""
    date = str(int_date)
    if len(date) == 8:
        return str(date[0:4] + '-' + date[4:6] + '-' + date[6:8])
    elif len(date) == 10:
        return date",https://github.com/QUANTAXIS/QUANTAXIS/blob/bb1fe424e4108b62a1f712b81a05cf829297a5c0/QUANTAXIS/QAUtil/QADate.py#L74-L84
tfidf_python_100_1.0,format date,python,"def rollforward(self, date):
        if self.onOffset(date):
            return date
        else:
            return date + type(self)()",https://github.com/pydata/xarray/blob/6d93a95d05bdbfc33fff24064f67d29dd891ab58/xarray/coding/cftime_offsets.py#L141-L145
tfidf_python_100_1.0,format date,python,"def to_date(date):
    if isinstance(date, six.string_types):
        return parse(date).date()

    if isinstance(date, datetime.datetime):
        try:
            return date.date()
        except AttributeError:
            return date

    raise RQInvalidArgument(""unknown date value: {}"".format(date))",https://github.com/ricequant/rqalpha/blob/ac40a62d4e7eca9494b4d0a14f46facf5616820c/rqalpha/api/api_base.py#L913-L923
tfidf_python_100_1.0,format date,python,"def check_date(date):
    if isinstance(date, six.string_types):
        return Date.parseISO(date)
    else:
        return Date.fromDateTime(date)",https://github.com/iLampard/x-utils/blob/291d92832ee0e0c89bc22e10ecf2f44445e0d300/xutils/date_utils/date.py#L254-L258
tfidf_python_100_1.0,format date,python,"def format_date(self, date):
        if isinstance(date, datetime.datetime):
            date = date.date()

        if isinstance(date, datetime.date):
            date = date.strftime('%d%m%y')

        if not patterns.DATE.match(date):
            raise ValueError(""Invalid date: "" + date)

        return date",https://github.com/Turbo87/aerofiles/blob/d8b7b04a1fcea5c98f89500de1164619a4ec7ef4/aerofiles/igc/writer.py#L24-L34
tfidf_python_100_1.0,format date,python,"def serialize(date):
        if isinstance(date, CustomDateFormat):
            return date.date_str

        if isinstance(date, datetime.datetime):
            date = date.date()
        assert isinstance(
            date, datetime.date
        ), 'Received not compatible date ""{}""'.format(repr(date))
        return date.isoformat()",https://github.com/eamigo86/graphene-django-extras/blob/b27fd6b5128f6b6a500a8b7a497d76be72d6a232/graphene_django_extras/base_types.py#L166-L175
tfidf_python_100_1.0,format date,python,"def serialize(date):
        if isinstance(date, datetime.datetime):
            date = date.date()
        assert isinstance(
            date, datetime.date
        ), 'Received not compatible date ""{}""'.format(repr(date))
        return date.isoformat()",https://github.com/graphql-python/graphene/blob/abff3d75a39bc8f2d1fdb48aafa1866cf47dfff6/graphene/types/datetime.py#L20-L26
tfidf_python_100_1.0,format date,python,"def endOfMonth(date):
        m = date.month()
        y = date.year()
        return Date(y, m, _month_length(m, Date.isLeap(y)))",https://github.com/iLampard/x-utils/blob/291d92832ee0e0c89bc22e10ecf2f44445e0d300/xutils/date_utils/date.py#L173-L176
tfidf_python_100_1.0,format date,python,"def isEndOfMonth(date):
        m = date.month()
        y = date.year()
        return date.dayOfMonth() == _month_length(m, Date.isLeap(y))",https://github.com/iLampard/x-utils/blob/291d92832ee0e0c89bc22e10ecf2f44445e0d300/xutils/date_utils/date.py#L179-L182
tfidf_python_100_1.0,format date,python,"def _containsdate(self, date):
        date = Date(date)
        return ((self.firstdate <= date <= self.lastdate) and
                ((date-self.firstdate) // self.stepsize))",https://github.com/hydpy-dev/hydpy/blob/1bc6a82cf30786521d86b36e27900c6717d3348d/hydpy/core/timetools.py#L1534-L1537
tfidf_python_100_1.0,format date,python,"def _get_date_type(date):
        """"""
        Returns the type of a date.

        :param str|datetime.date date: The date.

        :rtype: str
        """"""
        if isinstance(date, str):
            return 'str'

        if isinstance(date, datetime.date):
            return 'date'

        if isinstance(date, int):
            return 'int'

        raise ValueError('Unexpected type {0!s}'.format(date.__class__))",https://github.com/SetBased/py-etlt/blob/1c5b8ea60293c14f54d7845a9fe5c595021f66f2/etlt/helper/Type2Helper.py#L161-L178
tfidf_python_100_1.0,format date,python,"def __call__(cls, *args, **kwargs):
        date = args[0] if len(args) == 1 else kwargs['date']

        if date is None:
            date = get_configuration().get('VERSIONS', 'defaultversion')

        if isinstance(date, DictionaryVersion):
            return date

        if isinstance(date, str):
            if date.startswith('dictionary_'):
                date = _str_to_date(date.split('.')[0].split('_', maxsplit=1)[1])
            else:
                date = _str_to_date(date)
        elif isinstance(date, datetime.date):
            date = _str_to_date(_date_to_str(date))
        else:
            raise ValueError(""Invalid date format for dictionary version %s."" % _date_to_str(date))

        if date not in cls._instances:
            cls._instances[date] = super(DictionaryVersionSingleton, cls).__call__(date)

        return cls._instances[date]",https://github.com/IEMLdev/ieml/blob/4c842ba7e6165e2f1b4a4e2e98759f9f33af5f25/ieml/dictionary/version.py#L56-L78
tfidf_python_100_1.0,format date,python,"def convert_to_date_object(date):
    if isinstance(date, Constant):
        return convert_to_date_object(date.value)
    elif isinstance(date, Date) and date.start == date.end:
        return date.start
    else:
        try:
            return ensure_is_date_object(date)
        except TohuDateError:
            raise TohuTimestampError(f""Argument 'date' must represent some kind of constant date object. Got: {date}"")",https://github.com/maxalbert/tohu/blob/43380162fadec99cdd5c5c3152dd6b7d3a9d39a8/tohu/v6/derived_generators.py#L284-L293
tfidf_python_100_1.0,format date,python,"def __init__(self, date=None):
        """""" Parse the date string """"""
        if isinstance(date, datetime.date):
            self.date = date
        elif date is None or date.lower() == ""today"":
            self.date = TODAY
        elif date.lower() == ""yesterday"":
            self.date = TODAY - delta(days=1)
        else:
            try:
                self.date = datetime.date(*[int(i) for i in date.split(""-"")])
            except StandardError as error:
                log.debug(error)
                raise OptionError(
                    ""Invalid date format: '{0}', use YYYY-MM-DD."".format(date))
        self.datetime = datetime.datetime(
            self.date.year, self.date.month, self.date.day, 0, 0, 0)",https://github.com/psss/did/blob/04e4ee6f1aa14c0cae3ba9f9803871f3f98279cb/did/base.py#L189-L205
tfidf_python_100_1.0,readonly array,python,"def derive_readonly(self):
        """"""
        Figures out what fields should be readonly.  We iterate our field_config to find all
        that have a readonly of true
        """"""
        readonly = list(self.readonly)
        for key, value in self.field_config.items():
            if 'readonly' in value and value['readonly']:
                readonly.append(key)

        return readonly",https://github.com/nyaruka/smartmin/blob/488a676a4960555e4d216a7b95d6e01a4ad4efd8/smartmin/views.py#L914-L924
tfidf_python_100_1.0,readonly array,python,"def __init__(self, name, readonly=None):
        self.name = name
        self.readonly = readonly",https://github.com/matllubos/django-is-core/blob/3f87ec56a814738683c732dce5f07e0328c2300d/is_core/forms/fieldset.py#L87-L89
tfidf_python_100_1.0,readonly array,python,"def __init__(self, existingClaim=None, mountPath=None, hostPath=None, readOnly=False):  # noqa
        self.existingClaim = existingClaim
        self.mountPath = mountPath
        self.hostPath = hostPath
        self.readOnly = readOnly",https://github.com/polyaxon/polyaxon/blob/e1724f0756b1a42f9e7aa08a976584a84ef7f016/polyaxon/stores/schemas/volume.py#L21-L25
tfidf_python_100_1.0,readonly array,python,"def __init__(self, **kwargs):
        readOnly = {
            'componentType': self.componentType,
            'sizeSpec': self.sizeSpec
        }
        readOnly.update(kwargs)

        Asn1ItemBase.__init__(self, **readOnly)

        self._componentValues = []",https://github.com/etingof/pyasn1/blob/25cf116ef8d11bb0e08454c0f3635c9f4002c2d6/pyasn1/type/base.py#L461-L470
tfidf_python_100_1.0,readonly array,python,"def __init__(self, name, readonly=False):
        """"""Create a `DevicesField`.

        :param name: The name of the field. This is the name that's used to
            store the datum in the MAAS-side data dictionary.
        """"""
        super(ObjectFieldRelated, self).__init__(
            name, default=undefined, readonly=readonly)",https://github.com/maas/python-libmaas/blob/4092c68ef7fb1753efc843569848e2bcc3415002/maas/client/viscera/filesystem_groups.py#L74-L81
tfidf_python_100_1.0,readonly array,python,"def __init__(self, filename, readonly=None, writeonly=None):
        if readonly and writeonly:
            raise RuntimeError(""This interface cannot be both readonly and writeonly"")

        try:
            self.filename = filename.encode()
        except AttributeError:
            self.filename = filename
        self.readonly = readonly or self.readonly
        self.writeonly = writeonly or self.writeonly",https://github.com/cloudsigma/cgroupspy/blob/e705ac4ccdfe33d8ecc700e9a35a9556084449ca/cgroupspy/interfaces.py#L39-L48
tfidf_python_100_1.0,readonly array,python,"def __init__(self, *args, readonly=False, **kwargs):
        # Calling super() before setting readonly.
        # This way super().__init__ can set attributes even if readonly==True
        super().__init__(*args, **kwargs)
        self.__readonly = readonly",https://github.com/pasztorpisti/py-flags/blob/bc48adb5edd7340ea1a686622d7993b4bcf4bfc2/src/flags.py#L135-L139
tfidf_python_100_1.0,readonly array,python,"def __init__(self, **kwargs):
        readOnly = {
            'tagSet': self.tagSet,
            'subtypeSpec': self.subtypeSpec
        }

        readOnly.update(kwargs)

        self.__dict__.update(readOnly)

        self._readOnly = readOnly",https://github.com/etingof/pyasn1/blob/25cf116ef8d11bb0e08454c0f3635c9f4002c2d6/pyasn1/type/base.py#L40-L50
tfidf_python_100_1.0,readonly array,python,"def __init__(self, dbnames, conn_info, readonly=True):
        self.dbnames = cycle(dbnames)
        self.conn_info = conn_info
        self.conn_mapping = {}
        self.lock = threading.Lock()
        self.readonly = readonly",https://github.com/tilezen/tilequeue/blob/d7b9484ab92e246eb2773949c784ebb37c731e28/tilequeue/query/pool.py#L34-L39
tfidf_python_100_1.0,readonly array,python,"def __init__(self, hdfpath, readonly=0, complevel=None, complib=None, fletcher32=False, format=None):
        self.hdfpath = hdfpath
        self.readonly = readonly
        self._file_exists = None
        self._store = None
        self.format = format
        self.get_store_kwargs = {'complevel': complevel, 'complib': complib, 'fletcher32': fletcher32}",https://github.com/bpsmith/tia/blob/a7043b6383e557aeea8fc7112bbffd6e36a230e9/tia/bbg/datamgr.py#L202-L208
tfidf_python_100_1.0,readonly array,python,"def __init__(self, *args, **kwargs):
        super(ReadOnly, self).__init__(*args, **kwargs)

        self.readonly = True",https://github.com/swimlane/swimlane-python/blob/588fc503a76799bcdb5aecdf2f64a6ee05e3922d/swimlane/core/fields/base/__init__.py#L10-L13
tfidf_python_100_1.0,readonly array,python,"def __init__(self, uri, sasl_username=None, sasl_password=None,
                 readonly=False):
        self.uri = uri
        self.sasl_username = sasl_username
        self.sasl_password = sasl_password
        self.readonly = readonly",https://github.com/umago/virtualbmc/blob/47551d1427e8976da0449c5405e87a763180ad1a/virtualbmc/utils.py#L23-L28
tfidf_python_100_1.0,readonly array,python,"def readonly(self):
        if self.form and self.form.readonly is True:
            return True

        return self._readonly",https://github.com/stevelittlefish/easyforms/blob/f5dd2635b045beec9af970b249909f8429cedc57/easyforms/form.py#L185-L189
tfidf_python_100_1.0,readonly array,python,"def _relXactForReading(self):
        if not self.readonly:
            return
        self.txnrefcount -= 1
        if not self.txnrefcount:
            self._finiCoXact()",https://github.com/vertexproject/synapse/blob/22e67c5a8f6d7caddbcf34b39ab1bd2d6c4a6e0b/synapse/lib/lmdbslab.py#L281-L286
tfidf_python_100_1.0,readonly array,python,"def _set_import_info(self, new_import):
        if not self.readonly and \
           new_import is not None and not new_import == self._import_info:
            self._is_changed = True
            self._import_info = new_import",https://github.com/python-rope/rope/blob/1c9f9cd5964b099a99a9111e998f0dc728860688/rope/refactor/importutils/importinfo.py#L24-L28
tfidf_python_100_1.0,readonly array,python,"def __init__(self, *args, **kwargs):
        self.readonly = kwargs.pop('readonly', False)
        super(ClusterConnection, self).__init__(*args, **kwargs)",https://github.com/NoneGG/aredis/blob/204caad740ac13e5760d46444a2ba7632982a046/aredis/connection.py#L647-L649
tfidf_python_100_1.0,readonly array,python,"def update(self, values):
        if self.readonly:
            raise PermissionError('sheet was loaded readonly, '
                                  'if you want to write '
                                  'reinit with readonly=False')

        update_sheet_values(self.name,
                            self.sheet_name,
                            values,
                            spreadsheet_service=self.spreadsheet_service)",https://github.com/tgbugs/pyontutils/blob/3d913db29c177db39151592909a4f56170ef8b35/pyontutils/sheets.py#L146-L155
tfidf_python_100_1.0,readonly array,python,"def serialize(self, field, cstruct, **kw):
        readonly = kw.get('readonly', self.readonly)
        template = readonly and self.readonly_template or self.template
        values = self.get_template_values(field, cstruct, kw)
        return field.renderer(template, **values)",https://github.com/majerteam/deform_extensions/blob/fdad612e4889a40f1944611264b943866a3cb96e/deform_extensions/__init__.py#L816-L820
tfidf_python_100_1.0,readonly array,python,"def __init__(self, Session, mapped_class, geom_attr, readonly=False,
                 **kwargs):
        self.Session = Session
        self.mapped_class = mapped_class
        self.geom_attr = geom_attr
        self.readonly = readonly
        self.before_create = kwargs.get('before_create')
        self.before_update = kwargs.get('before_update')
        self.before_delete = kwargs.get('before_delete')",https://github.com/elemoine/papyrus/blob/764fb2326105df74fbd3dbcd7e58f4cb21956005/papyrus/protocol.py#L223-L231
tfidf_python_100_1.0,readonly array,python,"def __init__(
        self,
        source,
        destination,
        readonly=False,
    ):
        self.source = source
        self.destination = destination
        self.readonly = bool(readonly)",https://github.com/valohai/valohai-yaml/blob/3d2e92381633d84cdba039f6905df34c9633a2e1/valohai_yaml/objs/mount.py#L7-L15
tfidf_python_100_1.0,filter array,python,"def __DeviceProxy__get_property_list(self, filter, array=None):
    """"""
    get_property_list(self, filter, array=None) -> obj

            Get the list of property names for the device. The parameter
            filter allows the user to filter the returned name list. The
            wildcard character is '*'. Only one wildcard character is
            allowed in the filter parameter.

        Parameters :
                - filter[in] : (str) the filter wildcard
                - array[out] : (sequence obj or None) (optional, default is None)
                            an array to be filled with the property names. If None
                            a new list will be created internally with the values.

        Return     : the given array filled with the property names (or a new list
                    if array is None)

        Throws     : NonDbDevice, WrongNameSyntax,
                    ConnectionFailed (with database),
                    CommunicationFailed (with database)
                    DevFailed from database device

        New in PyTango 7.0.0
    """"""

    if array is None:
        new_array = StdStringVector()
        self._get_property_list(filter, new_array)
        return new_array

    if isinstance(array, StdStringVector):
        self._get_property_list(filter, array)
        return array
    elif isinstance(array, collections_abc.Sequence):
        new_array = StdStringVector()
        self._get_property_list(filter, new_array)
        StdStringVector_2_seq(new_array, array)
        return array

    raise TypeError('array must be a mutable sequence<string>')",https://github.com/tango-controls/pytango/blob/9cf78c517c9cdc1081ff6d080a9646a740cc1d36/tango/device_proxy.py#L704-L744
tfidf_python_100_1.0,filter array,python,"def underlying_variable(self):
        array = self
        while not isinstance(array, ArrayVariable):
            array = array.array
        return array",https://github.com/trailofbits/manticore/blob/54c5a15b1119c523ae54c09972413e8b97f11629/manticore/core/smtlib/expression.py#L687-L691
tfidf_python_100_1.0,filter array,python,"def reverse(array, i, j):
    while i < j:
        array[i], array[j] = array[j], array[i]
        i += 1
        j -= 1",https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/strings/reverse_words.py#L2-L6
tfidf_python_100_1.0,filter array,python,"def params(self):
        if not any(filter.params for filter in self):
            return None
        else:
            return Array(filter.params or Null() for filter in self)",https://github.com/brechtm/rinohtype/blob/40a63c4e5ad7550f62b6860f1812cb67cafb9dc7/src/rinoh/backend/pdf/filter.py#L459-L463
tfidf_python_100_1.0,filter array,python,"def tinyify(array, tiny_mode, small_mode):
  if tiny_mode:
    return [1 for _ in array]
  if small_mode:
    return [max(x // 4, 1) for x in array]
  return array",https://github.com/tensorflow/tensor2tensor/blob/272500b6efe353aeb638d2745ed56e519462ca31/tensor2tensor/layers/common_video.py#L502-L507
tfidf_python_100_1.0,filter array,python,"def _get_filter_string(self, filter_statement):
        """"""
        """"""

        if isinstance(filter_statement, QueryFilterContainer):
            filter_string = ' FILTER '
            is_first = True

            for filter in filter_statement.filters:
                if is_first:
                    is_first = False

                    filter_string += self._get_filter_condition_string(filter)
                else:
                    filter_string += ' %s %s ' % (
                        filter_statement.bit_operator,
                        self._get_filter_condition_string(filter)
                    )
        else:
            filter_string = ' FILTER %s' % self._get_filter_condition_string(filter_statement)

        return filter_string",https://github.com/saeschdivara/ArangoPy/blob/b924cc57bed71520fc2ef528b917daeb98e10eca/arangodb/query/advanced.py#L386-L407
tfidf_python_100_1.0,filter array,python,"def _maybe_to_sparse(array):
    """"""
    array must be SparseSeries or SparseArray
    """"""
    if isinstance(array, ABCSparseSeries):
        array = array.values.copy()
    return array",https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/arrays/sparse.py#L1803-L1809
tfidf_python_100_1.0,filter array,python,"def ensure_array(array):
    """"""
    Assert that the given array is an Array subclass (or numpy array).

    If the given array is a numpy.ndarray an appropriate NumpyArrayAdapter
    instance is created, otherwise the passed array must be a subclass of
    :class:`Array` else a TypeError will be raised.

    """"""
    if not isinstance(array, Array):
        if isinstance(array, np.ndarray):
            array = NumpyArrayAdapter(array)
        elif np.isscalar(array):
            array = ConstantArray([], array)
        else:
            raise TypeError('The given array should be a `biggus.Array` '
                            'instance, got {}.'.format(type(array)))
    return array",https://github.com/SciTools/biggus/blob/0a76fbe7806dd6295081cd399bcb76135d834d25/biggus/_init.py#L3289-L3306
tfidf_python_100_1.0,filter array,python,"def lcm(array):
        if len(array) == 0:
                return 0
        if len(array) == 1:
                return array[0]
        elif len(array) == 2:
                return array[0] * array[1] / gcd(array)
        else:
                return lcm([lcm(array[:len(array) / 2]), lcm(array[len(array) / 2:])])",https://github.com/ioguntol/AMP/blob/c227806bdd851b8c2d78afe0aedab43395705795/AMP/funcEval.py#L44-L52
tfidf_python_100_1.0,filter array,python,"def get_crc_datarange(self, inpt, vrfy_crc):
        return c_util.get_crc_datarange(array.array(""B"", inpt),
                            array.array(""B"", self.polynomial),
                            array.array(""B"", vrfy_crc),
                            array.array(""B"", self.start_value),
                            array.array(""B"", self.final_xor),
                            self.lsb_first, self.reverse_polynomial, self.reverse_all, self.little_endian)",https://github.com/jopohl/urh/blob/2eb33b125c8407964cd1092843cde5010eb88aae/src/urh/util/GenericCRC.py#L104-L110
tfidf_python_100_1.0,filter array,python,"def __init__(self, array, indexer_cls=BasicIndexer):
        self.array = as_indexable(array)
        self.indexer_cls = indexer_cls",https://github.com/pydata/xarray/blob/6d93a95d05bdbfc33fff24064f67d29dd891ab58/xarray/core/indexing.py#L447-L449
tfidf_python_100_1.0,filter array,python,"def __init__(self, array, lastPos=0):
        self.array = array
        self.lastPos = lastPos",https://github.com/radjkarl/fancyTools/blob/4c4d961003dc4ed6e46429a0c24f7e2bb52caa8b/fancytools/math/nearestPosition2.py#L24-L26
tfidf_python_100_1.0,filter array,python,"def string2bits(bit_str: str) -> array.array:
    return array.array(""B"", map(int, bit_str))",https://github.com/jopohl/urh/blob/2eb33b125c8407964cd1092843cde5010eb88aae/src/urh/util/util.py#L231-L232
tfidf_python_100_1.0,filter array,python,"def three_sum(array):
    """"""
    :param array: List[int]
    :return: Set[ Tuple[int, int, int] ]
    """"""
    res = set()
    array.sort()
    for i in range(len(array) - 2):
        if i > 0 and array[i] == array[i - 1]:
            continue
        l, r = i + 1, len(array) - 1
        while l < r:
            s = array[i] + array[l] + array[r]
            if s > 0:
                r -= 1
            elif s < 0:
                l += 1
            else:
                # found three sum
                res.add((array[i], array[l], array[r]))

                # remove duplicates
                while l < r and array[l] == array[l + 1]:
                    l += 1

                while l < r and array[r] == array[r - 1]:
                    r -= 1

                l += 1
                r -= 1
    return res",https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/arrays/three_sum.py#L18-L48
tfidf_python_100_1.0,filter array,python,"def __init__(self, auto_suggest, filter):
        assert isinstance(auto_suggest, AutoSuggest)

        self.auto_suggest = auto_suggest
        self.filter = to_cli_filter(filter)",https://github.com/wandb/client/blob/7d08954ed5674fee223cd85ed0d8518fe47266b2/wandb/vendor/prompt_toolkit/auto_suggest.py#L80-L84
tfidf_python_100_1.0,filter array,python,"def build_filter(self, filter):
        """"""
        Tries to build a :class:`filter.Filter` instance from the given filter.

        Raises ValueError if the :class:`filter.Filter` object can't be build
        from the given filter.
        """"""
        try:
            self.filter = Filter.from_string(filter, self.limit)
        except ValueError:
            raise

        return self",https://github.com/Frzk/Ellis/blob/39ce8987cbc503354cf1f45927344186a8b18363/ellis/rule.py#L75-L87
tfidf_python_100_1.0,filter array,python,"def __init__(self, array):
        self.array = as_indexable(array)
        self._copied = False",https://github.com/pydata/xarray/blob/6d93a95d05bdbfc33fff24064f67d29dd891ab58/xarray/core/indexing.py#L594-L596
tfidf_python_100_1.0,filter array,python,"def _with_attrs(array, **kwargs):
    array = AttrArray(array)
    for k, v in kwargs.items():
        setattr(array, k, v)
    return array",https://github.com/anntzer/mplcursors/blob/a4bce17a978162b5a1837cc419114c910e7992f9/lib/mplcursors/_pick_info.py#L105-L109
tfidf_python_100_1.0,filter array,python,"def removeFilter(self, filter):
        """""" Remove Registered Filter """"""
        filter = filter.split('#')
        del self.FILTERS[int(filter[1])]
        return True",https://github.com/Clivern/PyLogging/blob/46a1442ec63796302ec7fe3d49bd06a0f7a2fe70/pylogging/pylogging.py#L113-L117
tfidf_python_100_1.0,filter array,python,"def _get_list_filter(self):
        list_filter = super(InlineTableView, self)._get_list_filter()
        fk_name = _get_foreign_key(self.parent_instance.__class__, self.model, fk_name=self.fk_name).name
        list_filter['filter'] = filter = list_filter.get('filter', {})
        if 'filter' in list_filter:
            filter[fk_name] = self.parent_instance.pk
        return list_filter",https://github.com/matllubos/django-is-core/blob/3f87ec56a814738683c732dce5f07e0328c2300d/is_core/generic_views/inlines/inline_table_views.py#L20-L26
tfidf_python_100_1.0,map to json,python,"def make_dataset_from_sgf(sgf_filename, tf_record):
    pwcs = sgf_wrapper.replay_sgf_file(sgf_filename)
    tf_examples = map(_make_tf_example_from_pwc, pwcs)
    write_tf_examples(tf_record, tf_examples)",https://github.com/mlperf/training/blob/1c6ae725a81d15437a2b2df05cac0673fde5c3a4/reinforcement/tensorflow/minigo/preprocessing.py#L276-L279
tfidf_python_100_1.0,map to json,python,"def to_json(self):
        map = dict()
        map[""peer_pubkey""] = self.peer_pubkey
        map[""max_authorize""] = self.max_authorize
        map[""old_peerCost""] = self.old_peerCost
        map[""new_peer_cost""] = self.new_peer_cost
        map[""set_cost_view""] = self.set_cost_view
        map[""field1""] = self.field1
        map[""field2""] = self.field2
        map[""field3""] = self.field3
        map[""field4""] = self.field4
        return map",https://github.com/ontio/ontology-python-sdk/blob/ac88bdda941896c5d2ced08422a9c5179d3f9b19/ontology/smart_contract/native_contract/governance.py#L403-L414
tfidf_python_100_1.0,map to json,python,"def to_json(self):
        map = dict()
        map[""peer_pubkey""] = self.peer_pubkey
        map[""address""] = self.address.b58encode()
        map[""consensus_pos""] = self.consensus_pos
        map[""freeze_pos""] = self.freeze_pos
        map[""new_pos""] = self.new_pos
        map[""withdraw_pos""] = self.withdraw_pos
        map[""withdraw_freeze_pos""] = self.withdraw_freeze_pos
        map[""withdraw_unfreeze_pos""] = self.withdraw_unfreeze_pos
        return map",https://github.com/ontio/ontology-python-sdk/blob/ac88bdda941896c5d2ced08422a9c5179d3f9b19/ontology/smart_contract/native_contract/governance.py#L438-L448
tfidf_python_100_1.0,map to json,python,"def update_mapobj_from_dict(mapobj,map):
    if 'name' in map:
        mapobj.name = map['name']

    if 'extent' in map:
        e = map['extent']
        mapobj.setExtent(e['minx'],e['miny'],e['maxx'],e['maxy'])

    if 'projection' in map:
        mapobj.setProjection(map['projection'])
    
    if 'outputformats' in map:
        of = map['outputformats'][0]
        ofo = dict_to_outputformatobj(of)
        mapobj.appendOutputFormat(ofo)
        mapobj.selectOutputFormat(ofo.name)

    if 'imagetype' in map:
        mapobj.setImageType(map['imagetype'])

    if 'fontset' in map and map['fontset'] is not None:
        mapobj.setFontSet(map['fontset'])
    
    if 'web' in map:
        update_webobj_from_dict(mapobj.web, map['web'])
    
    if 'maxsize' in map:
        mapobj.maxsize = map['maxsize']

    if 'resolution' in map:
        mapobj.resolution = map['resolution']

    if 'symbolset' in map and map['symbolset'] is not None:
        mapobj.setSymbolSet(map['symbolset'])

    if 'units' in map:
        mapobj.units = mapscriptutils.units.lookup(map['units'])

    if 'height' in map and 'width' in map:
        mapobj.setSize(map['width'],map['height'])

    if 'imagecolor' in map:
        update_colorobj_from_dict(mapobj.imagecolor,map['imagecolor'])

    if 'layers' in map:
        for l in map['layers']:
            layerobj = mapscript.layerObj()
            update_layerobj_from_dict(layerobj,l,mapobj)
            mapobj.insertLayer(layerobj)

    return mapobj",https://github.com/camptocamp/Studio/blob/43cb7298434fb606b15136801b79b03571a2f27e/studio/lib/mapserializer.py#L538-L588
tfidf_python_100_1.0,map to json,python,"def __init__(self, map: Map):
        super(MapWrapper, self).__init__(map)
        self.enumerator = map.GetEnumerator()",https://github.com/CityOfZion/neo-python/blob/fe90f62e123d720d4281c79af0598d9df9e776fb/neo/SmartContract/Iterable/Wrapper.py#L14-L16
tfidf_python_100_1.0,map to json,python,"def map_keys(map):
    return List(
        [k for (k, v) in map.to_pairs()],
        use_comma=True)",https://github.com/Kronuz/pyScss/blob/fb32b317f6e2b4b4aad2b86a74844658ac4aa11e/scss/extension/core.py#L789-L792
tfidf_python_100_1.0,map to json,python,"def map_values(map):
    return List(
        [v for (k, v) in map.to_pairs()],
        use_comma=True)",https://github.com/Kronuz/pyScss/blob/fb32b317f6e2b4b4aad2b86a74844658ac4aa11e/scss/extension/core.py#L796-L799
tfidf_python_100_1.0,map to json,python,"def translate(self, map, to):
        for key, value in to.iteritems():
            map[value] = map[key]
            del map[key]
        return map",https://github.com/rackerlabs/python-lunrclient/blob/f26a450a422600f492480bfa42cbee50a5c7016f/lunrclient/lunr_shell.py#L55-L59
tfidf_python_100_1.0,map to json,python,"def map(self, map):
        return self.__class__([t.map(map) for t in self.terms])",https://github.com/klahnakoski/pyLibrary/blob/fa2dcbc48fda8d26999baef400e9a98149e0b982/jx_base/expressions.py#L3045-L3046
tfidf_python_100_1.0,map to json,python,"def __set_map__(self, **kwargs):
        """""" sets the subject predicat object json mapping

        kwargs:
            map: dictionary mapping 's', 'p', 'o' keys
        """"""
        if kwargs.get('map'):
            map = kwargs.pop('map',{})
            self.smap = map.get('s','s')
            self.pmap = map.get('p','p')
            self.omap = map.get('o','o')",https://github.com/KnowledgeLinks/rdfframework/blob/9ec32dcc4bed51650a4b392cc5c15100fef7923a/rdfframework/datasets/rdfdatasets.py#L96-L106
tfidf_python_100_1.0,map to json,python,"def getAnalysisRequests(self):
        backrefs = get_backreferences(self, 'AnalysisRequestSample')
        ars = map(get_object_by_uid, backrefs)
        return ars",https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/content/sample.py#L429-L432
tfidf_python_100_1.0,map to json,python,"def create_map_grid(map_file):
    if map_file[-3:] == ""map"":
        proj_dict, grid_dict = read_arps_map_file(map_file)
    else:
        proj_dict, grid_dict = read_ncar_map_file(map_file)
    mapping_data = make_proj_grids(proj_dict, grid_dict)
    return mapping_data, proj_dict, grid_dict",https://github.com/djgagne/hagelslag/blob/6fb6c3df90bf4867e13a97d3460b14471d107df1/hagelslag/util/create_model_grid_us_mask.py#L28-L34
tfidf_python_100_1.0,map to json,python,"def del_channel(self, map=None):
        if map is not None and self._fileno in map:
            del map[self._fileno]
            self.connected = False",https://github.com/etingof/pysnmp/blob/cde062dd42f67dfd2d7686286a322d40e9c3a4b7/pysnmp/carrier/asyncore/base.py#L96-L99
tfidf_python_100_1.0,map to json,python,"def Map(self: dict, f):
    if is_to_destruct(f):
        f = destruct_func(f)
    return map(f, self.items())",https://github.com/Xython/Linq.py/blob/ffb65f92f1df0d8161d5f835f5947554f6f33d72/linq/standard/dict.py#L34-L37
tfidf_python_100_1.0,map to json,python,"def _sprites(map):
    map = StringValue(map).value
    sprite_map = sprite_maps.get(map, {})
    return ListValue(sorted(s for s in sprite_map if not s.startswith('*')))",https://github.com/klen/zeta-library/blob/b76f89000f467e10ddcc94aded3f6c6bf4a0e5bd/zetalibrary/scss/__init__.py#L2892-L2895
tfidf_python_100_1.0,map to json,python,"def add_channel(self, map=None):
        if map is not None:
            map[self._fileno] = self
            self.connected = True",https://github.com/etingof/pysnmp/blob/cde062dd42f67dfd2d7686286a322d40e9c3a4b7/pysnmp/carrier/asyncore/base.py#L91-L94
tfidf_python_100_1.0,map to json,python,"def from_json(cls, json):
        return Track(
            json['num_samples'],
            json['duration'],
            json['sample_md5'],
            json['offset_seconds'],
            json['window_seconds'],
            json['analysis_sample_rate'],
            json['analysis_channels'],
            json['end_of_fade_in'],
            json['start_of_fade_out'],
            json['loudness'],
            json['tempo'],
            json['tempo_confidence'],
            json['time_signature'],
            json['time_signature_confidence'],
            json['key'],
            json['key_confidence'],
            json['mode'],
            json['mode_confidence'],
            json['codestring'],
            json['code_version'],
            json['echoprintstring'],
            json['echoprint_version'],
            json['synchstring'],
            json['synch_version'],
            json['rhythmstring'],
            json['rhythm_version']
        )",https://github.com/jingming/spotify/blob/d92c71073b2515f3c850604114133a7d2022d1a4/spotify/object/audio_analysis.py#L175-L203
tfidf_python_100_1.0,map to json,python,"def Map(self: Iterable, f):
    if is_to_destruct(f):
        f = destruct_func(f)
    return map(f, self)",https://github.com/Xython/Linq.py/blob/ffb65f92f1df0d8161d5f835f5947554f6f33d72/linq/standard/general.py#L27-L30
tfidf_python_100_1.0,map to json,python,"def __init__(self, json):
        """"""
        Initializes the ResultItem class from the JSON provided
        :param json: String. Raw JSON data to fetch information from
        """"""
        self.artist_name = json['artistName']
        self.type = None

        if 'wrapperType' in json:
            self.type = json['wrapperType']

            if 'collectionType' in json:
                self.collection_type = json['collectionType']
            elif 'artistType' in json:
                self.artist_type = json['artistType']
            elif 'kind' in json:
                self.track_type = json['kind']
        elif 'kind' in json:
            self.type = json['kind']

        if 'primaryGenreName' in json:
            self.primary_genre_name = json['primaryGenreName']

        if 'artistId' in json:
            self.artist_id = json['artistId']

        if 'artistLinkUrl' in json:
            self.artist_link_url = json['artistLinkUrl']

        if 'radioStationUrl' in json:
            self.artist_radio_url = json['radioStationUrl']

        if 'primaryGenreId' in json:
            self.primary_genre_id = json['primaryGenreId']

        if 'amgArtistId' in json:
            self.artist_amg_id = json['amgArtistId']

        if 'artistViewUrl' in json:
            self.artist_view_url = json['artistViewUrl']

        if 'trackName' in json:
            self.track_name = json['trackName']

        if 'trackId' in json:
            self.track_id = json['trackId']

        if 'trackCensoredName' in json:
            self.track_censored_name = json['trackCensoredName']

        if 'trackViewUrl' in json:
            self.track_view_url = json['trackViewUrl']

        if 'trackCount' in json:
            self.track_count = json['trackCount']

        if 'trackNumber' in json:
            self.track_number = json['trackNumber']

        if 'previewUrl' in json:
            self.preview_url = json['previewUrl']

        if 'artworkUrl30' in json:
            self.artwork_url_30 = json['artworkUrl30']

        if 'artworkUrl60' in json:
            self.artwork_url_60 = json['artworkUrl60']

        if 'artworkUrl100' in json:
            self.artwork_url_100 = json['artworkUrl100']

        if 'artworkUrl512' in json:
            self.artwork_url_512 = json['artworkUrl512']

        if 'collectionName' in json:
            self.collection_name = json['collectionName']

        if 'collectionId' in json:
            self.collection_id = json['collectionId']

        if 'collectionCensoredName' in json:
            self.collection_censored_name = json['collectionCensoredName']

        if 'collectionViewUrl' in json:
            self.collection_view_url = json['collectionViewUrl']

        if 'collectionPrice' in json:
            self.collection_price = json['collectionPrice']

        if 'trackPrice' in json:
            self.track_price = json['trackPrice']

        if 'trackRentalPrice' in json:
            self.track_rental_price = json['trackRentalPrice']

        if 'collectionHdPrice' in json:
            self.collection_hd_price = json['collectionHdPrice']

        if 'trackHdPrice' in json:
            self.track_hd_price = json['trackHdPrice']

        if 'trackHdRentalPrice' in json:
            self.track_hd_rental_price = json['trackHdRentalPrice']

        if 'releaseDate' in json:
            self.release_date = json['releaseDate']

        if 'collectionExplicitness' in json:
            self.collection_explicitness = json['collectionExplicitness']

        if 'trackExplicitness' in json:
            self.track_explicitness = json['trackExplicitness']

        if 'trackTimeMillis' in json:
            self.track_time = json['trackTimeMillis']

        if 'discCount' in json:
            self.disc_count = json['discCount']

        if 'discNumber' in json:
            self.disc_number = json['discNumber']

        if 'country' in json:
            self.country = json['country']

        if 'currency' in json:
            self.currency = json['currency']

        if 'copyright' in json:
            self.copyright = json['copyright']

        if 'contentAdvisoryRating' in json:
            self.content_advisory_rating = json['contentAdvisoryRating']

        if 'shortDescription' in json:
            self.short_description = json['shortDescription']

        if 'longDescription' in json:
            self.long_description = json['longDescription']

        if 'isStreamable' in json:
            self.is_streamable = json['isStreamable']

        if 'fileSizeBytes' in json:
            self.file_size = json['fileSizeBytes']

        if 'genres' in json:
            self.genres = json['genres']

        if 'price' in json:
            self.price = json['price']

        if 'description' in json:
            self.description = json['description']

        if 'genreIds' in json:
            self.genre_ids = json['genreIds']

        if 'artistIds' in json:
            self.artist_ids = json['artistIds']

        if 'formattedPrice' in json:
            self.formatted_price = json['formattedPrice']

        if 'averageUserRating' in json:
            self.average_user_rating = json['averageUserRating']

        if 'userRatingCount' in json:
            self.user_rating_count = json['userRatingCount']

        if 'screenshotUrls' in json:
            self.screenshot_urls = json['screenshotUrls']

        if 'ipadScreenshotUrls' in json:
            self.ipad_screenshot_urls = json['ipadScreenshotUrls']

        if 'features' in json:
            self.features = json['features']

        if 'supportedDevices' in json:
            self.supported_devices = json['supportedDevices']

        if 'advisories' in json:
            self.advisories = json['advisories']

        if 'isGameCenterEnabled' in json:
            self.is_game_center_enabled = json['isGameCenterEnabled']

        if 'userRatingCountForCurrentVersion' in json:
            self.user_rating_count_for_current_version = json['userRatingCountForCurrentVersion']

        if 'languageCodesISO2A' in json:
            self.language_codes = json['languageCodesISO2A']

        if 'averageUserRatingForCurrentVersion' in json:
            self.average_user_rating_for_current_version = json['averageUserRatingForCurrentVersion']

        if 'trackContentRating' in json:
            self.track_content_rating = json['trackContentRating']

        if 'version' in json:
            self.version = json['version']

        if 'sellerName' in json:
            self.seller_name = json['sellerName']

        if 'sellerUrl' in json:
            self.seller_url = json['sellerUrl']

        if 'bundleId' in json:
            self.bundle_id = json['bundleId']

        if 'releaseNotes' in json:
            self.release_notes = json['releaseNotes']

        if 'isVppDeviceBasedLicensingEnabled' in json:
            self.is_vpp_device_based_licensing_enabled = json['isVppDeviceBasedLicensingEnabled']

        if 'minimumOsVersion' in json:
            self.minimum_os_version = json['minimumOsVersion']",https://github.com/sleepyfran/itunespy/blob/0e7e931b135b5e0daae49ba68e9167ff4ac73eb5/itunespy/result_item.py#L19-L238
tfidf_python_100_1.0,map to json,python,"def map_get_nested3(map, keys, default=Null()):
    for key in keys:
        map = map.to_dict().get(key, None)
        if map is None:
            return default

    return map",https://github.com/Kronuz/pyScss/blob/fb32b317f6e2b4b4aad2b86a74844658ac4aa11e/scss/extension/core.py#L815-L821
tfidf_python_100_1.0,parse json file,python,"def parse(self, file, outfile=None):
        """"""Parse a BGI (basic gene info) JSON file
        """"""
        file = self._ensure_file(file)
        obj = json.load(file)
        items = obj['data']
        return [self.transform_item(item) for item in items]",https://github.com/biolink/ontobio/blob/4e512a7831cfe6bc1b32f2c3be2ba41bc5cf7345/ontobio/io/entityparser.py#L197-L203
tfidf_python_100_1.0,parse json file,python,"def _parse_by_expat(self, fileptr):
        vasprun = VasprunxmlExpat(fileptr)
        vasprun.parse()
        return vasprun.get_forces()[-1]",https://github.com/atztogo/phonopy/blob/869cc2ba9e7d495d5f4cf6942415ab3fc9e2a10f/phonopy/interface/vasp.py#L614-L617
tfidf_python_100_1.0,parse json file,python,"def parse(self, json_value):
        return {k: self.value_parser.parse(v) for k, v in json_value.items()}",https://github.com/antidot/Pyckson/blob/44e625164a53081eb46b8d4bc38f947a575de505/src/pyckson/parsers/base.py#L60-L61
tfidf_python_100_1.0,parse json file,python,"def franchise(id):
    json = requests.get(""{}/franchises/{}"".format(BASE, id)).json()
    return parse.parse_franchise(json[""franchises""][0])",https://github.com/mhostetter/nhl/blob/32c91cc392826e9de728563d57ab527421734ee1/nhl/statsapi/api.py#L28-L30
tfidf_python_100_1.0,parse json file,python,"def conference(id):
    json = requests.get(""{}/conferences/{}"".format(BASE, id)).json()
    return parse.parse_conference(json[""conferences""][0])",https://github.com/mhostetter/nhl/blob/32c91cc392826e9de728563d57ab527421734ee1/nhl/statsapi/api.py#L8-L10
tfidf_python_100_1.0,parse json file,python,"def parse(cls, json):
        return User(
            id=json.get('id'),
            name=json.get('name'),
            email=json.get('email', None) or json.get('login'),
            status=json.get('status'),
            roles=json.get('roles', None) or ([json['role']] if 'role' in json else list()),
            attributes=json.get('attributes', dict()),
            create_time=DateTime.parse(json.get('createTime')),
            last_login=DateTime.parse(json.get('lastLogin')),
            text=json.get('text', None),
            update_time=DateTime.parse(json.get('updateTime')),
            email_verified=json.get('email_verified', None)
        )",https://github.com/alerta/python-alerta-client/blob/7eb367b5fe87d5fc20b54dea8cddd7f09e251afa/alertaclient/models/user.py#L33-L46
tfidf_python_100_1.0,parse json file,python,"def load_json(self, file=None):
        if file is None:
            file = getattr(self.config, ""needs_file"", ""needs.json"")
        if not os.path.isabs(file):
            file = os.path.join(self.confdir, file)

        if not os.path.exists(file):
            self.log.warning(""Could not load needs json file {0}"".format(file))
        else:
            with open(file, ""r"") as needs_file:
                needs_file_content = needs_file.read()
            try:
                needs_list = json.loads(needs_file_content)
            except json.JSONDecodeError:
                self.log.warning(""Could not decode json file {0}"".format(file))
            else:
                self.needs_list = needs_list",https://github.com/useblocks/sphinxcontrib-needs/blob/f49af4859a74e9fe76de5b9133c01335ac6ae191/sphinxcontrib/needs/utils.py#L170-L186
tfidf_python_100_1.0,parse json file,python,"def parseLine(self, slots):

		numSlots = len(slots)

		initialParse = Parse(self, numSlots)
		parses = initialParse.extend(slots[0])
		parses[0].comparisonNums.add(1)

		boundedParses=[]


		for slotN in range(1, numSlots):

			newParses = []
			for parse in parses:
				newParses.append(parse.extend(slots[slotN]))

			for parseSetIndex in range(len(newParses)):

				parseSet = newParses[parseSetIndex]

				for parseIndex in range(len(parseSet)):

					parse = parseSet[parseIndex]
					parse.comparisonParses = []

					if len(parseSet) > 1 and parseIndex == 0:
						parse.comparisonNums.add(parseSetIndex)

					for comparisonIndex in parse.comparisonNums:

						# should be a label break, but not supported in Python
						# find better solution; redundant checking
						if parse.isBounded:
							break

						try:
							for comparisonParse in newParses[comparisonIndex]:

								if parse is comparisonParse:
									continue

								if not comparisonParse.isBounded:

									if parse.canCompare(comparisonParse):

										boundingRelation = parse.boundingRelation(comparisonParse)

										if boundingRelation == Bounding.bounds:
											# print parse.__report__()
											# print '--> bounds -->'
											# print comparisonParse.__report__()
											# print
											comparisonParse.isBounded = True
											comparisonParse.boundedBy = parse

										elif boundingRelation == Bounding.bounded:
											# print
											# print comparisonParse.__report__()
											# print '--> bounds -->'
											# print parse.__report__()
											# print
											parse.isBounded = True
											parse.boundedBy = comparisonParse
											break

										elif boundingRelation == Bounding.equal:
											parse.comparisonParses.append(comparisonParse)

									else:
										parse.comparisonParses.append(comparisonParse)
						except IndexError:
							pass

			parses = []
			#boundedParses=[]
			parseNum = 0

			for parseSet in newParses:
				for parse in parseSet:
					if parse.isBounded:
						boundedParses+=[parse]
					elif parse.score() >= 1000:
						parse.unmetrical = True
						boundedParses+=[parse]
					else:
						parse.parseNum = parseNum
						parseNum += 1
						parses.append(parse)


			for parse in parses:

				parse.comparisonNums = set()

				for compParse in parse.comparisonParses:
					if not compParse.isBounded:
						parse.comparisonNums.add(compParse.parseNum)



		return parses,boundedParses",https://github.com/quadrismegistus/prosodic/blob/8af66ed9be40c922d03a0b09bc11c87d2061b618/prosodic/Meter.py#L347-L448
tfidf_python_100_1.0,parse json file,python,"def from_json(cls, json):
        return Track(
            json['num_samples'],
            json['duration'],
            json['sample_md5'],
            json['offset_seconds'],
            json['window_seconds'],
            json['analysis_sample_rate'],
            json['analysis_channels'],
            json['end_of_fade_in'],
            json['start_of_fade_out'],
            json['loudness'],
            json['tempo'],
            json['tempo_confidence'],
            json['time_signature'],
            json['time_signature_confidence'],
            json['key'],
            json['key_confidence'],
            json['mode'],
            json['mode_confidence'],
            json['codestring'],
            json['code_version'],
            json['echoprintstring'],
            json['echoprint_version'],
            json['synchstring'],
            json['synch_version'],
            json['rhythmstring'],
            json['rhythm_version']
        )",https://github.com/jingming/spotify/blob/d92c71073b2515f3c850604114133a7d2022d1a4/spotify/object/audio_analysis.py#L175-L203
tfidf_python_100_1.0,parse json file,python,"def parse(self, method, payload):
        try:
            json = self.json_lib.loads(payload)
        except Exception as e:
            raise TweepError('Failed to parse JSON payload: %s' % e)

        needs_cursors = 'cursor' in method.session.params
        if needs_cursors and isinstance(json, dict) \
                and 'previous_cursor' in json \
                and 'next_cursor' in json:
            cursors = json['previous_cursor'], json['next_cursor']
            return json, cursors
        else:
            return json",https://github.com/tweepy/tweepy/blob/cc3894073905811c4d9fd816202f93454ed932da/tweepy/parsers.py#L50-L63
tfidf_python_100_1.0,parse json file,python,"def _url_encode_files(file):
        if hasattr(file, ""name""):
            file = (urllib.parse.quote(file.name), file)
        return file",https://github.com/offu/WeRoBot/blob/fd42109105b03f9acf45ebd9dcabb9d5cff98f3c/werobot/client.py#L46-L49
tfidf_python_100_1.0,parse json file,python,"def import_job():
    file = request.files['file']
    if (file and allowed_file(file.filename, ['json'])):
        dagobah.add_job_from_json(file.read(), destructive=True)",https://github.com/thieman/dagobah/blob/e624180c2291034960302c9e0b818b65b5a7ee11/dagobah/daemon/api.py#L473-L476
tfidf_python_100_1.0,parse json file,python,"def _parse_contents(self, contents_file_path):
        '''
        Parse the contents .json file and return the dictionary

        Parameters
        ----------
        contents_file_path: str
            Path to file containing .json file to parse.

        Returns
        -------
        contents_dict: dict
            parsed .json file
        '''
        logging.debug(""Parsing %s"" % (contents_file_path))
        contents_dict = json.load(open(contents_file_path))
        return contents_dict",https://github.com/geronimp/graftM/blob/c82576517290167f605fd0bc4facd009cee29f48/graftm/create.py#L54-L70
tfidf_python_100_1.0,parse json file,python,"def __init__(self, json):
        """"""
        Initializes the ResultItem class from the JSON provided
        :param json: String. Raw JSON data to fetch information from
        """"""
        self.artist_name = json['artistName']
        self.type = None

        if 'wrapperType' in json:
            self.type = json['wrapperType']

            if 'collectionType' in json:
                self.collection_type = json['collectionType']
            elif 'artistType' in json:
                self.artist_type = json['artistType']
            elif 'kind' in json:
                self.track_type = json['kind']
        elif 'kind' in json:
            self.type = json['kind']

        if 'primaryGenreName' in json:
            self.primary_genre_name = json['primaryGenreName']

        if 'artistId' in json:
            self.artist_id = json['artistId']

        if 'artistLinkUrl' in json:
            self.artist_link_url = json['artistLinkUrl']

        if 'radioStationUrl' in json:
            self.artist_radio_url = json['radioStationUrl']

        if 'primaryGenreId' in json:
            self.primary_genre_id = json['primaryGenreId']

        if 'amgArtistId' in json:
            self.artist_amg_id = json['amgArtistId']

        if 'artistViewUrl' in json:
            self.artist_view_url = json['artistViewUrl']

        if 'trackName' in json:
            self.track_name = json['trackName']

        if 'trackId' in json:
            self.track_id = json['trackId']

        if 'trackCensoredName' in json:
            self.track_censored_name = json['trackCensoredName']

        if 'trackViewUrl' in json:
            self.track_view_url = json['trackViewUrl']

        if 'trackCount' in json:
            self.track_count = json['trackCount']

        if 'trackNumber' in json:
            self.track_number = json['trackNumber']

        if 'previewUrl' in json:
            self.preview_url = json['previewUrl']

        if 'artworkUrl30' in json:
            self.artwork_url_30 = json['artworkUrl30']

        if 'artworkUrl60' in json:
            self.artwork_url_60 = json['artworkUrl60']

        if 'artworkUrl100' in json:
            self.artwork_url_100 = json['artworkUrl100']

        if 'artworkUrl512' in json:
            self.artwork_url_512 = json['artworkUrl512']

        if 'collectionName' in json:
            self.collection_name = json['collectionName']

        if 'collectionId' in json:
            self.collection_id = json['collectionId']

        if 'collectionCensoredName' in json:
            self.collection_censored_name = json['collectionCensoredName']

        if 'collectionViewUrl' in json:
            self.collection_view_url = json['collectionViewUrl']

        if 'collectionPrice' in json:
            self.collection_price = json['collectionPrice']

        if 'trackPrice' in json:
            self.track_price = json['trackPrice']

        if 'trackRentalPrice' in json:
            self.track_rental_price = json['trackRentalPrice']

        if 'collectionHdPrice' in json:
            self.collection_hd_price = json['collectionHdPrice']

        if 'trackHdPrice' in json:
            self.track_hd_price = json['trackHdPrice']

        if 'trackHdRentalPrice' in json:
            self.track_hd_rental_price = json['trackHdRentalPrice']

        if 'releaseDate' in json:
            self.release_date = json['releaseDate']

        if 'collectionExplicitness' in json:
            self.collection_explicitness = json['collectionExplicitness']

        if 'trackExplicitness' in json:
            self.track_explicitness = json['trackExplicitness']

        if 'trackTimeMillis' in json:
            self.track_time = json['trackTimeMillis']

        if 'discCount' in json:
            self.disc_count = json['discCount']

        if 'discNumber' in json:
            self.disc_number = json['discNumber']

        if 'country' in json:
            self.country = json['country']

        if 'currency' in json:
            self.currency = json['currency']

        if 'copyright' in json:
            self.copyright = json['copyright']

        if 'contentAdvisoryRating' in json:
            self.content_advisory_rating = json['contentAdvisoryRating']

        if 'shortDescription' in json:
            self.short_description = json['shortDescription']

        if 'longDescription' in json:
            self.long_description = json['longDescription']

        if 'isStreamable' in json:
            self.is_streamable = json['isStreamable']

        if 'fileSizeBytes' in json:
            self.file_size = json['fileSizeBytes']

        if 'genres' in json:
            self.genres = json['genres']

        if 'price' in json:
            self.price = json['price']

        if 'description' in json:
            self.description = json['description']

        if 'genreIds' in json:
            self.genre_ids = json['genreIds']

        if 'artistIds' in json:
            self.artist_ids = json['artistIds']

        if 'formattedPrice' in json:
            self.formatted_price = json['formattedPrice']

        if 'averageUserRating' in json:
            self.average_user_rating = json['averageUserRating']

        if 'userRatingCount' in json:
            self.user_rating_count = json['userRatingCount']

        if 'screenshotUrls' in json:
            self.screenshot_urls = json['screenshotUrls']

        if 'ipadScreenshotUrls' in json:
            self.ipad_screenshot_urls = json['ipadScreenshotUrls']

        if 'features' in json:
            self.features = json['features']

        if 'supportedDevices' in json:
            self.supported_devices = json['supportedDevices']

        if 'advisories' in json:
            self.advisories = json['advisories']

        if 'isGameCenterEnabled' in json:
            self.is_game_center_enabled = json['isGameCenterEnabled']

        if 'userRatingCountForCurrentVersion' in json:
            self.user_rating_count_for_current_version = json['userRatingCountForCurrentVersion']

        if 'languageCodesISO2A' in json:
            self.language_codes = json['languageCodesISO2A']

        if 'averageUserRatingForCurrentVersion' in json:
            self.average_user_rating_for_current_version = json['averageUserRatingForCurrentVersion']

        if 'trackContentRating' in json:
            self.track_content_rating = json['trackContentRating']

        if 'version' in json:
            self.version = json['version']

        if 'sellerName' in json:
            self.seller_name = json['sellerName']

        if 'sellerUrl' in json:
            self.seller_url = json['sellerUrl']

        if 'bundleId' in json:
            self.bundle_id = json['bundleId']

        if 'releaseNotes' in json:
            self.release_notes = json['releaseNotes']

        if 'isVppDeviceBasedLicensingEnabled' in json:
            self.is_vpp_device_based_licensing_enabled = json['isVppDeviceBasedLicensingEnabled']

        if 'minimumOsVersion' in json:
            self.minimum_os_version = json['minimumOsVersion']",https://github.com/sleepyfran/itunespy/blob/0e7e931b135b5e0daae49ba68e9167ff4ac73eb5/itunespy/result_item.py#L19-L238
tfidf_python_100_1.0,parse json file,python,"def franchises():
    json = requests.get(""{}/franchises/{}"".format(BASE, """")).json()
    return List(parse.parse_franchise(item) for item in json[""franchises""])",https://github.com/mhostetter/nhl/blob/32c91cc392826e9de728563d57ab527421734ee1/nhl/statsapi/api.py#L33-L35
tfidf_python_100_1.0,parse json file,python,"def divisions():
    json = requests.get(""{}/divisions/{}"".format(BASE, """")).json()
    return List(parse.parse_division(item) for item in json[""divisions""])",https://github.com/mhostetter/nhl/blob/32c91cc392826e9de728563d57ab527421734ee1/nhl/statsapi/api.py#L23-L25
tfidf_python_100_1.0,parse json file,python,"def venues():
    json = requests.get(""{}/venues/{}"".format(BASE, """")).json()
    return List(parse.parse_venue(item) for item in json[""venues""])",https://github.com/mhostetter/nhl/blob/32c91cc392826e9de728563d57ab527421734ee1/nhl/statsapi/api.py#L72-L74
tfidf_python_100_1.0,parse json file,python,"def conferences():
    json = requests.get(""{}/conferences/{}"".format(BASE, """")).json()
    return List(parse.parse_conference(item) for item in json[""conferences""])",https://github.com/mhostetter/nhl/blob/32c91cc392826e9de728563d57ab527421734ee1/nhl/statsapi/api.py#L13-L15
tfidf_python_100_1.0,parse json file,python,"def from_json(cls, json):
        return Section(
            json['start'],
            json['duration'],
            json['confidence'],
            json['loudness'],
            json['tempo'],
            json['tempo_confidence'],
            json['key'],
            json['key_confidence'],
            json['mode'],
            json['mode_confidence'],
            json['time_signature'],
            json['time_signature_confidence']
        )",https://github.com/jingming/spotify/blob/d92c71073b2515f3c850604114133a7d2022d1a4/spotify/object/audio_analysis.py#L78-L92
tfidf_python_100_1.0,parse json file,python,"def from_json(cls, json):
        return Segment(
            json['start'],
            json['duration'],
            json['confidence'],
            json['loudness_start'],
            json['loudness_max_time'],
            json['loudness_max'],
            json['loudness_end'],
            json['pitches'],
            json['timbre']
        )",https://github.com/jingming/spotify/blob/d92c71073b2515f3c850604114133a7d2022d1a4/spotify/object/audio_analysis.py#L110-L121
tfidf_python_100_1.0,get current observable value,python,"def _notify_subscribers(self, value, observables):
        for observable in observables.values():
            observable.send(value)",https://github.com/ravendb/ravendb-python-client/blob/383739db2594303e3cc469134aa16156f6f80acd/pyravendb/changes/database_changes.py#L102-L104
tfidf_python_100_1.0,get current observable value,python,"def map(self, observable, func):
        observable.attach(obj)
        self.observables.append(observable)
        self.dispatch_dict[observable] = func
        self.dispatch(observable)",https://github.com/cidles/pressagio/blob/2b3b89ae82316b929244e4c63e393682b2a57e57/src/pressagio/observer.py#L63-L67
tfidf_python_100_1.0,get current observable value,python,"def make_dict_observable(matrix_observable):
    """"""Convert an observable in matrix form to dictionary form.

    Takes in a diagonal observable as a matrix and converts it to a dictionary
    form. Can also handle a list sorted of the diagonal elements.

    Args:
        matrix_observable (list): The observable to be converted to dictionary
        form. Can be a matrix or just an ordered list of observed values

    Returns:
        Dict: A dictionary with all observable states as keys, and corresponding
        values being the observed value for that state
    """"""
    dict_observable = {}
    observable = np.array(matrix_observable)
    observable_size = len(observable)
    observable_bits = int(np.ceil(np.log2(observable_size)))
    binary_formater = '0{}b'.format(observable_bits)
    if observable.ndim == 2:
        observable = observable.diagonal()
    for state_no in range(observable_size):
        state_str = format(state_no, binary_formater)
        dict_observable[state_str] = observable[state_no]
    return dict_observable",https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/quantum_info/analyzation/make_observable.py#L13-L37
tfidf_python_100_1.0,get current observable value,python,"def update_observables(self, monitor, observables):
        for observable in observables:
            config_observable = self.config.get_observable(monitor.name, observable.name, observable.group)
            observable.update_usign_observable(config_observable)",https://github.com/Nekmo/simple-monitor-alert/blob/11d6dbd3c0b3b9a210d6435208066f5636f1f44e/simple_monitor_alert/monitor.py#L263-L266
tfidf_python_100_1.0,get current observable value,python,"def evaluate_all(self):
        observables = self.execute_all()
        for observable in observables:
            result = observable.evaluate()
            self.results.update_observable_result(observable, not result)
            log_evaluate(observable, result)
            yield observable, result
        self.results.write()",https://github.com/Nekmo/simple-monitor-alert/blob/11d6dbd3c0b3b9a210d6435208066f5636f1f44e/simple_monitor_alert/sma.py#L222-L229
tfidf_python_100_1.0,get current observable value,python,"def __init__(self, observable, fail, **kwargs):
        super(ObservableCommunication, self).__init__(**kwargs)
        self.observable = observable
        from simple_monitor_alert.utils.system import get_hostname
        self['observable'] = observable
        self['fail'] = fail
        self['hostname'] = get_hostname()
        self['level'] = self.observable.get_line_value('level', 'warning')
        self['subject'] = '{} [{}] {}'.format(self['hostname'], 'ERROR' if fail else 'SOLVED',
                                              observable.get_verbose_name())
        self['name'] = observable.get_verbose_name()
        self['observable_name'] = observable.name
        self['extra_info'] = observable.get_line_value('extra_info') or '(No more info available)'
        self['condition'] = get_verbose_condition(observable)
        self['message'] = self.get_message()",https://github.com/Nekmo/simple-monitor-alert/blob/11d6dbd3c0b3b9a210d6435208066f5636f1f44e/simple_monitor_alert/alerts.py#L42-L56
tfidf_python_100_1.0,get current observable value,python,"def evaluate_and_alert(self):
        observables = self.evaluate_all()
        fail_observables = [observable for (observable, result) in observables if not result]
        self.alert_all(fail_observables)",https://github.com/Nekmo/simple-monitor-alert/blob/11d6dbd3c0b3b9a210d6435208066f5636f1f44e/simple_monitor_alert/sma.py#L209-L212
tfidf_python_100_1.0,get current observable value,python,"def _trigger_event(
            self, holder: T.Any, alt_name: str, action: str, *event_args: T.Any
    ) -> None:
        """"""Triggers an event on the associated Observable object.
        The Holder is the object this property is a member of, alt_name
        is used as the event name when self.event is not set, action is
        prepended to the event name and event_args are passed through
        to the registered event handlers.""""""

        if isinstance(self.observable, Observable):
            observable = self.observable
        elif isinstance(self.observable, str):
            observable = getattr(holder, self.observable)
        elif isinstance(holder, Observable):
            observable = holder
        else:
            raise TypeError(
                ""This ObservableProperty is no member of an Observable ""
                ""object. Specify where to find the Observable object for ""
                ""triggering events with the observable keyword argument ""
                ""when initializing the ObservableProperty.""
            )

        name = alt_name if self.event is None else self.event
        event = ""{}_{}"".format(action, name)
        observable.trigger(event, *event_args)",https://github.com/timofurrer/observable/blob/a6a764efaf9408a334bdb1ddf4327d9dbc4b8eaa/observable/property.py#L70-L95
tfidf_python_100_1.0,get current observable value,python,"def average_data(counts, observable):
    """"""Compute the mean value of an diagonal observable.

    Takes in a diagonal observable in dictionary, list or matrix format and then
    calculates the sum_i value(i) P(i) where value(i) is the value of the
    observable for state i.

    Args:
        counts (dict): a dict of outcomes from an experiment
        observable (dict or matrix or list): The observable to be averaged over.
        As an example, ZZ on qubits can be given as:
        * dict: {""00"": 1, ""11"": 1, ""01"": -1, ""10"": -1}
        * matrix: [[1, 0, 0, 0], [0, -1, 0, 0, ], [0, 0, -1, 0], [0, 0, 0, 1]]
        * matrix diagonal (list): [1, -1, -1, 1]

    Returns:
        Double: Average of the observable
    """"""
    if not isinstance(observable, dict):
        observable = make_dict_observable(observable)
    temp = 0
    tot = sum(counts.values())
    for key in counts:
        if key in observable:
            temp += counts[key] * observable[key] / tot
    return temp",https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/quantum_info/analyzation/average.py#L13-L38
tfidf_python_100_1.0,get current observable value,python,"def __init__(self, observable: Observable, key, callback):
        self._observable = observable
        self._key = key
        self._callback = callback",https://github.com/eumis/pyviews/blob/80a868242ee9cdc6f4ded594b3e0544cc238ed55/pyviews/binding/implementations.py#L32-L35
tfidf_python_100_1.0,get current observable value,python,"def copy_observers_to(self, observable):
        observable._everything_observers.extend(self._everything_observers)

        for observable_name, observer in self._observers.items():
            observable.add_observer(observer, observable_name=observable_name)",https://github.com/jor-/util/blob/0eb0be84430f88885f4d48335596ca8881f85587/util/observable/decorator.py#L91-L95
tfidf_python_100_1.0,get current observable value,python,"def __init__(
            self, *args: T.Any,
            event: str = None, observable: T.Union[Observable, str] = None,
            **kwargs: T.Any
    ) -> None:
        super().__init__(*args, **kwargs)
        self.event = event
        self.observable = observable",https://github.com/timofurrer/observable/blob/a6a764efaf9408a334bdb1ddf4327d9dbc4b8eaa/observable/property.py#L37-L44
tfidf_python_100_1.0,get current observable value,python,"def create_with(
            cls, event: str = None, observable: T.Union[str, Observable] = None
    ) -> T.Callable[..., ""ObservableProperty""]:
        """"""Creates a partial application of ObservableProperty with
        event and observable preset.""""""

        return functools.partial(cls, event=event, observable=observable)",https://github.com/timofurrer/observable/blob/a6a764efaf9408a334bdb1ddf4327d9dbc4b8eaa/observable/property.py#L102-L108
tfidf_python_100_1.0,get current observable value,python,"def __init__(self):
        super(Observable, self).__init__()
        self.__observers = ObserverStore()
        # build the list of observable element.
        self.__observables = ObservableStore(self.__getTaggedProperties())",https://github.com/fredericklussier/ObservablePy/blob/fd7926a0568621f80b1d567d18f199976f1fa4e8/observablePy/Observable.py#L50-L54
tfidf_python_100_1.0,get current observable value,python,"def __init__(self, target: BindingTarget, observable: Observable, prop):
        super().__init__()
        self._target = target
        self._observable = observable
        self._prop = prop",https://github.com/eumis/pyviews/blob/80a868242ee9cdc6f4ded594b3e0544cc238ed55/pyviews/binding/implementations.py#L159-L163
tfidf_python_100_1.0,get current observable value,python,"def get_monitor_params(self, monitor):
        observables = self.config.get_monitor_observables(monitor.name)
        if isinstance(observables, dict):
            observables = observables.values()
        return dict(filter(lambda x: x[1] is not None, [(observable.get_verbose_name_group(), observable.get_param())
                                                        for observable in observables]))",https://github.com/Nekmo/simple-monitor-alert/blob/11d6dbd3c0b3b9a210d6435208066f5636f1f44e/simple_monitor_alert/monitor.py#L210-L215
tfidf_python_100_1.0,get current observable value,python,"def __init__(self, observable):
        if not CardMonitoringThread.instance:
            CardMonitoringThread.instance = \
               CardMonitoringThread.__CardMonitoringThreadSingleton(observable)
            CardMonitoringThread.instance.start()",https://github.com/LudovicRousseau/pyscard/blob/62e675028086c75656444cc21d563d9f08ebf8e7/smartcard/CardMonitoring.py#L210-L214
tfidf_python_100_1.0,get current observable value,python,"def log_evaluate(observable, result=None, use_logger=True):
    from simple_monitor_alert.utils.system import get_hostname
    result = result or observable.evaluate()
    level = 'success' if result else observable.get_line_value('level') or 'warning'
    msg = '{} - - Trigger: [{}] ({}) {}. '.format(get_hostname(), level,
                                                  getattr(getattr(observable, 'monitor', None), 'name', '?'),
                                           observable.get_verbose_name_group())
    msg += ('Result: {}' if result else 'Assertion {} failed').format(get_verbose_condition(observable))
    if observable.param_used:
        msg += '. Param used: {}'.format(observable.param_used)
    extra_info = observable.get_line_value('extra_info')
    if extra_info:
        msg += '. Extra info: {}'.format(extra_info)
    if use_logger:
        getattr(logger, 'info' if result else 'warning')(msg)
    else:
        return msg",https://github.com/Nekmo/simple-monitor-alert/blob/11d6dbd3c0b3b9a210d6435208066f5636f1f44e/simple_monitor_alert/monitor.py#L37-L53
tfidf_python_100_1.0,get current observable value,python,"def on_start(self, connection_context, op_id, params):
        try:
            execution_result = self.execute(connection_context.request_context, params)
            if not isinstance(execution_result, Observable):
                # pylint cannot find of method
                observable = Observable.of(execution_result, GQL_COMPLETE)  # pylint: disable=E1101
            else:
                observable = execution_result
            observable.subscribe(
                SubscriptionObserver(
                    connection_context,
                    op_id,
                    self.send_execution_result,
                    self.send_error,
                    self.on_close,
                )
            )
        # appropriate to catch all errors here
        except Exception as e:  # pylint: disable=W0703
            self.send_error(connection_context, op_id, str(e))",https://github.com/dagster-io/dagster/blob/4119f8c773089de64831b1dfb9e168e353d401dc/python_modules/dagit/dagit/subscription_server.py#L26-L45
tfidf_python_100_1.0,get current observable value,python,"def subscribe_fields(
    exe_context,  # type: ExecutionContext
    parent_type,  # type: GraphQLObjectType
    source_value,  # type: Any
    fields,  # type: DefaultOrderedDict
):
    # type: (...) -> Observable
    subscriber_exe_context = SubscriberExecutionContext(exe_context)

    def on_error(error):
        subscriber_exe_context.report_error(error)

    def map_result(data):
        # type: (Dict[str, Any]) -> ExecutionResult
        if subscriber_exe_context.errors:
            result = ExecutionResult(data=data, errors=subscriber_exe_context.errors)
        else:
            result = ExecutionResult(data=data)
        subscriber_exe_context.reset()
        return result

    observables = []  # type: List[Observable]

    # assert len(fields) == 1, ""Can only subscribe one element at a time.""

    for response_name, field_asts in fields.items():
        result = subscribe_field(
            subscriber_exe_context,
            parent_type,
            source_value,
            field_asts,
            [response_name],
        )
        if result is Undefined:
            continue

        def catch_error(error):
            subscriber_exe_context.errors.append(error)
            return Observable.just(None)

        # Map observable results
        observable = result.catch_exception(catch_error).map(
            lambda data: map_result({response_name: data})
        )
        return observable
        observables.append(observable)

    return Observable.merge(observables)",https://github.com/graphql-python/graphql-core/blob/d8e9d3abe7c209eb2f51cf001402783bfd480596/graphql/execution/executor.py#L265-L312
tfidf_python_100_1.0,get name of enumerated value,python,"def enumerate_tiles(tiles):
  """"""
  Convenience
  """"""
  enumerated = []
  for key in tiles.keys():
    enumerated.append((key[0], key[1], tiles[key]))
  return enumerated",https://github.com/tensorflow/lucid/blob/d1a1e2e4fd4be61b89b8cba20dc425a5ae34576e/lucid/scratch/atlas_pipeline/grid.py#L86-L93
tfidf_python_100_1.0,get name of enumerated value,python,"def values(ns_key):
    '''Return the allowed values for an enumerated namespace.

    Parameters
    ----------
    ns_key : str
        Namespace key identifier

    Returns
    -------
    values : list

    Raises
    ------
    NamespaceError
        If `ns_key` is not found, or does not have enumerated values

    Examples
    --------
    >>> jams.schema.values('tag_gtzan')
    ['blues', 'classical', 'country', 'disco', 'hip-hop', 'jazz',
     'metal', 'pop', 'reggae', 'rock']
    '''

    if ns_key not in __NAMESPACE__:
        raise NamespaceError('Unknown namespace: {:s}'.format(ns_key))

    if 'enum' not in __NAMESPACE__[ns_key]['value']:
        raise NamespaceError('Namespace {:s} is not enumerated'.format(ns_key))

    return copy.copy(__NAMESPACE__[ns_key]['value']['enum'])",https://github.com/marl/jams/blob/b16778399b9528efbd71434842a079f7691a7a66/jams/schema.py#L121-L151
tfidf_python_100_1.0,get name of enumerated value,python,"def build_flowable(self):
        return styleds.List([item.flowable() for item in self.li],
                            style='enumerated')",https://github.com/brechtm/rinohtype/blob/40a63c4e5ad7550f62b6860f1812cb67cafb9dc7/src/rinoh/frontend/epub/nodes.py#L160-L162
tfidf_python_100_1.0,get name of enumerated value,python,"def show(self):
        """"""
        When dynamic, not all argument values may be available.
        """"""
        copied = self.copy()
        enumerated = [el for el in enumerate(copied)]
        for (group_ind, specs) in enumerated:
            if len(enumerated) > 1: print(""Group %d"" % group_ind)
            ordering = self.constant_keys + self.varying_keys
            # Ordered nicely by varying_keys definition.
            spec_lines = [', '.join(['%s=%s' % (k, s[k]) for k in ordering]) for s in specs]
            print('\n'.join(['%d: %s' % (i,l) for (i,l) in enumerate(spec_lines)]))

        print('Remaining arguments not available for %s' % self.__class__.__name__)",https://github.com/ioam/lancet/blob/1fbbf88fa0e8974ff9ed462e3cb11722ddebdd6e/lancet/dynamic.py#L108-L121
tfidf_python_100_1.0,get name of enumerated value,python,"def _makePairPosRule(pair, rtl=False):
        enumerated = pair.firstIsClass ^ pair.secondIsClass
        value = otRound(pair.value)
        if rtl and ""N"" in pair.bidiTypes:
            # numbers are always shaped LTR even in RTL scripts
            rtl = False
        valuerecord = ast.ValueRecord(
            xPlacement=value if rtl else None,
            yPlacement=0 if rtl else None,
            xAdvance=value,
            yAdvance=0 if rtl else None,
        )
        return ast.PairPosStatement(
            glyphs1=pair.side1,
            valuerecord1=valuerecord,
            glyphs2=pair.side2,
            valuerecord2=None,
            enumerated=enumerated,
        )",https://github.com/googlefonts/ufo2ft/blob/915b986558e87bee288765d9218cc1cd4ebf7f4c/Lib/ufo2ft/featureWriters/kernFeatureWriter.py#L377-L395
tfidf_python_100_1.0,get name of enumerated value,python,"def register_value(cl_str, name, value):
    _extra_values[cl_str] = _extra_values.get(cl_str, {})
    _extra_values[cl_str][name] = value",https://github.com/pythongssapi/python-gssapi/blob/b6efe72aa35a4c1fe21b397e15fcb41611e365ce/gssapi/raw/_enum_extensions/__init__.py#L7-L9
tfidf_python_100_1.0,get name of enumerated value,python,"def to_dict(cls):
        """"""Make dictionary version of enumerated class.

        Dictionary created this way can be used with def_num.

        Returns:
          A dict (name) -> number
        """"""
        return dict((item.name, item.number) for item in iter(cls))",https://github.com/google/apitools/blob/f3745a7ea535aa0e88b0650c16479b696d6fd446/apitools/base/protorpclite/messages.py#L526-L534
tfidf_python_100_1.0,get name of enumerated value,python,"def add(self, key, value=None):
        """"""
        Adds the new key to this enumerated type.
        
        :param      key | <str>
        """"""
        if value is None:
            value = 2 ** (len(self))

        self[key] = value
        setattr(self, key, self[key])
        return value",https://github.com/bitesofcode/projex/blob/d31743ec456a41428709968ab11a2cf6c6c76247/projex/enum.py#L102-L113
tfidf_python_100_1.0,get name of enumerated value,python,"def verify(self, obj):
        """"""Verify that the object conforms to this verifier's schema.

        Args:
            obj (object): A python object to verify

        Raises:
            ValidationError: If there is a problem verifying the object, a
                ValidationError is thrown with at least the reason key set indicating
                the reason for the lack of validation.
        """"""

        if obj not in self.options:
            raise ValidationError(""Object is not in list of enumerated options"",
                                  reason='not in list of enumerated options', object=obj, options=self.options)

        return obj",https://github.com/iotile/coretools/blob/2d794f5f1346b841b0dcd16c9d284e9bf2f3c6ec/iotilecore/iotile/core/utilities/schema_verify/enum_verify.py#L16-L32
tfidf_python_100_1.0,get name of enumerated value,python,"def _apply_ranges(self, pat, hosts):
        """"""
        given a pattern like foo, that matches hosts, return all of hosts
        given a pattern like foo[0:5], where foo matches hosts, return the first 6 hosts
        """""" 

        (loose_pattern, limits) = self._enumeration_info(pat)
        if not limits:
            return hosts

        (left, right) = limits
        enumerated = enumerate(hosts)
        if left == '':
            left = 0
        if right == '':
            right = 0
        left=int(left)
        right=int(right)
        enumerated = [ h for (i,h) in enumerated if i>=left and i<=right ]
        return enumerated",https://github.com/cirruscluster/cirruscluster/blob/977409929dd81322d886425cdced10608117d5d7/cirruscluster/ext/ansible/inventory/__init__.py#L174-L193
tfidf_python_100_1.0,get name of enumerated value,python,"def build_flowable(self):
        # TODO: handle different numbering styles
        return rt.List([item.flowable() for item in self.list_item],
                       style='enumerated')",https://github.com/brechtm/rinohtype/blob/40a63c4e5ad7550f62b6860f1812cb67cafb9dc7/src/rinoh/frontend/rst/nodes.py#L459-L462
tfidf_python_100_1.0,get name of enumerated value,python,"def native_name(self, key, value):
    name = self.get('name', {})
    name.setdefault('native_names', []).append(value.get('a'))
    return name",https://github.com/inspirehep/inspire-dojson/blob/17f3789cd3d5ae58efa1190dc0eea9efb9c8ca59/inspire_dojson/hepnames/rules.py#L700-L703
tfidf_python_100_1.0,get name of enumerated value,python,"def get_const_string(self, name, value):
        return PyConstString(name=name, value=value, to_case=None)",https://github.com/valdergallo/pyconst/blob/af4cbc8d91ffab601ac5e45e5480f20c5462064d/pyconst/const.py#L99-L100
tfidf_python_100_1.0,get name of enumerated value,python,"def get(self, name):
        value = self.settings_state.get_value(name)
        if value is None:
            value = self.get_default_value(name)
        elif name in _CODECS:
            value = _CODECS[name].decode(value)
        return value",https://github.com/alvarogzp/telegram-bot-framework/blob/7b597a415c1901901c677976cb13100fc3083107/bot/action/standard/chatsettings/__init__.py#L28-L34
tfidf_python_100_1.0,get name of enumerated value,python,"def _public_notes(self, key, value):
    if 'Formerly' in value.get('a'):
        name_item = self.get('name', {})
        previous_names_list = name_item.get('previous_names', [])
        previous_name = value.get('a').replace('Formerly ', '')
        previous_names_list.extend(force_list(previous_name))
        self.setdefault('name', {})['previous_names'] = previous_names_list
    else:
        return {
            'source': value.get('9'),
            'value': value.get('a'),
        }",https://github.com/inspirehep/inspire-dojson/blob/17f3789cd3d5ae58efa1190dc0eea9efb9c8ca59/inspire_dojson/hepnames/rules.py#L473-L484
tfidf_python_100_1.0,get name of enumerated value,python,"def create_feature(self, **kwargs):
        """"""
        Create an enumerated sequence feature
        

        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please define a `callback` function
        to be invoked when receiving the response.
        >>> def callback_function(response):
        >>>     pprint(response)
        >>>
        >>> thread = api.create_feature(callback=callback_function)

        :param callback function: The callback function
            for asynchronous request. (optional)
        :param FeatureRequest body: 
        :return: Feature
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('callback'):
            return self.create_feature_with_http_info(**kwargs)
        else:
            (data) = self.create_feature_with_http_info(**kwargs)
            return data",https://github.com/nmdp-bioinformatics/SeqAnn/blob/5ce91559b0a4fbe4fb7758e034eb258202632463/seqann/feature_client/apis/features_api.py#L53-L78
tfidf_python_100_1.0,get name of enumerated value,python,"def _format_attribute(self, name, value):
        formatted_attr = {
            'name': name,
        }
        if value is not None:
            formatted_attr['value'] = value
        return formatted_attr",https://github.com/spulec/moto/blob/4a286c4bc288933bb023396e2784a6fdbb966bc9/moto/ecs/models.py#L364-L370
tfidf_python_100_1.0,get name of enumerated value,python,"def thesis_info(self, key, value):
    return {
        'b': value.get('a'),
        'c': value.get('b'),
        'd': value.get('c'),
    }",https://github.com/inspirehep/inspire-dojson/blob/17f3789cd3d5ae58efa1190dc0eea9efb9c8ca59/inspire_dojson/cds/rules.py#L314-L319
tfidf_python_100_1.0,get name of enumerated value,python,"def get_feature_by_query(self, **kwargs):
        """"""
        Retrieve an enumerated sequence feature
        

        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please define a `callback` function
        to be invoked when receiving the response.
        >>> def callback_function(response):
        >>>     pprint(response)
        >>>
        >>> thread = api.get_feature_by_query(callback=callback_function)

        :param callback function: The callback function
            for asynchronous request. (optional)
        :param str locus: locus name or URI
        :param str term: Sequence Ontology (SO) term name, accession, or URI
        :param int rank: feature rank, must be at least 1
        :param int accession: accession, must be at least 1
        :return: Feature
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('callback'):
            return self.get_feature_by_query_with_http_info(**kwargs)
        else:
            (data) = self.get_feature_by_query_with_http_info(**kwargs)
            return data",https://github.com/nmdp-bioinformatics/SeqAnn/blob/5ce91559b0a4fbe4fb7758e034eb258202632463/seqann/feature_client/apis/features_api.py#L279-L307
tfidf_python_100_1.0,get name of enumerated value,python,"def validate_default_element(self, value):
        """"""Validate default element of Enum field.

        Enum fields allow for delayed resolution of default values
        when the type of the field has not been resolved. The default
        value of a field may be a string or an integer. If the Enum
        type of the field has been resolved, the default value is
        validated against that type.

        Args:
          value: Value to validate.

        Raises:
          ValidationError if value is not expected message type.

        """"""
        if isinstance(value, (six.string_types, six.integer_types)):
            # Validation of the value does not happen for delayed resolution
            # enumerated types.  Ignore if type is not yet resolved.
            if self.__type:
                self.__type(value)
            return value

        return super(EnumField, self).validate_default_element(value)",https://github.com/google/apitools/blob/f3745a7ea535aa0e88b0650c16479b696d6fd446/apitools/base/protorpclite/messages.py#L1786-L1809
tfidf_python_100_1.0,encode url,python,"def _new_curl_object_for_url(url):
    c = _new_curl_object()
    c.setopt(c.URL, url.encode('ascii'))
    return c",https://github.com/galaxyproject/pulsar/blob/9ab6683802884324652da0a9f0808c7eb59d3ab4/pulsar/client/transport/curl.py#L106-L109
tfidf_python_100_1.0,encode url,python,"def encode(self, f):
        num_bytes_written = 0
        num_bytes_written += MqttFixedHeader.encode(self, f)
        num_bytes_written += self.encode_body(f)

        return num_bytes_written",https://github.com/kcallin/mqtt-codec/blob/0f754250cc3f44f4376777e7e8b3676c5a4d413a/mqtt_codec/packet.py#L359-L364
tfidf_python_100_1.0,encode url,python,"def encode(self, s):
    return super(ByteTextEncoderWithEos, self).encode(s) + [text_encoder.EOS_ID]",https://github.com/tensorflow/tensor2tensor/blob/272500b6efe353aeb638d2745ed56e519462ca31/tensor2tensor/data_generators/speech_recognition.py#L38-L39
tfidf_python_100_1.0,encode url,python,"def create_list_view_class(detail_view_class, name):
    encode = None
    if 'encode' in dir(detail_view_class):
        def encode(self):
            return RdyToFlattenList([detail_view_class(obj).encode()
                                     for obj in self.model])

    return _create_collection_view(
        detail_view_class, name, encode, OperatorListView
    )",https://github.com/osrg/ryu/blob/6f906e72c92e10bd0264c9b91a2f7bb85b97780c/ryu/services/protocols/bgp/operator/views/base.py#L301-L310
tfidf_python_100_1.0,encode url,python,"def __init__(self, url):
        """"""Constructor.

        :param url: can be either of str or bytes type.
        """"""
        if self.__py3:
            if type(url) is bytes:
                self.url = bytes(url)
            else:
                self.url = url.encode()
        else:
            self.url = str(url)",https://github.com/afilipovich/gglsbl/blob/89c4665bd6487a3689ccb6b1f3e53ff85e056103/gglsbl/protocol.py#L173-L184
tfidf_python_100_1.0,encode url,python,"def encode_url(url):
    if url == unquote(url):
        return quote(url.encode('utf-8'), safe='~@#$&()*!+=:;,.?/\'')
    else:
        return url",https://github.com/thumbor/thumbor/blob/558ccdd6e3bc29e1c9ee3687372c4b3eb05ac607/thumbor/loaders/http_loader.py#L23-L27
tfidf_python_100_1.0,encode url,python,"def image_proxify(url):

    if url.startswith('//'):
        url = 'https:' + url

    if not request.preferences.get_value('image_proxy'):
        return url

    if url.startswith('data:image/jpeg;base64,'):
        return url

    if settings.get('result_proxy'):
        return proxify(url)

    h = new_hmac(settings['server']['secret_key'], url.encode('utf-8'))

    return '{0}?{1}'.format(url_for('image_proxy'),
                            urlencode(dict(url=url.encode('utf-8'), h=h)))",https://github.com/asciimoo/searx/blob/a84caa22cf947e973c10aa968d35fb2bdda6d048/searx/webapp.py#L290-L307
tfidf_python_100_1.0,encode url,python,"def encode(self, o):
        if isinstance(o, SimpleLazyObject):
            o = str(o)
        return super().encode(o)",https://github.com/bitcaster-io/bitcaster/blob/04625a4b67c1ad01e5d38faa3093828b360d4a98/src/bitcaster/utils/json.py#L14-L17
tfidf_python_100_1.0,encode url,python,"def encode(self):
        pld=b""""
        for x in self.payload:
            pld+=x.encode()
        plen=len(pld)
        pld=b"""".join([super().encode(),self.cmd.encode(),pack("">B"",plen),pld])
        return pld",https://github.com/frawau/aioblescan/blob/02d12e90db3ee6df7be6513fec171f20dc533de3/aioblescan/aioblescan.py#L745-L751
tfidf_python_100_1.0,encode url,python,"def create_dict_view_class(detail_view_class, name):
    encode = None
    if 'encode' in dir(detail_view_class):
        def encode(self):
            dict_to_flatten = {}
            for key, obj in self.model.items():
                dict_to_flatten[key] = detail_view_class(obj).encode()
            return RdyToFlattenDict(dict_to_flatten)

    return _create_collection_view(
        detail_view_class, name, encode, OperatorDictView
    )",https://github.com/osrg/ryu/blob/6f906e72c92e10bd0264c9b91a2f7bb85b97780c/ryu/services/protocols/bgp/operator/views/base.py#L286-L297
tfidf_python_100_1.0,encode url,python,"def encode(self, o):
    return super(CoercingEncoder, self).encode(self.default(o))",https://github.com/pantsbuild/pants/blob/b72e650da0df685824ffdcc71988b8c282d0962d/src/python/pants/base/hash_utils.py#L103-L104
tfidf_python_100_1.0,encode url,python,"def makePickle(self, record):
        return self.logmaticKey.encode() + "" "".encode() + self.formatter.format(record).encode() + ""\n"".encode()",https://github.com/logmatic/logmatic-python/blob/15d39c8c05f903054aaed9a5213f47756c0870ec/logmatic/__init__.py#L53-L54
tfidf_python_100_1.0,encode url,python,"def get_evaluation_by_id(evaluation_id, campus):
    url = ""%s/%s"" % (IAS_PREFIX, evaluation_id)
    return _json_to_evaluation(get_resource_by_campus(url, campus))",https://github.com/uw-it-aca/uw-restclients/blob/e12dcd32bf5296b6ebdf71798031594afb7852cb/restclients/iasystem/evaluation.py#L32-L34
tfidf_python_100_1.0,encode url,python,"def _custom_document_loader(url):
    if url in _CONTEXTS:
        return _CONTEXTS[url]

    requested_ctx = _default_document_loader(url)
    _CONTEXTS[url] = requested_ctx
    return requested_ctx",https://github.com/COALAIP/pycoalaip/blob/cecc8f6ff4733f0525fafcee63647753e832f0be/coalaip/jsonld.py#L13-L19
tfidf_python_100_1.0,encode url,python,"def encode(self, o):
        if isinstance(o, OrderedDict):
            return '{'+','.join(( self.encode(k)+':'+self.encode(v) for (k,v) in o.items() ))+'}'
        else:
            return json.JSONEncoder.encode(self, o)",https://github.com/uogbuji/versa/blob/f092ffc7ed363a5b170890955168500f32de0dd5/tools/py/util.py#L187-L191
tfidf_python_100_1.0,encode url,python,"def get_static_url(self):
        subfix = 'ajax_select'
        url = get_static_url(subfix)
        if subfix not in url:
            url += 'ajax_select/'
        return url",https://github.com/django-inplaceedit/django-inplaceedit-extra-fields/blob/cd4e29d320b626a35ed4d34ca07dbb05b449cc3b/inplaceeditform_extra_fields/fields.py#L57-L62
tfidf_python_100_1.0,encode url,python,"def encode(self, o):
        for w in self.objects.values():
            r = w.encode(o)
            if r is not None:
                return r",https://github.com/nocarryr/json-object-factory/blob/732578c29b2d3a96a5f73edb64676b0a8b4f34b5/jsonfactory/registry.py#L45-L49
tfidf_python_100_1.0,encode url,python,"def url(self):
        """"""URL of the album, relative to its parent.""""""
        url = self.name.encode('utf-8')
        return url_quote(url) + '/' + self.url_ext",https://github.com/saimn/sigal/blob/912ca39991355d358dc85fd55c7aeabdd7acc386/sigal/gallery.py#L416-L419
tfidf_python_100_1.0,encode url,python,"def get_multipart_fields(self, data):
        for name, value in data.items():
            yield [self.part_boundary.encode(), self.FIELD.format(name).encode(), ''.encode(), str(value).encode()]",https://github.com/imtapps/generic-request-signer/blob/34c18856ffda6305bd4cd931bd20365bf161d1de/generic_request_signer/factory.py#L136-L138
tfidf_python_100_1.0,encode url,python,"def get_parse_candidate(crawl_candidate):
        if crawl_candidate.raw_html:
            return RawHelper.get_parsing_candidate(crawl_candidate.url, crawl_candidate.raw_html)
        return URLHelper.get_parsing_candidate(crawl_candidate.url)",https://github.com/goose3/goose3/blob/e6994b1b1826af2720a091d1bff5ca15594f558d/goose3/crawler.py#L248-L251
tfidf_python_100_1.0,create cookie,python,"def get_multi_auth_cookie(self, cookie):
        rp_query_cookie = self.cookie_dealer.get_cookie_value(
            cookie, UserAuthnMethod.MULTI_AUTH_COOKIE)

        if rp_query_cookie:
            return rp_query_cookie[0]
        return """"",https://github.com/IdentityPython/oidcendpoint/blob/6c1d729d51bfb6332816117fe476073df7a1d823/src/oidcendpoint/user_authn/user.py#L115-L121
tfidf_python_100_1.0,create cookie,python,"def _cookie_to_id(id_type, cookie):
        if id_type == REST_VLANID:
            rest_id = cookie >> COOKIE_SHIFT_VLANID
        elif id_type == REST_ADDRESSID:
            rest_id = cookie & UINT32_MAX
        else:
            assert id_type == REST_ROUTEID
            rest_id = (cookie & UINT32_MAX) >> COOKIE_SHIFT_ROUTEID

        return rest_id",https://github.com/osrg/ryu/blob/6f906e72c92e10bd0264c9b91a2f7bb85b97780c/ryu/app/rest_router.py#L627-L636
tfidf_python_100_1.0,create cookie,python,"def __init__(self, cookie_id, *args, **kwargs):
        self.cookie = cookie_id
        self.backtest_history = QA_fetch_backtest_history(cookie=self.cookie)
        self.backtest_info = QA_fetch_backtest_info(account_cookie=self.cookie)",https://github.com/QUANTAXIS/QUANTAXIS/blob/bb1fe424e4108b62a1f712b81a05cf829297a5c0/QUANTAXIS/QAApplication/QAResult.py#L37-L40
tfidf_python_100_1.0,create cookie,python,"def set_cookie_id(self, cookie):
        self.auth_cookie = cookie
        if cookie is None:
            self.authentified= False
        else:
            self.authentified = True",https://github.com/HydrelioxGitHub/pybbox/blob/bedcdccab5d18d36890ef8bf414845f2dec18b5c/pybbox/bboxAuth.py#L40-L45
tfidf_python_100_1.0,create cookie,python,"def get_word_size_in_bytes_from_cookie(cookie):
    if (get_cookie_base(cookie) == V2_ENCODING_COOKIE_BASE) or \
       (get_cookie_base(cookie) == V2_COMPRESSION_COOKIE_BASE):
        return V2_MAX_WORD_SIZE_IN_BYTES
    return (cookie & 0xf0) >> 4",https://github.com/HdrHistogram/HdrHistogram_py/blob/cb99981b0564a62e1aa02bd764efa6445923f8f7/hdrh/codec.py#L58-L62
tfidf_python_100_1.0,create cookie,python,"def lwp_cookie_str(cookie):
    """"""Return string representation of Cookie in an the LWP cookie file format.

    Actually, the format is extended a bit -- see module docstring.

    """"""
    h = [(cookie.name, cookie.value),
         (""path"", cookie.path),
         (""domain"", cookie.domain)]
    if cookie.port is not None: h.append((""port"", cookie.port))
    if cookie.path_specified: h.append((""path_spec"", None))
    if cookie.port_specified: h.append((""port_spec"", None))
    if cookie.domain_initial_dot: h.append((""domain_dot"", None))
    if cookie.secure: h.append((""secure"", None))
    if cookie.expires: h.append((""expires"",
                               time2isoz(float(cookie.expires))))
    if cookie.discard: h.append((""discard"", None))
    if cookie.comment: h.append((""comment"", cookie.comment))
    if cookie.comment_url: h.append((""commenturl"", cookie.comment_url))

    keys = sorted(cookie._rest.keys())
    for k in keys:
        h.append((k, str(cookie._rest[k])))

    h.append((""version"", str(cookie.version)))

    return join_header_words([h])",https://github.com/PythonCharmers/python-future/blob/c423752879acc05eebc29b0bb9909327bd5c7308/src/future/backports/http/cookiejar.py#L1817-L1843
tfidf_python_100_1.0,create cookie,python,"def get_cookie(self, cookie):
        for x in self.session.cookie_jar:
            if x.key == cookie:
                return x.value
        raise KeyError(cookie)",https://github.com/mayfield/syndicate/blob/917af976dacb7377bdf0cb616f47e0df5afaff1a/syndicate/adapters/aio.py#L52-L56
tfidf_python_100_1.0,create cookie,python,"def set_ok(self, cookie, request):
        if not DefaultCookiePolicy.set_ok(self, cookie, request):
            return False

        try:
            new_cookie_length = (self.cookie_length(cookie.domain) +
                                 len(cookie.path) + len(cookie.name) +
                                 len(cookie.value or ''))
        except TypeError:
            # cookiejar is not infallible #220
            _logger.debug('Cookie handling error', exc_info=1)
            return False

        if new_cookie_length >= 4100:
            return False

        if self.count_cookies(cookie.domain) >= 50:
            cookies = self.cookie_jar._cookies
            try:
                cookies[cookie.domain][cookie.path][cookie.name]
            except KeyError:
                return False

        if not wpull.util.is_ascii(str(cookie)):
            return False

        return True",https://github.com/ArchiveTeam/wpull/blob/ddf051aa3322479325ba20aa778cb2cb97606bf5/wpull/cookie.py#L30-L56
tfidf_python_100_1.0,create cookie,python,"def flush(self):
        if self.session is None:
            raise NoSessionBound(""CookieStorage is not bound to any session"")

        self.data = []
        for cookie in self.session.cookies:
            self.data.append({
                ""version"": cookie.version,
                ""name"": cookie.name,
                ""value"": cookie.value,
                ""port"": cookie.port,
                ""domain"": cookie.domain,
                ""path"": cookie.path,
                ""secure"": cookie.secure,
                ""expires"": cookie.expires,
                ""discard"": cookie.discard,
                ""comment"": cookie.comment,
                ""comment_url"": cookie.comment_url,
                ""rfc2109"": cookie.rfc2109
            })
        self.save()
        return self.data",https://github.com/maxpowel/scrapium/blob/bc12c425aa5978f953a87d05920ba0f61a00409c/scrapium/scrapium.py#L34-L55
tfidf_python_100_1.0,create cookie,python,"def add_cookie(self, cookie):
		"""""" Add new cookie (or replace if there is cookie with the same name already)

		:param cookie: cookie to add
		:return: None
		""""""
		if self.__ro_flag:
			raise RuntimeError('Read-only cookie-jar changing attempt')
		self.__cookies[cookie.name()] = cookie",https://github.com/a1ezzz/wasp-general/blob/1029839d33eb663f8dec76c1c46754d53c1de4a9/wasp_general/network/web/cookies.py#L243-L251
tfidf_python_100_1.0,create cookie,python,"def save(self):
        """"""
        Adds the session cookie to headers.
        """"""
        if self.data:
            cookie = self.create_cookie()
            cookie_len = len(cookie)

            if cookie_len > 4093:
                raise SessionError('Cookie too long! The cookie size {0} '
                                   'is more than 4093 bytes.'
                                   .format(cookie_len))

            self.adapter.set_header('Set-Cookie', cookie)

            # Reset data
            self._data = {}",https://github.com/authomatic/authomatic/blob/90a9ce60cc405ae8a2bf5c3713acd5d78579a04e/authomatic/core.py#L394-L410
tfidf_python_100_1.0,create cookie,python,"def _get_cookies(self):
        all_cookies = self._driver.get_cookies()
        return dict((cookie['name'], cookie['value']) for cookie in all_cookies)",https://github.com/horejsek/python-webdriverwrapper/blob/a492f79ab60ed83d860dd817b6a0961500d7e3f5/webdriverwrapper/download.py#L61-L63
tfidf_python_100_1.0,create cookie,python,"def get_netscape_cookie_spec(self, cookie, request_host):
        # FIXME: Now cookie.domain could not be None
        # request_host is not needed anymore
        host = make_unicode(cookie.domain) or request_host
        if cookie.get_nonstandard_attr('HttpOnly'):
            host = '#HttpOnly_' + host
        items = [
            host,
            u'TRUE',
            make_unicode(cookie.path),
            u'TRUE' if cookie.secure else u'FALSE',
            make_unicode(str(
                cookie.expires if cookie.expires
                else YEAR_2030_EPOCH_TIME
            )),
            make_unicode(cookie.name),
            make_unicode(cookie.value),
        ]
        return (u'\t'.join(items)).encode('utf-8')",https://github.com/lorien/grab/blob/8b301db2a08c830245b61c589e58af6234f4db79/grab/transport/curl.py#L454-L472
tfidf_python_100_1.0,create cookie,python,"def _set_session_cookie(self, response):
        if 'set-cookie' not in response.headers:
            return

        for cookie in response.headers['set-cookie'].split(';'):
            if cookie.startswith(self.SESSION_COOKIE_NAME):
                self._session_id = cookie",https://github.com/gaqzi/py-gocd/blob/6fe5b62dea51e665c11a343aba5fc98e130c5c63/gocd/server.py#L202-L208
tfidf_python_100_1.0,create cookie,python,"def get_all(self, key):
        return [cookie for cookie in self.all_cookies
                if cookie.name == key]",https://github.com/sashahart/cookies/blob/ab8185e06f221eaf65305f15e05852393723ac95/cookies.py#L1018-L1020
tfidf_python_100_1.0,create cookie,python,"def add(self, *args, **kwargs):
        """"""Add Cookie objects by their names, or create new ones under
        specified names.

        Any unnamed arguments are interpreted as existing cookies, and
        are added under the value in their .name attribute. With keyword
        arguments, the key is interpreted as the cookie name and the
        value as the UNENCODED value stored in the cookie.
        """"""
        # Only the first one is accessible through the main interface,
        # others accessible through get_all (all_cookies).
        for cookie in args:
            self.all_cookies.append(cookie)
            if cookie.name in self:
                continue
            self[cookie.name] = cookie
        for key, value in kwargs.items():
            cookie = self.cookie_class(key, value)
            self.all_cookies.append(cookie)
            if key in self:
                continue
            self[key] = cookie",https://github.com/sashahart/cookies/blob/ab8185e06f221eaf65305f15e05852393723ac95/cookies.py#L995-L1016
tfidf_python_100_1.0,create cookie,python,"def render_request(self, sort=True):
        """"""Render the dict's Cookie objects into a string formatted for HTTP
        request headers (simple 'Cookie: ' style).
        """"""
        if not sort:
            return (""; "".join(
                cookie.render_request() for cookie in self.values()))
        return (""; "".join(sorted(
            cookie.render_request() for cookie in self.values())))",https://github.com/sashahart/cookies/blob/ab8185e06f221eaf65305f15e05852393723ac95/cookies.py#L1127-L1135
tfidf_python_100_1.0,create cookie,python,"def _store_cookies(self, response_obj):
        for cookie in response_obj.cookies:
            try:
                self.domain_dict[cookie.host.lstrip()].append(cookie)
            except KeyError:
                self.domain_dict[cookie.host.lstrip()] = [cookie]",https://github.com/theelous3/asks/blob/ea522ea971ecb031d488a6301dc2718516cadcd6/asks/cookie_utils.py#L19-L24
tfidf_python_100_1.0,create cookie,python,"def parse_cookie_header(header):
    '''
    Parse the ""Set-cookie"" header, and return a list of cookies.

    This function is here because Tornado's HTTPClient doesn't handle cookies.
    '''
    attribs = ('expires', 'path', 'domain', 'version', 'httponly', 'secure', 'comment', 'max-age')

    # Split into cookie(s); handles headers with multiple cookies defined
    morsels = []
    for item in header.split(';'):
        item = item.strip()
        if ',' in item and 'expires' not in item:
            for part in item.split(','):
                morsels.append(part)
        else:
            morsels.append(item)

    # Break down morsels into actual cookies
    cookies = []
    cookie = {}
    value_set = False
    for morsel in morsels:
        parts = morsel.split('=')
        if parts[0].lower() in attribs:
            if parts[0] in cookie:
                cookies.append(cookie)
                cookie = {}
            if len(parts) > 1:
                cookie[parts[0]] = '='.join(parts[1:])
            else:
                cookie[parts[0]] = True
        else:
            if value_set is True:
                # This is a new cookie; save the old one and clear for this one
                cookies.append(cookie)
                cookie = {}
                value_set = False
            cookie[parts[0]] = '='.join(parts[1:])
            value_set = True

    if cookie:
        # Set the last cookie that was processed
        cookies.append(cookie)

    # These arguments are required by cookielib.Cookie()
    reqd = (
        'version',
        'port',
        'port_specified',
        'domain',
        'domain_specified',
        'domain_initial_dot',
        'path',
        'path_specified',
        'secure',
        'expires',
        'discard',
        'comment',
        'comment_url',
        'rest',
    )

    ret = []
    for cookie in cookies:
        name = None
        value = None
        for item in list(cookie):
            if item in attribs:
                continue
            name = item
            value = cookie.pop(item)

        # cookielib.Cookie() requires an epoch
        if 'expires' in cookie:
            cookie['expires'] = salt.ext.six.moves.http_cookiejar.http2time(cookie['expires'])

        # Fill in missing required fields
        for req in reqd:
            if req not in cookie:
                cookie[req] = ''
        if cookie['version'] == '':
            cookie['version'] = 0
        if cookie['rest'] == '':
            cookie['rest'] = {}
        if cookie['expires'] == '':
            cookie['expires'] = 0

        if 'httponly' in cookie:
            del cookie['httponly']
        ret.append(salt.ext.six.moves.http_cookiejar.Cookie(name=name, value=value, **cookie))

    return ret",https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/utils/http.py#L861-L953
tfidf_python_100_1.0,create cookie,python,"def set_ok_path(self, cookie, request):
        if cookie.path_specified:
            req_path = request_path(request)
            if ((cookie.version > 0 or
                 (cookie.version == 0 and self.strict_ns_set_path)) and
                not req_path.startswith(cookie.path)):
                _debug(""   path attribute %s is not a prefix of request ""
                       ""path %s"", cookie.path, req_path)
                return False
        return True",https://github.com/PythonCharmers/python-future/blob/c423752879acc05eebc29b0bb9909327bd5c7308/src/future/backports/http/cookiejar.py#L990-L999
tfidf_python_100_1.0,how to empty array,python,"def cloud_cover_to_irradiance(self, cloud_cover, how='clearsky_scaling',
                                  **kwargs):
        """"""
        Convert cloud cover to irradiance. A wrapper method.

        Parameters
        ----------
        cloud_cover : Series
        how : str, default 'clearsky_scaling'
            Selects the method for conversion. Can be one of
            clearsky_scaling or liujordan.
        **kwargs
            Passed to the selected method.

        Returns
        -------
        irradiance : DataFrame
            Columns include ghi, dni, dhi
        """"""

        how = how.lower()
        if how == 'clearsky_scaling':
            irrads = self.cloud_cover_to_irradiance_clearsky_scaling(
                cloud_cover, **kwargs)
        elif how == 'liujordan':
            irrads = self.cloud_cover_to_irradiance_liujordan(
                cloud_cover, **kwargs)
        else:
            raise ValueError('invalid how argument')

        return irrads",https://github.com/pvlib/pvlib-python/blob/2e844a595b820b43d1170269781fa66bd0ccc8a3/pvlib/forecast.py#L539-L569
tfidf_python_100_1.0,how to empty array,python,"def execute_arbitrary_series_groupby(op, data, _, aggcontext=None, **kwargs):
    how = op.how
    if how is None:
        how = 'first'

    if how not in {'first', 'last'}:
        raise com.OperationNotDefinedError(
            'Arbitrary {!r} is not supported'.format(how)
        )
    return aggcontext.agg(data, how)",https://github.com/ibis-project/ibis/blob/1e39a5fd9ef088b45c155e8a5f541767ee8ef2e7/ibis/pandas/execution/generic.py#L451-L460
tfidf_python_100_1.0,how to empty array,python,"def time_i8merge(self, how):
        merge(self.left, self.right, how=how)",https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/asv_bench/benchmarks/join_merge.py#L213-L214
tfidf_python_100_1.0,how to empty array,python,"def Parse(self, how):
        '''Parse the message.
        '''
        if type(how) == types.ClassType: how = how.typecode
        return how.parse(self.body_root, self)",https://github.com/rameshg87/pyremotevbox/blob/123dffff27da57c8faa3ac1dd4c68b1cf4558b1a/pyremotevbox/ZSI/parse.py#L322-L326
tfidf_python_100_1.0,how to empty array,python,"def set_mode(how):
    """""" Sets the behavior of the API

    :param how: if 'remote' all the execution is performed on the remote server; if 'local' all
           it is executed locally. Default = 'local'
    :return: None
    """"""
    global __mode
    if how == ""local"":
        __mode = how
    elif how == ""remote"":
        __mode = how
    else:
        raise ValueError(""how must be 'local' or 'remote'"")",https://github.com/DEIB-GECO/PyGMQL/blob/e58b2f9402a86056dcda484a32e3de0bb06ed991/gmql/settings.py#L79-L92
tfidf_python_100_1.0,how to empty array,python,"def empty_like(array, dtype=None, keepmeta=True):
    """"""Create an array of empty with the same shape and type as the input array.

    Args:
        array (xarray.DataArray): The shape and data-type of it define
            these same attributes of the output array.
        dtype (data-type, optional): If spacified, this function overrides
            the data-type of the output array.
        keepmeta (bool, optional): Whether *coords, attrs, and name of the input
            array are kept in the output one. Default is True.

    Returns:
        array (decode.array): Decode array without initializing entries.
    """"""
    if keepmeta:
        return dc.empty(array.shape, dtype,
            tcoords=array.dca.tcoords, chcoords=array.dca.chcoords,
            scalarcoords=array.dca.scalarcoords, attrs=array.attrs, name=array.name
        )
    else:
        return dc.empty(array.shape, dtype)",https://github.com/deshima-dev/decode/blob/e789e174cd316e7ec8bc55be7009ad35baced3c0/decode/core/array/functions.py#L185-L205
tfidf_python_100_1.0,how to empty array,python,"def dropna(self, how='any'):
        if how not in ('any', 'all'):
            raise ValueError(""invalid how option: {0}"".format(how))

        if self.hasnans:
            return self._shallow_copy(self.values[~self._isnan])
        return self._shallow_copy()",https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/indexes/base.py#L1961-L1967
tfidf_python_100_1.0,how to empty array,python,"def set_meta_profiling(how):
    """""" Enables or disables the profiling of metadata at the loading of a GMQLDataset

    :param how: True if you want to analyze the metadata when a GMQLDataset is created
                by a load_from_*. False otherwise. (Default=True)
    :return: None
    """"""
    global __metadata_profiling
    if isinstance(how, bool):
        __metadata_profiling = how
    else:
        raise TypeError(""how must be boolean. {} was provided"".format(type(how)))",https://github.com/DEIB-GECO/PyGMQL/blob/e58b2f9402a86056dcda484a32e3de0bb06ed991/gmql/settings.py#L136-L147
tfidf_python_100_1.0,how to empty array,python,"def __init__(self, item, how):
        self.item = item
        self.how = how",https://github.com/kayak/pypika/blob/bfed26e963b982ecdb9697b61b67d76b493f2115/pypika/queries.py#L1029-L1031
tfidf_python_100_1.0,how to empty array,python,"def underlying_variable(self):
        array = self
        while not isinstance(array, ArrayVariable):
            array = array.array
        return array",https://github.com/trailofbits/manticore/blob/54c5a15b1119c523ae54c09972413e8b97f11629/manticore/core/smtlib/expression.py#L687-L691
tfidf_python_100_1.0,how to empty array,python,"def set_progress(how):
    """""" Enables or disables the progress bars for the loading, writing and downloading
    of datasets

    :param how: True if you want the progress bar, False otherwise
    :return: None

    Example::

        import gmql as gl

        gl.set_progress(True)   # abilitates progress bars
        # ....do something...
        gl.set_progress(False)  # removes progress bars
        # ....do something...
    """"""
    global __progress_bar
    if isinstance(how, bool):
        __progress_bar = how
    else:
        raise ValueError(
            ""how must be a boolean. {} was found"".format(type(how)))",https://github.com/DEIB-GECO/PyGMQL/blob/e58b2f9402a86056dcda484a32e3de0bb06ed991/gmql/settings.py#L107-L128
tfidf_python_100_1.0,how to empty array,python,"def erase_in_display(self, how=0, *args, **kwargs):
        """"""Overloaded to reset history state.""""""
        super(HistoryScreen, self).erase_in_display(how, *args, **kwargs)

        if how == 3:
            self._reset_history()",https://github.com/selectel/pyte/blob/8adad489f86da1788a7995720c344a2fa44f244e/pyte/screens.py#L1204-L1209
tfidf_python_100_1.0,how to empty array,python,"def align(self, alignraster, how=np.mean, cxsize=None, cysize=None):
        '''
        geo.align(geo2, how=np.mean)

        Returns both georasters aligned and with the same pixelsize
        '''
        return align_georasters(self, alignraster, how=how, cxsize=cxsize, cysize=cysize)",https://github.com/ozak/georasters/blob/0612bd91bb2a2cb2f1d59ba89c1ff131dae27d70/georasters/georasters.py#L862-L868
tfidf_python_100_1.0,how to empty array,python,"def reverse(array, i, j):
    while i < j:
        array[i], array[j] = array[j], array[i]
        i += 1
        j -= 1",https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/strings/reverse_words.py#L2-L6
tfidf_python_100_1.0,how to empty array,python,"def shutdown(self, how):
        if how not in (SHUT_RD, SHUT_WR, SHUT_RDWR):
            raise _socket.error(22, ""Invalid argument"")
        self.__commstate = how",https://github.com/pybluez/pybluez/blob/e0dc4093dcbaa3ecb3fa24f8ccf22bbfe6b57fc9/macos/_bluetoothsockets.py#L572-L575
tfidf_python_100_1.0,how to empty array,python,"def tinyify(array, tiny_mode, small_mode):
  if tiny_mode:
    return [1 for _ in array]
  if small_mode:
    return [max(x // 4, 1) for x in array]
  return array",https://github.com/tensorflow/tensor2tensor/blob/272500b6efe353aeb638d2745ed56e519462ca31/tensor2tensor/layers/common_video.py#L502-L507
tfidf_python_100_1.0,how to empty array,python,"def _maybe_to_sparse(array):
    """"""
    array must be SparseSeries or SparseArray
    """"""
    if isinstance(array, ABCSparseSeries):
        array = array.values.copy()
    return array",https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/arrays/sparse.py#L1803-L1809
tfidf_python_100_1.0,how to empty array,python,"def ensure_array(array):
    """"""
    Assert that the given array is an Array subclass (or numpy array).

    If the given array is a numpy.ndarray an appropriate NumpyArrayAdapter
    instance is created, otherwise the passed array must be a subclass of
    :class:`Array` else a TypeError will be raised.

    """"""
    if not isinstance(array, Array):
        if isinstance(array, np.ndarray):
            array = NumpyArrayAdapter(array)
        elif np.isscalar(array):
            array = ConstantArray([], array)
        else:
            raise TypeError('The given array should be a `biggus.Array` '
                            'instance, got {}.'.format(type(array)))
    return array",https://github.com/SciTools/biggus/blob/0a76fbe7806dd6295081cd399bcb76135d834d25/biggus/_init.py#L3289-L3306
tfidf_python_100_1.0,how to empty array,python,"def lcm(array):
        if len(array) == 0:
                return 0
        if len(array) == 1:
                return array[0]
        elif len(array) == 2:
                return array[0] * array[1] / gcd(array)
        else:
                return lcm([lcm(array[:len(array) / 2]), lcm(array[len(array) / 2:])])",https://github.com/ioguntol/AMP/blob/c227806bdd851b8c2d78afe0aedab43395705795/AMP/funcEval.py#L44-L52
tfidf_python_100_1.0,how to empty array,python,"def erase_in_display(self, how=0, *args, **kwargs):
        """"""Erases display in a specific way.

        Character attributes are set to cursor attributes.

        :param int how: defines the way the line should be erased in:

            * ``0`` -- Erases from cursor to end of screen, including
              cursor position.
            * ``1`` -- Erases from beginning of screen to cursor,
              including cursor position.
            * ``2`` and ``3`` -- Erases complete display. All lines
              are erased and changed to single-width. Cursor does not
              move.
        :param bool private: when ``True`` only characters marked as
                             eraseable are affected **not implemented**.

        .. versionchanged:: 0.8.1

           The method accepts any number of positional arguments as some
           ``clear`` implementations include a ``;`` after the first
           parameter causing the stream to assume a ``0`` second parameter.
        """"""
        if how == 0:
            interval = range(self.cursor.y + 1, self.lines)
        elif how == 1:
            interval = range(self.cursor.y)
        elif how == 2 or how == 3:
            interval = range(self.lines)

        self.dirty.update(interval)
        for y in interval:
            line = self.buffer[y]
            for x in line:
                line[x] = self.cursor.attrs

        if how == 0 or how == 1:
            self.erase_in_line(how)",https://github.com/selectel/pyte/blob/8adad489f86da1788a7995720c344a2fa44f244e/pyte/screens.py#L771-L808
tfidf_python_100_1.0,how to get current date,python,"def date_0utc(date):
    """"""Get the 0 UTC date for a date

    Parameters
    ----------
    date : ee.Date

    Returns
    -------
    ee.Date

    """"""
    return ee.Date.fromYMD(date.get('year'), date.get('month'),
                           date.get('day'))",https://github.com/Open-ET/openet-core-beta/blob/f2b81ccf87bf7e7fe1b9f3dd1d4081d0ec7852db/openet/core/utils.py#L82-L95
tfidf_python_100_1.0,how to get current date,python,"def cloud_cover_to_irradiance(self, cloud_cover, how='clearsky_scaling',
                                  **kwargs):
        """"""
        Convert cloud cover to irradiance. A wrapper method.

        Parameters
        ----------
        cloud_cover : Series
        how : str, default 'clearsky_scaling'
            Selects the method for conversion. Can be one of
            clearsky_scaling or liujordan.
        **kwargs
            Passed to the selected method.

        Returns
        -------
        irradiance : DataFrame
            Columns include ghi, dni, dhi
        """"""

        how = how.lower()
        if how == 'clearsky_scaling':
            irrads = self.cloud_cover_to_irradiance_clearsky_scaling(
                cloud_cover, **kwargs)
        elif how == 'liujordan':
            irrads = self.cloud_cover_to_irradiance_liujordan(
                cloud_cover, **kwargs)
        else:
            raise ValueError('invalid how argument')

        return irrads",https://github.com/pvlib/pvlib-python/blob/2e844a595b820b43d1170269781fa66bd0ccc8a3/pvlib/forecast.py#L539-L569
tfidf_python_100_1.0,how to get current date,python,"def execute_arbitrary_series_groupby(op, data, _, aggcontext=None, **kwargs):
    how = op.how
    if how is None:
        how = 'first'

    if how not in {'first', 'last'}:
        raise com.OperationNotDefinedError(
            'Arbitrary {!r} is not supported'.format(how)
        )
    return aggcontext.agg(data, how)",https://github.com/ibis-project/ibis/blob/1e39a5fd9ef088b45c155e8a5f541767ee8ef2e7/ibis/pandas/execution/generic.py#L451-L460
tfidf_python_100_1.0,how to get current date,python,"def _discover_publication_date(opf_xmldoc, date_html=None):
    date = __discover_dc(opf_xmldoc, 'date')

    if not date and date_html is not None:
        date = _find_publish_date_from_dom(date_html)

    return date",https://github.com/paulocheque/epub-meta/blob/3f0efb9f29a286b1a6896ad05422b23f10e10164/epub_meta/collector.py#L165-L171
tfidf_python_100_1.0,how to get current date,python,"def _get_crime_categories(self, date=None):
        if date not in self.crime_categories:
            self._populate_crime_categories(date=date)
        return self.crime_categories[date]",https://github.com/rkhleics/police-api-client-python/blob/b5c1e493487eb2409e2c04ed9fbd304f73d89fdc/police_api/__init__.py#L166-L169
tfidf_python_100_1.0,how to get current date,python,"def time_i8merge(self, how):
        merge(self.left, self.right, how=how)",https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/asv_bench/benchmarks/join_merge.py#L213-L214
tfidf_python_100_1.0,how to get current date,python,"def to_date_string(date):
    if isinstance(date, numpy.int64) or isinstance(date, int):
        date = str(date)
        date = '%s-%s-%s' % (date[:4], date[4:6], date[6:8])
        return date",https://github.com/CxAalto/gtfspy/blob/bddba4b74faae6c1b91202f19184811e326547e5/gtfspy/util.py#L192-L196
tfidf_python_100_1.0,how to get current date,python,"def Parse(self, how):
        '''Parse the message.
        '''
        if type(how) == types.ClassType: how = how.typecode
        return how.parse(self.body_root, self)",https://github.com/rameshg87/pyremotevbox/blob/123dffff27da57c8faa3ac1dd4c68b1cf4558b1a/pyremotevbox/ZSI/parse.py#L322-L326
tfidf_python_100_1.0,how to get current date,python,"def QA_util_date_int2str(int_date):
    """"""
    ç±»ådatetime.datatime
    :param date: int 8ä½æ´æ°
    :return: ç±»åstr
    """"""
    date = str(int_date)
    if len(date) == 8:
        return str(date[0:4] + '-' + date[4:6] + '-' + date[6:8])
    elif len(date) == 10:
        return date",https://github.com/QUANTAXIS/QUANTAXIS/blob/bb1fe424e4108b62a1f712b81a05cf829297a5c0/QUANTAXIS/QAUtil/QADate.py#L74-L84
tfidf_python_100_1.0,how to get current date,python,"def rollforward(self, date):
        if self.onOffset(date):
            return date
        else:
            return date + type(self)()",https://github.com/pydata/xarray/blob/6d93a95d05bdbfc33fff24064f67d29dd891ab58/xarray/coding/cftime_offsets.py#L141-L145
tfidf_python_100_1.0,how to get current date,python,"def check_date(date):
    if isinstance(date, six.string_types):
        return Date.parseISO(date)
    else:
        return Date.fromDateTime(date)",https://github.com/iLampard/x-utils/blob/291d92832ee0e0c89bc22e10ecf2f44445e0d300/xutils/date_utils/date.py#L254-L258
tfidf_python_100_1.0,how to get current date,python,"def getDate(self):
        ""returns the GMT response datetime or None""
        date = self.headers.get('date')
        if date:
            date = self.convertTimeString(date)
        return date",https://github.com/hendrix/hendrix/blob/175af011a7e5822b772bfec0e11a46466bb8688d/hendrix/contrib/cache/__init__.py#L75-L80
tfidf_python_100_1.0,how to get current date,python,"def format_date(self, date):
        if isinstance(date, datetime.datetime):
            date = date.date()

        if isinstance(date, datetime.date):
            date = date.strftime('%d%m%y')

        if not patterns.DATE.match(date):
            raise ValueError(""Invalid date: "" + date)

        return date",https://github.com/Turbo87/aerofiles/blob/d8b7b04a1fcea5c98f89500de1164619a4ec7ef4/aerofiles/igc/writer.py#L24-L34
tfidf_python_100_1.0,how to get current date,python,"def isEndOfMonth(date):
        m = date.month()
        y = date.year()
        return date.dayOfMonth() == _month_length(m, Date.isLeap(y))",https://github.com/iLampard/x-utils/blob/291d92832ee0e0c89bc22e10ecf2f44445e0d300/xutils/date_utils/date.py#L179-L182
tfidf_python_100_1.0,how to get current date,python,"def endOfMonth(date):
        m = date.month()
        y = date.year()
        return Date(y, m, _month_length(m, Date.isLeap(y)))",https://github.com/iLampard/x-utils/blob/291d92832ee0e0c89bc22e10ecf2f44445e0d300/xutils/date_utils/date.py#L173-L176
tfidf_python_100_1.0,how to get current date,python,"def _containsdate(self, date):
        date = Date(date)
        return ((self.firstdate <= date <= self.lastdate) and
                ((date-self.firstdate) // self.stepsize))",https://github.com/hydpy-dev/hydpy/blob/1bc6a82cf30786521d86b36e27900c6717d3348d/hydpy/core/timetools.py#L1534-L1537
tfidf_python_100_1.0,how to get current date,python,"def get_date(date):
    """"""
    Get the date from a value that could be a date object or a string.

    :param date: The date object or string.

    :returns: The date object.
    """"""
    if type(date) is str:
        return datetime.strptime(date, '%Y-%m-%d').date()
    else:
        return date",https://github.com/Ex-Mente/auxi.0/blob/2dcdae74154f136f8ca58289fe5b20772f215046/auxi/core/helpers.py#L23-L34
tfidf_python_100_1.0,how to get current date,python,"def __call__(cls, *args, **kwargs):
        date = args[0] if len(args) == 1 else kwargs['date']

        if date is None:
            date = get_configuration().get('VERSIONS', 'defaultversion')

        if isinstance(date, DictionaryVersion):
            return date

        if isinstance(date, str):
            if date.startswith('dictionary_'):
                date = _str_to_date(date.split('.')[0].split('_', maxsplit=1)[1])
            else:
                date = _str_to_date(date)
        elif isinstance(date, datetime.date):
            date = _str_to_date(_date_to_str(date))
        else:
            raise ValueError(""Invalid date format for dictionary version %s."" % _date_to_str(date))

        if date not in cls._instances:
            cls._instances[date] = super(DictionaryVersionSingleton, cls).__call__(date)

        return cls._instances[date]",https://github.com/IEMLdev/ieml/blob/4c842ba7e6165e2f1b4a4e2e98759f9f33af5f25/ieml/dictionary/version.py#L56-L78
tfidf_python_100_1.0,how to get current date,python,"def isworkday(self, date):
        """"""
        Check if a given date is a work date, ignoring holidays.

        Args:
            date (date, datetime or str): Date to be checked.

        Returns:
            bool: True if the date is a work date, False otherwise.
        """"""
        date = parsefun(date)
        return self.weekdaymap[date.weekday()].isworkday",https://github.com/antoniobotelho/py-business-calendar/blob/92365fbddd043e41e33b01f1ddd9dd6a5094c031/business_calendar/business_calendar.py#L188-L199
tfidf_python_100_1.0,how to get current date,python,"def set_mode(how):
    """""" Sets the behavior of the API

    :param how: if 'remote' all the execution is performed on the remote server; if 'local' all
           it is executed locally. Default = 'local'
    :return: None
    """"""
    global __mode
    if how == ""local"":
        __mode = how
    elif how == ""remote"":
        __mode = how
    else:
        raise ValueError(""how must be 'local' or 'remote'"")",https://github.com/DEIB-GECO/PyGMQL/blob/e58b2f9402a86056dcda484a32e3de0bb06ed991/gmql/settings.py#L79-L92
tfidf_python_100_1.0,how to make the checkbox checked,python,"def set_checkbox(self, data, uncheck_other_boxes=True):
        """"""Set the *checked*-attribute of input elements of type ""checkbox""
        specified by ``data`` (i.e. check boxes).

        :param data: Dict of ``{name: value, ...}``.
            In the family of checkboxes whose *name*-attribute is ``name``,
            check the box whose *value*-attribute is ``value``. All boxes in
            the family can be checked (unchecked) if ``value`` is True (False).
            To check multiple specific boxes, let ``value`` be a tuple or list.
        :param uncheck_other_boxes: If True (default), before checking any
            boxes specified by ``data``, uncheck the entire checkbox family.
            Consider setting to False if some boxes are checked by default when
            the HTML is served.
        """"""
        for (name, value) in data.items():
            # Case-insensitive search for type=checkbox
            checkboxes = self.find_by_type(""input"", ""checkbox"", {'name': name})
            if not checkboxes:
                raise InvalidFormMethod(""No input checkbox named "" + name)

            # uncheck if requested
            if uncheck_other_boxes:
                self.uncheck_all(name)

            # Wrap individual values (e.g. int, str) in a 1-element tuple.
            if not isinstance(value, list) and not isinstance(value, tuple):
                value = (value,)

            # Check or uncheck one or more boxes
            for choice in value:
                choice_str = str(choice)  # Allow for example literal numbers
                for checkbox in checkboxes:
                    if checkbox.attrs.get(""value"", ""on"") == choice_str:
                        checkbox[""checked""] = """"
                        break
                    # Allow specifying True or False to check/uncheck
                    elif choice is True:
                        checkbox[""checked""] = """"
                        break
                    elif choice is False:
                        if ""checked"" in checkbox.attrs:
                            del checkbox.attrs[""checked""]
                        break
                else:
                    raise LinkNotFoundError(
                        ""No input checkbox named %s with choice %s"" %
                        (name, choice)
                    )",https://github.com/MechanicalSoup/MechanicalSoup/blob/027a270febf5bcda6a75db60ea9838d631370f4b/mechanicalsoup/form.py#L99-L146
tfidf_python_100_1.0,how to make the checkbox checked,python,"def addCheckBox(self, *args, **kwargs):
        checkbox = BeakerxCheckbox(description=self.getDescription(args, kwargs))
        checkbox.value = getValue(kwargs, 'value', False)
        self.children += (checkbox,)
        self.components[checkbox.description] = checkbox
        return checkbox",https://github.com/twosigma/beakerx/blob/404de61ed627d9daaf6b77eb4859e7cb6f37413f/beakerx/beakerx/easyform/easyform.py#L112-L117
tfidf_python_100_1.0,how to make the checkbox checked,python,"def get_checkbox_state_by_name(self, checkbox_text):
        return [checkbox.get_active() for checkbox in self.checkboxes if checkbox.get_label() == checkbox_text]",https://github.com/DLR-RM/RAFCON/blob/24942ef1a904531f49ab8830a1dbb604441be498/source/rafcon/gui/utils/dialog.py#L233-L234
tfidf_python_100_1.0,how to make the checkbox checked,python,"def render(self, obj):
        checked = bool(Accessor(self.field).resolve(obj)) if self.field else False
        if checked:
            return mark_safe('<input checked type=""checkbox"">')
        else:
            return mark_safe('<input type=""checkbox"">')",https://github.com/shymonk/django-datatable/blob/f20a6ed2ce31aa7488ff85b4b0e80fe1ad94ec44/table/columns/checkboxcolumn.py#L16-L21
tfidf_python_100_1.0,how to make the checkbox checked,python,"def checkbox_uncheck(self, force_check=False):
        """"""
        Wrapper to uncheck a checkbox
        """"""
        if self.get_attribute('checked'):
            self.click(force_click=force_check)",https://github.com/Shapeways/coyote_framework/blob/cb29899b984a21d56bf65d0b1d907073948fe16c/coyote_framework/webdriver/webdriverwrapper/WebElementWrapper.py#L662-L667
tfidf_python_100_1.0,how to make the checkbox checked,python,"def assert_checked_checkbox(self, value):
    """"""Assert the checkbox with label (recommended), name or id is checked.""""""
    check_box = find_field(world.browser, 'checkbox', value)
    assert check_box, ""Cannot find checkbox '{}'."".format(value)
    assert check_box.is_selected(), ""Check box should be selected.""",https://github.com/aloetesting/aloe_webdriver/blob/65d847da4bdc63f9c015cb19d4efdee87df8ffad/aloe_webdriver/__init__.py#L564-L568
tfidf_python_100_1.0,how to make the checkbox checked,python,"def checkbox_check(self, force_check=False):
        """"""
        Wrapper to check a  checkbox
        """"""
        if not self.get_attribute('checked'):
            self.click(force_click=force_check)",https://github.com/Shapeways/coyote_framework/blob/cb29899b984a21d56bf65d0b1d907073948fe16c/coyote_framework/webdriver/webdriverwrapper/WebElementWrapper.py#L655-L660
tfidf_python_100_1.0,how to make the checkbox checked,python,"def create_checkbox(self, parent, label, getter, setter):
        checkbox = QtGui.QCheckBox(label, parent)
        checkbox.setChecked(getter())

        def stateChanged(state):
            value = state == QtCore.Qt.Checked
            setter(value)

        checkbox.stateChanged.connect(stateChanged)
        return checkbox",https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-ui/vaex/ui/layers.py#L2427-L2436
tfidf_python_100_1.0,how to make the checkbox checked,python,"def set_checked(self, checked):
        if self._clone_original:
            self._clone_original.set_checked(checked)
        else:
            self._checked = checked",https://github.com/subdownloader/subdownloader/blob/bbccedd11b18d925ad4c062b5eb65981e24d0433/subdownloader/client/gui/searchFileModel.py#L27-L31
tfidf_python_100_1.0,how to make the checkbox checked,python,"def check_checkbox(self, value):
    """"""Check the checkbox with label (recommended), name or id.""""""
    check_box = find_field(world.browser, 'checkbox', value)
    assert check_box, ""Cannot find checkbox '{}'."".format(value)
    if not check_box.is_selected():
        check_box.click()",https://github.com/aloetesting/aloe_webdriver/blob/65d847da4bdc63f9c015cb19d4efdee87df8ffad/aloe_webdriver/__init__.py#L542-L547
tfidf_python_100_1.0,how to make the checkbox checked,python,"def handle_positioned_check_box(self, checked):
        if checked:
            self.__scan_hardware_source.validate_probe_position()
        else:
            self.__scan_hardware_source.probe_position = None",https://github.com/nion-software/nionswift-instrumentation-kit/blob/b20c4fff17e840e8cb3d544705faf5bd05f1cbf7/nionswift_plugin/nion_instrumentation_ui/ScanControlPanel.py#L382-L386
tfidf_python_100_1.0,how to make the checkbox checked,python,"def input_check(self, name, label, multi_line=False):
        """"""
        {% if multiple_choice_1 %}
            {% set checked = ""checked"" %}
        {% else %}
            {% set checked = """" %}
        {% endif %}
        <input type=""checkbox"" name=""multiple_choice_1"" value=""multiple_choice_1"" {{checked}}> multiple_choice_1
        """"""
        lines = list()
        lines.append('{%% if %s %%}' % name)
        lines.append(self.tab + '{% set checked = ""checked"" %}')
        lines.append('{% else %}')
        lines.append(self.tab  + '{% set checked = """" %}')
        lines.append('{% endif %}')
        if multi_line:
            line_break = ""<br>""
        else:
            line_break = """"
        lines.append('<input type=""checkbox"" name=""%s"" value=""%s"" {{checked}}> %s %s' % (name, name, label, line_break))        
        return ""\n"".join(lines)",https://github.com/MacHu-GWU/angora-project/blob/689a60da51cd88680ddbe26e28dbe81e6b01d275/angora/markup/html.py#L85-L105
tfidf_python_100_1.0,how to make the checkbox checked,python,"def set_value(self, checked, update_ui=1):
        if checked:
            self.attributes['checked'] = 'checked'
        else:
            if 'checked' in self.attributes:
                del self.attributes['checked']",https://github.com/dddomodossola/remi/blob/85206f62220662bb7ecd471042268def71ccad28/remi/gui.py#L2776-L2781
tfidf_python_100_1.0,how to make the checkbox checked,python,"def _checked_change(self, *args):
        for pony in self._missing_ponies.values():
            pony.checked = self.checked",https://github.com/Arzaroth/CelestiaSunrise/blob/a2dcc5e905114705cd4a93dd37b5fd8a9858c467/celestia/gui/missingponiesframe.py#L50-L52
tfidf_python_100_1.0,how to make the checkbox checked,python,"def handle_ac_line_sync_check_box(self, checked):
        frame_parameters = self.__scan_hardware_source.get_frame_parameters(self.__scan_hardware_source.selected_profile_index)
        frame_parameters.ac_line_sync = checked
        self.__scan_hardware_source.set_frame_parameters(self.__scan_hardware_source.selected_profile_index, frame_parameters)",https://github.com/nion-software/nionswift-instrumentation-kit/blob/b20c4fff17e840e8cb3d544705faf5bd05f1cbf7/nionswift_plugin/nion_instrumentation_ui/ScanControlPanel.py#L388-L391
tfidf_python_100_1.0,how to make the checkbox checked,python,"def get_selected_events(self, time_selection=None):
        """"""Returns which events are present in one time window.

        Parameters
        ----------
        time_selection : tuple of float
            start and end of the window of interest

        Returns
        -------
        list of dict
            list of events in the window of interest
        """"""
        events = []
        for checkbox in self.idx_eventtype_list:
            if checkbox.checkState() == Qt.Checked:
                events.extend(self.annot.get_events(name=checkbox.text(),
                                                    time=time_selection))

        return events",https://github.com/wonambi-python/wonambi/blob/1d8e3d7e53df8017c199f703bcab582914676e76/wonambi/widgets/notes.py#L738-L757
tfidf_python_100_1.0,how to make the checkbox checked,python,"def __str__(self):
        checked = """"
        if self.checked:
            checked = ""checked='checked'""
        return (
            ""<input ""
            ""type='radio' ""
            ""name='%(name)s' ""
            ""value='%(value)s' ""
            ""%(checked)s>""
            ""%(description)s""
            ""</input><br />"" % {
                'name': self.name,
                'value': self.value,
                'checked': checked,
                'description': self.description
            }
        )",https://github.com/ic-labs/django-icekit/blob/c507ea5b1864303732c53ad7c5800571fca5fa94/icekit/middleware.py#L119-L136
tfidf_python_100_1.0,how to make the checkbox checked,python,"def on_event_pre(self, e: Event) -> None:
        """"""Set values set on browser before calling event listeners.""""""
        super().on_event_pre(e)
        ct_msg = e.init.get('currentTarget', dict())
        if e.type in ('input', 'change'):
            # Update user inputs
            if self.type.lower() == 'checkbox':
                self._set_attribute('checked', ct_msg.get('checked'))
            elif self.type.lower() == 'radio':
                self._set_attribute('checked', ct_msg.get('checked'))
                for other in self._find_grouped_nodes():
                    if other is not self:
                        other._remove_attribute('checked')
            else:
                self._set_attribute('value', ct_msg.get('value'))",https://github.com/miyakogi/wdom/blob/a21bcd23e94baceee71161829f6897bee3fd39c1/wdom/element.py#L859-L873
tfidf_python_100_1.0,how to make the checkbox checked,python,"def __init__(self, checked=False, user_data='', **kwargs):
        """"""
        Args:
            checked (bool):
            user_data (str):
            kwargs: See Widget.__init__()
        """"""
        super(CheckBox, self).__init__('checkbox', user_data, **kwargs)
        self.set_value(checked)
        self.attributes[Widget.EVENT_ONCHANGE] = \
            ""var params={};params['value']=document.getElementById('%(emitter_identifier)s').checked;"" \
            ""sendCallbackParam('%(emitter_identifier)s','%(event_name)s',params);""% \
            {'emitter_identifier':str(self.identifier), 'event_name':Widget.EVENT_ONCHANGE}",https://github.com/dddomodossola/remi/blob/85206f62220662bb7ecd471042268def71ccad28/remi/gui.py#L2755-L2767
tfidf_python_100_1.0,how to make the checkbox checked,python,"def to_lookup(self, checked):
        if checked is None:
            return {}
        if self.revert:
            return {self.lookup: not checked}
        else:
            return {self.lookup: checked}",https://github.com/freevoid/django-datafilters/blob/99051b3b3e97946981c0e9697576b0100093287c/datafilters/specs/extra.py#L47-L53
tfidf_python_100_1.0,initializing array,python,"def initializing(self, initializing):
        validate_attribute(initializing, 'rts_profile.Initializing',
                           expected_type=Initialize, required=False)
        self._initializing = initializing",https://github.com/gbiggs/rtsprofile/blob/fded6eddcb0b25fe9808b1b12336a4413ea00905/rtsprofile/rts_profile.py#L500-L503
tfidf_python_100_1.0,initializing array,python,"def _plot_by_priority(self, priority, fmtos, initializing=False):
        def update(fmto):
            other_fmto = self._shared.get(fmto.key)
            if other_fmto:
                self.logger.debug(""%s is shared with %s"", fmto.key,
                                  other_fmto.plotter.logger.name)
                other_fmto.share(fmto, initializing=initializing)
            # but if not, share them
            else:
                if initializing:
                    self.logger.debug(""Initializing %s"", fmto.key)
                    fmto.initialize_plot(fmto.value)
                else:
                    self.logger.debug(""Updating %s"", fmto.key)
                    fmto.update(fmto.value)
            try:
                fmto.lock.release()
            except RuntimeError:
                pass

        self._initializing = initializing

        self.logger.debug(
            ""%s formatoptions with priority %i"",
            ""Initializing"" if initializing else ""Updating"", priority)

        if priority >= START or priority == END:
            for fmto in fmtos:
                update(fmto)
        elif priority == BEFOREPLOTTING:
            for fmto in fmtos:
                update(fmto)
            self._make_plot()

        self._initializing = False",https://github.com/Chilipp/psyplot/blob/75a0a15a9a1dd018e79d2df270d56c4bf5f311d5/psyplot/plotter.py#L1529-L1563
tfidf_python_100_1.0,initializing array,python,"def underlying_variable(self):
        array = self
        while not isinstance(array, ArrayVariable):
            array = array.array
        return array",https://github.com/trailofbits/manticore/blob/54c5a15b1119c523ae54c09972413e8b97f11629/manticore/core/smtlib/expression.py#L687-L691
tfidf_python_100_1.0,initializing array,python,"def reverse(array, i, j):
    while i < j:
        array[i], array[j] = array[j], array[i]
        i += 1
        j -= 1",https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/strings/reverse_words.py#L2-L6
tfidf_python_100_1.0,initializing array,python,"def tinyify(array, tiny_mode, small_mode):
  if tiny_mode:
    return [1 for _ in array]
  if small_mode:
    return [max(x // 4, 1) for x in array]
  return array",https://github.com/tensorflow/tensor2tensor/blob/272500b6efe353aeb638d2745ed56e519462ca31/tensor2tensor/layers/common_video.py#L502-L507
tfidf_python_100_1.0,initializing array,python,"def _maybe_to_sparse(array):
    """"""
    array must be SparseSeries or SparseArray
    """"""
    if isinstance(array, ABCSparseSeries):
        array = array.values.copy()
    return array",https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/arrays/sparse.py#L1803-L1809
tfidf_python_100_1.0,initializing array,python,"def ensure_array(array):
    """"""
    Assert that the given array is an Array subclass (or numpy array).

    If the given array is a numpy.ndarray an appropriate NumpyArrayAdapter
    instance is created, otherwise the passed array must be a subclass of
    :class:`Array` else a TypeError will be raised.

    """"""
    if not isinstance(array, Array):
        if isinstance(array, np.ndarray):
            array = NumpyArrayAdapter(array)
        elif np.isscalar(array):
            array = ConstantArray([], array)
        else:
            raise TypeError('The given array should be a `biggus.Array` '
                            'instance, got {}.'.format(type(array)))
    return array",https://github.com/SciTools/biggus/blob/0a76fbe7806dd6295081cd399bcb76135d834d25/biggus/_init.py#L3289-L3306
tfidf_python_100_1.0,initializing array,python,"def lcm(array):
        if len(array) == 0:
                return 0
        if len(array) == 1:
                return array[0]
        elif len(array) == 2:
                return array[0] * array[1] / gcd(array)
        else:
                return lcm([lcm(array[:len(array) / 2]), lcm(array[len(array) / 2:])])",https://github.com/ioguntol/AMP/blob/c227806bdd851b8c2d78afe0aedab43395705795/AMP/funcEval.py#L44-L52
tfidf_python_100_1.0,initializing array,python,"def get_crc_datarange(self, inpt, vrfy_crc):
        return c_util.get_crc_datarange(array.array(""B"", inpt),
                            array.array(""B"", self.polynomial),
                            array.array(""B"", vrfy_crc),
                            array.array(""B"", self.start_value),
                            array.array(""B"", self.final_xor),
                            self.lsb_first, self.reverse_polynomial, self.reverse_all, self.little_endian)",https://github.com/jopohl/urh/blob/2eb33b125c8407964cd1092843cde5010eb88aae/src/urh/util/GenericCRC.py#L104-L110
tfidf_python_100_1.0,initializing array,python,"def __init__(self, array, indexer_cls=BasicIndexer):
        self.array = as_indexable(array)
        self.indexer_cls = indexer_cls",https://github.com/pydata/xarray/blob/6d93a95d05bdbfc33fff24064f67d29dd891ab58/xarray/core/indexing.py#L447-L449
tfidf_python_100_1.0,initializing array,python,"def __init__(self, array, lastPos=0):
        self.array = array
        self.lastPos = lastPos",https://github.com/radjkarl/fancyTools/blob/4c4d961003dc4ed6e46429a0c24f7e2bb52caa8b/fancytools/math/nearestPosition2.py#L24-L26
tfidf_python_100_1.0,initializing array,python,"def string2bits(bit_str: str) -> array.array:
    return array.array(""B"", map(int, bit_str))",https://github.com/jopohl/urh/blob/2eb33b125c8407964cd1092843cde5010eb88aae/src/urh/util/util.py#L231-L232
tfidf_python_100_1.0,initializing array,python,"def three_sum(array):
    """"""
    :param array: List[int]
    :return: Set[ Tuple[int, int, int] ]
    """"""
    res = set()
    array.sort()
    for i in range(len(array) - 2):
        if i > 0 and array[i] == array[i - 1]:
            continue
        l, r = i + 1, len(array) - 1
        while l < r:
            s = array[i] + array[l] + array[r]
            if s > 0:
                r -= 1
            elif s < 0:
                l += 1
            else:
                # found three sum
                res.add((array[i], array[l], array[r]))

                # remove duplicates
                while l < r and array[l] == array[l + 1]:
                    l += 1

                while l < r and array[r] == array[r - 1]:
                    r -= 1

                l += 1
                r -= 1
    return res",https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/arrays/three_sum.py#L18-L48
tfidf_python_100_1.0,initializing array,python,"def __init__(self, array):
        self.array = as_indexable(array)
        self._copied = False",https://github.com/pydata/xarray/blob/6d93a95d05bdbfc33fff24064f67d29dd891ab58/xarray/core/indexing.py#L594-L596
tfidf_python_100_1.0,initializing array,python,"def _with_attrs(array, **kwargs):
    array = AttrArray(array)
    for k, v in kwargs.items():
        setattr(array, k, v)
    return array",https://github.com/anntzer/mplcursors/blob/a4bce17a978162b5a1837cc419114c910e7992f9/lib/mplcursors/_pick_info.py#L105-L109
tfidf_python_100_1.0,initializing array,python,"def __init__(self, array: Array):
        super(ArrayWrapper, self).__init__(array)
        self.enumerator = array.GetEnumerator()",https://github.com/CityOfZion/neo-python/blob/fe90f62e123d720d4281c79af0598d9df9e776fb/neo/SmartContract/Iterable/Wrapper.py#L7-L9
tfidf_python_100_1.0,initializing array,python,"def print_r(array):
    if not isinstance(array, dict):
        array = dict([(k, k) for k in array])
    for k, v in array.items():
        print(""[%s] => %s "" % (k, v))",https://github.com/m32/endesive/blob/973091dc69847fe2df594c80ac9235a8d08460ff/endesive/pdf/fpdf/php.py#L15-L19
tfidf_python_100_1.0,initializing array,python,"def calculate_entropy(array, norm=False):
    array = array/array.sum()

    if norm:
        array = array - array.min()
        array = array/array.sum()

    entropy = sum(-np.multiply(np.log(array[array > 0.0]), array[array > 0.0]))
    return entropy",https://github.com/MKLab-ITI/reveal-graph-embedding/blob/eda862687aa5a64b79c6b12de1b4dca6ce986dc8/reveal_graph_embedding/embedding/implicit.py#L14-L22
tfidf_python_100_1.0,initializing array,python,"def __init__(self, array: List[int]) -> None:
        assert len(array) >= 781
        self.array = array",https://github.com/niklasf/python-chess/blob/d91f986ca3e046b300a0d7d9ee2a13b07610fe1a/chess/polyglot.py#L238-L240
tfidf_python_100_1.0,initializing array,python,"def max_subarray(array):
    max_so_far = max_now = array[0]
    for i in range(1, len(array)):
        max_now = max(array[i], max_now + array[i])
        max_so_far = max(max_so_far, max_now)
    return max_so_far",https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/dp/max_subarray.py#L2-L7
tfidf_python_100_1.0,how to reverse a string,python,"def cloud_cover_to_irradiance(self, cloud_cover, how='clearsky_scaling',
                                  **kwargs):
        """"""
        Convert cloud cover to irradiance. A wrapper method.

        Parameters
        ----------
        cloud_cover : Series
        how : str, default 'clearsky_scaling'
            Selects the method for conversion. Can be one of
            clearsky_scaling or liujordan.
        **kwargs
            Passed to the selected method.

        Returns
        -------
        irradiance : DataFrame
            Columns include ghi, dni, dhi
        """"""

        how = how.lower()
        if how == 'clearsky_scaling':
            irrads = self.cloud_cover_to_irradiance_clearsky_scaling(
                cloud_cover, **kwargs)
        elif how == 'liujordan':
            irrads = self.cloud_cover_to_irradiance_liujordan(
                cloud_cover, **kwargs)
        else:
            raise ValueError('invalid how argument')

        return irrads",https://github.com/pvlib/pvlib-python/blob/2e844a595b820b43d1170269781fa66bd0ccc8a3/pvlib/forecast.py#L539-L569
tfidf_python_100_1.0,how to reverse a string,python,"def execute_arbitrary_series_groupby(op, data, _, aggcontext=None, **kwargs):
    how = op.how
    if how is None:
        how = 'first'

    if how not in {'first', 'last'}:
        raise com.OperationNotDefinedError(
            'Arbitrary {!r} is not supported'.format(how)
        )
    return aggcontext.agg(data, how)",https://github.com/ibis-project/ibis/blob/1e39a5fd9ef088b45c155e8a5f541767ee8ef2e7/ibis/pandas/execution/generic.py#L451-L460
tfidf_python_100_1.0,how to reverse a string,python,"def run(reverse):
    if reverse:
        do_more_random_stuff()
        do_random_stuff()
        do_random_stuff()
    else:
        do_random_stuff()
        do_random_stuff()
        do_more_random_stuff()

    do_random_stuff()",https://github.com/IDSIA/sacred/blob/72633776bed9b5bddf93ae7d215188e61970973a/examples/06_randomness.py#L76-L86
tfidf_python_100_1.0,how to reverse a string,python,"def depgraph_to_dotsrc(dep_graph, show_cycles, nodot, reverse):
    """"""Convert the dependency graph (DepGraph class) to dot source code.
    """"""
    if show_cycles:
        dotsrc = cycles2dot(dep_graph, reverse=reverse)
    elif not nodot:
        dotsrc = dep2dot(dep_graph, reverse=reverse)
    else:
        dotsrc = None
    return dotsrc",https://github.com/thebjorn/pydeps/blob/1e6715b7bea47a40e8042821b57937deaaa0fdc3/pydeps/pydeps.py#L54-L63
tfidf_python_100_1.0,how to reverse a string,python,"def mycarta(color_name, reverse=False):
    return pc.mycarta.get_map(color_name, reverse=reverse)",https://github.com/wcchin/colouringmap/blob/3a34ab564e8e0a361fb25809782de7fdf4728f3d/colouringmap/get_colours.py#L101-L102
tfidf_python_100_1.0,how to reverse a string,python,"def tableau(color_name, reverse=False):
    return pc.tableau.get_map(color_name, reverse=reverse)",https://github.com/wcchin/colouringmap/blob/3a34ab564e8e0a361fb25809782de7fdf4728f3d/colouringmap/get_colours.py#L104-L105
tfidf_python_100_1.0,how to reverse a string,python,"def cmocean_diverging(color_name, reverse=False):
    return pc.cmocean.diverging.get_map(color_name, reverse=reverse)",https://github.com/wcchin/colouringmap/blob/3a34ab564e8e0a361fb25809782de7fdf4728f3d/colouringmap/get_colours.py#L77-L78
tfidf_python_100_1.0,how to reverse a string,python,"def wesanderson(color_name, reverse=False):
    return pc.wesanderson.get_map(color_name, reverse=reverse)",https://github.com/wcchin/colouringmap/blob/3a34ab564e8e0a361fb25809782de7fdf4728f3d/colouringmap/get_colours.py#L107-L108
tfidf_python_100_1.0,how to reverse a string,python,"def cubehelix(color_name, reverse=False):
    return pc.cubehelix.get_map(color_name, reverse=reverse)",https://github.com/wcchin/colouringmap/blob/3a34ab564e8e0a361fb25809782de7fdf4728f3d/colouringmap/get_colours.py#L95-L96
tfidf_python_100_1.0,how to reverse a string,python,"def reverse(self, tvsubtitles):
        if tvsubtitles in self.from_tvsubtitles:
            return self.from_tvsubtitles[tvsubtitles]

        return self.alpha2_converter.reverse(tvsubtitles)",https://github.com/Diaoul/subliminal/blob/a952dfb2032eb0fd6eb1eb89f04080923c11c4cf/subliminal/converters/tvsubtitles.py#L21-L25
tfidf_python_100_1.0,how to reverse a string,python,"def reverse(self, addic7ed):
        if addic7ed in self.from_addic7ed:
            return self.from_addic7ed[addic7ed]

        return self.name_converter.reverse(addic7ed)",https://github.com/Diaoul/subliminal/blob/a952dfb2032eb0fd6eb1eb89f04080923c11c4cf/subliminal/converters/addic7ed.py#L28-L32
tfidf_python_100_1.0,how to reverse a string,python,"def time_i8merge(self, how):
        merge(self.left, self.right, how=how)",https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/asv_bench/benchmarks/join_merge.py#L213-L214
tfidf_python_100_1.0,how to reverse a string,python,"def Parse(self, how):
        '''Parse the message.
        '''
        if type(how) == types.ClassType: how = how.typecode
        return how.parse(self.body_root, self)",https://github.com/rameshg87/pyremotevbox/blob/123dffff27da57c8faa3ac1dd4c68b1cf4558b1a/pyremotevbox/ZSI/parse.py#L322-L326
tfidf_python_100_1.0,how to reverse a string,python,"def iterator(self, with_scores=False, reverse=False):
        if with_scores and not reverse:
            return self.search(None)
        return self.range(
            0,
            -1,
            with_scores=with_scores,
            reverse=reverse)",https://github.com/coleifer/walrus/blob/82bf15a6613487b5b5fefeb488f186d7e0106547/walrus/containers.py#L655-L662
tfidf_python_100_1.0,how to reverse a string,python,"def sort(self, reverse=False):
        self.rolls = sorted(self.rolls, reverse=reverse)",https://github.com/StarlitGhost/pyhedrals/blob/74b3a48ecc2b73a27ded913e4152273cd5ba9cc7/pyhedrals/pyhedrals.py#L59-L60
tfidf_python_100_1.0,how to reverse a string,python,"def items(self, reverse=None):
        """""" @reverse: #bool True to return revranked scores
            -> yields |(member, score)| #tuple pairs in the sorted set
        """"""
        reverse = reverse if reverse is not None else self.reversed
        for member in self.iter(withscores=True, reverse=reverse):
            yield member",https://github.com/jaredLunde/redis_structures/blob/b9cce5f5c85db5e12c292633ff8d04e3ae053294/redis_structures/__init__.py#L2231-L2237
tfidf_python_100_1.0,how to reverse a string,python,"def _get_sorting_message(request, key):
        """"""Parses the reverse query into a list of ClientSortControls protobuf
        messages.
        """"""
        control_list = []
        reverse = request.url.query.get('reverse', None)
        if reverse is None:
            return control_list

        if reverse.lower() == """":
            control_list.append(client_list_control_pb2.ClientSortControls(
                reverse=True,
                keys=key.split("","")
            ))
        elif reverse.lower() != 'false':
            control_list.append(client_list_control_pb2.ClientSortControls(
                reverse=True,
                keys=reverse.split("","")
            ))

        return control_list",https://github.com/hyperledger/sawtooth-core/blob/8cf473bc2207e51f02bd182d825158a57d72b098/rest_api/sawtooth_rest_api/route_handlers.py#L940-L960
tfidf_python_100_1.0,how to reverse a string,python,"def reverse(self):
        """""" Reverse all bumpers """"""
        if not self.test_drive and self.bumps:
            map(lambda b: b.reverse(), self.bumpers)",https://github.com/maxzheng/bumper-lib/blob/32a9dec5448673825bb2d7d92fa68882b597f794/bumper/__init__.py#L199-L202
tfidf_python_100_1.0,how to reverse a string,python,"def values(self, reverse=None):
        """""" @reverse: #bool True to return revranked scores
            -> yields :prop:cast scores in the sorted set
        """"""
        reverse = reverse if reverse is not None else self.reversed
        for member, score in self.items(reverse=reverse):
            yield self.cast(score)",https://github.com/jaredLunde/redis_structures/blob/b9cce5f5c85db5e12c292633ff8d04e3ae053294/redis_structures/__init__.py#L2223-L2229
tfidf_python_100_1.0,how to reverse a string,python,"def set_mode(how):
    """""" Sets the behavior of the API

    :param how: if 'remote' all the execution is performed on the remote server; if 'local' all
           it is executed locally. Default = 'local'
    :return: None
    """"""
    global __mode
    if how == ""local"":
        __mode = how
    elif how == ""remote"":
        __mode = how
    else:
        raise ValueError(""how must be 'local' or 'remote'"")",https://github.com/DEIB-GECO/PyGMQL/blob/e58b2f9402a86056dcda484a32e3de0bb06ed991/gmql/settings.py#L79-L92
tfidf_python_100_1.0,read properties file,python,"def read_from_dict(self, properties):
        super().read_from_dict(properties)
        self.__properties = properties.get(""properties"")",https://github.com/nion-software/nionswift/blob/d43693eaf057b8683b9638e575000f055fede452/nion/swift/model/DataStructure.py#L96-L98
tfidf_python_100_1.0,read properties file,python,"def get_interactive_session_properties(self):
        properties = {}
        telemetry_core.set_custom_properties(properties, 'StartTime', str(self.interactive_start_time))
        telemetry_core.set_custom_properties(properties, 'EndTime', str(self.interactive_end_time))
        telemetry_core.set_custom_properties(properties, 'OutsideGestures', self.tracked_data['outside_gesture'])
        telemetry_core.set_custom_properties(properties, 'ExitGestures', self.tracked_data['exit_code_gesture'])
        telemetry_core.set_custom_properties(properties, 'QueryGestures', self.tracked_data['query_gesture'])
        telemetry_core.set_custom_properties(properties, 'ScopeChanges', self.tracked_data['scope_changes'])
        telemetry_core.set_custom_properties(properties, 'TutorialRuns', self.tracked_data['ran_tutorial'])
        telemetry_core.set_custom_properties(properties, 'ExampleScrollingActions',
                                             self.tracked_data['scroll_examples'])
        telemetry_core.set_custom_properties(properties, 'ConfigurationChanges', self.tracked_data['open_config'])
        telemetry_core.set_custom_properties(properties, 'DefaultToggles', self.tracked_data['toggle_default'])
        telemetry_core.set_custom_properties(properties, 'SymbolBindingToggles',
                                             self.tracked_data['toggle_symbol_bindings'])
        telemetry_core.set_custom_properties(properties, 'CliCommandsUsed', self.tracked_data['cli_commands_used'])
        return properties",https://github.com/Azure/azure-cli-extensions/blob/3d4854205b0f0d882f688cfa12383d14506c2e35/src/interactive/azext_interactive/azclishell/telemetry.py#L31-L47
tfidf_python_100_1.0,read properties file,python,"def __init__(self, **properties):
        for k in properties.keys():
            properties[k] = coerceoption(properties[k])
        _Struct.__init__(self, **properties)",https://github.com/mesosphere/deimos/blob/b4deead93b6e2ddf4e4a42e33777755942da0b7a/deimos/config.py#L154-L157
tfidf_python_100_1.0,read properties file,python,"def restore_properties(self, properties: typing.Tuple) -> None:
        self.complex_display_type = properties[0]
        self.display_limits = properties[1]
        self.color_map_id = properties[2]
        self.sequence_index = properties[3]
        self.collection_index = properties[4]
        self.slice_center = properties[5]
        self.slice_interval = properties[6]",https://github.com/nion-software/nionswift/blob/d43693eaf057b8683b9638e575000f055fede452/nion/swift/model/DisplayItem.py#L695-L702
tfidf_python_100_1.0,read properties file,python,"def merge_properties(properties, new_properties):
	for p in new_properties:
		if properties.get(p, None) is None:
			properties[p] = new_properties[p]",https://github.com/cf-platform-eng/tile-generator/blob/56b602334edb38639bc7e01b1e9e68e43f9e6828/tile_generator/erb.py#L104-L107
tfidf_python_100_1.0,read properties file,python,"def GetProperties(arguments, propertyType, errorInfoMsg, yamlData):
    properties = {}
    if propertyType in yamlData:
        properties = yamlData[propertyType]
    return properties",https://github.com/DIPSAS/SwarmManagement/blob/c9ef1165b240c145d42e2d363925c8200fc19f43/SwarmManagement/SwarmTools.py#L163-L167
tfidf_python_100_1.0,read properties file,python,"def get_all_properties(self):
        properties = self.properties.copy()

        for subschema in self.all_of:
            subschema_props = subschema.get_all_properties()
            properties.update(subschema_props)

        return properties",https://github.com/p1c2u/openapi-core/blob/f274836c4dd45729b1634aff8758c63323173947/openapi_core/schema/schemas/models.py#L109-L116
tfidf_python_100_1.0,read properties file,python,"def __init__(self, fieldSchemas=None, properties=None,):
    self.fieldSchemas = fieldSchemas
    self.properties = properties",https://github.com/cloudera/impyla/blob/547fa2ba3b6151e2a98b3544301471a643212dc3/impala/_thrift_gen/hive_metastore/ttypes.py#L5845-L5847
tfidf_python_100_1.0,read properties file,python,"def write_to_dict(self):
        properties = super().write_to_dict()
        properties[""properties""] = copy.deepcopy(self.__properties)
        return properties",https://github.com/nion-software/nionswift/blob/d43693eaf057b8683b9638e575000f055fede452/nion/swift/model/DataStructure.py#L100-L103
tfidf_python_100_1.0,read properties file,python,"def migrate_to_v2(reader_info_list):
    for reader_info in reader_info_list:
        storage_handler = reader_info.storage_handler
        properties = reader_info.properties
        try:
            version = properties.get(""version"", 0)
            if version <= 1:
                if ""spatial_calibrations"" in properties:
                    properties[""intrinsic_spatial_calibrations""] = properties[""spatial_calibrations""]
                    del properties[""spatial_calibrations""]
                if ""intensity_calibration"" in properties:
                    properties[""intrinsic_intensity_calibration""] = properties[""intensity_calibration""]
                    del properties[""intensity_calibration""]
                if ""data_source_uuid"" in properties:
                    # for now, this is not translated into v2. it was an extra item.
                    del properties[""data_source_uuid""]
                if ""properties"" in properties:
                    old_properties = properties[""properties""]
                    new_properties = properties.setdefault(""hardware_source"", dict())
                    new_properties.update(copy.deepcopy(old_properties))
                    if ""session_uuid"" in new_properties:
                        del new_properties[""session_uuid""]
                    del properties[""properties""]
                temp_data = storage_handler.read_data()
                if temp_data is not None:
                    properties[""master_data_dtype""] = str(temp_data.dtype)
                    properties[""master_data_shape""] = temp_data.shape
                properties[""displays""] = [{}]
                properties[""uuid""] = properties.get(""uuid"", str(uuid.uuid4()))  # assign a new uuid if it doesn't exist
                properties[""version""] = 2
                logging.getLogger(""migration"").debug(""Updated {} to {} (ndata1)"".format(storage_handler.reference, properties[""version""]))
        except Exception as e:
            logging.debug(""Error reading %s"", storage_handler.reference)
            import traceback
            traceback.print_exc()
            traceback.print_stack()",https://github.com/nion-software/nionswift/blob/d43693eaf057b8683b9638e575000f055fede452/nion/swift/model/Migration.py#L704-L739
tfidf_python_100_1.0,read properties file,python,"def all(cls, client, properties=None):
        if properties is None:
            properties = []

        if ""name"" not in properties:
            properties.append(""name"")

        return client.find_entity_views(cls.__name__, properties=properties)",https://github.com/psphere-project/psphere/blob/83a252e037c3d6e4f18bcd37380998bc9535e591/psphere/managedobjects.py#L107-L114
tfidf_python_100_1.0,read properties file,python,"def set_properties(self, properties):
        """"""Sets properties and text of this info from a dictionary""""""
        if isinstance(properties, dict):
            self.properties = properties
            self.sync_properties()
        else:
            self.text = properties",https://github.com/svinota/mdns/blob/295f6407132616a0ff7401124b9057d89555f91d/mdns/zeroconf.py#L1399-L1405
tfidf_python_100_1.0,read properties file,python,"def _get_wmi_sampler(self, instance_key, wmi_class, properties, tag_by="""", **kwargs):
        """"""
        Create and cache a WMISampler for the given (class, properties)
        """"""
        properties = list(properties) + [tag_by] if tag_by else list(properties)

        if instance_key not in self.wmi_samplers:
            wmi_sampler = WMISampler(self.log, wmi_class, properties, **kwargs)
            self.wmi_samplers[instance_key] = wmi_sampler

        return self.wmi_samplers[instance_key]",https://github.com/DataDog/integrations-core/blob/ebd41c873cf9f97a8c51bf9459bc6a7536af8acd/datadog_checks_base/datadog_checks/base/checks/win/wmi/__init__.py#L237-L247
tfidf_python_100_1.0,read properties file,python,"def __init__(self, properties):
        super(GenericTexture, self).__init__()
        self.properties = properties",https://github.com/rdeits/meshcat-python/blob/aa3865143120f5ace8e62aab71d825e33674d277/src/meshcat/geometry.py#L193-L195
tfidf_python_100_1.0,read properties file,python,"def data(self):
        properties = self.properties
        return self.properties.data if properties else UNDEFINED",https://github.com/pasztorpisti/py-flags/blob/bc48adb5edd7340ea1a686622d7993b4bcf4bfc2/src/flags.py#L607-L609
tfidf_python_100_1.0,read properties file,python,"def __init__(self, properties=None):
        super(DataProviderContext, self).__init__()
        self.properties = properties",https://github.com/Microsoft/azure-devops-python-api/blob/4777ffda2f5052fabbaddb2abe9cb434e0cf1aa8/azure-devops/azure/devops/v5_0/contributions/models.py#L280-L282
tfidf_python_100_1.0,read properties file,python,"def properties(self):
        """"""Return various properties of the binary tree.

        :return: Binary tree properties.
        :rtype: dict

        **Example**:

        .. doctest::

            >>> from binarytree import Node
            >>>
            >>> root = Node(1)
            >>> root.left = Node(2)
            >>> root.right = Node(3)
            >>> root.left.left = Node(4)
            >>> root.left.right = Node(5)
            >>> props = root.properties
            >>>
            >>> props['height']         # equivalent to root.height
            2
            >>> props['size']           # equivalent to root.size
            5
            >>> props['max_leaf_depth'] # equivalent to root.max_leaf_depth
            2
            >>> props['min_leaf_depth'] # equivalent to root.min_leaf_depth
            1
            >>> props['max_node_value'] # equivalent to root.max_node_value
            5
            >>> props['min_node_value'] # equivalent to root.min_node_value
            1
            >>> props['leaf_count']     # equivalent to root.leaf_count
            3
            >>> props['is_balanced']    # equivalent to root.is_balanced
            True
            >>> props['is_bst']         # equivalent to root.is_bst
            False
            >>> props['is_complete']    # equivalent to root.is_complete
            True
            >>> props['is_max_heap']    # equivalent to root.is_max_heap
            False
            >>> props['is_min_heap']    # equivalent to root.is_min_heap
            True
            >>> props['is_perfect']     # equivalent to root.is_perfect
            False
            >>> props['is_strict']      # equivalent to root.is_strict
            True
        """"""
        properties = _get_tree_properties(self)
        properties.update({
            'is_bst': _is_bst(self),
            'is_balanced': _is_balanced(self) >= 0
        })
        return properties",https://github.com/joowani/binarytree/blob/23cb6f1e60e66b96133259031e97ec03e932ba13/binarytree/__init__.py#L1388-L1441
tfidf_python_100_1.0,read properties file,python,"def __init__(self, parent=None, properties=None):
        """"""
        Initiaize placeable.

        :parent: is a parent :Placeable: or :None:
        :properties: is a :dict: that will be stored within
        self.placeable
        """"""

        # For performance optimization reasons we are tracking children
        # by name and by reference
        self.children_by_name = OrderedDict()
        self.children_by_reference = OrderedDict()
        self._coordinates = Vector(0, 0, 0)

        self.parent = parent

        if properties is None:
            properties = {}

        self.properties = properties

        if 'radius' in properties:
            properties['width'] = properties['radius'] * 2
            properties['length'] = properties['radius'] * 2

        if 'diameter' in properties:
            properties['width'] = properties['diameter']
            properties['length'] = properties['diameter']

        if 'depth' in properties:
            properties['height'] = properties['depth']

        for dimension in ['length', 'width', 'height']:
            if dimension not in properties:
                properties[dimension] = 0",https://github.com/Opentrons/opentrons/blob/a7c15cc2636ecb64ab56c7edc1d8a57163aaeadf/api/src/opentrons/legacy_api/containers/placeable.py#L92-L127
tfidf_python_100_1.0,read properties file,python,"def properties_cache(self):
        properties = dict(self.o)
        for name in self.RESERVED_ATTRIBUTE_NAMES:
            properties[name] = None
            del properties[name]
        return properties",https://github.com/wharris/dougrain/blob/45062a1562fc34793e40c6253a93aa91eb4cf855/dougrain/document.py#L276-L281
tfidf_python_100_1.0,read properties file,python,"def _get_alias_transformation_properties(self):
        properties = dict()
        self.set_custom_properties(properties, 'StartTime', str(self.start_time))
        self.set_custom_properties(properties, 'EndTime', str(self.end_time))
        self.set_custom_properties(properties, 'Version', VERSION)
        self.set_custom_properties(properties, 'ExecutionTimeMs', self.execution_time)
        self.set_custom_properties(properties, 'FullCommandTableLoaded', str(self.full_command_table_loaded))
        self.set_custom_properties(properties, 'CollidedAliases', ','.join(self.collided_aliases))
        self.set_custom_properties(properties, 'AliasesHit', ','.join(self.aliases_hit))
        self.set_custom_properties(properties, 'NumberOfAliasRegistered', self.number_of_aliases_registered)
        self.set_custom_properties(properties, 'ActionType', 'Transformation')

        return properties",https://github.com/Azure/azure-cli-extensions/blob/3d4854205b0f0d882f688cfa12383d14506c2e35/src/alias/azext_alias/telemetry.py#L59-L71
tfidf_python_100_1.0,copy to clipboard,python,"def copy(self):
        """"""Copy text to clipboard""""""
        cliptxt = self._sel_to_text( self.selectedIndexes() )
        clipboard = QApplication.clipboard()
        clipboard.setText(cliptxt)",https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/variableexplorer/widgets/arrayeditor.py#L526-L530
tfidf_python_100_1.0,copy to clipboard,python,"def paste(self):
        """"""Import text/data/code from clipboard""""""
        clipboard = QApplication.clipboard()
        cliptext = ''
        if clipboard.mimeData().hasText():
            cliptext = to_text_string(clipboard.text())
        if cliptext.strip():
            self.import_from_string(cliptext, title=_(""Import from clipboard""))
        else:
            QMessageBox.warning(self, _( ""Empty clipboard""),
                                _(""Nothing to be imported from clipboard.""))",https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/variableexplorer/widgets/collectionseditor.py#L1242-L1252
tfidf_python_100_1.0,copy to clipboard,python,"def create_clipboard(self, text, selection=Gdk.SELECTION_CLIPBOARD):
        """"""
        Function creates a clipboard
        """"""
        clipboard = Gtk.Clipboard.get(selection)
        clipboard.set_text('\n'.join(text), -1)
        clipboard.store()
        return clipboard",https://github.com/devassistant/devassistant/blob/2dbfeaa666a64127263664d18969c55d19ecc83e/devassistant/gui/gui_helper.py#L466-L473
tfidf_python_100_1.0,copy to clipboard,python,"def copyFilepath( self ):
        """"""
        Copies the current filepath contents to the current clipboard.
        """"""
        clipboard = QApplication.instance().clipboard()
        clipboard.setText(self.filepath())
        clipboard.setText(self.filepath(), clipboard.Selection)",https://github.com/bitesofcode/projexui/blob/f18a73bec84df90b034ca69b9deea118dbedfc4d/projexui/widgets/xfilepathedit.py#L136-L142
tfidf_python_100_1.0,copy to clipboard,python,"def update_clipboard(self, text):
        '''å°ææ¬å¤å¶å°ç³»ç»åªè´´æ¿éé¢'''
        clipboard = Gtk.Clipboard.get(Gdk.SELECTION_CLIPBOARD)
        clipboard.set_text(text, -1)
        self.toast(_('{0} copied to clipboard'.format(text)))",https://github.com/XuShaohua/bcloud/blob/4b54e0fdccf2b3013285fef05c97354cfa31697b/bcloud/App.py#L492-L496
tfidf_python_100_1.0,copy to clipboard,python,"def copy_request_to_clipboard(self):
        txt = "" "".join(self.context_model.request)
        clipboard = app.clipboard()
        clipboard.setText(txt)
        with app.status(""Copied request to clipboard""):
            pass",https://github.com/nerdvegas/rez/blob/1d3b846d53b5b5404edfe8ddb9083f9ceec8c5e7/src/rezgui/windows/ContextSubWindow.py#L103-L108
tfidf_python_100_1.0,copy to clipboard,python,"def copy_to_clipboard(self, copy=True):
        """"""
        Copies the selected items to the clipboard
        :param copy: True to copy, False to cut.
        """"""
        urls = self.selected_urls()
        if not urls:
            return
        mime = self._UrlListMimeData(copy)
        mime.set_list(urls)
        clipboard = QtWidgets.QApplication.clipboard()
        clipboard.setMimeData(mime)",https://github.com/pyQode/pyqode.core/blob/a99ec6cd22d519394f613309412f8329dc4e90cb/pyqode/core/widgets/filesystem_treeview.py#L347-L358
tfidf_python_100_1.0,copy to clipboard,python,"def get_clipboard(self):
        """"""Returns the clipboard content

        If a bitmap is contained then it is returned.
        Otherwise, the clipboard text is returned.

        """"""

        bmpdata = wx.BitmapDataObject()
        textdata = wx.TextDataObject()

        if self.clipboard.Open():
            is_bmp_present = self.clipboard.GetData(bmpdata)
            self.clipboard.GetData(textdata)
            self.clipboard.Close()
        else:
            wx.MessageBox(_(""Can't open the clipboard""), _(""Error""))

        if is_bmp_present:
            return bmpdata.GetBitmap()
        else:
            return textdata.GetText()",https://github.com/manns/pyspread/blob/0e2fd44c2e0f06605efc3058c20a43a8c1f9e7e0/pyspread/src/lib/clipboard.py#L74-L95
tfidf_python_100_1.0,copy to clipboard,python,"def __init__(self, resource_set=None, command_stack_class=CommandStack):
        self.resource_set = resource_set or ResourceSet()
        self.__stack = command_stack_class()
        self.clipboard = []",https://github.com/pyecore/pyecore/blob/22b67ad8799594f8f44fd8bee497583d4f12ed63/pyecore/commands.py#L342-L345
tfidf_python_100_1.0,copy to clipboard,python,"def get(cbname):
    """""" Get the contents of the given clipboard. """"""
    _checkTkInit()
    if cbname == 'PRIMARY':
        try:
            return _theRoot.selection_get(selection='PRIMARY')
        except:
            return None
    if cbname == 'CLIPBOARD':
        try:
            return _theRoot.selection_get(selection='CLIPBOARD')
        except:
            return None
    raise RuntimeError(""Unexpected clipboard name: ""+str(cbname))",https://github.com/spacetelescope/stsci.tools/blob/9a022503ad24ca54ce83331482dfa3ff6de9f403/lib/stsci/tools/clipboard_helper.py#L58-L71
tfidf_python_100_1.0,copy to clipboard,python,"def copy_figure(self):
        """"""Copy figure from figviewer to clipboard.""""""
        if self.figviewer and self.figviewer.figcanvas.fig:
            self.figviewer.figcanvas.copy_figure()",https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/plots/widgets/figurebrowser.py#L349-L352
tfidf_python_100_1.0,copy to clipboard,python,"def get_user_clipboard(user):
    if is_authenticated(user):
        clipboard = Clipboard.objects.get_or_create(user=user)[0]
        return clipboard",https://github.com/divio/django-filer/blob/946629087943d41eff290f07bfdf240b8853dd88/filer/models/tools.py#L17-L20
tfidf_python_100_1.0,copy to clipboard,python,"def set_clipboard(clipboard):
    '''
    Explicitly sets the clipboard mechanism. The ""clipboard mechanism"" is how
    the copy() and paste() functions interact with the operating system to
    implement the copy/paste feature. The clipboard parameter must be one of:
        - pbcopy
        - pbobjc (default on Mac OS X)
        - gtk
        - qt
        - xclip
        - xsel
        - klipper
        - windows (default on Windows)
        - no (this is what is set when no clipboard mechanism can be found)
    '''
    global copy, paste

    clipboard_types = {'pbcopy': init_osx_pbcopy_clipboard,
                       'pyobjc': init_osx_pyobjc_clipboard,
                       'gtk': init_gtk_clipboard,
                       'qt': init_qt_clipboard, # TODO - split this into 'qtpy', 'pyqt4', and 'pyqt5'
                       'xclip': init_xclip_clipboard,
                       'xsel': init_xsel_clipboard,
                       'klipper': init_klipper_clipboard,
                       'windows': init_windows_clipboard,
                       'no': init_no_clipboard}

    if clipboard not in clipboard_types:
        raise ValueError('Argument must be one of %s' % (', '.join([repr(_) for _ in clipboard_types.keys()])))

    # Sets pyperclip's copy() and paste() functions:
    copy, paste = clipboard_types[clipboard]()",https://github.com/asweigart/pyperclip/blob/09abcb63fa5a6ac9afa3b11def604b059b943b6b/src/pyperclip/__init__.py#L563-L594
tfidf_python_100_1.0,copy to clipboard,python,"def get_clipboard(self):
        """"""
        Read text from the clipboard
        
        Usage: C{clipboard.get_clipboard()}

        @return: text contents of the clipboard
        @rtype: C{str}
        @raise Exception: if no text was found on the clipboard
        """"""
        Gdk.threads_enter()
        text = self.clipBoard.wait_for_text()
        Gdk.threads_leave()
        if text is not None:
            return text
        else:
            raise Exception(""No text found on clipboard"")",https://github.com/autokey/autokey/blob/35decb72f286ce68cd2a1f09ace8891a520b58d1/lib/autokey/scripting.py#L874-L890
tfidf_python_100_1.0,copy to clipboard,python,"def __getClipboard(self):
        self.text = self.clipBoard.text(QClipboard.Clipboard)
        self.sem.release()",https://github.com/autokey/autokey/blob/35decb72f286ce68cd2a1f09ace8891a520b58d1/lib/autokey/scripting.py#L804-L806
tfidf_python_100_1.0,copy to clipboard,python,"def fill_clipboard(self, contents):
        """"""
        Copy text into the clipboard
        
        Usage: C{clipboard.fill_clipboard(contents)}
        
        @param contents: string to be placed in the selection
        """"""
        Gdk.threads_enter()
        if Gtk.get_major_version() >= 3:
            self.clipBoard.set_text(contents, -1)
        else:
            self.clipBoard.set_text(contents)
        Gdk.threads_leave()",https://github.com/autokey/autokey/blob/35decb72f286ce68cd2a1f09ace8891a520b58d1/lib/autokey/scripting.py#L859-L872
tfidf_python_100_1.0,copy to clipboard,python,"def paste_clipboard(self, event):
        """"""
        Send the clipboard content as user input to the CPU.
        """"""
        log.critical(""paste clipboard"")
        clipboard = self.root.clipboard_get()
        for line in clipboard.splitlines():
            log.critical(""paste line: %s"", repr(line))
            self.add_user_input(line + ""\r"")",https://github.com/jedie/DragonPy/blob/6659e5b5133aab26979a498ee7453495773a4f6c/dragonpy/core/gui.py#L219-L227
tfidf_python_100_1.0,copy to clipboard,python,"def set_clipboard(clipboard):
    global copy, paste

    clipboard_types = {'osx': init_osx_clipboard,
                       'gtk': init_gtk_clipboard,
                       'qt': init_qt_clipboard,
                       'xclip': init_xclip_clipboard,
                       'xsel': init_xsel_clipboard,
                       'klipper': init_klipper_clipboard,
                       'windows': init_windows_clipboard,
                       'no': init_no_clipboard}

    copy, paste = clipboard_types[clipboard]()",https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/io/clipboard/__init__.py#L103-L115
tfidf_python_100_1.0,copy to clipboard,python,"def __Copy_Report_pushButton__clicked(self, checked):
        """"""
        Defines the slot triggered by **Copy_Report_pushButton** Widget when clicked.

        :param checked: Checked state.
        :type checked: bool
        """"""

        clipboard = QApplication.clipboard()
        clipboard.setText(self.__view.page().mainFrame().toPlainText())",https://github.com/KelSolaar/Umbra/blob/66f45f08d9d723787f1191989f8b0dda84b412ce/umbra/reporter.py#L413-L422
tfidf_python_100_1.0,copy to clipboard,python,"def __fillClipboard(self, string):
        self.clipBoard.setText(string, QClipboard.Clipboard)
        self.sem.release()",https://github.com/autokey/autokey/blob/35decb72f286ce68cd2a1f09ace8891a520b58d1/lib/autokey/scripting.py#L788-L790
tfidf_python_100_1.0,convert html to pdf,python,"def render_pdf(html, stylesheets=None,
               download_filename=None, automatic_download=True):
    """"""Render a PDF to a response with the correct ``Content-Type`` header.

    :param html:
        Either a :class:`weasyprint.HTML` object or an URL to be passed
        to :func:`flask_weasyprint.HTML`. The latter case requires
        a request context.
    :param stylesheets:
        A list of user stylesheets, passed to
        :meth:`~weasyprint.HTML.write_pdf`
    :param download_filename:
        If provided, the ``Content-Disposition`` header is set so that most
        web browser will show the ""Save asâ¦"" dialog with the value as the
        default filename.
    :param automatic_download:
        If True, the browser will automatic download file.
    :returns: a :class:`flask.Response` object.

    """"""
    if not hasattr(html, 'write_pdf'):
        html = HTML(html)
    pdf = html.write_pdf(stylesheets=stylesheets)
    response = current_app.response_class(pdf, mimetype='application/pdf')
    if download_filename:
        if automatic_download:
            value = 'attachment'
        else:
            value = 'inline'
        response.headers.add('Content-Disposition', value,
                             filename=download_filename)
    return response",https://github.com/Kozea/Flask-WeasyPrint/blob/ec0403a2b96c10a09159f75409ecb63342ce7518/flask_weasyprint/__init__.py#L190-L221
tfidf_python_100_1.0,convert html to pdf,python,"def shift_time_subscripts(self, t):
        pdf = self.copy()
        pdf.Vars = shift_time_subscripts(pdf.Vars, t)
        pdf.Cond = shift_time_subscripts(pdf.Cond, t)
        pdf.Scope = shift_time_subscripts(pdf.Scope, t)
        pdf.Param = shift_time_subscripts(pdf.Param, t)
        return pdf",https://github.com/TheVinhLuong102/ProbabPy/blob/a67d39a15fb70ff9d5f6f2836e42264a97a0496e/ProbabPy/__init__.py#L161-L167
tfidf_python_100_1.0,convert html to pdf,python,"def __call__(self):
        html = super(PrintView, self).__call__()
        pdf = createPdf(html)
        filename = ""{}.pdf"".format(self.context.getId())
        return self.download(pdf, filename)",https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/browser/supplyorder.py#L162-L166
tfidf_python_100_1.0,convert html to pdf,python,"def gauss_neg_log_dens(pdf, var_and_param_names_and_values={}, **kw_var_and_param_names_and_values):
    var_and_param_names_and_values = combine_dict_and_kwargs(var_and_param_names_and_values,
                                                             kw_var_and_param_names_and_values)
    if not pdf.PreProcessed:
        pdf.preprocess()
    if pdf.LogDetCov is None:
        neg_log_dens = (pdf.NumDims * log(2 * pi) + log(det(pdf.Cov)) +
                        det(pdf.DemeanedVarVector * pdf.Cov.inverse() * pdf.DemeanedVarVector.T)) / 2
    else:
        neg_log_dens = (pdf.NumDims * log(2 * pi) + pdf.LogDetCov +
                        det(pdf.DemeanedVarVector * Matrix(pdf.InvCov) * pdf.DemeanedVarVector.T)) / 2
    return sympy_xreplace(neg_log_dens, var_and_param_names_and_values)",https://github.com/TheVinhLuong102/ProbabPy/blob/a67d39a15fb70ff9d5f6f2836e42264a97a0496e/ProbabPy/__init__.py#L408-L419
tfidf_python_100_1.0,convert html to pdf,python,"def put_line(self):
        self.pdf.ln(2)
        self.pdf.line(self.pdf.get_x(),self.pdf.get_y(),self.pdf.get_x()+187,self.pdf.get_y())
        self.pdf.ln(3)",https://github.com/m32/endesive/blob/973091dc69847fe2df594c80ac9235a8d08460ff/endesive/pdf/fpdf/html.py#L391-L394
tfidf_python_100_1.0,convert html to pdf,python,"def serve_pdf_from_html(html: str,
                        offered_filename: str = ""test.pdf"",
                        **kwargs) -> HttpResponse:
    """"""
    Same args as ``pdf_from_html()``.
    WATCH OUT: may not apply e.g. wkhtmltopdf options as you'd wish.
    """"""
    pdf = get_pdf_from_html(html, **kwargs)
    return serve_buffer(pdf,
                        offered_filename=offered_filename,
                        content_type=MimeType.PDF,
                        as_attachment=False,
                        as_inline=True)",https://github.com/RudolfCardinal/pythonlib/blob/0b84cb35f38bd7d8723958dae51b480a829b7227/cardinal_pythonlib/django/serve.py#L197-L209
tfidf_python_100_1.0,convert html to pdf,python,"def multiply(self, *probability_density_functions_to_multiply):
        pdf = self.copy()
        for pdf_to_multiply in probability_density_functions_to_multiply:
            pdf = pdf.__mul__(pdf_to_multiply)
        return pdf",https://github.com/TheVinhLuong102/ProbabPy/blob/a67d39a15fb70ff9d5f6f2836e42264a97a0496e/ProbabPy/__init__.py#L129-L133
tfidf_python_100_1.0,convert html to pdf,python,"def _get_pdf_list(self, input_pdfs):
        """"""
        Generate list of PDF documents.

        :param input_pdfs: List of PDFs or a directory path
             Directory - Scans directory contents
             List - Filters list to assert all list items are paths to PDF documents
        :return: List of PDF paths
        """"""
        if isinstance(input_pdfs, list):
            return [pdf for pdf in input_pdfs if self.validate(pdf)]
        elif os.path.isdir(input_pdfs):
            return [os.path.join(input_pdfs, pdf) for pdf in os.listdir(input_pdfs) if self.validate(pdf)]",https://github.com/mrstephenneal/pdfconduit/blob/993421cc087eefefe01ff09afabd893bcc2718ec/pdf/transform/merge.py#L24-L36
tfidf_python_100_1.0,convert html to pdf,python,"def display_links(self, max_rows=100):
        html = []
        pdf = []
        j = []
        for i, l in enumerate(self.links):
            html.append(self.html_word_list[i][1])
            for k, b in enumerate(self.pdf_word_list):
                if b[0] == self.links[self.html_word_list[i][0]]:
                    pdf.append(b[1])
                    j.append(k)
                    break
        try:
            assert len(pdf) == len(html)
        except Exception:
            self.logger.exception(""PDF and HTML are not the same length"")

        total = 0
        match = 0
        for i, word in enumerate(html):
            total += 1
            if word == pdf[i]:
                match += 1
        self.logger.info((match, total, match / total))

        data = {
            # 'i': range(len(self.links)),
            ""html"": html,
            ""pdf"": pdf,
            ""j"": j,
        }
        pd.set_option(""display.max_rows"", max_rows)
        self.logger.info(pd.DataFrame(data, columns=[""html"", ""pdf"", ""j""]))
        pd.reset_option(""display.max_rows"")",https://github.com/HazyResearch/fonduer/blob/4520f86a716f03dcca458a9f4bddac75b4e7068f/src/fonduer/parser/visual_linker.py#L287-L319
tfidf_python_100_1.0,convert html to pdf,python,"def merge_pdfs(pdf_filepaths, out_filepath):
    """""" Merge all the PDF files in `pdf_filepaths` in a new PDF file `out_filepath`.

    Parameters
    ----------
    pdf_filepaths: list of str
        Paths to PDF files.

    out_filepath: str
        Path to the result PDF file.

    Returns
    -------
    path: str
        The output file path.
    """"""
    merger = PdfFileMerger()
    for pdf in pdf_filepaths:
        merger.append(PdfFileReader(open(pdf, 'rb')))

    merger.write(out_filepath)

    return out_filepath",https://github.com/PythonSanSebastian/docstamp/blob/b43808f2e15351b0b2f0b7eade9c7ef319c9e646/docstamp/pdf_utils.py#L9-L31
tfidf_python_100_1.0,convert html to pdf,python,"def gauss_max(pdf):
    pdf = pdf.copy()
    for var, value in pdf.Scope.items():
        if value is None:
            pdf.Scope[var] = pdf.Param[('Mean', var)]
    return pdf",https://github.com/TheVinhLuong102/ProbabPy/blob/a67d39a15fb70ff9d5f6f2836e42264a97a0496e/ProbabPy/__init__.py#L422-L427
tfidf_python_100_1.0,convert html to pdf,python,"def to_pdf(self, digest, blob, mime_type):
        cache_key = ""pdf:"" + digest
        pdf = self.cache.get(cache_key)
        if pdf:
            return pdf

        for handler in self.handlers:
            if handler.accept(mime_type, ""application/pdf""):
                pdf = handler.convert(blob)
                self.cache[cache_key] = pdf
                return pdf
        raise HandlerNotFound(f""No handler found to convert from {mime_type} to PDF"")",https://github.com/abilian/abilian-core/blob/0a71275bf108c3d51e13ca9e093c0249235351e3/abilian/services/conversion/service.py#L112-L123
tfidf_python_100_1.0,convert html to pdf,python,"def text_extract(path, password=None):
    """"""Extract text from a PDF file""""""
    pdf = Info(path, password).pdf

    return [pdf.getPage(i).extractText() for i in range(pdf.getNumPages())]",https://github.com/mrstephenneal/pdfconduit/blob/993421cc087eefefe01ff09afabd893bcc2718ec/pdf/conduit/extract.py#L44-L48
tfidf_python_100_1.0,convert html to pdf,python,"def file_claims_pdfa(filename):
    """"""Determines if the file claims to be PDF/A compliant

    This only checks if the XMP metadata contains a PDF/A marker. It does not
    do full PDF/A validation.
    """"""

    with pikepdf.open(filename) as pdf:
        pdfmeta = pdf.open_metadata()
        if not pdfmeta.pdfa_status:
            return {
                'pass': False,
                'output': 'pdf',
                'conformance': 'No PDF/A metadata in XMP',
            }
        valid_part_conforms = {'1A', '1B', '2A', '2B', '2U', '3A', '3B', '3U'}
        conformance = f'PDF/A-{pdfmeta.pdfa_status}'
        pdfa_dict = {}
        if pdfmeta.pdfa_status in valid_part_conforms:
            pdfa_dict['pass'] = True
            pdfa_dict['output'] = 'pdfa'
        pdfa_dict['conformance'] = conformance
    return pdfa_dict",https://github.com/jbarlow83/OCRmyPDF/blob/79c84eefa353632a3d7ccddbd398c6678c1c1777/src/ocrmypdf/pdfa.py#L127-L149
tfidf_python_100_1.0,convert html to pdf,python,"def __init__(self, versionhash, language, pdf):
        # We don't need it
        del language
        super(WCADocumentJSON, self).__init__(list)
        self.versionhash = versionhash
        self.urls = {'regulations': REGULATIONS_ROOT,
                     'guidelines': REGULATIONS_ROOT + ""guidelines.html"",
                     'pdf': pdf}",https://github.com/thewca/wca-regulations-compiler/blob/3ebbd8fe8fec7c9167296f59b2677696fe61a954/wrc/codegen/cgjson.py#L19-L26
tfidf_python_100_1.0,convert html to pdf,python,"def _resolved_objects(pdf, xobject):
        """"""Retrieve rotatation info.""""""
        return [pdf.getPage(i).get(xobject) for i in range(pdf.getNumPages())][0]",https://github.com/mrstephenneal/pdfconduit/blob/993421cc087eefefe01ff09afabd893bcc2718ec/pdf/utils/info.py#L28-L30
tfidf_python_100_1.0,convert html to pdf,python,"def update_xmp_pdfversion(pdf, version):

    with pdf.open_metadata(set_pikepdf_as_editor=False, update_docinfo=False) as meta:
        if 'pdf:PDFVersion' in meta:
            meta['pdf:PDFVersion'] = version",https://github.com/pikepdf/pikepdf/blob/07154f4dec007e2e9c0c6a8c07b964fd06bc5f77/src/pikepdf/_cpphelpers.py#L55-L59
tfidf_python_100_1.0,convert html to pdf,python,"def pypdf3(pdf_files, output):
        # Create PDF file merger object
        pdf_merger = PdfFileMerger()

        # Appending pdfs one by one
        for pdf in pdf_files:
            pdf_merger.append(pdf)

        # writing combined pdf to output pdf file
        with open(output, 'wb') as f:
            pdf_merger.write(f)
        pdf_merger.close()
        return output",https://github.com/mrstephenneal/pdfconduit/blob/993421cc087eefefe01ff09afabd893bcc2718ec/pdf/transform/merge.py#L46-L58
tfidf_python_100_1.0,convert html to pdf,python,"def gather_links(pdf):
    for page in pdf.pages:
        yield from check_page(pdf, page)
        yield from check_page_annots(pdf, page)",https://github.com/pikepdf/pikepdf/blob/07154f4dec007e2e9c0c6a8c07b964fd06bc5f77/examples/find_links.py#L63-L66
tfidf_python_100_1.0,convert html to pdf,python,"def import_from_pypower_ppc(network, ppc, overwrite_zero_s_nom=None):
    """"""
    Import network from PYPOWER PPC dictionary format version 2.

    Converts all baseMVA to base power of 1 MVA.

    For the meaning of the pypower indices, see also pypower/idx_*.

    Parameters
    ----------
    ppc : PYPOWER PPC dict
    overwrite_zero_s_nom : Float or None, default None

    Examples
    --------
    >>> network.import_from_pypower_ppc(ppc)
    """"""


    version = ppc[""version""]
    if int(version) != 2:
        logger.warning(""Warning, importing from PYPOWER may not work if PPC version is not 2!"")

    logger.warning(""Warning: Note that when importing from PYPOWER, some PYPOWER features not supported: areas, gencosts, component status"")


    baseMVA = ppc[""baseMVA""]

    #dictionary to store pandas DataFrames of PyPower data
    pdf = {}


    # add buses

    #integer numbering will be bus names
    index = np.array(ppc['bus'][:,0],dtype=int)

    columns = [""type"",""Pd"",""Qd"",""Gs"",""Bs"",""area"",""v_mag_pu_set"",""v_ang_set"",""v_nom"",""zone"",""v_mag_pu_max"",""v_mag_pu_min""]

    pdf[""buses""] = pd.DataFrame(index=index,columns=columns,data=ppc['bus'][:,1:len(columns)+1])

    if (pdf[""buses""][""v_nom""] == 0.).any():
        logger.warning(""Warning, some buses have nominal voltage of 0., setting the nominal voltage of these to 1."")
        pdf['buses'].loc[pdf['buses']['v_nom'] == 0.,'v_nom'] = 1.


    #rename controls
    controls = ["""",""PQ"",""PV"",""Slack""]
    pdf[""buses""][""control""] = pdf[""buses""].pop(""type"").map(lambda i: controls[int(i)])

    #add loads for any buses with Pd or Qd
    pdf['loads'] = pdf[""buses""].loc[pdf[""buses""][[""Pd"",""Qd""]].any(axis=1), [""Pd"",""Qd""]]
    pdf['loads']['bus'] = pdf['loads'].index
    pdf['loads'].rename(columns={""Qd"" : ""q_set"", ""Pd"" : ""p_set""}, inplace=True)
    pdf['loads'].index = [""L""+str(i) for i in range(len(pdf['loads']))]


    #add shunt impedances for any buses with Gs or Bs

    shunt = pdf[""buses""].loc[pdf[""buses""][[""Gs"",""Bs""]].any(axis=1), [""v_nom"",""Gs"",""Bs""]]

    #base power for shunt is 1 MVA, so no need to rebase here
    shunt[""g""] = shunt[""Gs""]/shunt[""v_nom""]**2
    shunt[""b""] = shunt[""Bs""]/shunt[""v_nom""]**2
    pdf['shunt_impedances'] = shunt.reindex(columns=[""g"",""b""])
    pdf['shunt_impedances'][""bus""] = pdf['shunt_impedances'].index
    pdf['shunt_impedances'].index = [""S""+str(i) for i in range(len(pdf['shunt_impedances']))]

    #add gens

    #it is assumed that the pypower p_max is the p_nom

    #could also do gen.p_min_pu = p_min/p_nom

    columns = ""bus, p_set, q_set, q_max, q_min, v_set_pu, mva_base, status, p_nom, p_min, Pc1, Pc2, Qc1min, Qc1max, Qc2min, Qc2max, ramp_agc, ramp_10, ramp_30, ramp_q, apf"".split("", "")

    index = [""G""+str(i) for i in range(len(ppc['gen']))]

    pdf['generators'] = pd.DataFrame(index=index,columns=columns,data=ppc['gen'][:,:len(columns)])


    #make sure bus name is an integer
    pdf['generators']['bus'] = np.array(ppc['gen'][:,0],dtype=int)

    #add branchs
    ## branch data
    # fbus, tbus, r, x, b, rateA, rateB, rateC, ratio, angle, status, angmin, angmax

    columns = 'bus0, bus1, r, x, b, s_nom, rateB, rateC, tap_ratio, phase_shift, status, v_ang_min, v_ang_max'.split("", "")


    pdf['branches'] = pd.DataFrame(columns=columns,data=ppc['branch'][:,:len(columns)])

    pdf['branches']['original_index'] = pdf['branches'].index

    pdf['branches'][""bus0""] = pdf['branches'][""bus0""].astype(int)
    pdf['branches'][""bus1""] = pdf['branches'][""bus1""].astype(int)

    # s_nom = 0 indicates an unconstrained line
    zero_s_nom = pdf['branches'][""s_nom""] == 0.
    if zero_s_nom.any():
        if overwrite_zero_s_nom is not None:
            pdf['branches'].loc[zero_s_nom, ""s_nom""] = overwrite_zero_s_nom
        else:
            logger.warning(""Warning: there are {} branches with s_nom equal to zero, ""
                  ""they will probably lead to infeasibilities and should be ""
                  ""replaced with a high value using the `overwrite_zero_s_nom` ""
                  ""argument."".format(zero_s_nom.sum()))

    # determine bus voltages of branches to detect transformers
    v_nom = pdf['branches'].bus0.map(pdf['buses'].v_nom)
    v_nom_1 = pdf['branches'].bus1.map(pdf['buses'].v_nom)

    # split branches into transformers and lines
    transformers = ((v_nom != v_nom_1)
                    | ((pdf['branches'].tap_ratio != 0.) & (pdf['branches'].tap_ratio != 1.)) #NB: PYPOWER has strange default of 0. for tap ratio
                    | (pdf['branches'].phase_shift != 0))
    pdf['transformers'] = pd.DataFrame(pdf['branches'][transformers])
    pdf['lines'] = pdf['branches'][~ transformers].drop([""tap_ratio"", ""phase_shift""], axis=1)

    #convert transformers from base baseMVA to base s_nom
    pdf['transformers']['r'] = pdf['transformers']['r']*pdf['transformers']['s_nom']/baseMVA
    pdf['transformers']['x'] = pdf['transformers']['x']*pdf['transformers']['s_nom']/baseMVA
    pdf['transformers']['b'] = pdf['transformers']['b']*baseMVA/pdf['transformers']['s_nom']

    #correct per unit impedances
    pdf['lines'][""r""] = v_nom**2*pdf['lines'][""r""]/baseMVA
    pdf['lines'][""x""] = v_nom**2*pdf['lines'][""x""]/baseMVA
    pdf['lines'][""b""] = pdf['lines'][""b""]*baseMVA/v_nom**2


    if (pdf['transformers']['tap_ratio'] == 0.).any():
        logger.warning(""Warning, some transformers have a tap ratio of 0., setting the tap ratio of these to 1."")
        pdf['transformers'].loc[pdf['transformers']['tap_ratio'] == 0.,'tap_ratio'] = 1.


    #name them nicely
    pdf['transformers'].index = [""T""+str(i) for i in range(len(pdf['transformers']))]
    pdf['lines'].index = [""L""+str(i) for i in range(len(pdf['lines']))]

    #TODO

    ##-----  OPF Data  -----##
    ## generator cost data
    # 1 startup shutdown n x1 y1 ... xn yn
    # 2 startup shutdown n c(n-1) ... c0

    for component in [""Bus"",""Load"",""Generator"",""Line"",""Transformer"",""ShuntImpedance""]:
        import_components_from_dataframe(network,pdf[network.components[component][""list_name""]],component)

    network.generators[""control""] = network.generators.bus.map(network.buses[""control""])

    #for consistency with pypower, take the v_mag set point from the generators
    network.buses.loc[network.generators.bus,""v_mag_pu_set""] = np.asarray(network.generators[""v_set_pu""])",https://github.com/PyPSA/PyPSA/blob/46954b1b3c21460550f7104681517065279a53b7/pypsa/io.py#L732-L885
tfidf_python_100_1.0,json to xml conversion,python,"def convert_field(self, value, conversion):
        if conversion in self.extra_converters:
            return self.extra_converters[conversion](value)
        return super(XFormatter, self).convert_field(value, conversion)",https://github.com/sci-bots/pygtkhelpers/blob/3a6e6d6340221c686229cd1c951d7537dae81b07/pygtkhelpers/utils.py#L253-L256
tfidf_python_100_1.0,json to xml conversion,python,"def format_field(self, value, conversion):
        if conversion in self.extra_converters:
            return self.extra_converters[conversion](value)
        return super(XFormatter, self).format_field(value, conversion)",https://github.com/sci-bots/pygtkhelpers/blob/3a6e6d6340221c686229cd1c951d7537dae81b07/pygtkhelpers/utils.py#L258-L261
tfidf_python_100_1.0,json to xml conversion,python,"def from_dict(conversion):
    if not conversion:
        conversion = None

    elif ""a"" in conversion:
        conversion[""conversion_type""] = v4c.CONVERSION_TYPE_LIN
        conversion = v4b.ChannelConversion(**conversion)

    elif ""formula"" in conversion:
        conversion[""conversion_type""] = v4c.CONVERSION_TYPE_ALG
        conversion = v4b.ChannelConversion(**conversion)

    elif all(key in conversion for key in [f""P{i}"" for i in range(1, 7)]):
        conversion[""conversion_type""] = v4c.CONVERSION_TYPE_RAT
        conversion = v4b.ChannelConversion(**conversion)

    elif ""raw_0"" in conversion and ""phys_0"" in conversion:
        conversion[""conversion_type""] = v4c.CONVERSION_TYPE_TAB
        nr = 0
        while f""phys_{nr}"" in conversion:
            nr += 1
        conversion[""val_param_nr""] = nr * 2
        conversion = v4b.ChannelConversion(**conversion)

    elif ""upper_0"" in conversion and ""phys_0"" in conversion:
        conversion[""conversion_type""] = v4c.CONVERSION_TYPE_RTAB
        nr = 0
        while f""phys_{nr}"" in conversion:
            nr += 1
        conversion[""val_param_nr""] = nr * 3 + 1
        conversion = v4b.ChannelConversion(**conversion)

    elif ""val_0"" in conversion and ""text_0"" in conversion:
        conversion[""conversion_type""] = v4c.CONVERSION_TYPE_TABX
        nr = 0
        while f""text_{nr}"" in conversion:
            nr += 1
        conversion[""ref_param_nr""] = nr + 1
        conversion = v4b.ChannelConversion(**conversion)

    elif ""upper_0"" in conversion and ""text_0"" in conversion:
        conversion[""conversion_type""] = v4c.CONVERSION_TYPE_RTABX
        nr = 0
        while f""text_{nr}"" in conversion:
            nr += 1
        conversion[""ref_param_nr""] = nr + 1
        conversion = v4b.ChannelConversion(**conversion)

    else:
        conversion = v4b.ChannelConversion(
            conversion_type=v4c.CONVERSION_TYPE_NON
        )

    return conversion",https://github.com/danielhrisca/asammdf/blob/3c7a1fd19c957ceebe4dcdbb2abf00806c2bdb66/asammdf/blocks/conversion_utils.py#L252-L305
tfidf_python_100_1.0,json to xml conversion,python,"def convert_field(self, value, conversion):
        # do any conversion on the resulting object
        if conversion is None:
            return value
        elif conversion == 's':
            return str(value)
        elif conversion == 'r':
            return repr(value)
        elif conversion == 'a':
            return ascii(value)
        raise ValueError(""Unknown conversion specifier {0!s}"".format(conversion))",https://github.com/cuescience/goat/blob/d76f44b9ec5dc40ad33abca50830c0d7492ef152/goat/string.py#L122-L132
tfidf_python_100_1.0,json to xml conversion,python,"def conversion_transfer(conversion, version=3):
    """""" convert between mdf4 and mdf3 channel conversions

    Parameters
    ----------
    conversion : block
        channel conversion
    version : int
        target mdf version

    Returns
    -------
    conversion : block
        channel conversion for specified version

    """"""

    if version <= 3:
        if conversion is None:
            conversion = v3b.ChannelConversion(conversion_type=v3c.CONVERSION_TYPE_NONE)
        else:
            conversion_type = conversion[""conversion_type""]
            if conversion[""id""] == b""CC"":
                pass
            else:
                unit = conversion.unit.strip("" \r\n\t\0"").encode(""latin-1"")

                if conversion_type == v4c.CONVERSION_TYPE_NON:
                    conversion = v3b.ChannelConversion(
                        unit=unit, conversion_type=v3c.CONVERSION_TYPE_NONE
                    )

                elif conversion_type == v4c.CONVERSION_TYPE_LIN:
                    conversion = v3b.ChannelConversion(
                        unit=unit,
                        conversion_type=v3c.CONVERSION_TYPE_LINEAR,
                        a=conversion.a,
                        b=conversion.b,
                    )

                elif conversion_type == v4c.CONVERSION_TYPE_RAT:
                    conversion = v3b.ChannelConversion(
                        unit=unit,
                        conversion_type=v3c.CONVERSION_TYPE_RAT,
                        P1=conversion.P1,
                        P2=conversion.P2,
                        P3=conversion.P3,
                        P4=conversion.P4,
                        P5=conversion.P5,
                        P6=conversion.P6,
                    )

                elif conversion_type == v4c.CONVERSION_TYPE_TAB:
                    conversion_ = {}
                    conversion_[""ref_param_nr""] = conversion.val_param_nr // 2
                    for i in range(conversion.val_param_nr // 2):
                        conversion_[f""raw_{i}""] = conversion[f""raw_{i}""]
                        conversion_[f""phys_{i}""] = conversion[f""phys_{i}""]

                    conversion = v3b.ChannelConversion(
                        unit=unit,
                        conversion_type=v3c.CONVERSION_TYPE_TAB,
                        **conversion_,
                    )

                elif conversion_type == v4c.CONVERSION_TYPE_TABI:
                    conversion_ = {}
                    conversion_[""ref_param_nr""] = conversion.val_param_nr // 2
                    for i in range(conversion.val_param_nr // 2):
                        conversion_[f""raw_{i}""] = conversion[f""raw_{i}""]
                        conversion_[f""phys_{i}""] = conversion[f""phys_{i}""]

                    conversion = v3b.ChannelConversion(
                        unit=unit,
                        conversion_type=v3c.CONVERSION_TYPE_TABI,
                        **conversion_,
                    )

                elif conversion_type == v4c.CONVERSION_TYPE_ALG:
                    formula = conversion.formula.replace(""X"", ""X1"")
                    conversion = v3b.ChannelConversion(
                        formula=formula,
                        unit=unit,
                        conversion_type=v3c.CONVERSION_TYPE_FORMULA,
                    )

                elif conversion_type == v4c.CONVERSION_TYPE_RTAB:
                    nr = (conversion.val_param_nr - 1) // 3
                    kargs = {
                        ""ref_param_nr"": nr,
                        ""conversion_type"": v3c.CONVERSION_TYPE_TABI,
                    }

                    for i in range(nr):
                        l_ = conversion[f""lower_{i}""]
                        u_ = conversion[f""upper_{i}""]
                        p_ = conversion[f""phys_{i}""]
                        kargs[f""raw_{i}""] = l_
                        kargs[f""raw_{i}""] = u_ - 0.000_001
                        kargs[f""phys_{i}""] = p_
                        kargs[f""phys_{i}""] = p_

                    conversion = v3b.ChannelConversion(unit=unit, **kargs)

                elif conversion_type == v4c.CONVERSION_TYPE_TABX:
                    nr = conversion.val_param_nr
                    kargs = {
                        ""ref_param_nr"": nr,
                        ""unit"": unit,
                        ""conversion_type"": v3c.CONVERSION_TYPE_TABX,
                    }
                    for i in range(nr):
                        kargs[f""param_val_{i}""] = conversion[f""val_{i}""]
                        kargs[f""text_{i}""] = conversion.referenced_blocks[
                            f""text_{i}""
                        ].text

                    conversion = v3b.ChannelConversion(**kargs)

                elif conversion_type == v4c.CONVERSION_TYPE_RTABX:
                    nr = conversion.val_param_nr // 2
                    kargs = {
                        ""ref_param_nr"": nr + 1,
                        ""unit"": unit,
                        ""conversion_type"": v3c.CONVERSION_TYPE_RTABX,
                    }
                    for i in range(nr):
                        kargs[f""lower_{i}""] = conversion[f""lower_{i}""]
                        kargs[f""upper_{i}""] = conversion[f""upper_{i}""]
                        kargs[f""text_{i}""] = conversion.referenced_blocks[
                            f""text_{i}""
                        ].text

                    new_conversion = v3b.ChannelConversion(**kargs)

                    new_conversion.referenced_blocks[""default_addr""] = v3b.TextBlock(
                        text=conversion.referenced_blocks[""default_addr""].text
                    )
                    new_conversion

                    conversion = new_conversion

    else:
        if conversion is None or conversion[""id""] == b""##CC"":
            pass
        else:
            conversion_type = conversion[""conversion_type""]
            unit = conversion[""unit_field""].decode(""latin-1"").strip("" \r\n\t\0"")
            if conversion_type == v3c.CONVERSION_TYPE_NONE:
                conversion = v4b.ChannelConversion(
                    conversion_type=v4c.CONVERSION_TYPE_NON
                )

            elif conversion_type == v3c.CONVERSION_TYPE_LINEAR:
                conversion = v4b.ChannelConversion(
                    conversion_type=v4c.CONVERSION_TYPE_LIN,
                    a=conversion.a,
                    b=conversion.b,
                )

            elif conversion_type == v3c.CONVERSION_TYPE_RAT:
                conversion = v4b.ChannelConversion(
                    conversion_type=v4c.CONVERSION_TYPE_RAT,
                    P1=conversion.P1,
                    P2=conversion.P2,
                    P3=conversion.P3,
                    P4=conversion.P4,
                    P5=conversion.P5,
                    P6=conversion.P6,
                )

            elif conversion_type == v3c.CONVERSION_TYPE_FORMULA:
                formula = conversion.formula
                conversion = v4b.ChannelConversion(
                    conversion_type=v4c.CONVERSION_TYPE_ALG, formula=formula
                )

            elif conversion_type == v3c.CONVERSION_TYPE_TAB:
                conversion_ = {}
                conversion_[""val_param_nr""] = conversion.ref_param_nr * 2
                for i in range(conversion.val_param_nr):
                    conversion_[f""raw_{i}""] = conversion[f""raw_{i}""]
                    conversion_[f""phys_{i}""] = conversion[f""phys_{i}""]

                conversion = v4b.ChannelConversion(
                    conversion_type=v4c.CONVERSION_TYPE_TAB, **conversion_
                )

            elif conversion_type == v3c.CONVERSION_TYPE_TABI:
                conversion_ = {}
                conversion_[""val_param_nr""] = conversion.ref_param_nr * 2
                for i in range(conversion.ref_param_nr):
                    conversion_[f""raw_{i}""] = conversion[f""raw_{i}""]
                    conversion_[f""phys_{i}""] = conversion[f""phys_{i}""]

                conversion = v4b.ChannelConversion(
                    conversion_type=v4c.CONVERSION_TYPE_TABI, **conversion_
                )

            elif conversion_type == v3c.CONVERSION_TYPE_TABX:

                nr = conversion[""ref_param_nr""]
                kargs = {
                    ""val_param_nr"": nr,
                    ""ref_param_nr"": nr + 1,
                    ""conversion_type"": v4c.CONVERSION_TYPE_TABX,
                }
                for i in range(nr):
                    kargs[f""val_{i}""] = conversion[f""param_val_{i}""]
                    kargs[f""text_{i}""] = conversion[f""text_{i}""]

                conversion = v4b.ChannelConversion(**kargs)

            elif conversion_type == v3c.CONVERSION_TYPE_RTABX:
#                print('IN', conversion)

                nr = conversion[""ref_param_nr""] - 1
                kargs = {
                    ""val_param_nr"": nr * 2,
                    ""ref_param_nr"": nr + 1,
                    ""conversion_type"": v4c.CONVERSION_TYPE_RTABX,
                    ""default_addr"": conversion.referenced_blocks[""default_addr""].text,
                }
                for i in range(nr):
                    kargs[f""lower_{i}""] = conversion[f""lower_{i}""]
                    kargs[f""upper_{i}""] = conversion[f""upper_{i}""]
                    kargs[f""text_{i}""] = conversion.referenced_blocks[f""text_{i}""].text

                conversion = v4b.ChannelConversion(**kargs)

#                print('OUT', conversion)
#                print('\n'*3)

            conversion.unit = unit

    return conversion",https://github.com/danielhrisca/asammdf/blob/3c7a1fd19c957ceebe4dcdbb2abf00806c2bdb66/asammdf/blocks/conversion_utils.py#L14-L249
tfidf_python_100_1.0,json to xml conversion,python,"def convert_field(self, value, conversion):
        """"""When field missing, store conversion specifier.""""""
        if isinstance(value, MissingField):
            if conversion is not None:
                value.conversion = conversion
            return value

        return super(FormatterWrapper, self).convert_field(value, conversion)",https://github.com/blubberdiblub/eztemplate/blob/ab5b2b4987c045116d130fd83e216704b8edfb5d/eztemplate/engines/string_formatter_engine.py#L72-L79
tfidf_python_100_1.0,json to xml conversion,python,"def mach2tas(M, h):
    """""" True airspeed (tas) to mach number conversion """"""
    a = vsound(h)
    tas = M * a
    return tas",https://github.com/xoolive/traffic/blob/d1a8878098f16759f6b6e0e8d8b8f32e34a680a8/traffic/core/aero.py#L306-L310
tfidf_python_100_1.0,json to xml conversion,python,"def tas2mach(tas, h):
    """""" True airspeed (tas) to mach number conversion """"""
    a = vsound(h)
    M = tas / a
    return M",https://github.com/xoolive/traffic/blob/d1a8878098f16759f6b6e0e8d8b8f32e34a680a8/traffic/core/aero.py#L299-L303
tfidf_python_100_1.0,json to xml conversion,python,"def vmach2tas(M, h):
    """""" True airspeed (tas) to mach number conversion """"""
    a = vvsound(h)
    tas = M * a
    return tas",https://github.com/xoolive/traffic/blob/d1a8878098f16759f6b6e0e8d8b8f32e34a680a8/traffic/core/aero.py#L95-L99
tfidf_python_100_1.0,json to xml conversion,python,"def vtas2mach(tas, h):
    """""" True airspeed (tas) to mach number conversion """"""
    a = vvsound(h)
    M = tas / a
    return M",https://github.com/xoolive/traffic/blob/d1a8878098f16759f6b6e0e8d8b8f32e34a680a8/traffic/core/aero.py#L88-L92
tfidf_python_100_1.0,json to xml conversion,python,"def _inject_conversion(self, value, conversion):
        """"""
        value: '{x}', conversion: 's' -> '{x!s}'
        """"""
        t = type(value)
        return value[:-1] + t(u'!') + conversion + t(u'}')",https://github.com/5monkeys/content-io/blob/8c8519c74cbadab871f7151c0e02252cb5753759/cio/utils/formatters.py#L63-L68
tfidf_python_100_1.0,json to xml conversion,python,"def json(self, conversion: _Text = 'badgerfish') -> Mapping:
        """"""A JSON Representation of the XML.  Default is badgerfish.
        :param conversion: Which conversion method to use. (`learn more <https://github.com/sanand0/xmljson#conventions>`_)
        """"""
        if not self._json:

            if conversion is 'badgerfish':
                from xmljson import badgerfish as serializer

            elif conversion is 'abdera':
                from xmljson import abdera as serializer

            elif conversion is 'cobra':
                from xmljson import cobra as serializer

            elif conversion is 'gdata':
                from xmljson import gdata as serializer

            elif conversion is 'parker':
                from xmljson import parker as serializer

            elif conversion is 'yahoo':
                from xmljson import yahoo as serializer

            self._json = json.dumps(serializer.data(etree.fromstring(self.xml)))

        return self._json",https://github.com/erinxocon/requests-xml/blob/923571ceae4ddd4f2f57a2fc8780d89b50f3e7a1/requests_xml.py#L177-L203
tfidf_python_100_1.0,json to xml conversion,python,"def get_xml(self):
        xml = '<evEncMDFe>'
        xml += self.descEvento.xml
        xml += self.nProt.xml
        xml += self.dtEnc.xml
        xml += self.cUF.xml
        xml += self.cMun.xml
        xml += '</evEncMDFe>'
        return xml",https://github.com/aricaldeira/PySPED/blob/42905693e913f32db2c23f4e067f94af28a8164a/pysped/mdfe/leiaute/evtencmdfe_300.py#L61-L69
tfidf_python_100_1.0,json to xml conversion,python,"def get_xml(self):
        xml = XMLNFe.get_xml(self)
        xml += '<infCad>'
        xml += self.IE.xml
        xml += self.CNPJ.xml
        xml += self.CPF.xml
        xml += self.UF.xml
        xml += self.cSit.xml
        xml += self.xNome.xml
        xml += self.xFant.xml
        xml += self.xRegApur.xml
        xml += self.CNAE.xml
        xml += self.dIniAtiv.xml
        xml += self.dUltSit.xml
        xml += self.dBaixa.xml
        xml += self.IEUnica.xml
        xml += self.IEAtual.xml
        xml += self.ender.xml
        xml += '</infCad>'
        return xml",https://github.com/aricaldeira/PySPED/blob/42905693e913f32db2c23f4e067f94af28a8164a/pysped/nfe/leiaute/conscad_101.py#L166-L185
tfidf_python_100_1.0,json to xml conversion,python,"def get_xml(self):
        xml = XMLNFe.get_xml(self)
        xml += '<evEPECCTe>'
        xml += self.descEvento.xml
        xml += self.xJust.xml
        xml += self.vICMS.xml
        xml += self.vTPrest.xml
        xml += self.vCarga.xml
        xml += self.Toma4.xml
        xml += self.Modal.xml
        xml += self.UFIni.xml
        xml += self.UFFIm.xml
        xml += self.tpCTe.xml
        xml += self.dhEmi.xml

        xml += '</evEPECCTe>'

        return xml",https://github.com/aricaldeira/PySPED/blob/42905693e913f32db2c23f4e067f94af28a8164a/pysped/cte/leiaute/eventoscte_300.py#L91-L108
tfidf_python_100_1.0,json to xml conversion,python,"def get_xml(self):
        xml = XMLNFe.get_xml(self)
        xml += ABERTURA
        xml += self.versao.xml
        xml += self.tpAmb.xml
        xml += self.verAplic.xml
        xml += self.cStat.xml
        xml += self.xMotivo.xml
        xml += self.dhResp.xml
        xml += self.indCont.xml
        xml += self.ultNSU.xml

        for n in self.resNFe:
            xml += n.xml

        for n in self.resCanc:
            xml += n.xml

        for n in self.resCCe:
            xml += n.xml

        xml += '</retConsNFeDest>'
        return xml",https://github.com/aricaldeira/PySPED/blob/42905693e913f32db2c23f4e067f94af28a8164a/pysped/nfe/leiaute/consnfedest_101.py#L231-L253
tfidf_python_100_1.0,json to xml conversion,python,"def get_xml(self):
        xml = XMLNFe.get_xml(self)
        xml += ABERTURA
        xml += '<resNFe>'
        xml += self.chNFe.xml
        xml += self.CNPJ.xml
        xml += self.CPF.xml
        xml += self.xNome.xml
        xml += self.IE.xml
        xml += self.dhEmi.xml
        xml += self.tpNF.xml
        xml += self.vNF.xml
        xml += self.digVal.xml
        xml += self.dhRecbto.xml
        xml += self.cSitNFe.xml
        xml += self.cSitConf.xml
        xml += '</resNFe>'
        return xml",https://github.com/aricaldeira/PySPED/blob/42905693e913f32db2c23f4e067f94af28a8164a/pysped/cte/leiaute/distdfeint_100.py#L195-L212
tfidf_python_100_1.0,json to xml conversion,python,"def get_xml(self):
        xml = XMLNFe.get_xml(self)
        xml += '<ret>'
        xml += self.NSU.xml
        xml += self.chNFe.xml
        xml += self.CNPJ.xml
        xml += self.CPF.xml
        xml += self.xNome.xml
        xml += self.IE.xml
        xml += self.dEmi.xml
        xml += self.tpNF.xml
        xml += self.vNF.xml
        xml += self.digVal.xml
        xml += self.dhRecbto.xml
        xml += self.cSitNFe.xml
        xml += self.cSitConf.xml
        xml += '</resNFe>'
        xml += '</ret>'
        return xml",https://github.com/aricaldeira/PySPED/blob/42905693e913f32db2c23f4e067f94af28a8164a/pysped/nfe/leiaute/consnfedest_101.py#L118-L136
tfidf_python_100_1.0,json to xml conversion,python,"def get_xml(self):
        xml = XMLNFe.get_xml(self)
        xml += '<infGTV>'
        xml += self.nDoc.xml
        xml += self.id.xml
        xml += self.serie.xml
        xml += self.subserie.xml
        xml += self.dEmi.xml
        xml += self.nDV.xml
        xml += self.qCarga.xml
        for iesp in self.infEspecie:
            xml += iesp.xml
        xml += self.rem.xml
        xml += self.dest.xml
        xml += self.placa.xml
        xml += self.UF.xml
        xml += self.RNTRC.xml
        xml += '</infGTV>'
        return xml",https://github.com/aricaldeira/PySPED/blob/42905693e913f32db2c23f4e067f94af28a8164a/pysped/cte/leiaute/eventoscte_300.py#L350-L368
tfidf_python_100_1.0,json to xml conversion,python,"def get_xml(self):
        xml = XMLNFe.get_xml(self)
        xml += ABERTURA
        xml += self.versao.xml
        xml += self.tpAmb.xml
        xml += self.verAplic.xml
        xml += self.cStat.xml
        xml += self.xMotivo.xml
        xml += self.cUF.xml
        xml += self.dhRecbto.xml
        xml += self.tMed.xml
        xml += self.dhRetorno.xml
        xml += self.xObs.xml
        xml += '</retConsStatServ>'
        return xml",https://github.com/aricaldeira/PySPED/blob/42905693e913f32db2c23f4e067f94af28a8164a/pysped/nfe/leiaute/consstatserv_107.py#L100-L114
tfidf_python_100_1.0,how to randomly pick a number,python,"def __iter__(self):
        while True:
            pick_label, pick_other = self.rng.choice(10, size=2, replace=False)
            yield [self.pick(pick_label), self.pick(pick_label), self.pick(pick_other)]",https://github.com/tensorpack/tensorpack/blob/d7a13cb74c9066bc791d7aafc3b744b60ee79a9f/examples/SimilarityLearning/embedding_data.py#L58-L61
tfidf_python_100_1.0,how to randomly pick a number,python,"def pick(rest):
	""Pick between a few options""
	question = rest.strip()
	choices = util.splitem(question)
	if len(choices) == 1:
		return ""I can't pick if you give me only one choice!""
	else:
		pick = random.choice(choices)
		certainty = random.sample(phrases.certainty_opts, 1)[0]
		return ""%s... %s %s"" % (pick, certainty, pick)",https://github.com/yougov/pmxbot/blob/5da84a3258a0fd73cb35b60e39769a5d7bfb2ba7/pmxbot/commands.py#L368-L377
tfidf_python_100_1.0,how to randomly pick a number,python,"def pick_tile_size(self, seg_size, data_lengths, valid_chunks, valid_lengths):
        """""" Choose job tiles size based on science segment length """"""

        if len(valid_lengths) == 1:
            return data_lengths[0], valid_chunks[0], valid_lengths[0]
        else:
            # Pick the tile size that is closest to 1/3 of the science segment
            target_size = seg_size / 3
            pick, pick_diff = 0, abs(valid_lengths[0] - target_size)
            for i, size in enumerate(valid_lengths):
                if abs(size - target_size) < pick_diff:
                    pick, pick_diff  = i, abs(size - target_size)
            return data_lengths[pick], valid_chunks[pick], valid_lengths[pick]",https://github.com/gwastro/pycbc/blob/7a64cdd104d263f1b6ea0b01e6841837d05a4cb3/pycbc/workflow/jobsetup.py#L506-L518
tfidf_python_100_1.0,how to randomly pick a number,python,"def pick(a, index=0, dim=0): return GVExpr('pick', [a, index], make_dim(1, inferred=True))",https://github.com/clab/dynet/blob/21cc62606b74f81bb4b11a9989a6c2bd0caa09c5/python/dynet_viz.py#L250-L250
tfidf_python_100_1.0,how to randomly pick a number,python,"def cloud_cover_to_irradiance(self, cloud_cover, how='clearsky_scaling',
                                  **kwargs):
        """"""
        Convert cloud cover to irradiance. A wrapper method.

        Parameters
        ----------
        cloud_cover : Series
        how : str, default 'clearsky_scaling'
            Selects the method for conversion. Can be one of
            clearsky_scaling or liujordan.
        **kwargs
            Passed to the selected method.

        Returns
        -------
        irradiance : DataFrame
            Columns include ghi, dni, dhi
        """"""

        how = how.lower()
        if how == 'clearsky_scaling':
            irrads = self.cloud_cover_to_irradiance_clearsky_scaling(
                cloud_cover, **kwargs)
        elif how == 'liujordan':
            irrads = self.cloud_cover_to_irradiance_liujordan(
                cloud_cover, **kwargs)
        else:
            raise ValueError('invalid how argument')

        return irrads",https://github.com/pvlib/pvlib-python/blob/2e844a595b820b43d1170269781fa66bd0ccc8a3/pvlib/forecast.py#L539-L569
tfidf_python_100_1.0,how to randomly pick a number,python,"def execute_arbitrary_series_groupby(op, data, _, aggcontext=None, **kwargs):
    how = op.how
    if how is None:
        how = 'first'

    if how not in {'first', 'last'}:
        raise com.OperationNotDefinedError(
            'Arbitrary {!r} is not supported'.format(how)
        )
    return aggcontext.agg(data, how)",https://github.com/ibis-project/ibis/blob/1e39a5fd9ef088b45c155e8a5f541767ee8ef2e7/ibis/pandas/execution/generic.py#L451-L460
tfidf_python_100_1.0,how to randomly pick a number,python,"def __init__(self, **kwdargs):
        if not hasattr(self, 'cb'):
            Callback.Callbacks.__init__(self)
        self.cap = 'ball'
        self.cap_radius = 4
        self.editable = True
        self.pickable = False
        self.coord = 'data'
        self.ref_obj = None
        self.__dict__.update(kwdargs)
        self.data = None
        self.crdmap = None
        self.tag = None
        if not hasattr(self, 'kind'):
            self.kind = None
        # For debugging
        self.name = None
        self.viewer = None

        # For callbacks
        for name in ('edited', 'pick-down', 'pick-move', 'pick-up',
                     'pick-hover', 'pick-enter', 'pick-leave',
                     'pick-key'):
            self.enable_callback(name)",https://github.com/ejeschke/ginga/blob/a78c893ec6f37a837de851947e9bb4625c597915/ginga/canvas/CanvasObject.py#L52-L75
tfidf_python_100_1.0,how to randomly pick a number,python,"def time_i8merge(self, how):
        merge(self.left, self.right, how=how)",https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/asv_bench/benchmarks/join_merge.py#L213-L214
tfidf_python_100_1.0,how to randomly pick a number,python,"def stalta_pick(stream, stalen, ltalen, trig_on, trig_off, freqmin=False,
                freqmax=False, debug=0, show=False):
    """"""
    Basic sta/lta picker, suggest using alternative in obspy.

    Simple sta/lta (short-term average/long-term average) picker, using
    obspy's :func:`obspy.signal.trigger.classic_sta_lta` routine to generate
    the characteristic function.

    Currently very basic quick wrapper, there are many other (better) options
    in obspy in the :mod:`obspy.signal.trigger` module.

    :type stream: obspy.core.stream.Stream
    :param stream: The stream to pick on, can be any number of channels.
    :type stalen: float
    :param stalen: Length of the short-term average window in seconds.
    :type ltalen: float
    :param ltalen: Length of the long-term average window in seconds.
    :type trig_on: float
    :param trig_on: sta/lta ratio to trigger a detection/pick
    :type trig_off: float
    :param trig_off: sta/lta ratio to turn the trigger off - no further picks\
        will be made between exceeding trig_on until trig_off is reached.
    :type freqmin: float
    :param freqmin: Low-cut frequency in Hz for bandpass filter
    :type freqmax: float
    :param freqmax: High-cut frequency in Hz for bandpass filter
    :type debug: int
    :param debug: Debug output level from 0-5.
    :type show: bool
    :param show: Show picks on waveform.

    :returns: :class:`obspy.core.event.event.Event`

    .. rubric:: Example

    >>> from obspy import read
    >>> from eqcorrscan.utils.picker import stalta_pick
    >>> st = read()
    >>> event = stalta_pick(st, stalen=0.2, ltalen=4, trig_on=10,
    ...             trig_off=1, freqmin=3.0, freqmax=20.0)
    >>> print(event.creation_info.author)
    EQcorrscan

    .. warning::
        This function is not designed for accurate picking, rather it can give
        a first idea of whether picks may be possible.  Proceed with caution.
    """"""
    event = Event()
    event.origins.append(Origin())
    event.creation_info = CreationInfo(author='EQcorrscan',
                                       creation_time=UTCDateTime())
    event.comments.append(Comment(text='stalta'))
    picks = []
    for tr in stream:
        # We are going to assume, for now, that if the pick is made on the
        # horizontal channel then it is an S, otherwise we will assume it is
        # a P-phase: obviously a bad assumption...
        if tr.stats.channel[-1] == 'Z':
            phase = 'P'
        else:
            phase = 'S'
        if freqmin and freqmax:
            tr.detrend('simple')
            tr.filter('bandpass', freqmin=freqmin, freqmax=freqmax,
                      corners=3, zerophase=True)
        df = tr.stats.sampling_rate
        cft = classic_sta_lta(tr.data, int(stalen * df), int(ltalen * df))
        if debug > 3:
            plot_trigger(tr, cft, trig_on, trig_off)
        triggers = trigger_onset(cft, trig_on, trig_off)
        for trigger in triggers:
            on = tr.stats.starttime + (trigger[0] / df)
            # off = tr.stats.starttime + (trigger[1] / df)
            wav_id = WaveformStreamID(station_code=tr.stats.station,
                                      channel_code=tr.stats.channel,
                                      network_code=tr.stats.network)
            p = Pick(waveform_id=wav_id, phase_hint=phase, time=on)
            if debug > 2:
                print('Pick made:')
                print(p)
            picks.append(p)
    # QC picks
    pick_stations = list(set([pick.waveform_id.station_code
                              for pick in picks]))
    for pick_station in pick_stations:
        station_picks = [pick for pick in picks if
                         pick.waveform_id.station_code == pick_station]
        # If P-pick is after S-picks, remove it.
        p_time = [pick.time for pick in station_picks
                  if pick.phase_hint == 'P']
        s_time = [pick.time for pick in station_picks
                  if pick.phase_hint == 'S']
        if p_time > s_time:
            p_pick = [pick for pick in station_picks if pick.phase_hint == 'P']
            for pick in p_pick:
                print('P pick after S pick, removing P pick')
                picks.remove(pick)
    if show:
        plotting.pretty_template_plot(stream, picks=picks, title='Autopicks',
                                      size=(8, 9))
    event.picks = picks
    if len(event.picks) > 0:
        event.origins[0].time = min([pick.time for pick in event.picks]) - 1
        # event.origins[0].latitude = float('nan')
        # event.origins[0].longitude = float('nan')
    # Set arbitrary origin time
    return event",https://github.com/eqcorrscan/EQcorrscan/blob/3121b4aca801ee5d38f56ca297ce1c0f9515d9ff/eqcorrscan/utils/picker.py#L186-L293
tfidf_python_100_1.0,how to randomly pick a number,python,"def Parse(self, how):
        '''Parse the message.
        '''
        if type(how) == types.ClassType: how = how.typecode
        return how.parse(self.body_root, self)",https://github.com/rameshg87/pyremotevbox/blob/123dffff27da57c8faa3ac1dd4c68b1cf4558b1a/pyremotevbox/ZSI/parse.py#L322-L326
tfidf_python_100_1.0,how to randomly pick a number,python,"def cherry_pick(self):
        """""" git cherry-pick -x <commit_sha1> """"""
        cmd = [""git"", ""cherry-pick"", ""-x"", self.commit_sha1]
        try:
            self.run_cmd(cmd)
        except subprocess.CalledProcessError as err:
            click.echo(f""Error cherry-pick {self.commit_sha1}."")
            click.echo(err.output)
            raise CherryPickException(f""Error cherry-pick {self.commit_sha1}."")",https://github.com/python/core-workflow/blob/b93c76195f6db382cfcefee334380fb4c68d4e21/cherry_picker/cherry_picker/cherry_picker.py#L233-L241
tfidf_python_100_1.0,how to randomly pick a number,python,"def set_mode(how):
    """""" Sets the behavior of the API

    :param how: if 'remote' all the execution is performed on the remote server; if 'local' all
           it is executed locally. Default = 'local'
    :return: None
    """"""
    global __mode
    if how == ""local"":
        __mode = how
    elif how == ""remote"":
        __mode = how
    else:
        raise ValueError(""how must be 'local' or 'remote'"")",https://github.com/DEIB-GECO/PyGMQL/blob/e58b2f9402a86056dcda484a32e3de0bb06ed991/gmql/settings.py#L79-L92
tfidf_python_100_1.0,how to randomly pick a number,python,"def timestamp_to_datetime(timestamp, tzinfo):
    if tzinfo is None:
        pick = datetime.fromtimestamp(timestamp, tzlocal())
        pick = pick.astimezone(tzutc()).replace(tzinfo=None)
    else:
        pick = datetime.fromtimestamp(timestamp, tzinfo)

    return pick",https://github.com/joke2k/faker/blob/965824b61132e52d92d1a6ce470396dbbe01c96c/faker/providers/date_time/__init__.py#L28-L35
tfidf_python_100_1.0,how to randomly pick a number,python,"def write_correlations(event_list, wavbase, extract_len, pre_pick, shift_len,
                       lowcut=1.0, highcut=10.0, max_sep=8, min_link=8,
                       cc_thresh=0.0, plotvar=False, debug=0):
    """"""
    Write a dt.cc file for hypoDD input for a given list of events.

    Takes an input list of events and computes pick refinements by correlation.
    Outputs two files, dt.cc and dt.cc2, each provides a different weight,
    dt.cc uses weights of the cross-correlation, and dt.cc2 provides weights
    as the square of the cross-correlation.

    :type event_list: list
    :param event_list: List of tuples of event_id (int) and sfile (String)
    :type wavbase: str
    :param wavbase: Path to the seisan wave directory that the wavefiles in the
                    S-files are stored
    :type extract_len: float
    :param extract_len: Length in seconds to extract around the pick
    :type pre_pick: float
    :param pre_pick: Time before the pick to start the correlation window
    :type shift_len: float
    :param shift_len: Time to allow pick to vary
    :type lowcut: float
    :param lowcut: Lowcut in Hz - default=1.0
    :type highcut: float
    :param highcut: Highcut in Hz - default=10.0
    :type max_sep: float
    :param max_sep: Maximum separation between event pairs in km
    :type min_link: int
    :param min_link: Minimum links for an event to be paired
    :type cc_thresh: float
    :param cc_thresh: Threshold to include cross-correlation results.
    :type plotvar: bool
    :param plotvar: To show the pick-correction plots, defualts to False.
    :type debug: int
    :param debug: Variable debug levels from 0-5, higher=more output.

    .. warning:: This is not a fast routine!

    .. warning::
        In contrast to seisan's corr routine, but in accordance with the
        hypoDD manual, this outputs corrected differential time.

    .. note::
        Currently we have not implemented a method for taking
        unassociated event objects and wavefiles.  As such if you have events \
        with associated wavefiles you are advised to generate Sfiles for each \
        event using the sfile_util module prior to this step.

    .. note::
        There is no provision to taper waveforms within these functions, if you
        desire this functionality, you should apply the taper before calling
        this.  Note the :func:`obspy.Trace.taper` functions.
    """"""
    warnings.filterwarnings(action=""ignore"",
                            message=""Maximum of cross correlation "" +
                                    ""lower than 0.8: *"")
    corr_list = []
    f = open('dt.cc', 'w')
    f2 = open('dt.cc2', 'w')
    k_events = len(list(event_list))
    for i, master in enumerate(event_list):
        master_sfile = master[1]
        if debug > 1:
            print('Computing correlations for master: %s' % master_sfile)
        master_event_id = master[0]
        master_event = read_nordic(master_sfile)[0]
        master_picks = master_event.picks
        master_ori_time = master_event.origins[0].time
        master_location = (master_event.origins[0].latitude,
                           master_event.origins[0].longitude,
                           master_event.origins[0].depth / 1000.0)
        master_wavefiles = readwavename(master_sfile)
        masterpath = glob.glob(wavbase + os.sep + master_wavefiles[0])
        if masterpath:
            masterstream = read(masterpath[0])
        if len(master_wavefiles) > 1:
            for wavefile in master_wavefiles:
                try:
                    masterstream += read(os.join(wavbase, wavefile))
                except:
                    raise IOError(""Couldn't find wavefile"")
                    continue
        for j in range(i + 1, k_events):
            # Use this tactic to only output unique event pairings
            slave_sfile = event_list[j][1]
            if debug > 2:
                print('Comparing to event: %s' % slave_sfile)
            slave_event_id = event_list[j][0]
            slave_wavefiles = readwavename(slave_sfile)
            try:
                slavestream = read(wavbase + os.sep + slave_wavefiles[0])
            except:
                raise IOError('No wavefile found: ' + slave_wavefiles[0] +
                              ' ' + slave_sfile)
            if len(slave_wavefiles) > 1:
                for wavefile in slave_wavefiles:
                    try:
                        slavestream += read(wavbase + os.sep + wavefile)
                    except IOError:
                        print('No waveform found: %s' %
                              (wavbase + os.sep + wavefile))
                        continue
            # Write out the header line
            event_text = '#' + str(master_event_id).rjust(10) +\
                str(slave_event_id).rjust(10) + ' 0.0   \n'
            event_text2 = '#' + str(master_event_id).rjust(10) +\
                str(slave_event_id).rjust(10) + ' 0.0   \n'
            slave_event = read_nordic(slave_sfile)[0]
            slave_picks = slave_event.picks
            slave_ori_time = slave_event.origins[0].time
            slave_location = (slave_event.origins[0].latitude,
                              slave_event.origins[0].longitude,
                              slave_event.origins[0].depth / 1000.0)
            if dist_calc(master_location, slave_location) > max_sep:
                if debug > 0:
                    print('Seperation exceeds max_sep: %s' %
                          (dist_calc(master_location, slave_location)))
                continue
            links = 0
            phases = 0
            for pick in master_picks:
                if not hasattr(pick, 'phase_hint') or \
                                len(pick.phase_hint) == 0:
                    warnings.warn('No phase-hint for pick:')
                    print(pick)
                    continue
                if pick.phase_hint[0].upper() not in ['P', 'S']:
                    warnings.warn('Will only use P or S phase picks')
                    print(pick)
                    continue
                    # Only use P and S picks, not amplitude or 'other'
                # Find station, phase pairs
                # Added by Carolin
                slave_matches = [p for p in slave_picks
                                 if hasattr(p, 'phase_hint') and
                                 p.phase_hint == pick.phase_hint and
                                 p.waveform_id.station_code ==
                                 pick.waveform_id.station_code]

                if masterstream.select(station=pick.waveform_id.station_code,
                                       channel='*' +
                                       pick.waveform_id.channel_code[-1]):
                    mastertr = masterstream.\
                        select(station=pick.waveform_id.station_code,
                               channel='*' +
                               pick.waveform_id.channel_code[-1])[0]
                elif debug > 1:
                    print('No waveform data for ' +
                          pick.waveform_id.station_code + '.' +
                          pick.waveform_id.channel_code)
                    print(pick.waveform_id.station_code +
                          '.' + pick.waveform_id.channel_code +
                          ' ' + slave_sfile + ' ' + master_sfile)
                    break
                # Loop through the matches
                for slave_pick in slave_matches:
                    if slavestream.select(station=slave_pick.waveform_id.
                                          station_code,
                                          channel='*' + slave_pick.waveform_id.
                                          channel_code[-1]):
                        slavetr = slavestream.\
                            select(station=slave_pick.waveform_id.station_code,
                                   channel='*' + slave_pick.waveform_id.
                                   channel_code[-1])[0]
                    else:
                        print('No slave data for ' +
                              slave_pick.waveform_id.station_code + '.' +
                              slave_pick.waveform_id.channel_code)
                        print(pick.waveform_id.station_code +
                              '.' + pick.waveform_id.channel_code +
                              ' ' + slave_sfile + ' ' + master_sfile)
                        break
                    # Correct the picks
                    try:
                        correction, cc =\
                            xcorr_pick_correction(
                                pick.time, mastertr, slave_pick.time,
                                slavetr, pre_pick, extract_len - pre_pick,
                                shift_len, filter=""bandpass"",
                                filter_options={'freqmin': lowcut,
                                                'freqmax': highcut},
                                plot=plotvar)
                        # Get the differential travel time using the
                        # corrected time.
                        # Check that the correction is within the allowed shift
                        # This can occur in the obspy routine when the
                        # correlation function is increasing at the end of the
                        # window.
                        if abs(correction) > shift_len:
                            warnings.warn('Shift correction too large, ' +
                                          'will not use')
                            continue
                        correction = (pick.time - master_ori_time) -\
                            (slave_pick.time + correction - slave_ori_time)
                        links += 1
                        if cc >= cc_thresh:
                            weight = cc
                            phases += 1
                            # added by Caro
                            event_text += pick.waveform_id.station_code.\
                                ljust(5) + _cc_round(correction, 3).\
                                rjust(11) + _cc_round(weight, 3).rjust(8) +\
                                ' ' + pick.phase_hint + '\n'
                            event_text2 += pick.waveform_id.station_code\
                                .ljust(5) + _cc_round(correction, 3).\
                                rjust(11) +\
                                _cc_round(weight * weight, 3).rjust(8) +\
                                ' ' + pick.phase_hint + '\n'
                            if debug > 3:
                                print(event_text)
                        else:
                            print('cc too low: %s' % cc)
                        corr_list.append(cc * cc)
                    except:
                        msg = ""Couldn't compute correlation correction""
                        warnings.warn(msg)
                        continue
            if links >= min_link and phases > 0:
                f.write(event_text)
                f2.write(event_text2)
    if plotvar:
        plt.hist(corr_list, 150)
        plt.show()
    # f.write('\n')
    f.close()
    f2.close()
    return",https://github.com/eqcorrscan/EQcorrscan/blob/3121b4aca801ee5d38f56ca297ce1c0f9515d9ff/eqcorrscan/utils/catalog_to_dd.py#L403-L630
tfidf_python_100_1.0,how to randomly pick a number,python,"def _template_gen(picks, st, length, swin='all', prepick=0.05,
                  all_horiz=False, delayed=True, plot=False, min_snr=None,
                  debug=0):
    """"""
    Master function to generate a multiplexed template for a single event.

    Function to generate a cut template as :class:`obspy.core.stream.Stream`
    from a given set of picks and data.  Should be given pre-processed
    data (downsampled and filtered).

    :type picks: list
    :param picks: Picks to extract data around, where each pick in the \
        list is an obspy.core.event.origin.Pick object.
    :type st: obspy.core.stream.Stream
    :param st: Stream to extract templates from
    :type length: float
    :param length: Length of template in seconds
    :type swin: str
    :param swin:
        P, S, P_all, S_all or all, defaults to all: see note in
        :func:`eqcorrscan.core.template_gen.template_gen`
    :type prepick: float
    :param prepick:
        Length in seconds to extract before the pick time default is 0.05
        seconds.
    :type all_horiz: bool
    :param all_horiz:
        To use both horizontal channels even if there is only a pick on one
        of them.  Defaults to False.
    :type delayed: bool
    :param delayed:
        If True, each channel will begin relative to it's own pick-time, if
        set to False, each channel will begin at the same time.
    :type plot: bool
    :param plot:
        To plot the template or not, default is False. Plots are saved as
        `template-starttime_template.png` and `template-starttime_noise.png`,
        where `template-starttime` is the start-time of the template
    :type min_snr: float
    :param min_snr:
        Minimum signal-to-noise ratio for a channel to be included in the
        template, where signal-to-noise ratio is calculated as the ratio of
        the maximum amplitude in the template window to the rms amplitude in
        the whole window given.
    :type debug: int
    :param debug: Debug output level from 0-5.

    :returns: Newly cut template.
    :rtype: :class:`obspy.core.stream.Stream`

    .. note:: By convention templates are generated with P-phases on the \
        vertical channel and S-phases on the horizontal channels, normal \
        seismograph naming conventions are assumed, where Z denotes vertical \
        and N, E, R, T, 1 and 2 denote horizontal channels, either oriented \
        or not.  To this end we will **only** use Z channels if they have a \
        P-pick, and will use one or other horizontal channels **only** if \
        there is an S-pick on it.

    .. note:: swin argument: Setting to `P` will return only data for channels
        with P picks, starting at the pick time (minus the prepick).
        Setting to `S` will return only data for channels with
        S picks, starting at the S-pick time (minus the prepick)
        (except if `all_horiz=True` when all horizontal channels will
        be returned if there is an S pick on one of them). Setting to `all`
        will return channels with either a P or S pick (including both
        horizontals if `all_horiz=True`) - with this option vertical channels
        will start at the P-pick (minus the prepick) and horizontal channels
        will start at the S-pick time (minus the prepick).
        `P_all` will return cut traces starting at the P-pick time for all
        channels. `S_all` will return cut traces starting at the S-pick
        time for all channels.

    .. warning:: If there is no phase_hint included in picks, and swin=all, \
        all channels with picks will be used.
    """"""
    from eqcorrscan.utils.debug_log import debug_print
    from eqcorrscan.utils.plotting import pretty_template_plot as tplot
    from eqcorrscan.utils.plotting import noise_plot
    from eqcorrscan.core.bright_lights import _rms
    picks_copy = copy.deepcopy(picks)  # Work on a copy of the picks and leave
    # the users picks intact.
    if not isinstance(swin, list):
        swin = [swin]
    for _swin in swin:
        assert _swin in ['P', 'all', 'S', 'P_all', 'S_all']
    for pick in picks_copy:
        if not pick.waveform_id:
            debug_print(
                ""Pick not associated with waveform, will not use it: ""
                ""{0}"".format(pick), 1, debug)
            picks_copy.remove(pick)
            continue
        if not pick.waveform_id.station_code or not \
                pick.waveform_id.channel_code:
            debug_print(
                ""Pick not associated with a channel, will not use it:""
                "" {0}"".format(pick), 1, debug)
            picks_copy.remove(pick)
            continue
    for tr in st:
        # Check that the data can be represented by float16, and check they
        # are not all zeros
        if np.all(tr.data.astype(np.float16) == 0):
            debug_print(""Trace is all zeros at float16 level, either gain or ""
                        ""check. Not using in template: {0}"".format(tr), 4,
                        debug)
            st.remove(tr)
    # Get the earliest pick-time and use that if we are not using delayed.
    picks_copy.sort(key=lambda p: p.time)
    first_pick = picks_copy[0]
    if plot:
        stplot = st.slice(first_pick.time - 20,
                          first_pick.time + length + 90).copy()
        noise = stplot.copy()
    # Work out starttimes
    starttimes = []
    for _swin in swin:
        for tr in st:
            starttime = {'station': tr.stats.station,
                         'channel': tr.stats.channel, 'picks': []}
            station_picks = [pick for pick in picks_copy
                             if pick.waveform_id.station_code ==
                             tr.stats.station]
            if _swin == 'P_all':
                p_pick = [pick for pick in station_picks
                          if pick.phase_hint.upper()[0] == 'P']
                if len(p_pick) == 0:
                    continue
                starttime.update({'picks': p_pick})
            elif _swin == 'S_all':
                s_pick = [pick for pick in station_picks
                          if pick.phase_hint.upper()[0] == 'S']
                if len(s_pick) == 0:
                    continue
                starttime.update({'picks': s_pick})
            elif _swin == 'all':
                if all_horiz and tr.stats.channel[-1] in ['1', '2', '3',
                                                          'N', 'E']:
                    # Get all picks on horizontal channels
                    channel_pick = [
                        pick for pick in station_picks
                        if pick.waveform_id.channel_code[-1] in
                        ['1', '2', '3', 'N', 'E']]
                else:
                    channel_pick = [
                        pick for pick in station_picks
                        if pick.waveform_id.channel_code == tr.stats.channel]
                if len(channel_pick) == 0:
                    continue
                starttime.update({'picks': channel_pick})
            elif _swin == 'P':
                p_pick = [pick for pick in station_picks
                          if pick.phase_hint.upper()[0] == 'P' and
                          pick.waveform_id.channel_code == tr.stats.channel]
                if len(p_pick) == 0:
                    continue
                starttime.update({'picks': p_pick})
            elif _swin == 'S':
                if tr.stats.channel[-1] in ['Z', 'U']:
                    continue
                s_pick = [pick for pick in station_picks
                          if pick.phase_hint.upper()[0] == 'S']
                if not all_horiz:
                    s_pick = [pick for pick in s_pick
                              if pick.waveform_id.channel_code ==
                              tr.stats.channel]
                starttime.update({'picks': s_pick})
                if len(starttime['picks']) == 0:
                    continue
            if not delayed:
                starttime.update({'picks': [first_pick]})
            starttimes.append(starttime)
    # Cut the data
    st1 = Stream()
    for _starttime in starttimes:
        debug_print(""Working on channel %s.%s"" %
                    (_starttime['station'], _starttime['channel']),
                    debug_level=0, print_level=debug)
        tr = st.select(
            station=_starttime['station'], channel=_starttime['channel'])[0]
        debug_print(""Found Trace %s"" % tr.__str__(), debug_level=0,
                    print_level=debug)
        noise_amp = _rms(tr.data)
        used_tr = False
        for pick in _starttime['picks']:
            if not pick.phase_hint:
                warnings.warn(
                    ""Pick for {0}.{1} has no phase hint given, you should not ""
                    ""use this template for cross-correlation""
                    "" re-picking!"".format(
                        pick.waveform_id.station_code,
                        pick.waveform_id.channel_code))
            starttime = pick.time - prepick
            debug_print(
                ""Cutting "" + tr.stats.station + '.' + tr.stats.channel, 0,
                debug)
            tr_cut = tr.slice(
                starttime=starttime, endtime=starttime + length,
                nearest_sample=False).copy()
            if plot:
                noise.select(
                    station=_starttime['station'],
                    channel=_starttime['channel']).trim(
                        noise[0].stats.starttime, starttime)
            if len(tr_cut.data) == 0:
                debug_print(
                    ""No data provided for {0}.{1} starting at {2}"".format(
                        tr.stats.station, tr.stats.channel, starttime), 3,
                    debug)
                continue
            # Ensure that the template is the correct length
            if len(tr_cut.data) == (tr_cut.stats.sampling_rate *
                                    length) + 1:
                tr_cut.data = tr_cut.data[0:-1]
            debug_print(
                'Cut starttime = %s\nCut endtime %s' %
                (str(tr_cut.stats.starttime), str(tr_cut.stats.endtime)), 0,
                debug)
            if min_snr is not None and \
               max(tr_cut.data) / noise_amp < min_snr:
                debug_print(
                    ""Signal-to-noise ratio below threshold for {0}.{1}"".format(
                        tr_cut.stats.station, tr_cut.stats.channel), 3, debug)
                continue
            st1 += tr_cut
            used_tr = True
        if not used_tr:
            debug_print('No pick for ' + tr.stats.station + '.' +
                        tr.stats.channel, 0, debug)
    if plot:
        fig1 = tplot(st1, background=stplot, picks=picks_copy,
                     title='Template for ' + str(st1[0].stats.starttime),
                     show=False, return_figure=True)
        fig2 = noise_plot(
            signal=st1, noise=noise, show=False, return_figure=True)
        fig1.savefig(""{0}_template.png"".format(st1[0].stats.starttime))
        fig2.savefig(""{0}_noise.png"".format(st1[0].stats.starttime))
        del(stplot, fig1, fig2)
    return st1",https://github.com/eqcorrscan/EQcorrscan/blob/3121b4aca801ee5d38f56ca297ce1c0f9515d9ff/eqcorrscan/core/template_gen.py#L535-L773
tfidf_python_100_1.0,how to randomly pick a number,python,"def dropna(self, how='any'):
        if how not in ('any', 'all'):
            raise ValueError(""invalid how option: {0}"".format(how))

        if self.hasnans:
            return self._shallow_copy(self.values[~self._isnan])
        return self._shallow_copy()",https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/indexes/base.py#L1961-L1967
tfidf_python_100_1.0,how to randomly pick a number,python,"def write_catalog(event_list, max_sep=8, min_link=8, debug=0):
    """"""
    Generate a dt.ct for hypoDD for a series of events.

    Takes input event list from
    :func:`eqcorrscan.utils.catalog_to_dd.write_event` as a list of tuples of
    event id and sfile.  It will read the pick information from the seisan
    formated s-file using the sfile_util utilities.

    :type event_list: list
    :param event_list: List of tuples of event_id (int) and sfile (String)
    :type max_sep: float
    :param max_sep: Maximum separation between event pairs in km
    :type min_link: int
    :param min_link:
        Minimum links for an event to be paired, e.g. minimum number of picks
        from the same station and channel (and phase) that are shared between
        two events for them to be paired.
    :type debug: int
    :param debug: Debug output level.

    :returns: list of stations that have been used in this catalog

    .. note::
        We have not yet implemented a method for taking unassociated event
        objects and wavefiles.  As such if you have events with associated
        wavefiles you are advised to generate Sfiles for each event using
        the :mod:`eqcorrscan.utils.sfile_util` module prior to this step.
    """"""
    # Cope with possibly being passed a zip in python 3.x
    event_list = list(event_list)
    f = open('dt.ct', 'w')
    f2 = open('dt.ct2', 'w')
    fphase = open('phase.dat', 'w')
    stations = []
    evcount = 0
    for i, master in enumerate(event_list):
        master_sfile = master[1]
        master_event_id = master[0]
        master_event = read_nordic(master_sfile)[0]
        master_ori_time = master_event.origins[0].time
        master_location = (master_event.origins[0].latitude,
                           master_event.origins[0].longitude,
                           master_event.origins[0].depth / 1000)
        if len(master_event.magnitudes) > 0:
            master_magnitude = master_event.magnitudes[0].mag or ' '
        else:
            master_magnitude = ' '
        header = '# ' + \
            master_ori_time.strftime('%Y  %m  %d  %H  %M  %S.%f') +\
            ' ' + str(master_location[0]).ljust(8) + ' ' +\
            str(master_location[1]).ljust(8) + ' ' +\
            str(master_location[2]).ljust(4) + ' ' +\
            str(master_magnitude).ljust(4) + ' 0.0 0.0 0.0' +\
            str(master_event_id).rjust(4)
        fphase.write(header + '\n')
        for pick in master_event.picks:
            if not hasattr(pick, 'phase_hint') or len(pick.phase_hint) == 0:
                warnings.warn('No phase-hint for pick:')
                print(pick)
                continue
            if pick.phase_hint[0].upper() in ['P', 'S']:
                weight = [arrival.time_weight
                          for arrival in master_event.origins[0].arrivals
                          if arrival.pick_id == pick.resource_id][0]
                # Convert seisan weight to hypoDD 0-1 weights
                if weight == 0:
                    weight = 1.0
                elif weight == 9:
                    weight = 0.0
                else:
                    weight = 1 - weight / 4.0
                fphase.write(pick.waveform_id.station_code + '  ' +
                             _cc_round(pick.time -
                                       master_ori_time, 3).rjust(6) +
                             '   ' + str(weight).ljust(5) +
                             pick.phase_hint + '\n')
        for j in range(i + 1, len(event_list)):
            # Use this tactic to only output unique event pairings
            slave_sfile = event_list[j][1]
            slave_event_id = event_list[j][0]
            # Write out the header line
            event_text = '#' + str(master_event_id).rjust(10) +\
                str(slave_event_id).rjust(10) + '\n'
            event_text2 = '#' + str(master_event_id).rjust(10) +\
                str(slave_event_id).rjust(10) + '\n'
            slave_event = read_nordic(slave_sfile)[0]
            slave_ori_time = slave_event.origins[0].time
            slave_location = (slave_event.origins[0].latitude,
                              slave_event.origins[0].longitude,
                              slave_event.origins[0].depth / 1000)
            if dist_calc(master_location, slave_location) > max_sep:
                continue
            links = 0  # Count the number of linkages
            for pick in master_event.picks:
                if not hasattr(pick, 'phase_hint') or\
                                len(pick.phase_hint) == 0:
                    continue
                if pick.phase_hint[0].upper() not in ['P', 'S']:
                    continue
                    # Only use P and S picks, not amplitude or 'other'
                # Added by Carolin
                slave_matches = [p for p in slave_event.picks
                                 if hasattr(p, 'phase_hint') and
                                 p.phase_hint == pick.phase_hint and
                                 p.waveform_id.station_code.upper() ==
                                 pick.waveform_id.station_code.upper()]
                # Loop through the matches
                for slave_pick in slave_matches:
                    links += 1
                    master_weight = [arrival.time_weight
                                     for arrival in master_event.
                                     origins[0].arrivals
                                     if arrival.pick_id == pick.resource_id][0]
                    slave_weight = [arrival.time_weight
                                    for arrival in slave_event.
                                    origins[0].arrivals
                                    if arrival.pick_id ==
                                    slave_pick.resource_id][0]
                    master_weight = str(int(master_weight))
                    slave_weight = str(int(slave_weight))
                    event_text += pick.waveform_id.station_code.ljust(5) +\
                        _cc_round(pick.time - master_ori_time, 3).rjust(11) +\
                        _cc_round(slave_pick.time -
                                  slave_ori_time, 3).rjust(8) +\
                        _av_weight(master_weight, slave_weight).rjust(7) +\
                        ' ' + pick.phase_hint + '\n'
                    # Added by Carolin
                    event_text2 += pick.waveform_id.station_code.ljust(5) +\
                        _cc_round(pick.time - master_ori_time, 3).rjust(11) +\
                        _cc_round(slave_pick.time -
                                  slave_ori_time, 3).rjust(8) +\
                        _av_weight(master_weight, slave_weight).rjust(7) +\
                        ' ' + pick.phase_hint + '\n'
                    stations.append(pick.waveform_id.station_code)
            if links >= min_link:
                f.write(event_text)
                f2.write(event_text2)
                evcount += 1
    print('You have ' + str(evcount) + ' links')
    # f.write('\n')
    f.close()
    f2.close()
    fphase.close()
    return list(set(stations))",https://github.com/eqcorrscan/EQcorrscan/blob/3121b4aca801ee5d38f56ca297ce1c0f9515d9ff/eqcorrscan/utils/catalog_to_dd.py#L256-L400
tfidf_python_100_1.0,how to randomly pick a number,python,"def href_match_to_url(cls, match):
    def pick(group):
      return '' if group is None else group
    return unescape(pick(match.group(1)) or pick(match.group(2)) or pick(match.group(3)))",https://github.com/pantsbuild/pex/blob/87b2129d860250d3b9edce75b9cb62f9789ee521/pex/crawler.py#L45-L48
tfidf_python_100_1.0,how to randomly pick a number,python,"def extract_from_stream(stream, detections, pad=5.0, length=30.0):
    """"""
    Extract waveforms for a list of detections from a stream.

    :type stream: obspy.core.stream.Stream
    :param stream: Stream containing the detections.
    :type detections: list
    :param detections: list of eqcorrscan.core.match_filter.detection
    :type pad: float
    :param pad: Pre-detection extract time in seconds.
    :type length: float
    :param length: Total extracted length in seconds.

    :returns:
        list of :class:`obspy.core.stream.Stream`, one for each detection.
    :type: list
    """"""
    streams = []
    for detection in detections:
        cut_stream = Stream()
        for pick in detection.event.picks:
            tr = stream.select(station=pick.waveform_id.station_code,
                               channel=pick.waveform_id.channel_code)
            if len(tr) == 0:
                print('No data in stream for pick:')
                print(pick)
                continue
            cut_stream += tr.slice(
                starttime=pick.time - pad,
                endtime=pick.time - pad + length).copy()
        streams.append(cut_stream)
    return streams",https://github.com/eqcorrscan/EQcorrscan/blob/3121b4aca801ee5d38f56ca297ce1c0f9515d9ff/eqcorrscan/core/match_filter.py#L3923-L3954
tfidf_python_100_1.0,how to randomly pick a number,python,"def set_meta_profiling(how):
    """""" Enables or disables the profiling of metadata at the loading of a GMQLDataset

    :param how: True if you want to analyze the metadata when a GMQLDataset is created
                by a load_from_*. False otherwise. (Default=True)
    :return: None
    """"""
    global __metadata_profiling
    if isinstance(how, bool):
        __metadata_profiling = how
    else:
        raise TypeError(""how must be boolean. {} was provided"".format(type(how)))",https://github.com/DEIB-GECO/PyGMQL/blob/e58b2f9402a86056dcda484a32e3de0bb06ed991/gmql/settings.py#L136-L147
tfidf_python_100_1.0,normal distribution,python,"def __init__(self, numSamples, distribution):
    self.numSamples = numSamples
    self.distribution = distribution
    self.possibleValues = distribution.possibleValues",https://github.com/numenta/htmresearch/blob/70c096b09a577ea0432c3f3bfff4442d4871b7aa/htmresearch/frameworks/location/ambiguity_index.py#L107-L110
tfidf_python_100_1.0,normal distribution,python,"def get_distribution_time_index_frequency(self, distribution):
        if isinstance(distribution, dict):
            distribution = distribution
        else:
            distribution = self.get_distribution(distribution)

        return time_series.get_distribution_time_index_frequency(distribution)",https://github.com/datosgobar/pydatajson/blob/3141082ffbaa295e2deaf6ffbbc5a59f5859960e/pydatajson/core.py#L199-L205
tfidf_python_100_1.0,normal distribution,python,"def get_distribution_time_index(self, distribution):
        if isinstance(distribution, dict):
            distribution = distribution
        else:
            distribution = self.get_distribution(distribution)

        return time_series.get_distribution_time_index(distribution)",https://github.com/datosgobar/pydatajson/blob/3141082ffbaa295e2deaf6ffbbc5a59f5859960e/pydatajson/core.py#L191-L197
tfidf_python_100_1.0,normal distribution,python,"def generateObjects(numObjects, featuresPerObject, objectWidth, numFeatures,
                    distribution=""AllFeaturesEqual_Replacement""):
  assert featuresPerObject <= (objectWidth ** 2)

  objectFeatures = generateObjectFeatures(numObjects,
                                          featuresPerObject,
                                          numFeatures,
                                          distribution)

  return arrangeFeatures(objectFeatures, objectWidth)",https://github.com/numenta/htmresearch/blob/70c096b09a577ea0432c3f3bfff4442d4871b7aa/htmresearch/frameworks/location/object_generation.py#L107-L116
tfidf_python_100_1.0,normal distribution,python,"def setemission(self, state, distribution):
        self.nodes.add(state)
        if not isinstance(distribution, Distribution):
            distribution = Distribution(distribution)
        self.edges_toobservables[state] = distribution
        self.observablenodes.update(distribution.keys())",https://github.com/proycon/pynlpl/blob/7707f69a91caaa6cde037f0d0379f1d42500a68b/pynlpl/statistics.py#L454-L459
tfidf_python_100_1.0,normal distribution,python,"def run(self):
        # Write the syspaths file
        if getattr(self.distribution, 'salt_syspaths_hardcoded_path', None) is None:
            print('This command is not meant to be called on it\'s own')
            exit(1)

        # Write the system paths file
        open(self.distribution.salt_syspaths_hardcoded_path, 'w').write(
            INSTALL_SYSPATHS_TEMPLATE.format(
                date=DATE,
                root_dir=self.distribution.salt_root_dir,
                share_dir=self.distribution.salt_share_dir,
                config_dir=self.distribution.salt_config_dir,
                cache_dir=self.distribution.salt_cache_dir,
                sock_dir=self.distribution.salt_sock_dir,
                srv_root_dir=self.distribution.salt_srv_root_dir,
                base_file_roots_dir=self.distribution.salt_base_file_roots_dir,
                base_pillar_roots_dir=self.distribution.salt_base_pillar_roots_dir,
                base_master_roots_dir=self.distribution.salt_base_master_roots_dir,
                base_thorium_roots_dir=self.distribution.salt_base_thorium_roots_dir,
                logs_dir=self.distribution.salt_logs_dir,
                pidfile_dir=self.distribution.salt_pidfile_dir,
                spm_parent_path=self.distribution.salt_spm_parent_dir,
                spm_formula_path=self.distribution.salt_spm_formula_dir,
                spm_pillar_path=self.distribution.salt_spm_pillar_dir,
                spm_reactor_path=self.distribution.salt_spm_reactor_dir,
                home_dir=self.distribution.salt_home_dir,
            )
        )",https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/setup.py#L286-L314
tfidf_python_100_1.0,normal distribution,python,"def settransitions(self, state, distribution):
        self.nodes.add(state)
        if not isinstance(distribution, Distribution):
            distribution = Distribution(distribution)
        self.edges_out[state] = distribution
        self.nodes.update(distribution.keys())",https://github.com/proycon/pynlpl/blob/7707f69a91caaa6cde037f0d0379f1d42500a68b/pynlpl/statistics.py#L354-L359
tfidf_python_100_1.0,normal distribution,python,"def add(self, distribution):
        key = '%s-%s' % (distribution.name, distribution.version)
        self[key] = distribution",https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/pkginfo/index.py#L12-L14
tfidf_python_100_1.0,normal distribution,python,"def process_election_distribution(self, last_candidate_aggregates):
        distribution, *self.election_distributions_pending = self.election_distributions_pending
        return self.process_election(distribution, last_candidate_aggregates)",https://github.com/grahame/dividebatur/blob/adc1f6e8013943471f1679e3c94f9448a1e4a472/dividebatur/counter.py#L476-L478
tfidf_python_100_1.0,normal distribution,python,"def process_exclusion_distribution(self, last_candidate_aggregates):
        distribution, *self.exclusion_distributions_pending = self.exclusion_distributions_pending
        return self.process_exclusion(distribution, last_candidate_aggregates)",https://github.com/grahame/dividebatur/blob/adc1f6e8013943471f1679e3c94f9448a1e4a472/dividebatur/counter.py#L469-L471
tfidf_python_100_1.0,normal distribution,python,"def __init__(self, distribution):
        """"""
        Args:
            distribution (openturns.Distribution):
                1D distribution created in OpenTURNS.
        """"""
        Dist.__init__(self)
        if distribution.getDimension() != 1:
            raise Exception(""Only 1D OpenTURNS distribution are supported for now"")
        self.distribution = distribution",https://github.com/jonathf/chaospy/blob/25ecfa7bf5608dc10c0b31d142ded0e3755f5d74/chaospy/distributions/collection/openturns.py#L10-L19
tfidf_python_100_1.0,normal distribution,python,"def __init__(self, normal, w):
        self.normal = normal
        self.w = w",https://github.com/fogleman/pg/blob/124ea3803c788b2c98c4f3a428e5d26842a67b58/pg/csg.py#L59-L61
tfidf_python_100_1.0,normal distribution,python,"def install(self, install_prefix, distribution, configure=True, is_remote=False, in_project=False):
        if isinstance(distribution, string_types) or not distribution:
            distribution = MaltegoDistribution(distribution)
        if not isinstance(distribution, MtzDistribution):
            if distribution.version >= '3.4.0':
                raise ValueError(INCOMPATIBLE)
            print('Installing transform package %s...' % self.name, file=sys.stderr)

        install_prefix = self._init_install_prefix(install_prefix)

        self._install_transforms(install_prefix, distribution, in_project)
        self._install_entities(distribution)
        self._install_machines(distribution)

        if configure:
            self.configure(install_prefix, remote=is_remote)",https://github.com/redcanari/canari3/blob/322d2bae4b49ac728229f418b786b51fcc227352/src/canari/pkgutils/transform.py#L306-L321
tfidf_python_100_1.0,normal distribution,python,"def get_os_dist_info():
    """"""
        Returns the distribution info
    """"""

    distribution = platform.dist()
    dist_name = distribution[0].lower()
    dist_version_str = distribution[1]
    if dist_name and dist_version_str:
        return dist_name, dist_version_str
    else:
        return None, None",https://github.com/mongolab/mongoctl/blob/fab15216127ad4bf8ea9aa8a95d75504c0ef01a2/mongoctl/binary_repo.py#L442-L453
tfidf_python_100_1.0,normal distribution,python,"def as_random_variable(distribution,
                       sample_shape=(),
                       value=None):
  """"""Wrap an existing distribution as a traceable random variable.

  This enables the use of custom or user-provided distributions in
  Edward models. Unlike a bare `RandomVariable` object, this method
  wraps the constructor so it is included in the Edward trace and its
  values can be properly intercepted and overridden.

  Where possible, you should prefer the built-in constructors
  (`ed.Normal`, etc); these simultaneously construct a Distribution
  and a RandomVariable object so that the distribution parameters
  themselves may be intercepted and overridden. RVs constructed via
  `as_random_variable()` have a fixed distribution and may not support
  program transformations (e.g, conjugate marginalization) that rely
  on overriding distribution parameters.

  Args:
    distribution: tfd.Distribution governing the distribution of the random
      variable, such as sampling and log-probabilities.
    sample_shape: tf.TensorShape of samples to draw from the random variable.
      Default is `()` corresponding to a single sample.
    value: Fixed tf.Tensor to associate with random variable. Must have shape
      `sample_shape + distribution.batch_shape + distribution.event_shape`.
      Default is to sample from random variable according to `sample_shape`.

  Returns:
    rv: a `RandomVariable` wrapping the provided distribution.

  #### Example

  ```python
  from tensorflow_probability import distributions as tfd
  from tensorflow_probability import edward2 as ed

  def model():
    # equivalent to ed.Normal(0., 1., name='x')
    return ed.as_random_variable(tfd.Normal(0., 1., name='x'))

  log_joint = ed.make_log_joint_fn(model)
  output = log_joint(x=2.)
  ```
  """"""

  return _build_custom_rv(distribution=distribution,
                          sample_shape=sample_shape,
                          value=value,
                          name=_simple_name(distribution))",https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/edward2/generated_random_variables.py#L97-L145
tfidf_python_100_1.0,normal distribution,python,"def file_is_available(self):
        return self.distribution and self.distribution.storage.exists(self.distribution.name)",https://github.com/mvantellingen/localshop/blob/32310dc454720aefdea5bf4cea7f78a38c183954/src/localshop/apps/packages/models.py#L249-L250
tfidf_python_100_1.0,normal distribution,python,"def fixup_distribution(distribution, count):
    zeros = count - sum(distribution.values())
    distribution[0] = zeros + distribution.get(0, 0)
    return distribution",https://github.com/mixcloud/django-experiments/blob/1f45e9f8a108b51e44918daa647269b2b8d43f1d/experiments/admin_utils.py#L47-L50
tfidf_python_100_1.0,normal distribution,python,"def get_distributions(catalog, filter_in=None, filter_out=None,
                      meta_field=None, exclude_meta_fields=None,
                      only_time_series=False):
    """"""Devuelve lista de distribuciones del catÃ¡logo o de uno de sus metadatos.

    Args:
        catalog (dict, str or DataJson): RepresentaciÃ³n externa/interna de un
            catÃ¡logo. Una representaciÃ³n _externa_ es un path local o una
            URL remota a un archivo con la metadata de un catÃ¡logo, en
            formato JSON o XLSX. La representaciÃ³n _interna_ de un catÃ¡logo
            es un diccionario. Ejemplos: http://datos.gob.ar/data.json,
            http://www.ign.gob.ar/descargas/geodatos/catalog.xlsx,
            ""/energia/catalog.xlsx"".
        filter_in (dict): Devuelve los distribuciones cuyos atributos
            coinciden con los pasados en este diccionario. Ejemplo::
                {
                    ""dataset"": {
                        ""publisher"": {""name"": ""Ministerio de Ambiente""}
                    }
                }
            SÃ³lo se devolverÃ¡n los distribuciones que pertenezcan a un dataset
            de ese publisher_name.
        filter_out (dict): Devuelve los distribuciones cuyos atributos no
            coinciden con los pasados en este diccionario. Ejemplo::
                {
                    ""dataset"": {
                        ""publisher"": {""name"": ""Ministerio de Ambiente""}
                    }
                }
            SÃ³lo se devolverÃ¡n los distribuciones que no pertenezcan a un
            dataset de ese publisher_name.
        meta_field (str): Nombre de un metadato de Distribution. En lugar de
            devolver los objetos completos Distribution, devuelve una lista de
            valores para ese metadato presentes en el catÃ¡logo.
        exclude_meta_fields (list): Metadatos de Distribution que se quieren
            excluir de los objetos Distribution devueltos.
        only_time_series (bool): Si es verdadero, sÃ³lo devuelve distribuciones
            que sean distribuciones de series de tiempo.
    """"""

    filter_in = filter_in or {}
    filter_out = filter_out or {}
    catalog = read_catalog_obj(catalog)

    distributions = []
    for dataset in get_datasets(catalog, filter_in, filter_out):
        for distribution in dataset.get(""distribution"", []):
            # agrega el id del dataset
            distribution[""dataset_identifier""] = dataset[""identifier""]
            distributions.append(distribution)

    filtered_distributions = [
        distribution for distribution in distributions if
        _filter_dictionary(distribution, filter_in.get(""distribution""),
                           filter_out.get(""distribution""))
    ]

    # realiza filtros especiales
    if only_time_series:
        filtered_distributions = [distribution for distribution in
                                  filtered_distributions if
                                  distribution_has_time_index(distribution)]

    if meta_field:
        return [distribution[meta_field]
                for distribution in filtered_distributions
                if meta_field in distribution]

    if exclude_meta_fields:
        meta_filtered_distributions = []
        for distribution in filtered_distributions:
            distribution_meta_filtered = distribution.copy()
            for excluded_meta_field in exclude_meta_fields:
                distribution_meta_filtered.pop(excluded_meta_field, None)
            meta_filtered_distributions.append(distribution_meta_filtered)

        return meta_filtered_distributions

    else:
        return filtered_distributions",https://github.com/datosgobar/pydatajson/blob/3141082ffbaa295e2deaf6ffbbc5a59f5859960e/pydatajson/search.py#L101-L180
tfidf_python_100_1.0,normal distribution,python,"def get_distribution():
    distribution = platform.dist()
    verbose(""Detected system: {}"".format("" "".join(distribution)))
    distribution = distribution[0].lower()
    if distribution not in PACKAGE_TOOLS:
        warning(""Unknown distribution. Can't suggest packages to install"")
    return distribution",https://github.com/openpaperwork/paperwork-backend/blob/114b831e94e039e68b339751fd18250877abad76/paperwork_backend/shell_cmd.py#L61-L67
tfidf_python_100_1.0,normal distribution,python,"def run(self):
        if self.distribution.extras_require is None:
            self.distribution.extras_require = {}
        self.distribution.extras_require['tests'] = self.distribution.tests_require
        EggInfoCommand.run(self)",https://github.com/manahl/pytest-plugins/blob/54ca3c07a6334eebfa4928d7a0dc0708ab6e723c/common_setup.py#L38-L42
tfidf_python_100_1.0,nelder mead optimize,python,"def __init__(self, docsearch, optimize, progress_cb=dummy_progress_cb):
        self.docsearch = docsearch
        self.optimize = optimize
        self.progress_cb = progress_cb",https://github.com/openpaperwork/paperwork-backend/blob/114b831e94e039e68b339751fd18250877abad76/paperwork_backend/docsearch.py#L193-L196
tfidf_python_100_1.0,nelder mead optimize,python,"def __init__(self, optimize=False):
        super(MessageSerializer, self).__init__()
        self._optimize = optimize",https://github.com/foxdog-studios/pyddp/blob/a4ac0bd5d8a2f350e012fd65d79e0034a89d8e67/ddp/messages/message_serializer.py#L25-L27
tfidf_python_100_1.0,nelder mead optimize,python,"def infer_newX(self, Y_new, optimize=True):
        """"""
        Infer X for the new observed data *Y_new*.

        :param Y_new: the new observed data for inference
        :type Y_new: numpy.ndarray
        :param optimize: whether to optimize the location of new X (True by default)
        :type optimize: boolean
        :return: a tuple containing the posterior estimation of X and the model that optimize X
        :rtype: (:class:`~GPy.core.parameterization.variational.VariationalPosterior` and numpy.ndarray, :class:`~GPy.core.model.Model`)
        """"""
        from ..inference.latent_function_inference.inferenceX import infer_newX
        return infer_newX(self, Y_new, optimize=optimize)",https://github.com/SheffieldML/GPy/blob/54c32d79d289d622fb18b898aee65a2a431d90cf/GPy/core/gp.py#L666-L678
tfidf_python_100_1.0,nelder mead optimize,python,"def optimize(self):
        """"""Optimize index for faster by-document-id queries.""""""
        self.check_session()
        result = self.session.optimize()
        if self.autosession:
            self.commit()
        return result",https://github.com/RaRe-Technologies/gensim-simserver/blob/e7e59e836ef6d9da019a8c6b218ef0bdd998b2da/simserver/simserver.py#L959-L965
tfidf_python_100_1.0,nelder mead optimize,python,"def __init__(self, optimize = True, **attrs):
        '''
        API:
            __init__(self, optimize = True, **attrs):
        Description:
            Class constructor.
        Input:
            optimize: Optimizes find() if True.
            attrs: Graph attributes.
        Post:
            self.optimize will be updated.
        '''
        attrs['type'] = DIRECTED_GRAPH
        Graph.__init__(self, **attrs)
        self.sizes = {}
        self.optimize = optimize",https://github.com/coin-or/GiMPy/blob/51853122a50eb6019d06bbdedbfc396a833b5a22/src/gimpy/graph.py#L3399-L3414
tfidf_python_100_1.0,nelder mead optimize,python,"def commit(self, waitFlush=True, waitSearcher=True, optimize=False):
        xstr = '<commit'
        if optimize:
            xstr = '<optimize'
        if not waitSearcher:  # just handle deviations from the default
            if not waitFlush:
                xstr += ' waitFlush=""false"" waitSearcher=""false""'
            else:
                xstr += ' waitSearcher=""false""'
        xstr += '/>'
        return self.doUpdateXML(xstr)",https://github.com/DataONEorg/d1_python/blob/3ac4d4f3ca052d3e8641a6a329cab526c8ddcb0d/client_onedrive/src/d1_onedrive/impl/drivers/dokan/solrclient.py#L387-L397
tfidf_python_100_1.0,nelder mead optimize,python,"def optimize(self):
        return bayesopt.optimize(self.train_valid, self.dim, self.lower_bound, self.upper_bound, self.hyperparams)",https://github.com/vrasneur/pyfasttext/blob/7c0a14e7fd4c4841847223853f8cf6676cd064bb/examples/bayesopt_search_cont.py#L108-L109
tfidf_python_100_1.0,nelder mead optimize,python,"def commit(self, waitFlush=True, waitSearcher=True, optimize=False):
        xstr = '<commit'
        if optimize:
            xstr = '<optimize'
        if not waitSearcher:  # just handle deviations from the default
            if not waitFlush:
                xstr += ' waitFlush=""false"" waitSearcher=""false""'
            else:
                xstr += ' waitSearcher=""false""'
        xstr += '/>'
        return self.query('solr', xstr, do_post=True)",https://github.com/DataONEorg/d1_python/blob/3ac4d4f3ca052d3e8641a6a329cab526c8ddcb0d/lib_client/src/d1_client/solr_client.py#L395-L405
tfidf_python_100_1.0,nelder mead optimize,python,"def check_options_optimizing(options, log):
    if options.optimize >= 2:
        check_external_program(
            log=log,
            program='pngquant',
            package='pngquant',
            version_checker=pngquant.version,
            need_version='2.0.1',
            required_for='--optimize {2,3}',
        )

    if options.optimize >= 2:
        # Although we use JBIG2 for optimize=1, don't nag about it unless the
        # user is asking for more optimization
        check_external_program(
            log=log,
            program='jbig2',
            package='jbig2enc',
            version_checker=jbig2enc.version,
            need_version='0.28',
            required_for='--optimize {2,3} | --jbig2-lossy',
            recommended=True if not options.jbig2_lossy else False,
        )

    if options.optimize == 0 and any(
        [options.jbig2_lossy, options.png_quality, options.jpeg_quality]
    ):
        log.warning(
            ""The arguments --jbig2-lossy, --png-quality, and --jpeg-quality ""
            ""will be ignored because --optimize=0.""
        )",https://github.com/jbarlow83/OCRmyPDF/blob/79c84eefa353632a3d7ccddbd398c6678c1c1777/src/ocrmypdf/__main__.py#L640-L670
tfidf_python_100_1.0,nelder mead optimize,python,"def infer_newX(model, Y_new, optimize=True, init='L2'):
    """"""
    Infer the distribution of X for the new observed data *Y_new*.

    :param model: the GPy model used in inference
    :type model: GPy.core.Model
    :param Y_new: the new observed data for inference
    :type Y_new: numpy.ndarray
    :param optimize: whether to optimize the location of new X (True by default)
    :type optimize: boolean
    :return: a tuple containing the estimated posterior distribution of X and the model that optimize X
    :rtype: (GPy.core.parameterization.variational.VariationalPosterior, GPy.core.Model)
    """"""
    infr_m = InferenceX(model, Y_new, init=init)

    if optimize:
        infr_m.optimize()

    return infr_m.X, infr_m",https://github.com/SheffieldML/GPy/blob/54c32d79d289d622fb18b898aee65a2a431d90cf/GPy/inference/latent_function_inference/inferenceX.py#L9-L27
tfidf_python_100_1.0,nelder mead optimize,python,"def curve_fit(*args, **kwargs):
    with lock:
        import scipy.optimize
        return scipy.optimize.curve_fit(*args, **kwargs)",https://github.com/lbusoni/plico/blob/08a29da8f06e920470516838878a51ac83bab847/plico/utils/plico_scipy.py#L46-L49
tfidf_python_100_1.0,nelder mead optimize,python,"def leastsq(*args, **kwargs):
    with lock:
        import scipy.optimize
        return scipy.optimize.leastsq(*args, **kwargs)",https://github.com/lbusoni/plico/blob/08a29da8f06e920470516838878a51ac83bab847/plico/utils/plico_scipy.py#L16-L19
tfidf_python_100_1.0,nelder mead optimize,python,"def optimize(self, duplicate_manager=None):
        """"""
        Optimizes the acquisition function (uses a flag from the model to use gradients or not).
        """"""
        if not self.analytical_gradient_acq:
            out = self.optimizer.optimize(f=self.acquisition_function, duplicate_manager=duplicate_manager)
        else:
            out = self.optimizer.optimize(f=self.acquisition_function, f_df=self.acquisition_function_withGradients, duplicate_manager=duplicate_manager)
        return out",https://github.com/SheffieldML/GPyOpt/blob/255539dc5927819ca701e44fe3d76cd4864222fa/GPyOpt/acquisitions/base.py#L52-L60
tfidf_python_100_1.0,nelder mead optimize,python,"def optimize(self, env, compiler):
    return Try(self.test.optimize(env, compiler), self.body.optimize(env, compiler))",https://github.com/chaosim/dao/blob/d7ba65c98ee063aefd1ff4eabb192d1536fdbaaa/dao/interlang/element.py#L366-L367
tfidf_python_100_1.0,nelder mead optimize,python,"def optimize(self):
        for core in self.endpoints:
            self._send_solr_command(self.endpoints[core], ""{ \""optimize\"": {} }"")",https://github.com/izacus/pysolarized/blob/d820e20a45b63e5b88b0421eb703037a8d7ad4a3/pysolarized/solr.py#L154-L156
tfidf_python_100_1.0,nelder mead optimize,python,"def robot_wireless(optimize=True, verbose=True, plot=True):
    from matplotlib import pyplot as plt
    import GPy
    import pods

    data = pods.datasets.robot_wireless()
    # optimize
    m = GPy.models.BayesianGPLVM(data['Y'], 4, num_inducing=25)
    if optimize: m.optimize(messages=verbose, max_f_eval=10000)
    if plot:
        m.plot_latent()

    return m",https://github.com/SheffieldML/GPy/blob/54c32d79d289d622fb18b898aee65a2a431d90cf/GPy/examples/dimensionality_reduction.py#L626-L638
tfidf_python_100_1.0,nelder mead optimize,python,"def optimize(self, env, compiler):
    return self.__class__(self.item.optimize(env, compiler))",https://github.com/chaosim/dao/blob/d7ba65c98ee063aefd1ff4eabb192d1536fdbaaa/dao/interlang/vop.py#L708-L709
tfidf_python_100_1.0,nelder mead optimize,python,"def optimize(self, env, compiler):
    return ExpressionWithCode(self.exp, self.function.optimize(env, compiler))",https://github.com/chaosim/dao/blob/d7ba65c98ee063aefd1ff4eabb192d1536fdbaaa/dao/interlang/lamda.py#L548-L549
tfidf_python_100_1.0,nelder mead optimize,python,"def byte_compile(self, to_compile):
        if sys.dont_write_bytecode:
            return

        from distutils.util import byte_compile

        try:
            # try to make the byte compile messages quieter
            log.set_verbosity(self.verbose - 1)

            byte_compile(to_compile, optimize=0, force=1, dry_run=self.dry_run)
            if self.optimize:
                byte_compile(
                    to_compile, optimize=self.optimize, force=1,
                    dry_run=self.dry_run,
                )
        finally:
            log.set_verbosity(self.verbose)",https://github.com/pypa/setuptools/blob/83c667e0b2a98193851c07115d1af65011ed0fb6/setuptools/command/easy_install.py#L1257-L1274
tfidf_python_100_1.0,nelder mead optimize,python,"def initialize_batch(self, duplicate_manager=None,context_manager=None):

        x, _ = self.acquisition.optimize(duplicate_manager=duplicate_manager)

        return x",https://github.com/SheffieldML/GPyOpt/blob/255539dc5927819ca701e44fe3d76cd4864222fa/GPyOpt/core/evaluators/batch_random.py#L22-L26
tfidf_python_100_1.0,hash set for counting distinct elements,python,"def distinct(self):
        """"""
        Only return distinct row. 
        Return a new query set with distinct mark
        """"""
        new_query_set = self.clone()
        new_query_set.query.distinct = True
        return new_query_set",https://github.com/firstprayer/monsql/blob/6285c15b574c8664046eae2edfeb548c7b173efd/monsql/queryset.py#L110-L117
tfidf_python_100_1.0,hash set for counting distinct elements,python,"def estimate_background(self):
        counting={'a':.0,'c':.0,'g':.0,'t':.0}
        all=0.0
        
        for chr_id in self.genome.keys():
            if self.verbose:
                start_time = time.time()
                print 'Counting on:',chr_id

            for nt in counting.keys():
                
                count_nt=self.genome[chr_id][:].lower().count(nt)
                counting[nt]+=count_nt
                all+=count_nt

            print 'elapsed:',time.time() - start_time
        
        if self.verbose:
            print counting

        for nt in counting.keys():
            counting[nt]/=all
        
        return counting",https://github.com/lucapinello/Haystack/blob/cc080d741f36cd77b07c0b59d08ea6a4cf0ef2f7/haystack/bioutilities.py#L778-L801
tfidf_python_100_1.0,hash set for counting distinct elements,python,"def estimate_background(self):
        counting={'a':.0,'c':.0,'g':.0,'t':.0}
        all=0.0
        
        for chr_id in self.chr.keys():
            if self.verbose:
                print 'Counting on:',chr_id
            
            self.chr[chr_id].seek(0)
            self.chr[chr_id].readline()
            
            for line in self.chr[chr_id]:
                for nt in counting.keys():
                    count_nt=line.lower().count(nt)
                    counting[nt]+=count_nt
                    all+=count_nt
        
        if self.verbose:
            print counting
        
        for nt in counting.keys():
            counting[nt]/=all
            
        return counting",https://github.com/lucapinello/Haystack/blob/cc080d741f36cd77b07c0b59d08ea6a4cf0ef2f7/haystack/bioutilities.py#L621-L644
tfidf_python_100_1.0,hash set for counting distinct elements,python,"def __init__(
            self,
            elements,
            distinct=False,
            sort_key=None,
            sources=set([])):
        """"""
        Parameters
        ----------
        elements : list
            Collection of any class which  is compatible with the sort key

        distinct : bool
            Only keep distinct entries or allow duplicates.

        sort_key : fn
            Function which maps each element to a sorting criterion.

        sources : set
            Set of files from which this collection was generated.
        """"""
        self.distinct = distinct
        if distinct:
            elements = set(elements)
        self.sort_key = sort_key
        if self.sort_key is None:
            self.elements = elements
        else:
            self.elements = sorted(elements, key=sort_key)
        self.sources = sources",https://github.com/openvax/sercol/blob/e66a8e8c3c0b21e53eb8f73be4d23409fab311ae/sercol/collection.py#L26-L55
tfidf_python_100_1.0,hash set for counting distinct elements,python,"def __init__(self, values, distinct=False):
        self.distinct = distinct
        super(Return, self).__init__(values)",https://github.com/bruth/cypher/blob/4f962f51539ac5a667ab5a050b6b4052d4c10c0f/cypher/syntax.py#L507-L509
tfidf_python_100_1.0,hash set for counting distinct elements,python,"def is_quantized(x, N=1000, distinct=0.1):
    if isinstance(x, pd.DataFrame):
        return [is_quantized(x[c]) for c in x.columns]
    elif isinstance(x, np.ndarrayclass_or_type_or_tuple):
        if len(x.shape) == 1:
            return is_quantized(np.array(x))
        else:
            return [is_quantized(row) for row in x]
    else:
        N = min(N, len(x)) or len(x)
        if distinct <= 1:
            distinct = distinct * N
        M = len(set(x[:N]))
        if M <= distinct:
            return True
        else:
            return False",https://github.com/totalgood/twip/blob/5c0411d2acfbe5b421841072814c9152591c03f7/twip/plot.py#L24-L40
tfidf_python_100_1.0,hash set for counting distinct elements,python,"def __init__(self, col, distinct=False, **extra):
        super(SqlCount, self).__init__(col, distinct=distinct and 'DISTINCT ' or '', **extra)",https://github.com/henriquebastos/django-aggregate-if/blob/588c1487bc88a8996d4ee9c2c9d50fa4a4484872/aggregate_if.py#L88-L89
tfidf_python_100_1.0,hash set for counting distinct elements,python,"def get_distinct_sql(self):
        if self._distinct and self.distinct_ons:
            raise ValueError('Cannot combine distinct and distinct_on')
        if self._distinct:
            return 'DISTINCT '
        if self.distinct_ons:
            return 'DISTINCT ON ({0}) '.format(', '.join(f.get_sql() for f in self.distinct_ons))
        return ''",https://github.com/ambitioninc/django-query-builder/blob/113a7d845d3ddc6a45621b9880308e756f87c5bf/querybuilder/query.py#L1386-L1393
tfidf_python_100_1.0,hash set for counting distinct elements,python,"def __init__(self, expr, distinct=False):
        super().__init__(expr)
        self.distinct(distinct)",https://github.com/horejsek/python-sqlpuzzle/blob/d3a42ed1b339b8eafddb8d2c28a3a5832b3998dd/sqlpuzzle/_queryparts/functions.py#L48-L50
tfidf_python_100_1.0,hash set for counting distinct elements,python,"def distinct(self):
        if self.is_distinct:
            raise InvalidOperationException(""This is already a distinct query."")
        self.is_distinct = True
        self._select_tokens.insert(0, _Token(token=""distinct""))
        return self",https://github.com/ravendb/ravendb-python-client/blob/383739db2594303e3cc469134aa16156f6f80acd/pyravendb/store/session_query.py#L672-L677
tfidf_python_100_1.0,hash set for counting distinct elements,python,"def update_hashtableobj_from_dict(hashobj,hash):
    for k, v in hash.iteritems():
        hashobj.set(k,v)",https://github.com/camptocamp/Studio/blob/43cb7298434fb606b15136801b79b03571a2f27e/studio/lib/mapserializer.py#L528-L530
tfidf_python_100_1.0,hash set for counting distinct elements,python,"def grad3(hash, x, y, z):
	g = _GRAD3[hash % 16]
	return x*g[0] + y*g[1] + z*g[2]",https://github.com/caseman/noise/blob/bb32991ab97e90882d0e46e578060717c5b90dc5/perlin.py#L299-L301
tfidf_python_100_1.0,hash set for counting distinct elements,python,"def elfhash(s):
    """"""
    :param string: bytes

    >>> import base64
    >>> s = base64.b64encode(b'hello world')
    >>> elfhash(s)
    224648685
    """"""
    hash = 0
    x = 0
    for c in s:
        hash = (hash << 4) + c
        x = hash & 0xF0000000
        if x:
            hash ^= (x >> 24)
            hash &= ~x
    return (hash & 0x7FFFFFFF)",https://github.com/cosven/feeluown-core/blob/62dc64638f62971b16be0a75c0b8c7ae2999869e/fuocore/utils.py#L23-L40
tfidf_python_100_1.0,hash set for counting distinct elements,python,"def distinct(self, *args, **kwargs):
        return self._collection_with_options(kwargs).distinct(*args, **kwargs)",https://github.com/pricingassistant/mongokat/blob/61eaf4bc1c4cc359c6f9592ec97b9a04d9561411/mongokat/collection.py#L136-L137
tfidf_python_100_1.0,hash set for counting distinct elements,python,"def __hash__(self):
        hash = 17
        for b in self.GetByteArray():
            hash = hash * 31 + b
        #        print(""hash code %s "" % hash)
        return hash",https://github.com/CityOfZion/neo-python/blob/fe90f62e123d720d4281c79af0598d9df9e776fb/neo/VM/InteropService.py#L65-L70
tfidf_python_100_1.0,hash set for counting distinct elements,python,"def distinct(self, *args, **kwargs):
        return CursorWrapper(
            self.cursor.distinct(*args, **kwargs), self.instance)",https://github.com/openstates/billy/blob/5fc795347f12a949e410a8cfad0c911ea6bced67/billy/models/base.py#L414-L416
tfidf_python_100_1.0,hash set for counting distinct elements,python,"def __init__(self, elements=None):
        elements = elements or []
        self._set = set(elements)
        self._list = elements",https://github.com/michael-lazar/rtv/blob/ccef2af042566ad384977028cf0bde01bc524dda/rtv/config.py#L136-L139
tfidf_python_100_1.0,hash set for counting distinct elements,python,"def _extractElements(self):
        elements = []
        indexed_face_set = self._getIndexedFaceSet()
        if indexed_face_set is not None and 'coordIndex' in indexed_face_set:
            coordinate_indexes = indexed_face_set['coordIndex']
            elements = _convertToElementList(coordinate_indexes)

        return elements",https://github.com/ABI-Software/MeshParser/blob/08dc0ce7c44d0149b443261ff6d3708e28a928e7/src/meshparser/vrmlparser/parser.py#L64-L71
tfidf_python_100_1.0,hash set for counting distinct elements,python,"def tuple2list(elements):

	elements = list(elements)
	for i in xrange(len(elements)):
		if isinstance(elements[i], tuple):
			elements[i] = tuple2list(elements[i])
	return elements",https://github.com/lltk/lltk/blob/d171de55c1b97695fddedf4b02401ae27bf1d634/lltk/utils.py#L43-L49
tfidf_python_100_1.0,hash set for counting distinct elements,python,"def __hash__(self):
        return (hash(self.from_study) ^
                hash(self.pattern) ^
                hash(self.is_regex) ^
                hash(self.order) ^
                hash(self._repository) ^
                hash(self._skip_missing) ^
                hash(self._drop_if_missing) ^
                hash(self._fallback_to_default))",https://github.com/MonashBI/arcana/blob/d6271a29d13733d00422d11417af8d200be62acc/arcana/data/input.py#L52-L60
tfidf_python_100_1.0,how to get database table name,python,"def __init__(self, cxn, database, table):
        self.database = database
        self.table = table
        self.cxn = cxn",https://github.com/20c/munge/blob/e20fef8c24e48d4b0a5c387820fbb2b7bebb0af0/munge/codec/mysql.py#L21-L24
tfidf_python_100_1.0,how to get database table name,python,"def main(options):

    filename_database = options[""--database""]
    name_table        = options[""--table""]

    print(""\npyprel database examples\n"")

    if os.path.exists(filename_database):
        print(""create database {database}"".format(
            database = filename_database
        ))
        create_database(filename = ""database.db"")

    print(""access database {filename}"".format(
        filename = filename_database
    ))
    database = dataset.connect(
        ""sqlite:///{filename_database}"".format(
            filename_database = filename_database
        )
    )
    table = database[name_table]

    print(""add data to database"")
    table.insert(dict(
        name     = ""Legolas Greenleaf"",
        age      = 2000,
        country  = ""Mirkwood"",
        uuid4    = str(uuid.uuid4())
    ))
    table.insert(dict(
        name     = ""Cody Rapol"",
        age      = 30,
        country  = ""USA"",
        activity = ""DDR"",
        uuid4    = str(uuid.uuid4())
    ))

    print(
""""""
database tables:\n{tables}
\ntable {table} columns:\n{columns}
\ntable {table} row one:\n{row}
"""""".format(
            tables  = database.tables,
            table   = name_table,
            columns = database[name_table].columns,
            row     = [entry for entry in table.find(id = ""1"")]
        )
    )

    print(""table {table} printout:\n"".format(
        table = name_table
    ))

    print(
        pyprel.Table(
            contents = pyprel.table_dataset_database_table(
                table = database[name_table]
            )
        )
    )",https://github.com/wdbm/pyprel/blob/c1253ea3f8c60a2f5493a0d5a61ca3c84df7c21d/pyprel_examples_database.py#L54-L115
tfidf_python_100_1.0,how to get database table name,python,"def table(self, name, database=None, schema=None):
        """"""Create a table expression that references a particular a table
        called `name` in a MySQL database called `database`.

        Parameters
        ----------
        name : str
            The name of the table to retrieve.
        database : str, optional
            The database in which the table referred to by `name` resides. If
            ``None`` then the ``current_database`` is used.
        schema : str, optional
            The schema in which the table resides.  If ``None`` then the
            `public` schema is assumed.

        Returns
        -------
        table : TableExpr
            A table expression.
        """"""
        if database is not None and database != self.current_database:
            return self.database(name=database).table(name=name, schema=schema)
        else:
            alch_table = self._get_sqla_table(name, schema=schema)
            node = self.table_class(alch_table, self, self._schemas.get(name))
            return self.table_expr_class(node)",https://github.com/ibis-project/ibis/blob/1e39a5fd9ef088b45c155e8a5f541767ee8ef2e7/ibis/sql/mysql/client.py#L180-L205
tfidf_python_100_1.0,how to get database table name,python,"def __init__(self, table, database='default', client=None):
        self.database = database
        self.table = table
        self.client = client or get_default_client()",https://github.com/spotify/luigi/blob/c5eca1c3c3ee2a7eb612486192a0da146710a1e9/luigi/contrib/hive.py#L394-L397
tfidf_python_100_1.0,how to get database table name,python,"def table_metadata(self, database, table):
        ""Fetch table-specific metadata.""
        return (self.metadata(""databases"") or {}).get(database, {}).get(
            ""tables"", {}
        ).get(
            table, {}
        )",https://github.com/simonw/datasette/blob/11b352b4d52fd02a422776edebb14f12e4994d3b/datasette/app.py#L521-L527
tfidf_python_100_1.0,how to get database table name,python,"def table(self, name, database=None):
        """"""
        Create a table expression that references a particular table in the
        SQLite database

        Parameters
        ----------
        name : string
        database : string, optional
          name of the attached database that the table is located in.

        Returns
        -------
        table : TableExpr
        """"""
        alch_table = self._get_sqla_table(name, schema=database)
        node = self.table_class(alch_table, self)
        return self.table_expr_class(node)",https://github.com/ibis-project/ibis/blob/1e39a5fd9ef088b45c155e8a5f541767ee8ef2e7/ibis/sql/sqlite/client.py#L377-L394
tfidf_python_100_1.0,how to get database table name,python,"def get_template_processor(self, **kwargs):
        return get_template_processor(
            table=self, database=self.database, **kwargs)",https://github.com/apache/incubator-superset/blob/ca2996c78f679260eb79c6008e276733df5fb653/superset/connectors/sqla/models.py#L476-L478
tfidf_python_100_1.0,how to get database table name,python,"def __init__(self, name, database=None):
        self.name = name
        self.database = database",https://github.com/ibis-project/ibis/blob/1e39a5fd9ef088b45c155e8a5f541767ee8ef2e7/ibis/mapd/ddl.py#L236-L238
tfidf_python_100_1.0,how to get database table name,python,"def _fully_qualified_name(self, name, database):
        if bool(fully_qualified_re.search(name)):
            return name

        database = database or self.current_database
        return '{0}.`{1}`'.format(database, name)",https://github.com/ibis-project/ibis/blob/1e39a5fd9ef088b45c155e8a5f541767ee8ef2e7/ibis/clickhouse/client.py#L259-L264
tfidf_python_100_1.0,how to get database table name,python,"def get_template_processor(database, table=None, query=None, **kwargs):
    TP = template_processors.get(database.backend, BaseTemplateProcessor)
    return TP(database=database, table=table, query=query, **kwargs)",https://github.com/apache/incubator-superset/blob/ca2996c78f679260eb79c6008e276733df5fb653/superset/jinja_context.py#L217-L219
tfidf_python_100_1.0,how to get database table name,python,"def _load_one(table: LdapObjectClass, key: str, value: str, database: Optional[Database] = None) -> LdapObject:
        q = Q(**{key: value})
        result = get_one(table, q, database)
        return result",https://github.com/Karaage-Cluster/python-tldap/blob/61f1af74a3648cb6491e7eeb1ee2eb395d67bf59/tldap/database/__init__.py#L343-L346
tfidf_python_100_1.0,how to get database table name,python,"def _load_list(table: LdapObjectClass, key: str, value: str,
                   database: Optional[Database] = None) -> List[LdapObject]:
        q = Q(**{key: value})
        return list(search(table, q, database))",https://github.com/Karaage-Cluster/python-tldap/blob/61f1af74a3648cb6491e7eeb1ee2eb395d67bf59/tldap/database/__init__.py#L349-L352
tfidf_python_100_1.0,how to get database table name,python,"def get_table_index_names(self, table, database=None, **kwargs):
        return map(Index.get_name, self.get_table_indexes(table, database, **kwargs))",https://github.com/lokhman/pydbal/blob/53f396a2a18826e9fff178cd2c0636c1656cbaea/pydbal/schema.py#L323-L324
tfidf_python_100_1.0,how to get database table name,python,"def drop_table_or_view(self, name, database=None, force=False):
        """"""
        Attempt to drop a relation that may be a view or table
        """"""
        try:
            self.drop_table(name, database=database)
        except Exception as e:
            try:
                self.drop_view(name, database=database)
            except Exception:
                raise e",https://github.com/ibis-project/ibis/blob/1e39a5fd9ef088b45c155e8a5f541767ee8ef2e7/ibis/mapd/client.py#L681-L691
tfidf_python_100_1.0,how to get database table name,python,"def _load_table(self, name):
        """""" Reflect a given table from the database. """"""
        table = self._tables.get(name, None)
        if table is not None:
            return table
        if not self.engine.has_table(name):
            raise BindingException('Table does not exist: %r' % name,
                                   table=name)
        table = Table(name, self.meta, autoload=True)
        self._tables[name] = table
        return table",https://github.com/openspending/babbage/blob/9e03efe62e0be0cceabafd4de2a09cb8ec794b92/babbage/cube.py#L30-L40
tfidf_python_100_1.0,how to get database table name,python,"def _fully_qualified_name(self, name, database):
        if ddl._is_fully_qualified(name):
            return name

        database = database or self.current_database
        return '{0}.`{1}`'.format(database, name)",https://github.com/ibis-project/ibis/blob/1e39a5fd9ef088b45c155e8a5f541767ee8ef2e7/ibis/impala/client.py#L845-L850
tfidf_python_100_1.0,how to get database table name,python,"def get_table_column_names(self, table, database=None, **kwargs):
        return map(Column.get_name, self.get_table_columns(table, database, **kwargs))",https://github.com/lokhman/pydbal/blob/53f396a2a18826e9fff178cd2c0636c1656cbaea/pydbal/schema.py#L311-L312
tfidf_python_100_1.0,how to get database table name,python,"def exists_table(self, name, database=None):
        """"""
        Determine if the indicated table or view exists

        Parameters
        ----------
        name : string
        database : string, default None

        Returns
        -------
        if_exists : boolean
        """"""
        return len(self.list_tables(like=name, database=database)) > 0",https://github.com/ibis-project/ibis/blob/1e39a5fd9ef088b45c155e8a5f541767ee8ef2e7/ibis/clickhouse/client.py#L372-L385
tfidf_python_100_1.0,how to get database table name,python,"def _upgradeTableOid(store, table, createTable, postCreate=lambda: None):
    """"""
    Upgrade a table to have an explicit oid.

    Must be called in a transaction to avoid corrupting the database.
    """"""
    if _hasExplicitOid(store, table):
        return
    store.executeSchemaSQL(
        'ALTER TABLE *DATABASE*.{0} RENAME TO {0}_temp'.format(table))
    createTable()
    store.executeSchemaSQL(
        'INSERT INTO *DATABASE*.{0} '
        'SELECT oid, * FROM *DATABASE*.{0}_temp'.format(table))
    store.executeSchemaSQL('DROP TABLE *DATABASE*.{0}_temp'.format(table))
    postCreate()",https://github.com/twisted/axiom/blob/7de70bc8fe1bb81f9c2339fba8daec9eb2e92b68/axiom/upgrade.py#L268-L283
tfidf_python_100_1.0,how to get database table name,python,"def exists_table(self, name, database=None):
        """"""
        Determine if the indicated table or view exists

        Parameters
        ----------
        name : string
        database : string, default None

        Returns
        -------
        if_exists : boolean
        """"""
        return bool(self.list_tables(like=name, database=database))",https://github.com/ibis-project/ibis/blob/1e39a5fd9ef088b45c155e8a5f541767ee8ef2e7/ibis/mapd/client.py#L768-L781
tfidf_python_100_1.0,deserialize json,python,"def deserialize(yaml_doc, yaml_schema):
	mapping_schema = dict2colander.dict2colander(yaml_schema)
	return mapping_schema.deserialize(yaml_doc)",https://github.com/marrow/schema/blob/0c4c3e3b8c79d8bfeb8d7265cfa5b12a2e643152/example/thirdparty/dict2colander.py#L132-L134
tfidf_python_100_1.0,deserialize json,python,"def deserialize(self, data_raw, **kwargs):
        data = super(ModelView, self).deserialize(data_raw, **kwargs)
        return self.resolve_related(data)",https://github.com/4Catalyzer/flask-resty/blob/a8b6502a799c270ca9ce41c6d8b7297713942097/flask_resty/view.py#L357-L359
tfidf_python_100_1.0,deserialize json,python,"def deserialize(json, cls=None):
    """"""Deserialize a JSON string into a Python object.

    Args:
        json (str): the JSON string.
        cls (:py:class:`object`):
            if the ``json`` is deserialized into a ``dict`` and
            this argument is set,
            the ``dict`` keys are passed as keyword arguments to the
            given ``cls`` initializer.

    Returns:
        Python object representation of the given JSON string.
    """"""
    LOGGER.debug('deserialize(%s)', json)

    out = simplejson.loads(json)

    if isinstance(out, dict) and cls is not None:
        return cls(**out)

    return out",https://github.com/steenzout/python-serialization-json/blob/583568e14cc02ba0bf711f56b8a0a3ad142c696d/steenzout/serialization/json/__init__.py#L50-L71
tfidf_python_100_1.0,deserialize json,python,"def _unpickle(cls, string, attr):
        attr_class = cls._get_attr_class(attr)
        deserialize = getattr(attr_class, 'deserialize', None)
        if six.callable(deserialize):
            return deserialize(BytesIO(string))
        else:
            return attr_class(string)",https://github.com/maaku/python-bitcoin/blob/1b80c284170fd3f547cc45f4700ce169f3f99641/bitcoin/authtree.py#L176-L182
tfidf_python_100_1.0,deserialize json,python,"def deserialize(self, s):
        d = json.loads(s)
        self.__dict__ = d
        return self",https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-keyvault/azure/keyvault/_internal.py#L166-L169
tfidf_python_100_1.0,deserialize json,python,"def _deserialize(self, value):
        return {self.key_field.deserialize(k): self.nested_field.deserialize(v) for k, v in value.items()}",https://github.com/podhmo/swagger-marshmallow-codegen/blob/16d3373991c247b09e10b6f8f524db0c498283e8/swagger_marshmallow_codegen/fields.py#L45-L46
tfidf_python_100_1.0,deserialize json,python,"def deserialize(self, value, parent=None):
        result = []
        deserialize = super(ListField, self).deserialize
        for item in value:
            result.append(deserialize(item, parent=parent))
        return result",https://github.com/aiogram/aiogram/blob/2af930149ce2482547721e2c8755c10307295e48/aiogram/types/fields.py#L135-L140
tfidf_python_100_1.0,deserialize json,python,"def deserialize(self, data):
        self.header.deserialize(data)
        self.compose.deserialize(data[""payload""])
        self.release.deserialize(data[""payload""])
        if self.release.is_layered:
            self.base_product.deserialize(data[""payload""])
        self.variants.deserialize(data[""payload""])
        self.header.set_current_version()",https://github.com/release-engineering/productmd/blob/49256bf2e8c84124f42346241140b986ad7bfc38/productmd/composeinfo.py#L196-L203
tfidf_python_100_1.0,deserialize json,python,"def deserialize(self, parser):
        self.header.deserialize(parser)
        self.release.deserialize(parser)
        if self.release.is_layered:
            self.base_product.deserialize(parser)
        self.tree.deserialize(parser)
        self.variants.deserialize(parser)
        self.checksums.deserialize(parser)
        self.images.deserialize(parser)
        self.stage2.deserialize(parser)
        self.media.deserialize(parser)
        self.validate()
        self.header.set_current_version()
        return parser",https://github.com/release-engineering/productmd/blob/49256bf2e8c84124f42346241140b986ad7bfc38/productmd/treeinfo.py#L120-L133
tfidf_python_100_1.0,deserialize json,python,"def deserialize(self, value, **kwargs):
        if value is None:
            return None
        return self.nested_type.deserialize(value, **kwargs)",https://github.com/paylogic/halogen/blob/2dec0a67c506d02d1f51915fa7163f59764a0bde/halogen/types.py#L289-L292
tfidf_python_100_1.0,deserialize json,python,"def deserialize(self, node, cstruct):
        cstruct = super(BinaryValueType, self).deserialize(node, cstruct)
        return cstruct.decode('base64')",https://github.com/voidfiles/lark/blob/e3f202d88b97f7b985a358fe28df7e52204f7846/lark/redis/schemas.py#L113-L115
tfidf_python_100_1.0,deserialize json,python,"def __init__(self, deserialize, response, *args):

        super(GraphErrorException, self).__init__(deserialize, response, 'GraphError', *args)",https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-graphrbac/azure/graphrbac/models/graph_error_py3.py#L43-L45
tfidf_python_100_1.0,deserialize json,python,"def __init__(self, deserialize, response, *args):

        super(QueryFailureException, self).__init__(deserialize, response, 'QueryFailure', *args)",https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-mgmt-policyinsights/azure/mgmt/policyinsights/models/query_failure.py#L39-L41
tfidf_python_100_1.0,deserialize json,python,"def __init__(self, deserialize, response, *args):

        super(FabricErrorException, self).__init__(deserialize, response, 'FabricError', *args)",https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicefabric/azure/servicefabric/models/fabric_error.py#L48-L50
tfidf_python_100_1.0,deserialize json,python,"def __init__(self, deserialize, response, *args):

        super(ErrorModelException, self).__init__(deserialize, response, 'ErrorModel', *args)",https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-mgmt-servicefabric/azure/mgmt/servicefabric/models/error_model_py3.py#L43-L45
tfidf_python_100_1.0,deserialize json,python,"def __init__(self, deserialize, response, *args):

        super(CustomVisionErrorException, self).__init__(deserialize, response, 'CustomVisionError', *args)",https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-cognitiveservices-vision-customvision/azure/cognitiveservices/vision/customvision/training/models/custom_vision_error_py3.py#L116-L118
tfidf_python_100_1.0,deserialize json,python,"def __init__(self, deserialize, response, *args):

        super(DefaultErrorResponseException, self).__init__(deserialize, response, 'DefaultErrorResponse', *args)",https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-mgmt-web/azure/mgmt/web/models/default_error_response.py#L46-L48
tfidf_python_100_1.0,deserialize json,python,"def __init__(self, deserialize, response, *args):

        super(ErrorDetailsException, self).__init__(deserialize, response, 'ErrorDetails', *args)",https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-mgmt-iotcentral/azure/mgmt/iotcentral/models/error_details.py#L60-L62
tfidf_python_100_1.0,deserialize json,python,"def __init__(self, deserialize, response, *args):

        super(StorageSyncErrorException, self).__init__(deserialize, response, 'StorageSyncError', *args)",https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-mgmt-storagesync/azure/mgmt/storagesync/models/storage_sync_error_py3.py#L43-L45
tfidf_python_100_1.0,deserialize json,python,"def __init__(self, deserialize, response, *args):

        super(KeyVaultErrorException, self).__init__(deserialize, response, 'KeyVaultError', *args)",https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-keyvault/azure/keyvault/v7_0/models/key_vault_error_py3.py#L46-L48
tfidf_python_100_1.0,find int in string,python,"def html_to_red_green_blue(string):
    string = _extract_html_hex(string)
    if string is None:
        return None, None, None
    return int(string[:2], 16), int(string[2:4], 16), int(string[4:], 16)",https://github.com/jalanb/pysyte/blob/4e278101943d1ceb1a6bcaf6ddc72052ecf13114/pysyte/colours/colour_numbers.py#L103-L107
tfidf_python_100_1.0,find int in string,python,"def datetime_from_str(string):
    """"""

    Args:
        string: string of the form YYMMDD-HH_MM_SS, e.g 160930-18_43_01

    Returns: a datetime object

    """"""


    return datetime.datetime(year=2000+int(string[0:2]), month=int(string[2:4]), day=int(string[4:6]), hour=int(string[7:9]), minute=int(string[10:12]),second=int(string[13:15]))",https://github.com/LISE-B26/pylabcontrol/blob/67482e5157fcd1c40705e5c2cacfb93564703ed0/build/lib/pylabcontrol/src/core/helper_functions.py#L187-L198
tfidf_python_100_1.0,find int in string,python,"def _isint(string):
    """"""
    >>> _isint(""123"")
    True
    >>> _isint(""123.45"")
    False
    """"""
    return type(string) is int or \
           (isinstance(string, _binary_type) or isinstance(string, _text_type)) and \
           _isconvertible(int, string)",https://github.com/pgmpy/pgmpy/blob/9381a66aba3c3871d3ccd00672b148d17d63239e/pgmpy/extern/tabulate.py#L254-L263
tfidf_python_100_1.0,find int in string,python,"def _str_to_int(self, string, bits=8):
        i = 0
        if len(string) < 3:
            i = int(string, 10)
        elif string[0:2] == '0b' or string[0:3] == '-0b':
            i = int(string, 2)
        elif string[0:2] == '0o' or string[0:3] == '-0o':
            i = int(string, 8)
        elif string[0:2] == '0x' or string[0:3] == '-0x':
            i = int(string, 16)
        else:
            i = int(string)

        return self._get_complement(i, bits)",https://github.com/ymyzk/sasm/blob/3abf18affeb2f346b2a825018bfb6029c46d1512/sasm/instructions.py#L33-L46
tfidf_python_100_1.0,find int in string,python,"def _isint(string):
    """"""
    >>> _isint(""123"")
    True
    >>> _isint(""123.45"")
    False
    """"""
    return type(string) is int or \
        (isinstance(string, _binary_type) or
         isinstance(string, string_types)) and \
        _isconvertible(int, string)",https://github.com/solvebio/solvebio-python/blob/b29614643043afd19c1d8074e8f25c6700d51a73/solvebio/utils/tabulate.py#L160-L170
tfidf_python_100_1.0,find int in string,python,"def is_valid_hash(string):
    string = str(string)  # in case of being passed an int
    return len(string.strip()) == 64 and uses_only_hash_chars(string)",https://github.com/blockcypher/blockcypher-python/blob/7601ea21916957ff279384fd699527ff9c28a56e/blockcypher/utils.py#L303-L305
tfidf_python_100_1.0,find int in string,python,"def numerify(string):
    return int(''.join(str(_alphabet.index(c)) for c in string))",https://github.com/figo-connect/schwifty/blob/69376fade070dbfdf89c57a0060bc290f7a744bb/schwifty/iban.py#L29-L30
tfidf_python_100_1.0,find int in string,python,"def is_regex(string):
    """"""
    TODO: improve this!

    Returns True if the given string is considered a regular expression,
    False otherwise.
    It will be considered a regex if starts with a non alphabetic character
    and then correctly compiled by re.compile

    :param string: str

    """"""
    is_regex = False
    regex_chars = ['\\', '(', '+', '^', '$']
    for c in regex_chars:
        if string.find(c) > -1:
            return is_valid_regex(string)
    return is_regex",https://github.com/Neurita/boyle/blob/2dae7199849395a209c887d5f30506e1de8a9ad9/boyle/utils/strings.py#L147-L164
tfidf_python_100_1.0,find int in string,python,"def _to_number(cls, string):
        """"""Convert string to int or float.""""""
        try:
            if float(string) - int(string) == 0:
                return int(string)
            return float(string)
        except ValueError:
            try:
                return float(string)
            except ValueError:
                return string",https://github.com/dgomes/pyipma/blob/cd808abeb70dca0e336afdf55bef3f73973eaa71/pyipma/api.py#L35-L45
tfidf_python_100_1.0,find int in string,python,"def convert_string(string):
    """"""Convert string to int, float or bool.
    """"""
    if is_int(string):
        return int(string)
    elif is_float(string):
        return float(string)
    elif convert_bool(string)[0]:
        return convert_bool(string)[1]
    elif string == 'None':
        return None
    else:
        return string",https://github.com/theislab/scanpy/blob/9e4e5ee02e04cf618872d9b098e24f0542e8b227/scanpy/readwrite.py#L602-L614
tfidf_python_100_1.0,find int in string,python,"def value_to_bool(string):
    if isinstance(string, bool):
        return string

    if isinstance(string, int):
        string = str(string)

    if string.lower() in ['1', 'true']:
        return True

    return False",https://github.com/racker/rackspace-monitoring/blob/8a9929e5fd51826c0a392e21bc55acb2aefe54f7/rackspace_monitoring/utils.py#L29-L39
tfidf_python_100_1.0,find int in string,python,"def _loadAnyChar(parentContext, xmlElement, attributeToFormatMap):
    string = _safeGetRequiredAttribute(xmlElement, 'String', '')
    abstractRuleParams = _loadAbstractRuleParams(parentContext, xmlElement, attributeToFormatMap)
    return _parserModule.AnyChar(abstractRuleParams, string)",https://github.com/andreikop/qutepart/blob/109d76b239751318bcef06f39b2fbbf18687a40b/qutepart/syntax/loader.py#L276-L279
tfidf_python_100_1.0,find int in string,python,"def _loadStringDetect(parentContext, xmlElement, attributeToFormatMap):
    string = _safeGetRequiredAttribute(xmlElement, 'String', None)

    abstractRuleParams = _loadAbstractRuleParams(parentContext, xmlElement, attributeToFormatMap)
    return _parserModule.StringDetect(abstractRuleParams,
                                      string)",https://github.com/andreikop/qutepart/blob/109d76b239751318bcef06f39b2fbbf18687a40b/qutepart/syntax/loader.py#L281-L286
tfidf_python_100_1.0,find int in string,python,"def __init__(self, string):
        #: The string in question
        self.string = string
        super(UnterminatedColorError, self).__init__(string)",https://github.com/jwodder/txtble/blob/31d39ed6c15df13599c704a757cd36e1cd57cdd1/txtble/errors.py#L20-L23
tfidf_python_100_1.0,find int in string,python,"def __init__(self, string):
        #: The string in question
        self.string = string
        super(IndeterminateWidthError, self).__init__(string)",https://github.com/jwodder/txtble/blob/31d39ed6c15df13599c704a757cd36e1cd57cdd1/txtble/errors.py#L6-L9
tfidf_python_100_1.0,find int in string,python,"def html_to_ansi(string):
    i = html_to_small_ansi(string)
    if i is not None:
        return i
    i = html_to_int(string)
    return integer_to_ansi(i)",https://github.com/jalanb/pysyte/blob/4e278101943d1ceb1a6bcaf6ddc72052ecf13114/pysyte/colours/colour_numbers.py#L114-L119
tfidf_python_100_1.0,find int in string,python,"def _type(string, has_invisible=True, numparse=True):
    """"""The least generic type (type(None), int, float, str, unicode).

    >>> _type(None) is type(None)
    True
    >>> _type(""foo"") is type("""")
    True
    >>> _type(""1"") is type(1)
    True
    >>> _type('\x1b[31m42\x1b[0m') is type(42)
    True
    >>> _type('\x1b[31m42\x1b[0m') is type(42)
    True

    """"""

    if has_invisible and \
       (isinstance(string, _text_type) or isinstance(string, _binary_type)):
        string = _strip_invisible(string)

    if string is None:
        return _none_type
    elif hasattr(string, ""isoformat""):  # datetime.datetime, date, and time
        return _text_type
    elif _isbool(string):
        return _bool_type
    elif _isint(string) and numparse:
        return int
    elif _isint(string, _long_type) and numparse:
        return int
    elif _isnumber(string) and numparse:
        return float
    elif isinstance(string, _binary_type):
        return _binary_type
    else:
        return _text_type",https://github.com/raphaelvallat/pingouin/blob/58b19fa4fffbfe09d58b456e3926a148249e4d9b/pingouin/external/tabulate.py#L478-L513
tfidf_python_100_1.0,find int in string,python,"def is_cacheable(string, as_map_key=False):
    return string and len(string) >= MIN_SIZE_CACHEABLE \
                  and (as_map_key \
                  or (string[:2] in [""~#"", ""~$"", ""~:""]))",https://github.com/cognitect/transit-python/blob/59e27e7d322feaa3a7e8eb3de06ae96d8adb614f/transit/rolling_cache.py#L43-L46
tfidf_python_100_1.0,find int in string,python,"def string_to_paths(string):
    for c in ':, ;':
        if c in string:
            return strings_to_paths(string.split(c))
    return [makepath(string)]",https://github.com/jalanb/pysyte/blob/4e278101943d1ceb1a6bcaf6ddc72052ecf13114/pysyte/paths.py#L671-L675
tfidf_python_100_1.0,find int in string,python,"def stripnull(string):
    """"""Return string truncated at first null character.""""""
    i = string.find(b'\x00')
    return string if (i < 0) else string[:i]",https://github.com/newville/wxmplot/blob/8e0dc037453e5cdf18c968dc5a3d29efd761edee/examples/tifffile.py#L1788-L1791
tfidf_python_100_1.0,get current process id,python,"def _pyon_process_state_change_callback(self, process, state, container):

        if not hasattr(process, 'id'):
            # Process is in Pending state, which we ignore, because we don't
            # Have a process id for it
            return

        pidpyon = self._watched_processes.get(process.id)
        if pidpyon is None:
            self._log.warning(""Got callback for unknown process %s"" % process.id)
            return

        self._log.info(""Got callback for process %s with state %s"" % (process.id, ProcessStateEnum._str_map[state]))
        pidpyon._process_state_change(state)",https://github.com/nimbusproject/pidantic/blob/7d98dcbf449d0458279144d07a7b82dc933905cc/pidantic/pyon/pidpyon.py#L117-L130
tfidf_python_100_1.0,get current process id,python,"def __init__(self, control_dir, process):
        self.control_dir = control_dir
        self.process = process",https://github.com/pytest-dev/pytest-xprocess/blob/c3ee760b02dce2d0eed960b3ab0e28379853c3ef/xprocess.py#L176-L178
tfidf_python_100_1.0,get current process id,python,"def kill_mprocess(process):
    """"""kill process
    Args:
        process - Popen object for process
    """"""
    if process and proc_alive(process):
        process.terminate()
        process.communicate()
    return not proc_alive(process)",https://github.com/10gen/mongo-orchestration/blob/81fd2224205922ea2178b08190b53a33aec47261/mongo_orchestration/process.py#L263-L271
tfidf_python_100_1.0,get current process id,python,"def _sort_io_counters(process,
                      sortedby='io_counters',
                      sortedby_secondary='memory_percent'):
    """"""Specific case for io_counters
    Sum of io_r + io_w""""""
    return process[sortedby][0] - process[sortedby][2] + process[sortedby][1] - process[sortedby][3]",https://github.com/nicolargo/glances/blob/5bd4d587a736e0d2b03170b56926841d2a3eb7ee/glances/processes.py#L381-L386
tfidf_python_100_1.0,get current process id,python,"def __init__(self, process: Process):
        """"""Constructor.

        Args:
            process (Process): task process
        """"""
        self.process = process
        self.stopped_due_to_worker_shutdown = False",https://github.com/mozilla-releng/scriptworker/blob/8e97bbd83b9b578565ec57904c966dd6ae4ef0ae/scriptworker/task_process.py#L21-L28
tfidf_python_100_1.0,get current process id,python,"def Convert(self, metadata, process, token=None):
    """"""Converts Process to ExportedProcess.""""""

    result = ExportedProcess(
        metadata=metadata,
        pid=process.pid,
        ppid=process.ppid,
        name=process.name,
        exe=process.exe,
        cmdline="" "".join(process.cmdline),
        ctime=process.ctime,
        real_uid=process.real_uid,
        effective_uid=process.effective_uid,
        saved_uid=process.saved_uid,
        real_gid=process.real_gid,
        effective_gid=process.effective_gid,
        saved_gid=process.saved_gid,
        username=process.username,
        terminal=process.terminal,
        status=process.status,
        nice=process.nice,
        cwd=process.cwd,
        num_threads=process.num_threads,
        user_cpu_time=process.user_cpu_time,
        system_cpu_time=process.system_cpu_time,
        cpu_percent=process.cpu_percent,
        rss_size=process.RSS_size,
        vms_size=process.VMS_size,
        memory_percent=process.memory_percent)
    return [result]",https://github.com/google/grr/blob/5cef4e8e2f0d5df43ea4877e9c798e0bf60bfe74/grr/server/grr_response_server/export.py#L729-L758
tfidf_python_100_1.0,get current process id,python,"def set_process(self, process = None):
        """"""
        Manually set the parent process. Use with care!

        @type  process: L{Process}
        @param process: (Optional) Process object. Use C{None} for no process.
        """"""
        if process is None:
            self.__process = None
        else:
            global Process      # delayed import
            if Process is None:
                from winappdbg.process import Process
            if not isinstance(process, Process):
                msg  = ""Parent process must be a Process instance, ""
                msg += ""got %s instead"" % type(process)
                raise TypeError(msg)
            self.__process = process",https://github.com/fabioz/PyDev.Debugger/blob/ed9c4307662a5593b8a7f1f3389ecd0e79b8c503/pydevd_attach_to_process/winappdbg/module.py#L220-L237
tfidf_python_100_1.0,get current process id,python,"def __init__(self, process=None, alert_if=None, email_notification=None):
        self.process_set = {
            'process': process,
            'alert_if': alert_if,
            'email_notification': email_notification
        }",https://github.com/1and1/oneandone-cloudserver-sdk-python/blob/8155dda1ef611cab581d4ed54bf0c393405f0954/oneandone/client.py#L7300-L7305
tfidf_python_100_1.0,get current process id,python,"def process(self, request_adu):
        """""" Process request ADU and return response.

        :param request_adu: A bytearray containing the ADU request.
        :return: A bytearray containing the response of the ADU request.
        """"""
        validate_crc(request_adu)
        return super(RTUServer, self).process(request_adu)",https://github.com/AdvancedClimateSystems/uModbus/blob/0560a42308003f4072d988f28042b8d55b694ad4/umodbus/server/serial/rtu.py#L60-L67
tfidf_python_100_1.0,get current process id,python,"def consumer_stats_counter():
        """"""Return a new consumer stats counter instance.

        :rtype: dict

        """"""
        return {
            process.Process.ERROR: 0,
            process.Process.PROCESSED: 0,
            process.Process.REDELIVERED: 0
        }",https://github.com/gmr/rejected/blob/610a3e1401122ecb98d891b6795cca0255e5b044/rejected/mcp.py#L230-L240
tfidf_python_100_1.0,get current process id,python,"def kill_sub_processes(self):
        for process in self.sub_processes:
            process.terminate()
        for process in self.sub_processes:
            process.join(timeout=1)
        self.sub_processes = []",https://github.com/RLBot/RLBot/blob/3f9b6bec8b9baf4dcfff0f6cf3103c8744ac6234/src/main/python/rlbot/setup_manager.py#L322-L327
tfidf_python_100_1.0,get current process id,python,"def __init__(self, possible_values):
    self.depends_on_ids = {id(x): [] for x in possible_values}
    self.current = None",https://github.com/rix0rrr/gcl/blob/4e3bccc978a9c60aaaffd20f6f291c4d23775cdf/gcl/query.py#L357-L359
tfidf_python_100_1.0,get current process id,python,"def __init__(self, process):
        """"""
        :param StochasticProcess process: diffusion process to evolve

        """"""
        self.process = process
        self.diffusion_driver = process.diffusion_driver
        length = len(process) if len(process) > 1 else None
        super(GaussEvolutionProducer, self).__init__(process.evolve, State(process.start), length)",https://github.com/pbrisk/timewave/blob/cf641391d1607a424042724c8b990d43ee270ef6/timewave/stochasticproducer.py#L66-L74
tfidf_python_100_1.0,get current process id,python,"def set_process(self, process = None):
        """"""
        Manually set the parent Process object. Use with care!

        @type  process: L{Process}
        @param process: (Optional) Process object. Use C{None} for no process.
        """"""
        if process is None:
            self.dwProcessId = None
            self.__process   = None
        else:
            self.__load_Process_class()
            if not isinstance(process, Process):
                msg  = ""Parent process must be a Process instance, ""
                msg += ""got %s instead"" % type(process)
                raise TypeError(msg)
            self.dwProcessId = process.get_pid()
            self.__process = process",https://github.com/fabioz/PyDev.Debugger/blob/ed9c4307662a5593b8a7f1f3389ecd0e79b8c503/pydevd_attach_to_process/winappdbg/thread.py#L183-L200
tfidf_python_100_1.0,get current process id,python,"def spawn(self, process):
    """"""Spawn a process.

    Spawning a process binds it to this context and assigns the process a
    pid which is returned.  The process' ``initialize`` method is called.

    Note: A process cannot send messages until it is bound to a context.

    :param process: The process to bind to this context.
    :type process: :class:`Process`
    :return: The pid of the process.
    :rtype: :class:`PID`
    """"""
    self._assert_started()
    process.bind(self)
    self.http.mount_process(process)
    self._processes[process.pid] = process
    process.initialize()
    return process.pid",https://github.com/wickman/compactor/blob/52714be3d84aa595a212feccb4d92ec250cede2a/compactor/context.py#L170-L188
tfidf_python_100_1.0,get current process id,python,"def _get_local_dict():
    current = fibers.current()
    s = '_%s__local_dict__' % current.__class__.__name__
    if not hasattr(current, s):
        setattr(current, s, {})
    return getattr(current, s)",https://github.com/saghul/evergreen/blob/22f22f45892f397c23c3e09e6ea1ad4c00b3add8/evergreen/local.py#L10-L15
tfidf_python_100_1.0,get current process id,python,"def pause() -> None:
    """"""
    Pauses the current process indefinitely -- it will require another process to `resume()` it. When this resumption
    happens, the process returns from this function.
    """"""
    if _logger is not None:
        _log(INFO, ""Process"", local.name, ""pause"")
    Process.current().rsim()._gr.switch()",https://github.com/ElementAI/greensim/blob/f160e8b57d69f6ef469f2e991cc07b7721e08a91/greensim/__init__.py#L552-L559
tfidf_python_100_1.0,get current process id,python,"def recursive_terminate(process, use_psutil=True):
    if use_psutil and psutil is not None:
        _recursive_terminate_with_psutil(process)
    else:
        _recursive_terminate_without_psutil(process)",https://github.com/tomMoral/loky/blob/dc2d941d8285a96f3a5b666a4bd04875b0b25984/loky/backend/utils.py#L24-L28
tfidf_python_100_1.0,get current process id,python,"def _RegisterProcess(self, process):
    """"""Registers a process with the engine.

    Args:
      process (MultiProcessBaseProcess): process.

    Raises:
      KeyError: if the process is already registered with the engine.
      ValueError: if the process is missing.
    """"""
    if process is None:
      raise ValueError('Missing process.')

    if process.pid in self._processes_per_pid:
      raise KeyError(
          'Already managing process: {0!s} (PID: {1:d})'.format(
              process.name, process.pid))

    self._processes_per_pid[process.pid] = process",https://github.com/log2timeline/plaso/blob/9c564698d2da3ffbe23607a3c54c0582ea18a6cc/plaso/multi_processing/engine.py#L256-L274
tfidf_python_100_1.0,get current process id,python,"def _get_global_scope(self):
        current = self
        while current.parent is not None:
            current = current.parent
        return current",https://github.com/python-rope/rope/blob/1c9f9cd5964b099a99a9111e998f0dc728860688/rope/base/pyscopes.py#L72-L76
tfidf_python_100_1.0,regex case insensitive,python,"def _loadWordDetect(parentContext, xmlElement, attributeToFormatMap):
    word = _safeGetRequiredAttribute(xmlElement, ""String"", """")
    insensitive = _parseBoolAttribute(xmlElement.attrib.get(""insensitive"", ""false""))

    abstractRuleParams = _loadAbstractRuleParams(parentContext, xmlElement, attributeToFormatMap)

    return _parserModule.WordDetect(abstractRuleParams, word, insensitive)",https://github.com/andreikop/qutepart/blob/109d76b239751318bcef06f39b2fbbf18687a40b/qutepart/syntax/loader.py#L288-L294
tfidf_python_100_1.0,regex case insensitive,python,"def __init__(self, abstractRuleParams, word, insensitive):
        AbstractRule.__init__(self, abstractRuleParams)
        self.word = word
        self.insensitive = insensitive",https://github.com/andreikop/qutepart/blob/109d76b239751318bcef06f39b2fbbf18687a40b/qutepart/syntax/parser.py#L323-L326
tfidf_python_100_1.0,regex case insensitive,python,"def __init__(self, abstractRuleParams, words, insensitive):
        AbstractRule.__init__(self, abstractRuleParams)
        self.words = set(words)
        self.insensitive = insensitive",https://github.com/andreikop/qutepart/blob/109d76b239751318bcef06f39b2fbbf18687a40b/qutepart/syntax/parser.py#L352-L355
tfidf_python_100_1.0,regex case insensitive,python,"def __init__(self, regex, verbose_pattern=None):
        self.regex = regex
        self.verbose_pattern = verbose_pattern or regex",https://github.com/elastic/apm-agent-python/blob/2975663d7bd22282dc39336b2c37b37c12c7a774/elasticapm/conf/__init__.py#L114-L116
tfidf_python_100_1.0,regex case insensitive,python,"def __init__(self, regex, compiled_regex):
    self._compiled = compiled_regex
    self.regex = regex",https://github.com/google/openhtf/blob/655e85df7134db7bdf8f8fdd6ff9a6bf932e7b09/openhtf/util/validators.py#L207-L209
tfidf_python_100_1.0,regex case insensitive,python,"def __init__(self, regex, verbose_pattern, unit_multipliers):
        self.regex = regex
        self.verbose_pattern = verbose_pattern
        self.unit_multipliers = unit_multipliers",https://github.com/elastic/apm-agent-python/blob/2975663d7bd22282dc39336b2c37b37c12c7a774/elasticapm/conf/__init__.py#L127-L130
tfidf_python_100_1.0,regex case insensitive,python,"def string_in_list(tmp_str, strlist):
        # type: (AnyStr, List[AnyStr]) -> bool
        """"""Is tmp_str in strlist, case insensitive.""""""
        new_str_list = strlist[:]
        for i, str_in_list in enumerate(new_str_list):
            new_str_list[i] = str_in_list.lower()
        return tmp_str.lower() in new_str_list",https://github.com/lreis2415/PyGeoC/blob/9a92d1a229bb74298e3c57f27c97079980b5f729/pygeoc/utils.py#L475-L481
tfidf_python_100_1.0,regex case insensitive,python,"def regex(regex, case=False, _value=None, *args, **kwargs):
    if kwargs.get('case'):
        regex = re.compile(regex)
    else:
        regex = re.compile(regex, re.IGNORECASE)

    if not regex.match(_value):
        raise ValidationError('The _value must match the regex %s' % regex)

    return _value",https://github.com/petermelias/valhalla/blob/b08d7a4a94fd8d85a6f5ea86200ec60ef4525e3d/valhalla/filters/strings.py#L90-L99
tfidf_python_100_1.0,regex case insensitive,python,"def __repr__(self):
        mime_types = (
            ' mime_types={types!r}'.format(types=self.mime_types)
            if self.mime_types else ''
        )
        regex = (
            ' regex={regex!r}'.format(regex=self.regex) if self.regex else ''
        )
        return '<{cls}{mime_types}{regex}>'.format(
            cls=self.__class__.__name__, mime_types=mime_types, regex=regex
        )",https://github.com/fastmonkeys/pontus/blob/cf02fb22c4558b899e2dcbe437a1a525321c4f12/pontus/validators.py#L79-L89
tfidf_python_100_1.0,regex case insensitive,python,"def __init__(self, regex: Regex, cons_output_line: Callable[..., Either[str, A]]) -> None:
        self.regex = regex
        self.cons_output_line = cons_output_line",https://github.com/tek/myo/blob/3772a00a021cbf4efb55786e26881767d854afe8/myo/output/parser/base.py#L40-L42
tfidf_python_100_1.0,regex case insensitive,python,"def __init__(self, regex):
        create_init([""regex""])(self, regex)
        self.pattern = re.compile(regex)",https://github.com/bheinzerling/descriptors/blob/04fff864649fba9bd6a2d8f8b649cf30994e0e46/descriptors/handmade.py#L50-L52
tfidf_python_100_1.0,regex case insensitive,python,"def __init__(self, regex, flags=0, *args, **kwargs):
        super(Regexp, self).__init__(*args, **kwargs)
        if isinstance(regex, str):
            regex = re.compile(regex, flags)
        self.regex = regex

        self.message_values.update({""regex"": self.regex.pattern})",https://github.com/alfred82santa/dirty-validators/blob/95af84fb8e6452c8a6d88af496cbdb31bca7a608/dirty_validators/basic.py#L282-L288
tfidf_python_100_1.0,regex case insensitive,python,"def _compile_regex_if_needed(self):
        regex = self._path_regex()
        if regex:
            self._compiled_regex = re.compile(regex)",https://github.com/hugollm/gatekeeper/blob/f030626bf34560610fbfbd90625e80c29535b460/gatekeeper/endpoints/endpoint.py#L17-L20
tfidf_python_100_1.0,regex case insensitive,python,"def get_regex(resolver_or_pattern):
    """"""Utility method for django's deprecated resolver.regex""""""
    try:
        regex = resolver_or_pattern.regex
    except AttributeError:
        regex = resolver_or_pattern.pattern.regex
    return regex",https://github.com/getsentry/raven-python/blob/d891c20f0f930153f508e9d698d9de42e910face/raven/contrib/django/resolver.py#L11-L17
tfidf_python_100_1.0,regex case insensitive,python,"def set_regex(self, regex):
        if not (isinstance(regex, type(re.compile(''))) or
                isinstance(regex, types.ListType)):
            raise ValueError(""regex must be a Regex or a list of Regexes"")
        self.regex = regex
        return self",https://github.com/usc-isi-i2/dig-regex-extractor/blob/05ce9017e7e77fd3ffab941e4fba31327409b3c7/digRegexExtractor/regex_extractor.py#L15-L20
tfidf_python_100_1.0,regex case insensitive,python,"def __init__(self, regex, validator_cls):
        self.regex = regex
        self.validator = validator_cls()",https://github.com/dependencies-io/schema/blob/32c7b3eca6b814151b74994fcd0e3cc2b03e9115/dependencies_schema/v2/parsers.py#L9-L11
tfidf_python_100_1.0,regex case insensitive,python,"def grammar(self, completers: dict):
        regex, new_completers = self._cmdRegex()
        regex = _updateCompleterDict(completers, new_completers, regex)
        return rf""(\s* {regex} \s*)""",https://github.com/nicfit/nicfit.py/blob/8313f8edbc5e7361ddad496d6d818324b5236c7a/nicfit/shell/command.py#L49-L52
tfidf_python_100_1.0,regex case insensitive,python,"def __init__(self, name, regex, flags=0, skip=False):
    super(Regex, self).__init__(name, skip)
    if isinstance(regex, string_types):
      regex = re.compile(regex, flags)
    self.regex = regex",https://github.com/NiklasRosenstein-Python/nr-deprecated/blob/f9f8b89ea1b084841a8ab65784eaf68852686b2a/nr/strex.py#L538-L542
tfidf_python_100_1.0,regex case insensitive,python,"def get_regex(regex):
    """"""
    Ensure we have a compiled regular expression object.

        >>> import re
        >>> get_regex('string') # doctest: +ELLIPSIS
        <_sre.SRE_Pattern object at 0x...>
        >>> pattern = re.compile(r'string')
        >>> get_regex(pattern) is pattern
        True
        >>> get_regex(3) # doctest: +ELLIPSIS
        Traceback (most recent call last):
        ...
        TypeError: Invalid regex type: 3
    """"""
    if isinstance(regex, basestring):
        return re.compile(regex)
    elif not isinstance(regex, re._pattern_type):
        raise TypeError(""Invalid regex type: %r"" % (regex,))
    return regex",https://github.com/refnode/liquid/blob/8b2b5efc635b0dbfe610db9036fdb4ae3e3d5439/src/liquid/strscan.py#L516-L535
tfidf_python_100_1.0,regex case insensitive,python,"def __init__(self, regex):
        self.rx = re.compile(regex)
        self.__name__ = 'Regex[%s]' % regex",https://github.com/gem/oq-engine/blob/8294553a0b8aba33fd96437a35065d03547d0040/openquake/hazardlib/valid.py#L223-L225
tfidf_python_100_1.0,custom http error response,python,"def __str__(self):
        if 'error' in self.response:
            return 'HTTP Error %s: %s' % (self.status_code,
                                          self.response['error'])
        elif 'message' in self.response:
            return 'HTTP Error %s: %s' % (self.status_code,
                                          self.response['message'])
        else:
            return 'HTTP Error %s' % (self.status_code)",https://github.com/pycrest/PyCrest/blob/5c43899b5a0c6fda7a6441233a571bd04fa6b82f/pycrest/errors.py#L8-L16
tfidf_python_100_1.0,custom http error response,python,"def _do_post(self, url, **kwargs):
        """"""
        Convenient method for POST requests
        Returns http request status value from a POST request
        """"""
        #TODO:
        # Add error handling. Check for HTTP status here would be much more conveinent than in each calling method
        scaleioapi_post_headers = {'Content-type':'application/json','Version':'1.0'}
        try:
            response = self._session.post(url, headers=scaleioapi_post_headers, **kwargs)
            self.conn.logger.debug('_do_post() - HTTP response: %s', response.text)
            if response.status_code == requests.codes.ok:
                self.conn.logger.debug('_do_post() - HTTP response OK, data: %s', response.text)                
                return response
            else:
                self.conn.logger.error('_do_post() - HTTP response error: %s', response.status_code)
                self.conn.logger.error('_do_post() - HTTP response error, data: %s', response.text)                
                raise RuntimeError(""_do_post() - HTTP response error"" + response.status_code)
        except Exception as e:
            self.conn.logger.error(""_do_post() - Unhandled Error Occurred: %s"" % str(e)) 
            raise RuntimeError(""_do_post() - Communication error with ScaleIO gateway"")
        return response",https://github.com/swevm/scaleio-py/blob/d043a0137cb925987fd5c895a3210968ce1d9028/scaleiopy/api/scaleio/common/connection.py#L127-L148
tfidf_python_100_1.0,custom http error response,python,"def _do_get(self, url, **kwargs):
        """"""
        Convenient method for GET requests
        Returns http request status value from a POST request
        """"""
        #TODO:
        # Add error handling. Check for HTTP status here would be much more conveinent than in each calling method
        scaleioapi_post_headers = {'Content-type':'application/json','Version':'1.0'}
        try:
            #response = self._session.get(""{}/{}"".format(self._api_url, uri)).json()
            response = self._session.get(url)
            if response.status_code == requests.codes.ok:
                self.conn.logger.debug('_do_get() - HTTP response OK, data: %s', response.text)                
                return response
            else:
                self.conn.logger.error('_do_get() - HTTP response error: %s', response.status_code)
                self.conn.logger.error('_do_get() - HTTP response error, data: %s', response.text)                
                raise RuntimeError(""_do_get() - HTTP response error"" + response.status_code)
        except Exception as e:
            self.conn.logger.error(""_do_get() - Unhandled Error Occurred: %s"" % str(e)) 
            raise RuntimeError(""_do_get() - Communication error with ScaleIO gateway"")
        return response",https://github.com/swevm/scaleio-py/blob/d043a0137cb925987fd5c895a3210968ce1d9028/scaleiopy/api/scaleio/common/connection.py#L104-L125
tfidf_python_100_1.0,custom http error response,python,"def _http_check(self, response):
        if response.status != 200:
            raise NSQHttpError('http error <{}>'.format(response.status))
        return response.data",https://github.com/wtolson/gnsq/blob/0fd02578b2c9c5fa30626d78579db2a46c10edac/gnsq/httpclient.py#L77-L80
tfidf_python_100_1.0,custom http error response,python,"def client_didFinishGetRequestForStream_error_response_(self, client,
            outstream, error, response):
        self._cb_requestdone(error, response)",https://github.com/pybluez/pybluez/blob/e0dc4093dcbaa3ecb3fa24f8ccf22bbfe6b57fc9/macos/_obex.py#L386-L388
tfidf_python_100_1.0,custom http error response,python,"def client_didFinishPutRequestForStream_error_response_(self, client,
            instream, error, response):
        self._cb_requestdone(error, response)",https://github.com/pybluez/pybluez/blob/e0dc4093dcbaa3ecb3fa24f8ccf22bbfe6b57fc9/macos/_obex.py#L376-L378
tfidf_python_100_1.0,custom http error response,python,"def exc_http(self, msg='HTTP Error', response=None):
        self.exc_common(PYCBC_EXC_HTTP, msg, 0, objextra=response)",https://github.com/ardydedase/pycouchbase/blob/6f010b4d2ef41aead2366878d0cf0b1284c0db0e/couchbase-python-cffi/couchbase_ffi/_rtconfig.py#L97-L98
tfidf_python_100_1.0,custom http error response,python,"def __init__(self, error):  # type: (HTTPError) -> None
        super(UploadError, self).__init__(
            ""HTTP Error {}: {}"".format(
                error.response.status_code, error.response.reason
            )
        )",https://github.com/sdispater/poetry/blob/2d27acd76c165dd49f11934520a7973de7a3762a/poetry/masonry/publishing/uploader.py#L27-L32
tfidf_python_100_1.0,custom http error response,python,"def is_redirect_from_http_to_https(response):
    if response.status_code == 302:
        oldurl = response.url
        newurl = response.headers['location']
        if oldurl.startswith('http://') and oldurl.replace('http', 'https') == newurl:
            return True
    return False",https://github.com/EUDAT-B2SAFE/B2HANDLE/blob/a6d216d459644e01fbdfd5b318a535950bc5cdbb/b2handle/hsresponses.py#L14-L20
tfidf_python_100_1.0,custom http error response,python,"def _finishedrequest(self, error, response):
        if error in (_kOBEXSessionNoTransportError,
                     _kOBEXSessionTransportDiedError):
            self.__closetransport()
        self.__error = error
        self.__response = response
        self.__busy = False
        _macutil.interruptwait()",https://github.com/pybluez/pybluez/blob/e0dc4093dcbaa3ecb3fa24f8ccf22bbfe6b57fc9/macos/_obex.py#L283-L290
tfidf_python_100_1.0,custom http error response,python,"def format_error_response(response):
    content_type = get_mimetype_from_headers(response.headers)
    if content_type == 'application/json':
        error = '\n{}'.format(pretty_json(response.json()))
    else:
        error = response.reason or ''
    return ""HTTP Status: {} {}"".format(response.status_code, error)",https://github.com/caxap/python-boxview/blob/3322fe60df88867cd5d50951145235dc1a73f263/boxview/utils.py#L60-L66
tfidf_python_100_1.0,custom http error response,python,"def __init__(self, error, response):
        self.error = error
        self.response = response
        self.headers = response.headers",https://github.com/EricDalrymple91/strawpy/blob/0c4294fc2dca250a5c13a97e825ae21587278a91/strawpy/asyncstrawpy.py#L56-L59
tfidf_python_100_1.0,custom http error response,python,"def _generic_callback(self, response):
        errorstr = get_ipmi_error(response)
        if errorstr:
            response['error'] = errorstr
        self.lastresponse = response",https://github.com/openstack/pyghmi/blob/f710b1d30a8eed19a9e86f01f9351c737666f3e5/pyghmi/ipmi/private/session.py#L694-L698
tfidf_python_100_1.0,custom http error response,python,"def dcos_agents_state():
    response = dcos.http.get(agents_url())

    if response.status_code == 200:
        return response.json()
    else:
        return None",https://github.com/dcos/shakedown/blob/e2f9e2382788dbcd29bd18aa058b76e7c3b83b3e/shakedown/dcos/__init__.py#L77-L83
tfidf_python_100_1.0,custom http error response,python,"def find_standard_sakefile(settings):
    """"""Returns the filename of the appropriate sakefile""""""
    error = settings[""error""]
    if settings[""customsake""]:
        custom = settings[""customsake""]
        if not os.path.isfile(custom):
            error(""Specified sakefile '{}' doesn't exist"", custom)
            sys.exit(1)
        return custom
    # no custom specified, going over defaults in order
    for name in [""Sakefile"", ""Sakefile.yaml"", ""Sakefile.yml""]:
        if os.path.isfile(name):
            return name
    error(""Error: there is no Sakefile to read"")
    sys.exit(1)",https://github.com/tonyfischetti/sake/blob/b7ad20fe8e7137db99a20ac06b8da26492601b00/sakelib/acts.py#L118-L132
tfidf_python_100_1.0,custom http error response,python,"def existsWithError(self):
        response = self.client.headHelper(self.url)
        error = None
        if 'X-Error-Message' in response.headers:
            error = response.headers['X-Error-Message']
        return (response.status_code == 200, error)",https://github.com/algorithmiaio/algorithmia-python/blob/fe33e6524272ff7ca11c43d1d6985890e6c48a79/Algorithmia/datafile.py#L77-L82
tfidf_python_100_1.0,custom http error response,python,"def set_response(self, response):
        try:
            response = response.json()
            if ""Error"" in response:
                raise exceptions.ErrorResponseException(response[""Error""])
            return response[""Name""]
        except:
            raise response.raise_for_status()",https://github.com/ravendb/ravendb-python-client/blob/383739db2594303e3cc469134aa16156f6f80acd/pyravendb/commands/raven_commands.py#L596-L603
tfidf_python_100_1.0,custom http error response,python,"def _handle_esri_errors(self, response, error_message):
        if response.status_code != 200:
            raise EsriDownloadError('{}: {} HTTP {} {}'.format(
                response.request.url,
                error_message,
                response.status_code,
                response.text,
            ))

        try:
            data = response.json()
        except:
            self._logger.error(""Could not parse response from {} as JSON:\n\n{}"".format(
                response.request.url,
                response.text,
            ))
            raise

        error = data.get('error')
        if error:
            raise EsriDownloadError(""{}: {} {}"" .format(
                error_message,
                error['message'],
                ', '.join(error['details']),
            ))

        return data",https://github.com/openaddresses/pyesridump/blob/378155816559134b8d2b3de0d0f2fddc74f23fcd/esridump/dumper.py#L79-L105
tfidf_python_100_1.0,custom http error response,python,"def set_response(self, response):
        try:
            response = response.json()
            if ""Error"" in response:
                raise exceptions.InvalidOperationException(response[""Error""])
            return response
        except ValueError:
            raise response.raise_for_status()",https://github.com/ravendb/ravendb-python-client/blob/383739db2594303e3cc469134aa16156f6f80acd/pyravendb/commands/raven_commands.py#L423-L430
tfidf_python_100_1.0,custom http error response,python,"def __check_error(response):
        if 200 <= response.status_code < 300:
            pass
        else:
            error = Error.new_from_json_dict(response.json)
            raise LineBotApiError(response.status_code, error)",https://github.com/line/line-bot-sdk-python/blob/1b38bfc2497ff3e3c75be4b50e0f1b7425a07ce0/linebot/api.py#L579-L584
tfidf_python_100_1.0,how to determine a string is a valid word,python,"def alphasnake(string):
    """"""Convert to snakecase removing non alpha numerics
    Word #word -> word_word.
    """"""
    if string:
        string = "" "".join(
            [re.sub(r'\W+', '', word) for word in string.split()]
        )
        string = decamel_to_snake(string)
    return string",https://github.com/GreenBuildingRegistry/yaml-config/blob/3d4bf4cadd07d4c3b71674077bd7cf16efb6ea10/yamlconf/utils.py#L36-L45
tfidf_python_100_1.0,how to determine a string is a valid word,python,"def _romanize(word: str) -> str:
    """"""
    :param str word: Thai word to be romanized, should have already been tokenized.
    :return: Spells out how the Thai word should be pronounced.
    """"""
    if not isinstance(word, str) or not word:
        return """"

    word = _replace_vowels(_normalize(word))
    res = _RE_CONSONANT.findall(word)

    # 2-character word, all consonants
    if len(word) == 2 and len(res) == 2:
        word = list(word)
        word.insert(1, ""o"")
        word = """".join(word)

    word = _replace_consonants(word, res)

    return word",https://github.com/PyThaiNLP/pythainlp/blob/e9a300b8a99dfd1a67a955e7c06f62e4afe0fbca/pythainlp/transliterate/royin.py#L177-L196
tfidf_python_100_1.0,how to determine a string is a valid word,python,"def word_to_id(self, word):
        """"""Returns the integer id of a word string.""""""
        if word in self._vocab:
            return self._vocab[word]
        else:
            return self._unk_id",https://github.com/tensorlayer/tensorlayer/blob/aa9e52e36c7058a7e6fd81d36563ca6850b21956/tensorlayer/nlp.py#L226-L231
tfidf_python_100_1.0,how to determine a string is a valid word,python,"def syllabify(word):
    '''Syllabify the given word.'''

    word = replace_umlauts(word)

    word = apply_T1(word)
    word = apply_T2(word)
    word = apply_T4(word)
    word = apply_T5(word)
    word = apply_T6(word)
    word = apply_T7(word)

    word = replace_umlauts(word, put_back=True)[1:]  # FENCEPOST

    return word",https://github.com/tsnaomi/finnsyll/blob/6a42740311688c946a636a3e2304866c7aa041b3/finnsyll/prev/v01.py#L56-L70
tfidf_python_100_1.0,how to determine a string is a valid word,python,"def stem(self, word, early_english=False):
        """"""Return Porter stem.

        Parameters
        ----------
        word : str
            The word to stem
        early_english : bool
            Set to True in order to remove -eth & -est (2nd & 3rd person
            singular verbal agreement suffixes)

        Returns
        -------
        str
            Word stem

        Examples
        --------
        >>> stmr = Porter()
        >>> stmr.stem('reading')
        'read'
        >>> stmr.stem('suspension')
        'suspens'
        >>> stmr.stem('elusiveness')
        'elus'

        >>> stmr.stem('eateth', early_english=True)
        'eat'

        """"""
        # lowercase, normalize, and compose
        word = normalize('NFC', text_type(word.lower()))

        # Return word if stem is shorter than 2
        if len(word) < 3:
            return word

        # Re-map consonantal y to Y (Y will be C, y will be V)
        if word[0] == 'y':
            word = 'Y' + word[1:]
        for i in range(1, len(word)):
            if word[i] == 'y' and word[i - 1] in self._vowels:
                word = word[:i] + 'Y' + word[i + 1 :]

        # Step 1a
        if word[-1] == 's':
            if word[-4:] == 'sses':
                word = word[:-2]
            elif word[-3:] == 'ies':
                word = word[:-2]
            elif word[-2:] == 'ss':
                pass
            else:
                word = word[:-1]

        # Step 1b
        step1b_flag = False
        if word[-3:] == 'eed':
            if self._m_degree(word[:-3]) > 0:
                word = word[:-1]
        elif word[-2:] == 'ed':
            if self._has_vowel(word[:-2]):
                word = word[:-2]
                step1b_flag = True
        elif word[-3:] == 'ing':
            if self._has_vowel(word[:-3]):
                word = word[:-3]
                step1b_flag = True
        elif early_english:
            if word[-3:] == 'est':
                if self._has_vowel(word[:-3]):
                    word = word[:-3]
                    step1b_flag = True
            elif word[-3:] == 'eth':
                if self._has_vowel(word[:-3]):
                    word = word[:-3]
                    step1b_flag = True

        if step1b_flag:
            if word[-2:] in {'at', 'bl', 'iz'}:
                word += 'e'
            elif self._ends_in_doubled_cons(word) and word[-1] not in {
                'l',
                's',
                'z',
            }:
                word = word[:-1]
            elif self._m_degree(word) == 1 and self._ends_in_cvc(word):
                word += 'e'

        # Step 1c
        if word[-1] in {'Y', 'y'} and self._has_vowel(word[:-1]):
            word = word[:-1] + 'i'

        # Step 2
        if len(word) > 1:
            if word[-2] == 'a':
                if word[-7:] == 'ational':
                    if self._m_degree(word[:-7]) > 0:
                        word = word[:-5] + 'e'
                elif word[-6:] == 'tional':
                    if self._m_degree(word[:-6]) > 0:
                        word = word[:-2]
            elif word[-2] == 'c':
                if word[-4:] in {'enci', 'anci'}:
                    if self._m_degree(word[:-4]) > 0:
                        word = word[:-1] + 'e'
            elif word[-2] == 'e':
                if word[-4:] == 'izer':
                    if self._m_degree(word[:-4]) > 0:
                        word = word[:-1]
            elif word[-2] == 'g':
                if word[-4:] == 'logi':
                    if self._m_degree(word[:-4]) > 0:
                        word = word[:-1]
            elif word[-2] == 'l':
                if word[-3:] == 'bli':
                    if self._m_degree(word[:-3]) > 0:
                        word = word[:-1] + 'e'
                elif word[-4:] == 'alli':
                    if self._m_degree(word[:-4]) > 0:
                        word = word[:-2]
                elif word[-5:] == 'entli':
                    if self._m_degree(word[:-5]) > 0:
                        word = word[:-2]
                elif word[-3:] == 'eli':
                    if self._m_degree(word[:-3]) > 0:
                        word = word[:-2]
                elif word[-5:] == 'ousli':
                    if self._m_degree(word[:-5]) > 0:
                        word = word[:-2]
            elif word[-2] == 'o':
                if word[-7:] == 'ization':
                    if self._m_degree(word[:-7]) > 0:
                        word = word[:-5] + 'e'
                elif word[-5:] == 'ation':
                    if self._m_degree(word[:-5]) > 0:
                        word = word[:-3] + 'e'
                elif word[-4:] == 'ator':
                    if self._m_degree(word[:-4]) > 0:
                        word = word[:-2] + 'e'
            elif word[-2] == 's':
                if word[-5:] == 'alism':
                    if self._m_degree(word[:-5]) > 0:
                        word = word[:-3]
                elif word[-7:] in {'iveness', 'fulness', 'ousness'}:
                    if self._m_degree(word[:-7]) > 0:
                        word = word[:-4]
            elif word[-2] == 't':
                if word[-5:] == 'aliti':
                    if self._m_degree(word[:-5]) > 0:
                        word = word[:-3]
                elif word[-5:] == 'iviti':
                    if self._m_degree(word[:-5]) > 0:
                        word = word[:-3] + 'e'
                elif word[-6:] == 'biliti':
                    if self._m_degree(word[:-6]) > 0:
                        word = word[:-5] + 'le'

        # Step 3
        if word[-5:] in 'icate':
            if self._m_degree(word[:-5]) > 0:
                word = word[:-3]
        elif word[-5:] == 'ative':
            if self._m_degree(word[:-5]) > 0:
                word = word[:-5]
        elif word[-5:] in {'alize', 'iciti'}:
            if self._m_degree(word[:-5]) > 0:
                word = word[:-3]
        elif word[-4:] == 'ical':
            if self._m_degree(word[:-4]) > 0:
                word = word[:-2]
        elif word[-3:] == 'ful':
            if self._m_degree(word[:-3]) > 0:
                word = word[:-3]
        elif word[-4:] == 'ness':
            if self._m_degree(word[:-4]) > 0:
                word = word[:-4]

        # Step 4
        if word[-2:] == 'al':
            if self._m_degree(word[:-2]) > 1:
                word = word[:-2]
        elif word[-4:] in {'ance', 'ence'}:
            if self._m_degree(word[:-4]) > 1:
                word = word[:-4]
        elif word[-2:] in {'er', 'ic'}:
            if self._m_degree(word[:-2]) > 1:
                word = word[:-2]
        elif word[-4:] in {'able', 'ible'}:
            if self._m_degree(word[:-4]) > 1:
                word = word[:-4]
        elif word[-3:] == 'ant':
            if self._m_degree(word[:-3]) > 1:
                word = word[:-3]
        elif word[-5:] == 'ement':
            if self._m_degree(word[:-5]) > 1:
                word = word[:-5]
        elif word[-4:] == 'ment':
            if self._m_degree(word[:-4]) > 1:
                word = word[:-4]
        elif word[-3:] == 'ent':
            if self._m_degree(word[:-3]) > 1:
                word = word[:-3]
        elif word[-4:] in {'sion', 'tion'}:
            if self._m_degree(word[:-3]) > 1:
                word = word[:-3]
        elif word[-2:] == 'ou':
            if self._m_degree(word[:-2]) > 1:
                word = word[:-2]
        elif word[-3:] in {'ism', 'ate', 'iti', 'ous', 'ive', 'ize'}:
            if self._m_degree(word[:-3]) > 1:
                word = word[:-3]

        # Step 5a
        if word[-1] == 'e':
            if self._m_degree(word[:-1]) > 1:
                word = word[:-1]
            elif self._m_degree(word[:-1]) == 1 and not self._ends_in_cvc(
                word[:-1]
            ):
                word = word[:-1]

        # Step 5b
        if word[-2:] == 'll' and self._m_degree(word) > 1:
            word = word[:-1]

        # Change 'Y' back to 'y' if it survived stemming
        for i in range(len(word)):
            if word[i] == 'Y':
                word = word[:i] + 'y' + word[i + 1 :]

        return word",https://github.com/chrislit/abydos/blob/165466b3ff6afd8024a4c8660421b0c4e7773db9/abydos/stemmer/_porter.py#L139-L371
tfidf_python_100_1.0,how to determine a string is a valid word,python,"def stem(self, word, early_english=False):
        """"""Return the Porter2 (Snowball English) stem.

        Parameters
        ----------
        word : str
            The word to stem
        early_english : bool
            Set to True in order to remove -eth & -est (2nd & 3rd person
            singular verbal agreement suffixes)

        Returns
        -------
        str
            Word stem

        Examples
        --------
        >>> stmr = Porter2()
        >>> stmr.stem('reading')
        'read'
        >>> stmr.stem('suspension')
        'suspens'
        >>> stmr.stem('elusiveness')
        'elus'

        >>> stmr.stem('eateth', early_english=True)
        'eat'

        """"""
        # lowercase, normalize, and compose
        word = normalize('NFC', text_type(word.lower()))
        # replace apostrophe-like characters with U+0027, per
        # http://snowball.tartarus.org/texts/apostrophe.html
        word = word.replace('â', '\'')
        word = word.replace('â', '\'')

        # Exceptions 1
        if word in self._exception1dict:
            return self._exception1dict[word]
        elif word in self._exception1set:
            return word

        # Return word if stem is shorter than 3
        if len(word) < 3:
            return word

        # Remove initial ', if present.
        while word and word[0] == '\'':
            word = word[1:]
            # Return word if stem is shorter than 2
            if len(word) < 2:
                return word

        # Re-map vocalic Y to y (Y will be C, y will be V)
        if word[0] == 'y':
            word = 'Y' + word[1:]
        for i in range(1, len(word)):
            if word[i] == 'y' and word[i - 1] in self._vowels:
                word = word[:i] + 'Y' + word[i + 1 :]

        r1_start = self._sb_r1(word, self._r1_prefixes)
        r2_start = self._sb_r2(word, self._r1_prefixes)

        # Step 0
        if word[-3:] == '\'s\'':
            word = word[:-3]
        elif word[-2:] == '\'s':
            word = word[:-2]
        elif word[-1:] == '\'':
            word = word[:-1]
        # Return word if stem is shorter than 2
        if len(word) < 3:
            return word

        # Step 1a
        if word[-4:] == 'sses':
            word = word[:-2]
        elif word[-3:] in {'ied', 'ies'}:
            if len(word) > 4:
                word = word[:-2]
            else:
                word = word[:-1]
        elif word[-2:] in {'us', 'ss'}:
            pass
        elif word[-1] == 's':
            if self._sb_has_vowel(word[:-2]):
                word = word[:-1]

        # Exceptions 2
        if word in self._exception2set:
            return word

        # Step 1b
        step1b_flag = False
        if word[-5:] == 'eedly':
            if len(word[r1_start:]) >= 5:
                word = word[:-3]
        elif word[-5:] == 'ingly':
            if self._sb_has_vowel(word[:-5]):
                word = word[:-5]
                step1b_flag = True
        elif word[-4:] == 'edly':
            if self._sb_has_vowel(word[:-4]):
                word = word[:-4]
                step1b_flag = True
        elif word[-3:] == 'eed':
            if len(word[r1_start:]) >= 3:
                word = word[:-1]
        elif word[-3:] == 'ing':
            if self._sb_has_vowel(word[:-3]):
                word = word[:-3]
                step1b_flag = True
        elif word[-2:] == 'ed':
            if self._sb_has_vowel(word[:-2]):
                word = word[:-2]
                step1b_flag = True
        elif early_english:
            if word[-3:] == 'est':
                if self._sb_has_vowel(word[:-3]):
                    word = word[:-3]
                    step1b_flag = True
            elif word[-3:] == 'eth':
                if self._sb_has_vowel(word[:-3]):
                    word = word[:-3]
                    step1b_flag = True

        if step1b_flag:
            if word[-2:] in {'at', 'bl', 'iz'}:
                word += 'e'
            elif word[-2:] in self._doubles:
                word = word[:-1]
            elif self._sb_short_word(word, self._r1_prefixes):
                word += 'e'

        # Step 1c
        if (
            len(word) > 2
            and word[-1] in {'Y', 'y'}
            and word[-2] not in self._vowels
        ):
            word = word[:-1] + 'i'

        # Step 2
        if word[-2] == 'a':
            if word[-7:] == 'ational':
                if len(word[r1_start:]) >= 7:
                    word = word[:-5] + 'e'
            elif word[-6:] == 'tional':
                if len(word[r1_start:]) >= 6:
                    word = word[:-2]
        elif word[-2] == 'c':
            if word[-4:] in {'enci', 'anci'}:
                if len(word[r1_start:]) >= 4:
                    word = word[:-1] + 'e'
        elif word[-2] == 'e':
            if word[-4:] == 'izer':
                if len(word[r1_start:]) >= 4:
                    word = word[:-1]
        elif word[-2] == 'g':
            if word[-3:] == 'ogi':
                if (
                    r1_start >= 1
                    and len(word[r1_start:]) >= 3
                    and word[-4] == 'l'
                ):
                    word = word[:-1]
        elif word[-2] == 'l':
            if word[-6:] == 'lessli':
                if len(word[r1_start:]) >= 6:
                    word = word[:-2]
            elif word[-5:] in {'entli', 'fulli', 'ousli'}:
                if len(word[r1_start:]) >= 5:
                    word = word[:-2]
            elif word[-4:] == 'abli':
                if len(word[r1_start:]) >= 4:
                    word = word[:-1] + 'e'
            elif word[-4:] == 'alli':
                if len(word[r1_start:]) >= 4:
                    word = word[:-2]
            elif word[-3:] == 'bli':
                if len(word[r1_start:]) >= 3:
                    word = word[:-1] + 'e'
            elif word[-2:] == 'li':
                if (
                    r1_start >= 1
                    and len(word[r1_start:]) >= 2
                    and word[-3] in self._li
                ):
                    word = word[:-2]
        elif word[-2] == 'o':
            if word[-7:] == 'ization':
                if len(word[r1_start:]) >= 7:
                    word = word[:-5] + 'e'
            elif word[-5:] == 'ation':
                if len(word[r1_start:]) >= 5:
                    word = word[:-3] + 'e'
            elif word[-4:] == 'ator':
                if len(word[r1_start:]) >= 4:
                    word = word[:-2] + 'e'
        elif word[-2] == 's':
            if word[-7:] in {'fulness', 'ousness', 'iveness'}:
                if len(word[r1_start:]) >= 7:
                    word = word[:-4]
            elif word[-5:] == 'alism':
                if len(word[r1_start:]) >= 5:
                    word = word[:-3]
        elif word[-2] == 't':
            if word[-6:] == 'biliti':
                if len(word[r1_start:]) >= 6:
                    word = word[:-5] + 'le'
            elif word[-5:] == 'aliti':
                if len(word[r1_start:]) >= 5:
                    word = word[:-3]
            elif word[-5:] == 'iviti':
                if len(word[r1_start:]) >= 5:
                    word = word[:-3] + 'e'

        # Step 3
        if word[-7:] == 'ational':
            if len(word[r1_start:]) >= 7:
                word = word[:-5] + 'e'
        elif word[-6:] == 'tional':
            if len(word[r1_start:]) >= 6:
                word = word[:-2]
        elif word[-5:] in {'alize', 'icate', 'iciti'}:
            if len(word[r1_start:]) >= 5:
                word = word[:-3]
        elif word[-5:] == 'ative':
            if len(word[r2_start:]) >= 5:
                word = word[:-5]
        elif word[-4:] == 'ical':
            if len(word[r1_start:]) >= 4:
                word = word[:-2]
        elif word[-4:] == 'ness':
            if len(word[r1_start:]) >= 4:
                word = word[:-4]
        elif word[-3:] == 'ful':
            if len(word[r1_start:]) >= 3:
                word = word[:-3]

        # Step 4
        for suffix in (
            'ement',
            'ance',
            'ence',
            'able',
            'ible',
            'ment',
            'ant',
            'ent',
            'ism',
            'ate',
            'iti',
            'ous',
            'ive',
            'ize',
            'al',
            'er',
            'ic',
        ):
            if word[-len(suffix) :] == suffix:
                if len(word[r2_start:]) >= len(suffix):
                    word = word[: -len(suffix)]
                break
        else:
            if word[-3:] == 'ion':
                if (
                    len(word[r2_start:]) >= 3
                    and len(word) >= 4
                    and word[-4] in tuple('st')
                ):
                    word = word[:-3]

        # Step 5
        if word[-1] == 'e':
            if len(word[r2_start:]) >= 1 or (
                len(word[r1_start:]) >= 1
                and not self._sb_ends_in_short_syllable(word[:-1])
            ):
                word = word[:-1]
        elif word[-1] == 'l':
            if len(word[r2_start:]) >= 1 and word[-2] == 'l':
                word = word[:-1]

        # Change 'Y' back to 'y' if it survived stemming
        for i in range(0, len(word)):
            if word[i] == 'Y':
                word = word[:i] + 'y' + word[i + 1 :]

        return word",https://github.com/chrislit/abydos/blob/165466b3ff6afd8024a4c8660421b0c4e7773db9/abydos/stemmer/_porter2.py#L87-L377
tfidf_python_100_1.0,how to determine a string is a valid word,python,"def stem(self, word):
        """"""Return Snowball Dutch stem.

        Parameters
        ----------
        word : str
            The word to stem

        Returns
        -------
        str
            Word stem

        Examples
        --------
        >>> stmr = SnowballDutch()
        >>> stmr.stem('lezen')
        'lez'
        >>> stmr.stem('opschorting')
        'opschort'
        >>> stmr.stem('ongrijpbaarheid')
        'ongrijp'

        """"""
        # lowercase, normalize, decompose, filter umlauts & acutes out, and
        # compose
        word = normalize('NFC', text_type(word.lower()))
        word = word.translate(self._accented)

        for i in range(len(word)):
            if i == 0 and word[0] == 'y':
                word = 'Y' + word[1:]
            elif word[i] == 'y' and word[i - 1] in self._vowels:
                word = word[:i] + 'Y' + word[i + 1 :]
            elif (
                word[i] == 'i'
                and word[i - 1] in self._vowels
                and i + 1 < len(word)
                and word[i + 1] in self._vowels
            ):
                word = word[:i] + 'I' + word[i + 1 :]

        r1_start = max(3, self._sb_r1(word))
        r2_start = self._sb_r2(word)

        # Step 1
        if word[-5:] == 'heden':
            if len(word[r1_start:]) >= 5:
                word = word[:-3] + 'id'
        elif word[-3:] == 'ene':
            if len(word[r1_start:]) >= 3 and (
                word[-4] not in self._vowels and word[-6:-3] != 'gem'
            ):
                word = self._undouble(word[:-3])
        elif word[-2:] == 'en':
            if len(word[r1_start:]) >= 2 and (
                word[-3] not in self._vowels and word[-5:-2] != 'gem'
            ):
                word = self._undouble(word[:-2])
        elif word[-2:] == 'se':
            if (
                len(word[r1_start:]) >= 2
                and word[-3] not in self._not_s_endings
            ):
                word = word[:-2]
        elif word[-1:] == 's':
            if (
                len(word[r1_start:]) >= 1
                and word[-2] not in self._not_s_endings
            ):
                word = word[:-1]

        # Step 2
        e_removed = False
        if word[-1:] == 'e':
            if len(word[r1_start:]) >= 1 and word[-2] not in self._vowels:
                word = self._undouble(word[:-1])
                e_removed = True

        # Step 3a
        if word[-4:] == 'heid':
            if len(word[r2_start:]) >= 4 and word[-5] != 'c':
                word = word[:-4]
                if word[-2:] == 'en':
                    if len(word[r1_start:]) >= 2 and (
                        word[-3] not in self._vowels and word[-5:-2] != 'gem'
                    ):
                        word = self._undouble(word[:-2])

        # Step 3b
        if word[-4:] == 'lijk':
            if len(word[r2_start:]) >= 4:
                word = word[:-4]
                # Repeat step 2
                if word[-1:] == 'e':
                    if (
                        len(word[r1_start:]) >= 1
                        and word[-2] not in self._vowels
                    ):
                        word = self._undouble(word[:-1])
        elif word[-4:] == 'baar':
            if len(word[r2_start:]) >= 4:
                word = word[:-4]
        elif word[-3:] in ('end', 'ing'):
            if len(word[r2_start:]) >= 3:
                word = word[:-3]
                if (
                    word[-2:] == 'ig'
                    and len(word[r2_start:]) >= 2
                    and word[-3] != 'e'
                ):
                    word = word[:-2]
                else:
                    word = self._undouble(word)
        elif word[-3:] == 'bar':
            if len(word[r2_start:]) >= 3 and e_removed:
                word = word[:-3]
        elif word[-2:] == 'ig':
            if len(word[r2_start:]) >= 2 and word[-3] != 'e':
                word = word[:-2]

        # Step 4
        if (
            len(word) >= 4
            and word[-3] == word[-2]
            and word[-2] in {'a', 'e', 'o', 'u'}
            and word[-4] not in self._vowels
            and word[-1] not in self._vowels
            and word[-1] != 'I'
        ):
            word = word[:-2] + word[-1]

        # Change 'Y' and 'U' back to lowercase if survived stemming
        for i in range(0, len(word)):
            if word[i] == 'Y':
                word = word[:i] + 'y' + word[i + 1 :]
            elif word[i] == 'I':
                word = word[:i] + 'i' + word[i + 1 :]

        return word",https://github.com/chrislit/abydos/blob/165466b3ff6afd8024a4c8660421b0c4e7773db9/abydos/stemmer/_snowball_dutch.py#L74-L213
tfidf_python_100_1.0,how to determine a string is a valid word,python,"def stem(self, word, alternate_vowels=False):
        """"""Return Snowball German stem.

        Parameters
        ----------
        word : str
            The word to stem
        alternate_vowels : bool
            Composes ae as Ã¤, oe as Ã¶, and ue as Ã¼ before running the algorithm

        Returns
        -------
        str
            Word stem

        Examples
        --------
        >>> stmr = SnowballGerman()
        >>> stmr.stem('lesen')
        'les'
        >>> stmr.stem('graues')
        'grau'
        >>> stmr.stem('buchstabieren')
        'buchstabi'

        """"""
        # lowercase, normalize, and compose
        word = normalize('NFC', word.lower())
        word = word.replace('Ã', 'ss')

        if len(word) > 2:
            for i in range(2, len(word)):
                if word[i] in self._vowels and word[i - 2] in self._vowels:
                    if word[i - 1] == 'u':
                        word = word[: i - 1] + 'U' + word[i:]
                    elif word[i - 1] == 'y':
                        word = word[: i - 1] + 'Y' + word[i:]

        if alternate_vowels:
            word = word.replace('ae', 'Ã¤')
            word = word.replace('oe', 'Ã¶')
            word = word.replace('que', 'Q')
            word = word.replace('ue', 'Ã¼')
            word = word.replace('Q', 'que')

        r1_start = max(3, self._sb_r1(word))
        r2_start = self._sb_r2(word)

        # Step 1
        niss_flag = False
        if word[-3:] == 'ern':
            if len(word[r1_start:]) >= 3:
                word = word[:-3]
        elif word[-2:] == 'em':
            if len(word[r1_start:]) >= 2:
                word = word[:-2]
        elif word[-2:] == 'er':
            if len(word[r1_start:]) >= 2:
                word = word[:-2]
        elif word[-2:] == 'en':
            if len(word[r1_start:]) >= 2:
                word = word[:-2]
                niss_flag = True
        elif word[-2:] == 'es':
            if len(word[r1_start:]) >= 2:
                word = word[:-2]
                niss_flag = True
        elif word[-1:] == 'e':
            if len(word[r1_start:]) >= 1:
                word = word[:-1]
                niss_flag = True
        elif word[-1:] == 's':
            if (
                len(word[r1_start:]) >= 1
                and len(word) >= 2
                and word[-2] in self._s_endings
            ):
                word = word[:-1]

        if niss_flag and word[-4:] == 'niss':
            word = word[:-1]

        # Step 2
        if word[-3:] == 'est':
            if len(word[r1_start:]) >= 3:
                word = word[:-3]
        elif word[-2:] == 'en':
            if len(word[r1_start:]) >= 2:
                word = word[:-2]
        elif word[-2:] == 'er':
            if len(word[r1_start:]) >= 2:
                word = word[:-2]
        elif word[-2:] == 'st':
            if (
                len(word[r1_start:]) >= 2
                and len(word) >= 6
                and word[-3] in self._st_endings
            ):
                word = word[:-2]

        # Step 3
        if word[-4:] == 'isch':
            if len(word[r2_start:]) >= 4 and word[-5] != 'e':
                word = word[:-4]
        elif word[-4:] in {'lich', 'heit'}:
            if len(word[r2_start:]) >= 4:
                word = word[:-4]
                if word[-2:] in {'er', 'en'} and len(word[r1_start:]) >= 2:
                    word = word[:-2]
        elif word[-4:] == 'keit':
            if len(word[r2_start:]) >= 4:
                word = word[:-4]
                if word[-4:] == 'lich' and len(word[r2_start:]) >= 4:
                    word = word[:-4]
                elif word[-2:] == 'ig' and len(word[r2_start:]) >= 2:
                    word = word[:-2]
        elif word[-3:] in {'end', 'ung'}:
            if len(word[r2_start:]) >= 3:
                word = word[:-3]
                if (
                    word[-2:] == 'ig'
                    and len(word[r2_start:]) >= 2
                    and word[-3] != 'e'
                ):
                    word = word[:-2]
        elif word[-2:] in {'ig', 'ik'}:
            if len(word[r2_start:]) >= 2 and word[-3] != 'e':
                word = word[:-2]

        # Change 'Y' and 'U' back to lowercase if survived stemming
        for i in range(0, len(word)):
            if word[i] == 'Y':
                word = word[:i] + 'y' + word[i + 1 :]
            elif word[i] == 'U':
                word = word[:i] + 'u' + word[i + 1 :]

        # Remove umlauts
        _umlauts = dict(zip((ord(_) for _ in 'Ã¤Ã¶Ã¼'), 'aou'))
        word = word.translate(_umlauts)

        return word",https://github.com/chrislit/abydos/blob/165466b3ff6afd8024a4c8660421b0c4e7773db9/abydos/stemmer/_snowball_german.py#L51-L191
tfidf_python_100_1.0,how to determine a string is a valid word,python,"def unquote(word):
    if len(word) == 0:
        return word
    if word[0] == '""' and word[-1] == '""':
        return word[1:-1]
    elif word[0] == ""'"" and word[-1] == ""'"":
        return word[1:-1]
    return word",https://github.com/meejah/txtorcon/blob/14053b95adf0b4bd9dd9c317bece912a26578a93/txtorcon/torcontrolprotocol.py#L146-L153
tfidf_python_100_1.0,how to determine a string is a valid word,python,"def rebreath(word):
    if word == """":
        return word
    if word.startswith(""h""):
        word = add_necessary_breathing(word[1:], ROUGH)
    else:
        word = add_necessary_breathing(word)
    word = remove_redundant_macron(word)

    return word",https://github.com/jtauber/greek-accentuation/blob/330796cd97f7c7adcbecbd05bd91be984f9b9f67/greek_accentuation/syllabify.py#L283-L292
tfidf_python_100_1.0,how to determine a string is a valid word,python,"def _remove_quotes(cls, word):
        if len(word) > 0 and word.startswith((""'"", '""')) and word[0] == word[-1]:
            return word[1:-1]

        return word",https://github.com/iotile/typedargs/blob/0a5091a664b9b4d836e091e9ba583e944f438fd8/typedargs/shell.py#L127-L131
tfidf_python_100_1.0,how to determine a string is a valid word,python,"def encode(self, word):
        """"""Return the MRA personal numeric identifier (PNI) for a word.

        Parameters
        ----------
        word : str
            The word to transform

        Returns
        -------
        str
            The MRA PNI

        Examples
        --------
        >>> pe = MRA()
        >>> pe.encode('Christopher')
        'CHRPHR'
        >>> pe.encode('Niall')
        'NL'
        >>> pe.encode('Smith')
        'SMTH'
        >>> pe.encode('Schmidt')
        'SCHMDT'

        """"""
        if not word:
            return word
        word = word.upper()
        word = word.replace('Ã', 'SS')
        word = word[0] + ''.join(
            c for c in word[1:] if c not in self._uc_v_set
        )
        word = self._delete_consecutive_repeats(word)
        if len(word) > 6:
            word = word[:3] + word[-3:]
        return word",https://github.com/chrislit/abydos/blob/165466b3ff6afd8024a4c8660421b0c4e7773db9/abydos/phonetic/_mra.py#L43-L79
tfidf_python_100_1.0,how to determine a string is a valid word,python,"def stem(self, word):
        """"""
        Stem the word if it has more than two characters,
        otherwise return it as is.
        """"""

        if len(word) <= 2:
            return word
        else:
            word = self.remove_initial_apostrophe(word)
            word = self.set_ys(word)
            self.find_regions(word)

            word = self.strip_possessives(word)
            word = self.replace_suffixes_1(word)
            word = self.replace_suffixes_2(word)
            word = self.replace_ys(word)
            word = self.replace_suffixes_3(word)
            word = self.replace_suffixes_4(word)
            word = self.delete_suffixes(word)
            word = self.process_terminals(word)

            return word",https://github.com/evandempsey/porter2-stemmer/blob/949824b7767c25efb014ef738e682442fa70c10b/porter2stemmer/porter2stemmer.py#L38-L60
tfidf_python_100_1.0,how to determine a string is a valid word,python,"def remove_adverbs_ends(word):
        if len(word) > 4 and word[:-3] in {""nie"", ""wie""}:
            return word[:-2]
        if len(word) > 4 and word.endswith(""rze""):
            return word[:-2]
        return word",https://github.com/summanlp/textrank/blob/6844bbe8c4b2b468020ae0dfd6574a743f9ad442/summa/preprocessing/snowball.py#L3692-L3697
tfidf_python_100_1.0,how to determine a string is a valid word,python,"def _acceptable(self, word):
        if word and word[0] in {'a', 'e', 'i', 'o', 'u'}:
            return len(word) > 1
        return len(word) > 2 and self._has_vowel(word[1:])",https://github.com/chrislit/abydos/blob/165466b3ff6afd8024a4c8660421b0c4e7773db9/abydos/stemmer/_paice_husk.py#L178-L181
tfidf_python_100_1.0,how to determine a string is a valid word,python,"def add_keyword(self, word, or_operator=False):
        """""" Adds a given string or list to the current keyword list

        :param word: String or list of at least 2 character long keyword(s)
        :param or_operator: Boolean. Concatenates all elements of parameter \
        word with ``OR``. Is ignored is word is not a list. Thus it is \
        possible to search for ``foo OR bar``. Default value is False \
        which corresponds to a search of ``foo AND bar``.
        :raises: TwitterSearchException
        """"""

        if isinstance(word, str if py3k else basestring) and len(word) >= 2:
            self.searchterms.append(word if "" "" not in word else '""%s""' % word)
        elif isinstance(word, (tuple,list)):
            word = [ (i if "" "" not in i else '""%s""' % i)  for i in word ]
            self.searchterms += ["" OR "".join(word)] if or_operator else word
        else:
            raise TwitterSearchException(1000)",https://github.com/ckoepp/TwitterSearch/blob/627b9f519d49faf6b83859717f9082b3b2622aaf/TwitterSearch/TwitterSearchOrder.py#L132-L149
tfidf_python_100_1.0,how to determine a string is a valid word,python,"def _compile_dict_into_word(WORD):
    word = ''

    for i in xrange(1, len(WORD) + 1):
        word += WORD[i]

    return word",https://github.com/tsnaomi/finnsyll/blob/6a42740311688c946a636a3e2304866c7aa041b3/finnsyll/prev/v01.py#L45-L51
tfidf_python_100_1.0,how to determine a string is a valid word,python,"def _remove_possessives(word):
    if len(word) > 5:
        if word[-2:] in (""ov"", ""Å¯v""):
            return word[:-2]
        if word.endswith(""in""):
            return _palatalize(word[:-1])
    return word",https://github.com/miso-belica/sumy/blob/099ab4938e2c1b6a011297375586bac2953641b9/sumy/nlp/stemmers/czech.py#L91-L97
tfidf_python_100_1.0,how to determine a string is a valid word,python,"def escape(word):
    if ' ' in word:
        word = word.replace('""', '\\""')
        word = word.replace(""'"", ""\\'"")
        word = '""' + word + '""'
    return word",https://github.com/TakesxiSximada/jumon/blob/7b659882bc3d5036588dfbc4c901291be650468e/src/jumon/__init__.py#L73-L78
tfidf_python_100_1.0,how to determine a string is a valid word,python,"def stem(self, word):
        """"""Return Caumanns German stem.

        Parameters
        ----------
        word : str
            The word to stem

        Returns
        -------
        str
            Word stem

        Examples
        --------
        >>> stmr = Caumanns()
        >>> stmr.stem('lesen')
        'les'
        >>> stmr.stem('graues')
        'grau'
        >>> stmr.stem('buchstabieren')
        'buchstabier'

        """"""
        if not word:
            return ''

        upper_initial = word[0].isupper()
        word = normalize('NFC', text_type(word.lower()))

        # # Part 2: Substitution
        # 1. Change umlauts to corresponding vowels & Ã to ss
        word = word.translate(self._umlauts)
        word = word.replace('Ã', 'ss')

        # 2. Change second of doubled characters to *
        new_word = word[0]
        for i in range(1, len(word)):
            if new_word[i - 1] == word[i]:
                new_word += '*'
            else:
                new_word += word[i]
        word = new_word

        # 3. Replace sch, ch, ei, ie with $, Â§, %, &
        word = word.replace('sch', '$')
        word = word.replace('ch', 'Â§')
        word = word.replace('ei', '%')
        word = word.replace('ie', '&')
        word = word.replace('ig', '#')
        word = word.replace('st', '!')

        # # Part 1: Recursive Context-Free Stripping
        # 1. Remove the following 7 suffixes recursively
        while len(word) > 3:
            if (len(word) > 4 and word[-2:] in {'em', 'er'}) or (
                len(word) > 5 and word[-2:] == 'nd'
            ):
                word = word[:-2]
            elif (word[-1] in {'e', 's', 'n'}) or (
                not upper_initial and word[-1] in {'t', '!'}
            ):
                word = word[:-1]
            else:
                break

        # Additional optimizations:
        if len(word) > 5 and word[-5:] == 'erin*':
            word = word[:-1]
        if word[-1] == 'z':
            word = word[:-1] + 'x'

        # Reverse substitutions:
        word = word.replace('$', 'sch')
        word = word.replace('Â§', 'ch')
        word = word.replace('%', 'ei')
        word = word.replace('&', 'ie')
        word = word.replace('#', 'ig')
        word = word.replace('!', 'st')

        # Expand doubled
        word = ''.join(
            [word[0]]
            + [
                word[i - 1] if word[i] == '*' else word[i]
                for i in range(1, len(word))
            ]
        )

        # Finally, convert gege to ge
        if len(word) > 4:
            word = word.replace('gege', 'ge', 1)

        return word",https://github.com/chrislit/abydos/blob/165466b3ff6afd8024a4c8660421b0c4e7773db9/abydos/stemmer/_caumanns.py#L53-L146
tfidf_python_100_1.0,html entities replace,python,"def __init__(self, entities):
        self.entities = entities
        self.entities_by_id = dict((e.name, e) for e in entities)",https://github.com/lalinsky/mbdata/blob/1ec788834047ced8614ad9763e430afe1d1e65e7/mbdata/search.py#L38-L40
tfidf_python_100_1.0,html entities replace,python,"def make_entities_code(entities):
    entities_text = ""\n"".join(""    \""%s\"": u\""%s\"","" % (
        name, entities[name].encode(
            ""unicode-escape"").replace(""\"""", ""\\\""""))
        for name in sorted(entities.keys()))
    return """"""entities = {
%s
}"""""" % entities_text",https://github.com/html5lib/html5lib-python/blob/4b2275497b624c6e97150fa2eb16a7db7ed42111/utils/entities.py#L82-L89
tfidf_python_100_1.0,html entities replace,python,"def normalize_entities(cur_title):
    entities = {
        u'\u2014':'-',
        u'\u2013':'-',
        u'&mdash;': '-',
        u'&ndash;': '-',
        u'\u00A0': ' ',
        u'\u00AB': '""',
        u'\u00BB': '""',
        u'&quot;': '""',
    }
    for c, r in entities.items():
        if c in cur_title:
            cur_title = cur_title.replace(c, r)

    return cur_title",https://github.com/usc-isi-i2/etk/blob/aab077c984ea20f5e8ae33af622fe11d3c4df866/etk/extractors/readability/htmls.py#L25-L40
tfidf_python_100_1.0,html entities replace,python,"def add_entities(self,entities):
        self.response_count.append(len(entities))
        self._wrapper.process_inbound_entities(entities)
        return entities",https://github.com/MattParr/python-atws/blob/2128baf85d00dcc290ecf911d6c636ac0abe5f33/atws/wrapper.py#L129-L132
tfidf_python_100_1.0,html entities replace,python,"def schedule_entities_reindex(entities):
    """"""
    :param entities: as returned by :func:`get_entities_for_reindex`
    """"""
    entities = [(e[0], e[1], e[2], dict(e[3])) for e in entities]
    return index_update.apply_async(kwargs={""index"": ""default"", ""items"": entities})",https://github.com/abilian/abilian-core/blob/0a71275bf108c3d51e13ca9e093c0249235351e3/abilian/web/tags/admin.py#L57-L62
tfidf_python_100_1.0,html entities replace,python,"def __init__(self, entities=None, mode=None, **unknown_fields):
        '''
        entities : Entities
        mode : bool
        '''
        self.entities = Entities.from_json(entities) if entities else None
        self.mode = mode",https://github.com/juju/python-libjuju/blob/58f0011f4c57cd68830258952fa952eaadca6b38/juju/client/_definitions.py#L5000-L5006
tfidf_python_100_1.0,html entities replace,python,"def __init__(self, entities=None, **unknown_fields):
        '''
        entities : typing.Sequence[~EntityPortRange]
        '''
        self.entities = [EntityPortRange.from_json(o) for o in entities or []]",https://github.com/juju/python-libjuju/blob/58f0011f4c57cd68830258952fa952eaadca6b38/juju/client/_definitions.py#L3218-L3222
tfidf_python_100_1.0,html entities replace,python,"def __init__(self, entities=None, **unknown_fields):
        '''
        entities : typing.Sequence[~AgentGetEntitiesResult]
        '''
        self.entities = [AgentGetEntitiesResult.from_json(o) for o in entities or []]",https://github.com/juju/python-libjuju/blob/58f0011f4c57cd68830258952fa952eaadca6b38/juju/client/_definitions.py#L566-L570
tfidf_python_100_1.0,html entities replace,python,"def __init__(self, entities=None, **unknown_fields):
        '''
        entities : typing.Sequence[~EntityWorkloadVersion]
        '''
        self.entities = [EntityWorkloadVersion.from_json(o) for o in entities or []]",https://github.com/juju/python-libjuju/blob/58f0011f4c57cd68830258952fa952eaadca6b38/juju/client/_definitions.py#L3467-L3471
tfidf_python_100_1.0,html entities replace,python,"def __init__(self, entities=None, **unknown_fields):
        '''
        entities : typing.Sequence[~EntityCharmURL]
        '''
        self.entities = [EntityCharmURL.from_json(o) for o in entities or []]",https://github.com/juju/python-libjuju/blob/58f0011f4c57cd68830258952fa952eaadca6b38/juju/client/_definitions.py#L3207-L3211
tfidf_python_100_1.0,html entities replace,python,"def __init__(self, entities=None, **unknown_fields):
        '''
        entities : typing.Sequence[~EntityStatusArgs]
        '''
        self.entities = [EntityStatusArgs.from_json(o) for o in entities or []]",https://github.com/juju/python-libjuju/blob/58f0011f4c57cd68830258952fa952eaadca6b38/juju/client/_definitions.py#L9096-L9100
tfidf_python_100_1.0,html entities replace,python,"async def _replace_with_mention(self, entities, i, user):
        """"""
        Helper method to replace ``entities[i]`` to mention ``user``,
        or do nothing if it can't be found.
        """"""
        try:
            entities[i] = types.InputMessageEntityMentionName(
                entities[i].offset, entities[i].length,
                await self.get_input_entity(user)
            )
            return True            
        except (ValueError, TypeError):
            return False",https://github.com/LonamiWebs/Telethon/blob/1ead9757d366b58c1e0567cddb0196e20f1a445f/telethon/client/messageparse.py#L48-L60
tfidf_python_100_1.0,html entities replace,python,"def get(self):
        entities = self.get_repository().get_all()
        return self.render_template(entities=entities)",https://github.com/kyzima-spb/flask-pony/blob/6cf28d70b7ebf415d58fa138fcc70b8dd57432c7/flask_pony/views.py#L183-L185
tfidf_python_100_1.0,html entities replace,python,"def get_entity(self, request, **kwargs):
        entities = self.get_entity_list(request, **kwargs)
        if len(entities) == 0:
            return None
        elif len(entities) > 1:
            raise error_types.MultipleObjectsFound()
        else:
            return entities[0]",https://github.com/Aplopio/django_rip/blob/6b03962ccb778c1a95950a3803e5170c7a2392df/rip/generic_steps/default_entity_actions.py#L120-L127
tfidf_python_100_1.0,html entities replace,python,"def _dissociateFileFromJob(self, jobStoreFileID):
        entities = list(self.jobFileIDs.query_entities(filter=""RowKey eq '%s'"" % jobStoreFileID))
        if entities:
            assert len(entities) == 1
            jobStoreID = entities[0].PartitionKey
            self.jobFileIDs.delete_entity(partition_key=str(jobStoreID), row_key=str(jobStoreFileID))",https://github.com/DataBiosphere/toil/blob/a8252277ff814e7bee0971139c2344f88e44b644/src/toil/jobStores/azureJobStore.py#L490-L495
tfidf_python_100_1.0,html entities replace,python,"def create(self, user_says=True, entities=True):
        if user_says:
            self.create_user_says_skeleton()
        if entities:
            self.create_entity_skeleton()",https://github.com/treethought/flask-assistant/blob/9331b9796644dfa987bcd97a13e78e9ab62923d3/api_ai/schema_handlers.py#L352-L356
tfidf_python_100_1.0,html entities replace,python,"def boolean_intersection(self, entities, delete_first=True, delete_other=True):
        """"""Boolean intersection, see
        https://gmsh.info/doc/texinfo/gmsh.html#Boolean-operations input_entity
        and tool_entity are called object and tool in gmsh documentation.
        """"""
        assert len(entities) > 1
        return self._boolean_operation(
            ""BooleanIntersection"",
            [entities[0]],
            entities[1:],
            delete_first=delete_first,
            delete_other=delete_other,
        )",https://github.com/nschloe/pygmsh/blob/1a1a07481aebe6c161b60dd31e0fbe1ddf330d61/pygmsh/opencascade/geometry.py#L163-L175
tfidf_python_100_1.0,html entities replace,python,"def __init__(self, entity_class, entities):
        Query.__init__(self, entity_class)
        self.__entities = entities",https://github.com/helixyte/everest/blob/70c9b93c3061db5cb62428349d18b8fb8566411b/everest/repositories/memory/querying.py#L81-L83
tfidf_python_100_1.0,html entities replace,python,"def load_entities():
    """"""Load entities from JSON file.""""""
    path = os.path.join(TOPDIR, 'entities.json')
    entities = json.load(open(path))
    names = [i['name'] for i in entities]

    try:
        assert len(set(names)) == len(entities)
    except AssertionError:
        raise Exception('Entities with same name: %s' % [i for i in names if
                                                         names.count(i) > 1])

    entities = dict((k['name'], c.Entity(name=k['name'],
                                         dimensions=k['dimensions'],
                                         uri=k['URI'])) for k in entities)

    dimensions_ent = defaultdict(list)
    for ent in entities:
        if not entities[ent].dimensions:
            continue
        perms = get_dimension_permutations(entities, entities[ent].dimensions)
        for perm in perms:
            key = get_key_from_dimensions(perm)
            dimensions_ent[key].append(entities[ent])

    return entities, dimensions_ent",https://github.com/marcolagi/quantulum/blob/28b697dfa997116c1aa3ef63a3ceb8725bffd24f/quantulum/load.py#L59-L84
tfidf_python_100_1.0,html entities replace,python,"def list_entities(self):
        driver = self._driver()
        entities = driver.list_entities()
        print entities
        tmpl = env.get_template('list_entities.html')
        return tmpl.render(entities=entities)",https://github.com/racker/rackspace-monitoring/blob/8a9929e5fd51826c0a392e21bc55acb2aefe54f7/demo/web/app.py#L40-L45
tfidf_python_100_1.0,set file attrib hidden,python,"def update_attrib(attrib):
    new_attrib = {}
    for k in attrib:
        new_k = fix_ns(k)
        new_attrib[new_k] = attrib[k]
    return new_attrib",https://github.com/mattharrison/rst2odp/blob/4adbf29b28c8207ec882f792ded07e98b1d3e7d0/odplib/preso.py#L154-L159
tfidf_python_100_1.0,set file attrib hidden,python,"def _get_attrib():
    try:
        attrib = cache['attrib']
    except KeyError:
        attrib = LanguageTool._get_attrib()
        cache['attrib'] = attrib
    return attrib",https://github.com/myint/language-check/blob/58e419833ef28a9193fcaa21193616a8a14504a9/language_check/__init__.py#L511-L517
tfidf_python_100_1.0,set file attrib hidden,python,"def fold_text(self, prefix, hidden, postfix="""", file=None, align=0):
        """"""
        :param str prefix: always visible
        :param str hidden: hidden
            If this is sys.stdout, it will replace that stream,
            and collect the data during the context (in the `with` block).
        :param str postfix: always visible, right after. """" by default.
        :param io.TextIOBase|io.StringIO file: sys.stdout by default.
        :param int align: remove this number of initial chars from hidden
        """"""
        if file is None:
            file = sys.stdout
        # Extra logic: Multi-line hidden. Add initial ""\n"" if not there.
        if ""\n"" in hidden:
            if hidden[:1] != ""\n"":
                hidden = ""\n"" + hidden
        # Extra logic: A final ""\n"" of hidden, make it always visible such that it looks nicer.
        if hidden[-1:] == ""\n"":
            hidden = hidden[:-1]
            postfix += ""\n""
        if self.is_domterm():
            with self.logical_block(file=file):
                self.indentation(file=file)
                self.hide_button(file=file)
                file.write(prefix)
                if prefix.endswith(""\x1b[0m""):
                    file.write("" "")  # bug in DomTerm?
                with self.hide_button_span(2, file=file):
                    hidden_ls = hidden.split(""\n"")
                    hidden_ls = [s[align:] for s in hidden_ls]
                    hidden = ""\033]118\007"".join(hidden_ls)
                    file.write(hidden)
        else:
            file.write(prefix)
            file.write(hidden.replace(""\n"", ""\n ""))
        file.write(postfix)
        file.flush()",https://github.com/albertz/py_better_exchook/blob/3d524a027d7fc4e83e47e39a1978849561da69b3/better_exchook.py#L785-L821
tfidf_python_100_1.0,set file attrib hidden,python,"def update_gradients(self, dL_dF, X):
        hidden = self.mapping1.f(X)
        self.mapping2.update_gradients(dL_dF, hidden)
        self.mapping1.update_gradients(self.mapping2.gradients_X(dL_dF, hidden), X)",https://github.com/SheffieldML/GPy/blob/54c32d79d289d622fb18b898aee65a2a431d90cf/GPy/mappings/compound.py#L32-L35
tfidf_python_100_1.0,set file attrib hidden,python,"def gradients_X(self, dL_dF, X):
        hidden = self.mapping1.f(X)
        return self.mapping1.gradients_X(self.mapping2.gradients_X(dL_dF, hidden), X)",https://github.com/SheffieldML/GPy/blob/54c32d79d289d622fb18b898aee65a2a431d90cf/GPy/mappings/compound.py#L37-L39
tfidf_python_100_1.0,set file attrib hidden,python,"def __init__(self, hidden=None):
        super(HideStateModel, self).__init__()
        self.hidden = hidden",https://github.com/Microsoft/azure-devops-python-api/blob/4777ffda2f5052fabbaddb2abe9cb434e0cf1aa8/azure-devops/azure/devops/v5_0/work_item_tracking_process/models.py#L375-L377
tfidf_python_100_1.0,set file attrib hidden,python,"def get_entity_attrib(self, attrib):
        attrib_tokens = self.get(attrib, None)
        return attrib_tokens[self.word_start : self.word_end + 1]",https://github.com/HazyResearch/metal/blob/c24e3772e25ac6d0917b8b7af4c1bcb92928f84a/metal/contrib/info_extraction/mentions.py#L83-L85
tfidf_python_100_1.0,set file attrib hidden,python,"def _setup_rect(self, sub_element, xoff, yoff, xsize, ysize):
        sub_element.attrib['xOff'] = str(xoff)
        sub_element.attrib['yOff'] = str(yoff)
        sub_element.attrib['xSize'] = str(xsize)
        sub_element.attrib['ySize'] = str(ysize)",https://github.com/satellogic/telluric/blob/e752cd3ee71e339f79717e526fde362e80055d9e/telluric/base_vrt.py#L129-L133
tfidf_python_100_1.0,set file attrib hidden,python,"def __str__(self):
        attrib = ['key'] + [a.name for a in self._gene_attributes]
        attrib = ['{0}={1}'.format(a, getattr(self, a)) for a in attrib]
        return '{0}({1})'.format(self.__class__.__name__, "", "".join(attrib))",https://github.com/CodeReclaimers/neat-python/blob/e3dbe77c0d776eae41d598e6439e6ac02ab90b18/neat/genes.py#L18-L21
tfidf_python_100_1.0,set file attrib hidden,python,"def _initialize_from_column_xml(self, xmldata):
        for attrib in _ATTRIBUTES:
            self._apply_attribute(xmldata, attrib, lambda x: xmldata.attrib.get(x, None))",https://github.com/tableau/document-api-python/blob/9097a5b351622c5dd2653fa94624bc012316d8a4/tableaudocumentapi/field.py#L59-L61
tfidf_python_100_1.0,set file attrib hidden,python,"def hidden(rs_id):
    logger.debug(""hidden({rs_id})"".format(**locals()))
    if rs_id not in ReplicaSets():
        return send_result(404)
    hidden_docs = []
    for hidden_info in ReplicaSets().hidden(rs_id):
        hidden_info['links'] = _build_member_links(rs_id, hidden_info)
        hidden_docs.append(hidden_info)
    result = {
        'hidden': hidden_docs,
        'links': _build_member_parent_links(
            rs_id, 'get-replica-set-hidden-members')
    }
    return send_result(200, result)",https://github.com/10gen/mongo-orchestration/blob/81fd2224205922ea2178b08190b53a33aec47261/mongo_orchestration/apps/replica_sets.py#L239-L252
tfidf_python_100_1.0,set file attrib hidden,python,"def parse_transaction_item(trxn, term, cards):
    """"""
    """"""
    t = None
    card = None
    card_description = None
    icc_trxn = None
    timeout = None

    #card = cards[trxn.attrib['card']]
    try:
        card_description = trxn.attrib['card']
    except KeyError:
        for key, value in cards.items():
            card_description = value.get_description()
            break
    
    try:
        card = cards[card_description]
    except KeyError:
        pass

    try:
        icc_trxn = trxn.attrib['icc']
    except:
        pass

    try:
        timeout = trxn.attrib['timeout']
    except KeyError:
        pass

    try:
        t = Transaction(trxn.attrib['type'], card, term, icc_trxn, timeout)
    except KeyError:
        print('Error parsing {}: transaction type is not set'.format(filename))
        sys.exit()

    try:
        t.set_description(trxn.attrib['description'])
    except KeyError:
        pass

    try:
        num_of_trxns_to_repeat = int(trxn.attrib['repeat'])
    except KeyError:
        num_of_trxns_to_repeat = 1

    transactions = []
    for i in range(0, num_of_trxns_to_repeat):
        for attrib in trxn:
            if attrib.tag.lower() == 'amount':
                if attrib.text == 'random':
                    try:
                        min = int(attrib.attrib['min'])
                    except KeyError:
                        min = 0
                    try:
                        max = int(attrib.attrib['max'])
                    except KeyError:
                        max = 100
                    try:
                        exponent = int(attrib.attrib['exponent'])
                    except KeyError:
                        exponent = 1

                    random_amount = random.randint(min, max) * exponent
                    t.set_amount(random_amount)
                else:
                    t.set_amount(attrib.text)
            elif attrib.tag.lower() == 'pin':
                t.set_PIN(attrib.text)
            elif attrib.tag.lower() == 'stan':
                t.set_STAN(attrib.text)
            elif attrib.tag.lower() == 'currency':
                t.set_currency(attrib.text)
            elif attrib.tag.lower() == 'field48':
                t.set_field48_tags(attrib.attrib['tag'], attrib.text)
            elif attrib.tag.lower() == 'field54':
                t.set_field54(attrib.text)
            elif attrib.tag.lower() == 'account_from':
                t.set_account_from(attrib.text)
            elif attrib.tag.lower() == 'account_to':
                t.set_account_to(attrib.text)
            elif attrib.tag.lower() == 'response_code':
                t.set_expected_code(attrib.text)
            elif attrib.tag.lower() == 'response_action':
                if not t.set_expected_action(attrib.text):
                    print('Unknown response action: {}'.format(attrib.text))

        transactions.append(t)
    return transactions",https://github.com/timgabets/bpc8583/blob/1b8e95d73ad273ad9d11bff40d1af3f06f0f3503/examples/isoClient.py#L141-L232
tfidf_python_100_1.0,set file attrib hidden,python,"def readInstances(self):
        for instanceCount, instanceElement in enumerate(self.root.findall("".instance"")):
            instanceObject = self.instanceDescriptorClass()
            if instanceElement.attrib.get(""familyname""):
                instanceObject.familyName = instanceElement.attrib.get(""familyname"")
            if instanceElement.attrib.get(""stylename""):
                instanceObject.styleName = instanceElement.attrib.get(""stylename"")
            if instanceElement.attrib.get(""styleMapFamilyName""):
                instanceObject.styleMapFamilyName = instanceElement.attrib.get(""styleMapFamilyName"")
            if instanceElement.attrib.get(""styleMapStyleName""):
                instanceObject.styleMapStyleName = instanceElement.attrib.get(""styleMapStyleName"")
            if instanceElement.attrib.get(""styleMapFamilyName""):
                instanceObject.styleMapFamilyName = instanceElement.attrib.get(""styleMapFamilyName"")
            instanceObject.location = self.locationFromElement(instanceElement)
            instanceObject.filename = instanceElement.attrib.get('filename')
            for libElement in instanceElement.findall('.provideLib'):
                if libElement.attrib.get('state') == '1':
                    instanceObject.lib = True
            for libElement in instanceElement.findall('.provideInfo'):
                if libElement.attrib.get('state') == '1':
                    instanceObject.info = True
            self.documentObject.instances.append(instanceObject)",https://github.com/LettError/ufoProcessor/blob/7c63e1c8aba2f2ef9b12edb6560aa6c58024a89a/Lib/ufoProcessor/sp3.py#L384-L405
tfidf_python_100_1.0,set file attrib hidden,python,"def detach(hidden):
    if isinstance(hidden, (tuple, list)):
        hidden = [i.detach() for i in hidden]
    else:
        hidden = hidden.detach()
    return hidden",https://github.com/apache/incubator-mxnet/blob/1af29e9c060a4c7d60eeaacba32afdb9a7775ba7/example/gluon/word_language_model/train.py#L152-L157
tfidf_python_100_1.0,set file attrib hidden,python,"def detach(hidden):
    if isinstance(hidden, (tuple, list)):
        hidden = [detach(h) for h in hidden]
    else:
        hidden = hidden.detach()
    return hidden",https://github.com/dmlc/gluon-nlp/blob/4b83eb6bcc8881e5f1081a3675adaa19fac5c0ba/scripts/language_model/large_word_language_model.py#L292-L297
tfidf_python_100_1.0,set file attrib hidden,python,"def start(self, tag, attrib):
        """"""On start of element tag""""""
        if tag == E_CLINICAL_DATA:
            self.ref_state = AUDIT_REF_STATE
            self.context = Context(attrib[A_STUDY_OID],
                                   attrib[A_AUDIT_SUBCATEGORY_NAME],
                                   int(attrib[A_METADATA_VERSION_OID]))

        elif tag == E_SUBJECT_DATA:
            self.context.subject = Subject(
                attrib.get(A_SUBJECT_KEY),
                attrib.get(A_SUBJECT_NAME),
                attrib.get(A_SUBJECT_STATUS),
                attrib.get(A_TRANSACTION_TYPE, DEFAULT_TRANSACTION_TYPE),
            )
        elif tag == E_USER_REF:
            # Set the Signature or audit-record value depending on state
            self.get_parent_element().user_oid = attrib.get(A_USER_OID)

        elif tag == E_SOURCE_ID:
            self.state = STATE_SOURCE_ID

        elif tag == E_DATE_TIME_STAMP_:
            self.state = STATE_DATETIME

        elif tag == E_REASON_FOR_CHANGE:
            self.state = STATE_REASON_FOR_CHANGE

        elif tag == E_LOCATION_REF:
            # Set the Signature or audit-record value depending on state
            self.get_parent_element().location_oid = attrib.get(A_LOCATION_OID)

        elif tag == E_STUDY_EVENT_DATA:
            self.context.event = Event(
                attrib.get(A_STUDYEVENT_OID),
                attrib.get(A_STUDYEVENT_REPEAT_KEY),
                attrib.get(A_TRANSACTION_TYPE),
                attrib.get(A_INSTANCE_NAME),
                attrib.get(A_INSTANCE_OVERDUE),
                make_int(attrib.get(A_INSTANCE_ID), -1)
            )

        elif tag == E_FORM_DATA:
            self.context.form = Form(
                attrib.get(A_FORM_OID),
                int(attrib.get(A_FORM_REPEAT_KEY, 0)),
                attrib.get(A_TRANSACTION_TYPE),
                attrib.get(A_DATAPAGE_NAME),
                make_int(attrib.get(A_DATAPAGE_ID, -1)),
            )

        elif tag == E_ITEM_GROUP_DATA:
            self.context.itemgroup = ItemGroup(
                attrib.get(A_ITEMGROUP_OID),
                int(attrib.get(A_ITEMGROUP_REPEAT_KEY, 0)),
                attrib.get(A_TRANSACTION_TYPE),
                make_int(attrib.get(A_RECORD_ID, -1)),
            )

        elif tag == E_ITEM_DATA:
            self.context.item = Item(
                attrib.get(A_ITEM_OID),
                attrib.get(A_VALUE),
                yes_no_none(attrib.get(A_FREEZE)),
                yes_no_none(attrib.get(A_VERIFY)),
                yes_no_none(attrib.get(A_LOCK)),
                attrib.get(A_TRANSACTION_TYPE)
            )

        elif tag == E_QUERY:
            self.context.query = Query(
                make_int(attrib.get(A_QUERY_REPEAT_KEY, -1)),
                attrib.get(A_STATUS),
                attrib.get(A_RESPONSE),
                attrib.get(A_RECIPIENT),
                attrib.get(A_VALUE)  # Optional, depends on status
            )

        elif tag == E_PROTOCOL_DEVIATION:
            self.context.protocol_deviation = ProtocolDeviation(
                make_int(attrib.get(A_PROTCOL_DEVIATION_REPEAT_KEY, -1)),
                attrib.get(A_CODE),
                attrib.get(A_CLASS),
                attrib.get(A_STATUS),
                attrib.get(A_VALUE),
                attrib.get(A_TRANSACTION_TYPE)
            )

        elif tag == E_REVIEW:
            self.context.review = Review(
                attrib.get(A_GROUP_NAME),
                yes_no_none(attrib.get(A_REVIEWED)),
            )
        elif tag == E_COMMENT:
            self.context.comment = Comment(
                attrib.get(A_COMMENT_REPEAT_KEY),
                attrib.get(A_VALUE),
                attrib.get(A_TRANSACTION_TYPE)

            )
        elif tag == E_SIGNATURE:
            self.ref_state = SIGNATURE_REF_STATE

        elif tag == E_SIGNATURE_REF:
            self.context.signature.oid = attrib.get(A_SIGNATURE_OID)",https://github.com/mdsol/rwslib/blob/1a86bc072d408c009ed1de8bf6e98a1769f54d18/rwslib/extras/audit_event/parser.py#L151-L255
tfidf_python_100_1.0,set file attrib hidden,python,"def node(self, parent=None, tag='g', attrib=None, **extras):
        """"""Make a new svg node""""""
        if parent is None:
            parent = self.root
        attrib = attrib or {}
        attrib.update(extras)

        def in_attrib_and_number(key):
            return key in attrib and isinstance(attrib[key], Number)

        for pos, dim in (('x', 'width'), ('y', 'height')):
            if in_attrib_and_number(dim) and attrib[dim] < 0:
                attrib[dim] = -attrib[dim]
                if in_attrib_and_number(pos):
                    attrib[pos] = attrib[pos] - attrib[dim]

        for key, value in dict(attrib).items():
            if value is None:
                del attrib[key]

            attrib[key] = to_str(value)
            if key.endswith('_'):
                attrib[key.rstrip('_')] = attrib[key]
                del attrib[key]
            elif key == 'href':
                attrib[etree.QName('http://www.w3.org/1999/xlink',
                                   key)] = attrib[key]
                del attrib[key]
        return etree.SubElement(parent, tag, attrib)",https://github.com/Kozea/pygal/blob/5e25c98a59a0642eecd9fcc5dbfeeb2190fbb5e7/pygal/svg.py#L178-L206
tfidf_python_100_1.0,set file attrib hidden,python,"def __init__(self, tag, attrib={}, **extra):
        attrib = attrib.copy()
        attrib.update(extra)
        self.tag = tag
        self.attrib = attrib
        self._children = []",https://github.com/RedHatInsights/insights-core/blob/b57cbf8ed7c089672426ede0441e0a4f789ef4a1/insights/contrib/ElementTree.py#L207-L212
tfidf_python_100_1.0,set file attrib hidden,python,"def init_hidden(self, hidden):
        """"""
        Converts flattened hidden state (from sequence generator) into a tuple
        of hidden states.

        :param hidden: None or flattened hidden state for decoder RNN layers
        """"""
        if hidden is not None:
            # per-layer chunks
            hidden = hidden.chunk(self.num_layers)
            # (h, c) chunks for LSTM layer
            hidden = tuple(i.chunk(2) for i in hidden)
        else:
            hidden = [None] * self.num_layers

        self.next_hidden = []
        return hidden",https://github.com/mlperf/training/blob/1c6ae725a81d15437a2b2df05cac0673fde5c3a4/rnn_translator/pytorch/seq2seq/models/decoder.py#L149-L165
tfidf_python_100_1.0,set file attrib hidden,python,"def _field_type(field_element):
    if 'type' in field_element.attrib:
        return field_element.attrib['type']
    if 'domain' in field_element.attrib:
        return domains[field_element.attrib['domain']]",https://github.com/barryp/py-amqplib/blob/2b3a47de34b4712c111d0a55d7ff109dffc2a7b2/extras/generate_skeleton_0_8.py#L76-L80
tfidf_python_100_1.0,sorting multiple arrays based on another arrays sorted order,python,"def Free(self):
    '''
    Frees the memory used by all of the dynamically allocated C arrays.
    
    '''

    if self.arrays._calloc:
      _dbl_free(self.arrays._time)
      _dbl_free(self.arrays._flux)
      _dbl_free(self.arrays._bflx)
      _dbl_free(self.arrays._M)
      _dbl_free(self.arrays._E)
      _dbl_free(self.arrays._f)
      _dbl_free(self.arrays._r)
      _dbl_free(self.arrays._x)
      _dbl_free(self.arrays._y)
      _dbl_free(self.arrays._z)
      self.arrays._calloc = 0
    if self.arrays._balloc:  
      _dbl_free(self.arrays._b)
      self.arrays._balloc = 0
    if self.arrays._ialloc:
      _dbl_free(self.arrays._iarr)
      self.arrays._ialloc = 0",https://github.com/rodluger/pysyzygy/blob/d2b64251047cc0f0d0adeb6feab4054e7fce4b7a/pysyzygy/transit.py#L546-L569
tfidf_python_100_1.0,sorting multiple arrays based on another arrays sorted order,python,"def register_arrays(self, arrays):
        """"""
        Register arrays using a list of dictionaries defining the arrays.

        The list should itself contain dictionaries. i.e.

        .. code-block:: python

            D = [{ 'name':'uvw', 'shape':(3,'ntime','nbl'),'dtype':np.float32 },
                { 'name':'lm', 'shape':(2,'nsrc'),'dtype':np.float32 }]

        Parameters
        ----------
        arrays : A list or dict.
            A list or dictionary of dictionaries describing arrays.
        """"""

        if isinstance(arrays, collections.Mapping):
            arrays = arrays.itervalues()

        for ary in arrays:
            self.register_array(**ary)",https://github.com/ska-sa/hypercube/blob/6564a9e65ccd9ed7e7a71bd643f183e1ec645b29/hypercube/base_cube.py#L416-L437
tfidf_python_100_1.0,sorting multiple arrays based on another arrays sorted order,python,"def _init_fast_path(self, arrays, idxs_conf):
        self._init_common(arrays, idxs_conf)
        self._zipped = list(zip(self.arrays, self.idxs_conf, self.wildcard_conf))",https://github.com/alphatwirl/alphatwirl/blob/5138eeba6cd8a334ba52d6c2c022b33c61e3ba38/alphatwirl/summary/BackrefMultipleArrayReader.py#L57-L59
tfidf_python_100_1.0,sorting multiple arrays based on another arrays sorted order,python,"def _fitch_intersect(self, arrays):
        """"""
        Find the intersection of any number of 1D arrays.
        Return the sorted, unique values that are in all of the input arrays.
        Adapted from numpy.lib.arraysetops.intersect1d
        """"""
        def pairwise_intersect(arr1, arr2):
            s2 = set(arr2)
            b3 = [val for val in arr1 if val in s2]
            return b3

        arrays = list(arrays) # allow assignment
        N = len(arrays)
        while N > 1:
            arr1 = arrays.pop()
            arr2 = arrays.pop()
            arr = pairwise_intersect(arr1, arr2)
            arrays.append(arr)
            N = len(arrays)

        return arrays[0]",https://github.com/neherlab/treetime/blob/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0/treetime/treeanc.py#L1224-L1244
tfidf_python_100_1.0,sorting multiple arrays based on another arrays sorted order,python,"def build_frame(self, arrays):
    self._maybe_clear_deque()

    arrays = arrays if isinstance(arrays, list) else [arrays]

    sections = self._arrays_to_sections(arrays)
    self._save_section_info(arrays, sections)
    final_image = self._sections_to_image(sections)
    final_image = im_util.apply_colormap(final_image, self.config['colormap'])

    return final_image",https://github.com/tensorflow/tensorboard/blob/8e5f497b48e40f2a774f85416b8a35ac0693c35e/tensorboard/plugins/beholder/visualizer.py#L295-L305
tfidf_python_100_1.0,sorting multiple arrays based on another arrays sorted order,python,"def permute_data(arrays, random_state=None):
  """"""Permute multiple numpy arrays with the same order.""""""
  if any(len(a) != len(arrays[0]) for a in arrays):
    raise ValueError('All arrays must be the same length.')
  if not random_state:
    random_state = np.random
  order = random_state.permutation(len(arrays[0]))
  return [a[order] for a in arrays]",https://github.com/google/prettytensor/blob/75daa0b11252590f548da5647addc0ea610c4c45/prettytensor/tutorial/data_utils.py#L82-L89
tfidf_python_100_1.0,sorting multiple arrays based on another arrays sorted order,python,"def weld_arrays_to_vec_of_struct(arrays, weld_types):
    """"""Create a vector of structs from multiple vectors.

    Parameters
    ----------
    arrays : list of (numpy.ndarray or WeldObject)
        Arrays to put in a struct.
    weld_types : list of WeldType
        The Weld types of the arrays in the same order.

    Returns
    -------
    WeldObject
        Representation of this computation.

    """"""
    weld_obj = create_empty_weld_object()
    obj_ids = [get_weld_obj_id(weld_obj, array) for array in arrays]

    arrays = 'zip({})'.format(', '.join(obj_ids)) if len(obj_ids) > 1 else '{}'.format(obj_ids[0])
    input_types = struct_of('{e}', weld_types) if len(obj_ids) > 1 else '{}'.format(weld_types[0])
    res_types = struct_of('{e}', weld_types)
    to_merge = 'e' if len(obj_ids) > 1 else '{e}'

    weld_template = """"""result(
    for({arrays},
        appender[{res_types}],
        |b: appender[{res_types}], i: i64, e: {input_types}|
            merge(b, {to_merge})
    )    
)""""""

    weld_obj.weld_code = weld_template.format(arrays=arrays,
                                              input_types=input_types,
                                              res_types=res_types,
                                              to_merge=to_merge)

    return weld_obj",https://github.com/radujica/baloo/blob/f6e05e35b73a75e8a300754c6bdc575e5f2d53b9/baloo/weld/weld_utils.py#L313-L350
tfidf_python_100_1.0,sorting multiple arrays based on another arrays sorted order,python,"def dstack(tup):
    """"""Stack arrays in sequence depth wise (along third dimension), 
    handling ``RemoteArray`` and ``DistArray`` without moving data.

    Args:
      tup (sequence of array_like)

    Returns: 
      res: `ndarray`, if inputs were all local
           `RemoteArray`, if inputs were all on the same remote engine
           `DistArray`, if inputs were already scattered on different engines
    """"""
    # Follow numpy.dstack behavior for 1D and 2D arrays:
    arrays = list(tup)
    for i in range(len(arrays)):
        if arrays[i].ndim is 1:
            arrays[i] = arrays[i][np.newaxis, :]
        if arrays[i].ndim is 2:
            arrays[i] = arrays[i][:, :, np.newaxis]
    return concatenate(arrays, axis=2)",https://github.com/mattja/distob/blob/b0fc49e157189932c70231077ed35e1aa5717da9/distob/arrays.py#L1652-L1671
tfidf_python_100_1.0,sorting multiple arrays based on another arrays sorted order,python,"def __init__(self, arrays, idxs_conf, backref_idxs=None):

        # e.g.,
        # arrays = [[ ], [ ], [ ], [ ], [ ], [ ], [ ], [ ]]
        # idxs_conf = (0, '*', None, '*', None, None, None, None)
        # backref_idxs = [None, None, 1, None, 3, 1, 1, 3]

        #
        self._check_args(arrays, idxs_conf, backref_idxs)

        #
        self.take_fast_path = not self._is_backref_used(backref_idxs)

        if self.take_fast_path:
            self._init_fast_path(arrays, idxs_conf)
            return

        self._init_full_path(arrays, idxs_conf, backref_idxs)",https://github.com/alphatwirl/alphatwirl/blob/5138eeba6cd8a334ba52d6c2c022b33c61e3ba38/alphatwirl/summary/BackrefMultipleArrayReader.py#L7-L24
tfidf_python_100_1.0,sorting multiple arrays based on another arrays sorted order,python,"def check_same_ndim(*arrays):
    a = arrays[0]
    for b in arrays[1:]:
        if len(b.shape) != len(a.shape):
            raise ValueError(
                'arrays do not have same number of dimensions'
            )",https://github.com/cggh/scikit-allel/blob/3c979a57a100240ba959dd13f98839349530f215/allel/util.py#L106-L112
tfidf_python_100_1.0,sorting multiple arrays based on another arrays sorted order,python,"def concatenate(isclassmethod, cls_or_self, arrays, axis=0):
        if isclassmethod: 
            cls = cls_or_self
        else:
            self = cls_or_self
            cls = self.__class__
            arrays = (self,) + tuple(arrays)

        if not all(type(x) == type(arrays[0]) for x in arrays):
            raise TypeError(""cannot concatenate arrays of different type with AwkwardArray.concatenate"")

        for x in arrays:
            x.valid()

        if axis == 0:
            return cls._concatenate_axis0(arrays)
        elif axis == 1:
            return cls._concatenate_axis1(arrays)
        else:
            raise NotImplementedError(""axis > 1"")",https://github.com/scikit-hep/awkward-array/blob/1f878c4e11a4548fd977e230ce93eb5534db73a2/awkward/array/base.py#L375-L394
tfidf_python_100_1.0,sorting multiple arrays based on another arrays sorted order,python,"def reify_arrays(arrays, dims, copy=True):
    """"""
    Reify arrays, given the supplied dimensions. If copy is True,
    returns a copy of arrays else performs this inplace.
    """"""
    arrays = ({ k : AttrDict(**a) for k, a in arrays.iteritems() }
        if copy else arrays)

    for n, a in arrays.iteritems():
        a.shape = tuple(dims[v].extent_size if isinstance(v, str) else v
            for v in a.shape)

    return arrays",https://github.com/ska-sa/hypercube/blob/6564a9e65ccd9ed7e7a71bd643f183e1ec645b29/hypercube/util/__init__.py#L51-L63
tfidf_python_100_1.0,sorting multiple arrays based on another arrays sorted order,python,"def global_extrema(arrays):
  return min([x.min() for x in arrays]), max([x.max() for x in arrays])",https://github.com/tensorflow/tensorboard/blob/8e5f497b48e40f2a774f85416b8a35ac0693c35e/tensorboard/plugins/beholder/im_util.py#L31-L32
tfidf_python_100_1.0,sorting multiple arrays based on another arrays sorted order,python,"def from_arrays(cls, arrays, sortorder=None, names=None):
        """"""
        Convert arrays to MultiIndex.

        Parameters
        ----------
        arrays : list / sequence of array-likes
            Each array-like gives one level's value for each data point.
            len(arrays) is the number of levels.
        sortorder : int or None
            Level of sortedness (must be lexicographically sorted by that
            level).
        names : list / sequence of str, optional
            Names for the levels in the index.

        Returns
        -------
        index : MultiIndex

        See Also
        --------
        MultiIndex.from_tuples : Convert list of tuples to MultiIndex.
        MultiIndex.from_product : Make a MultiIndex from cartesian product
                                  of iterables.
        MultiIndex.from_frame : Make a MultiIndex from a DataFrame.

        Examples
        --------
        >>> arrays = [[1, 1, 2, 2], ['red', 'blue', 'red', 'blue']]
        >>> pd.MultiIndex.from_arrays(arrays, names=('number', 'color'))
        MultiIndex(levels=[[1, 2], ['blue', 'red']],
                   codes=[[0, 0, 1, 1], [1, 0, 1, 0]],
                   names=['number', 'color'])
        """"""
        error_msg = ""Input must be a list / sequence of array-likes.""
        if not is_list_like(arrays):
            raise TypeError(error_msg)
        elif is_iterator(arrays):
            arrays = list(arrays)

        # Check if elements of array are list-like
        for array in arrays:
            if not is_list_like(array):
                raise TypeError(error_msg)

        # Check if lengths of all arrays are equal or not,
        # raise ValueError, if not
        for i in range(1, len(arrays)):
            if len(arrays[i]) != len(arrays[i - 1]):
                raise ValueError('all arrays must be same length')

        from pandas.core.arrays.categorical import _factorize_from_iterables

        codes, levels = _factorize_from_iterables(arrays)
        if names is None:
            names = [getattr(arr, ""name"", None) for arr in arrays]

        return MultiIndex(levels=levels, codes=codes, sortorder=sortorder,
                          names=names, verify_integrity=False)",https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/indexes/multi.py#L292-L350
tfidf_python_100_1.0,sorting multiple arrays based on another arrays sorted order,python,"def _util_concatenate(cls, arrays):
        if all(isinstance(x, cls.numpy.ndarray) for x in arrays):
            return cls.numpy.concatenate(arrays)
        else:
            return arrays[0].concatenate(arrays[1:])",https://github.com/scikit-hep/awkward-array/blob/1f878c4e11a4548fd977e230ce93eb5534db73a2/awkward/array/base.py#L368-L372
tfidf_python_100_1.0,sorting multiple arrays based on another arrays sorted order,python,"def getContGroupArrays(arrays, groupPositions, arrayKeys=None):
    """"""Convinience function to generate a subset of arrays from specified array
    positions.

    :param arrays: a dictionary containing ``numpy.arrays``
    :param groupPositions: arrays positions that should be included in the
        subset of arrays
    :param arrayKeys: a list of ""arrays"" keys that should be included in the
        subset of arrays, if None all keys are selected

    :returns: a dictionary containing ``numpy.arrays``
    """"""
    if arrayKeys is None:
        arrayKeys = list(viewkeys(arrays))
    matchingArrays = dict()
    for key in arrayKeys:
        matchingArrays[key] = arrays[key][groupPositions]
    return matchingArrays",https://github.com/hollenstein/maspy/blob/f15fcfd24df306d8420540460d902aa3073ec133/maspy/featuregrouping.py#L323-L340
tfidf_python_100_1.0,sorting multiple arrays based on another arrays sorted order,python,"def _checkDimensionsListLike(arrays):
    """"""Check that each array in a list of arrays has the same size.

    """"""
    dim1 = len(arrays)
    dim2, dim3 = arrays[0].shape
    for aa in range(1, dim1):
        dim2_aa, dim3_aa = arrays[aa].shape
        if (dim2_aa != dim2) or (dim3_aa != dim3):
            raise _error.InvalidError(_MDPERR[""obj_square""])
    return dim1, dim2, dim3",https://github.com/sawcordwell/pymdptoolbox/blob/7c96789cc80e280437005c12065cf70266c11636/src/mdptoolbox/util.py#L94-L104
tfidf_python_100_1.0,sorting multiple arrays based on another arrays sorted order,python,"def cartesian(arrays, out=None):
    """"""
    Generate a cartesian product of input arrays.

    Parameters
    ----------
    arrays : list of array-like
        1-D arrays to form the cartesian product of.
    out : ndarray
        Array to place the cartesian product in.

    Returns
    -------
    out : ndarray
        2-D array of shape (M, len(arrays)) containing cartesian products
        formed of input arrays.

    Examples
    --------
    >>> cartesian(([1, 2, 3], [4, 5], [6, 7]))
    array([[1, 4, 6],
           [1, 4, 7],
           [1, 5, 6],
           [1, 5, 7],
           [2, 4, 6],
           [2, 4, 7],
           [2, 5, 6],
           [2, 5, 7],
           [3, 4, 6],
           [3, 4, 7],
           [3, 5, 6],
           [3, 5, 7]])

    """"""

    arrays = [np.asarray(x) for x in arrays]
    dtype = arrays[0].dtype

    n = np.prod([x.size for x in arrays])
    if out is None:
        out = np.zeros([n, len(arrays)], dtype=dtype)

    m = n // arrays[0].size
    out[:,0] = np.repeat(arrays[0], m)
    if arrays[1:]:
        cartesian(arrays[1:], out=out[0:m,1:])
        for j in range(1, arrays[0].size):
            out[j*m:(j+1)*m,1:] = out[0:m,1:]
    return out",https://github.com/EconForge/dolo/blob/d91ddf148b009bf79852d9aec70f3a1877e0f79a/dolo/numeric/misc.py#L3-L51
tfidf_python_100_1.0,sorting multiple arrays based on another arrays sorted order,python,"def concatenate_1d(arrays):
    """"""
    Concatenate 1D numpy arrays.
    Similar to np.concatenate but work with empty input and masked arrays.
    """"""
    if len(arrays) == 0:
        return np.array([])
    if len(arrays) == 1:
        return np.asanyarray(arrays[0])
    if any(map(np.ma.is_masked, arrays)):
        return np.ma.concatenate(arrays)
    return np.concatenate(arrays)",https://github.com/yymao/generic-catalog-reader/blob/bc6267ac41b9f68106ed6065184469ac13fdc0b6/GCR/utils.py#L29-L40
tfidf_python_100_1.0,sorting multiple arrays based on another arrays sorted order,python,"def average_arrays(arrays: List[mx.nd.NDArray]) -> mx.nd.NDArray:
    """"""
    Take a list of arrays of the same shape and take the element wise average.

    :param arrays: A list of NDArrays with the same shape that will be averaged.
    :return: The average of the NDArrays in the same context as arrays[0].
    """"""
    if not arrays:
        raise ValueError(""arrays is empty."")
    if len(arrays) == 1:
        return arrays[0]
    check_condition(all(arrays[0].shape == a.shape for a in arrays), ""nd array shapes do not match"")
    return mx.nd.add_n(*arrays) / len(arrays)",https://github.com/awslabs/sockeye/blob/5d64a1ee1ef3cbba17c6d1d94bc061020c43f6ab/sockeye/utils.py#L448-L460
tfidf_python_100_1.0,string similarity levenshtein,python,"def evaluate_extracted_tokens(gold_content, extr_content):
    """"""
    Evaluate the similarity between gold-standard and extracted content,
    typically for a single HTML document, as another way of evaluating the
    performance of an extractor model.

    Args:
        gold_content (str or Sequence[str]): Gold-standard content, either as a
            string or as an already-tokenized list of tokens.
        extr_content (str or Sequence[str]): Extracted content, either as a
            string or as an already-tokenized list of tokens.

    Returns:
        Dict[str, float]
    """"""
    if isinstance(gold_content, string_):
        gold_content = simple_tokenizer(gold_content)
    if isinstance(extr_content, string_):
        extr_content = simple_tokenizer(extr_content)
    gold_set = set(gold_content)
    extr_set = set(extr_content)
    jaccard = len(gold_set & extr_set) / len(gold_set | extr_set)
    levenshtein = dameraulevenshtein(gold_content, extr_content)
    return {'jaccard': jaccard, 'levenshtein': levenshtein}",https://github.com/dragnet-org/dragnet/blob/532c9d9f28e5b1b57f3cabc708218d3863a16322/dragnet/model_training.py#L51-L74
tfidf_python_100_1.0,string similarity levenshtein,python,"def similarity(document_1, document_2, k=0.5):
    return k * structural_similarity(document_1, document_2) + (1 - k) * style_similarity(document_1, document_2)",https://github.com/matiskay/html-similarity/blob/eef5586b1cf30134254690b2150260ef82cbd18f/html_similarity/similarity.py#L5-L6
tfidf_python_100_1.0,string similarity levenshtein,python,"def baseline_similarity(a, b, filter=True):
    if filter is True:
        similarity = -mean_squared_error(gauss_filt(a, 201), gauss_filt(b, 201)) ** 0.5
    else:
        similarity = -mean_squared_error(a, b) ** 0.5
    return similarity",https://github.com/SoftwareDefinedBuildings/XBOS/blob/c12d4fb14518ea3ae98c471c28e0710fdf74dd25/apps/consumption/iec.py#L36-L41
tfidf_python_100_1.0,string similarity levenshtein,python,"def sim_levenshtein(src, tar, mode='lev', cost=(1, 1, 1, 1)):
    """"""Return the Levenshtein similarity of two strings.

    This is a wrapper of :py:meth:`Levenshtein.sim`.

    Parameters
    ----------
    src : str
        Source string for comparison
    tar : str
        Target string for comparison
    mode : str
        Specifies a mode for computing the Levenshtein distance:

            - ``lev`` (default) computes the ordinary Levenshtein distance, in
              which edits may include inserts, deletes, and substitutions
            - ``osa`` computes the Optimal String Alignment distance, in which
              edits may include inserts, deletes, substitutions, and
              transpositions but substrings may only be edited once

    cost : tuple
        A 4-tuple representing the cost of the four possible edits: inserts,
        deletes, substitutions, and transpositions, respectively (by default:
        (1, 1, 1, 1))

    Returns
    -------
    float
        The Levenshtein similarity between src & tar

    Examples
    --------
    >>> round(sim_levenshtein('cat', 'hat'), 12)
    0.666666666667
    >>> round(sim_levenshtein('Niall', 'Neil'), 12)
    0.4
    >>> sim_levenshtein('aluminum', 'Catalan')
    0.125
    >>> sim_levenshtein('ATCG', 'TAGC')
    0.25

    """"""
    return Levenshtein().sim(src, tar, mode, cost)",https://github.com/chrislit/abydos/blob/165466b3ff6afd8024a4c8660421b0c4e7773db9/abydos/distance/_levenshtein.py#L301-L343
tfidf_python_100_1.0,string similarity levenshtein,python,"def levenshtein(src, tar, mode='lev', cost=(1, 1, 1, 1)):
    """"""Return the Levenshtein distance between two strings.

    This is a wrapper of :py:meth:`Levenshtein.dist_abs`.

    Parameters
    ----------
    src : str
        Source string for comparison
    tar : str
        Target string for comparison
    mode : str
        Specifies a mode for computing the Levenshtein distance:

            - ``lev`` (default) computes the ordinary Levenshtein distance, in
              which edits may include inserts, deletes, and substitutions
            - ``osa`` computes the Optimal String Alignment distance, in which
              edits may include inserts, deletes, substitutions, and
              transpositions but substrings may only be edited once

    cost : tuple
        A 4-tuple representing the cost of the four possible edits: inserts,
        deletes, substitutions, and transpositions, respectively (by default:
        (1, 1, 1, 1))

    Returns
    -------
    int (may return a float if cost has float values)
        The Levenshtein distance between src & tar

    Examples
    --------
    >>> levenshtein('cat', 'hat')
    1
    >>> levenshtein('Niall', 'Neil')
    3
    >>> levenshtein('aluminum', 'Catalan')
    7
    >>> levenshtein('ATCG', 'TAGC')
    3

    >>> levenshtein('ATCG', 'TAGC', mode='osa')
    2
    >>> levenshtein('ACTG', 'TAGC', mode='osa')
    4

    """"""
    return Levenshtein().dist_abs(src, tar, mode, cost)",https://github.com/chrislit/abydos/blob/165466b3ff6afd8024a4c8660421b0c4e7773db9/abydos/distance/_levenshtein.py#L206-L253
tfidf_python_100_1.0,string similarity levenshtein,python,"def similarity(a, b, cls=JsonDiffer, **kwargs):
    return cls(**kwargs).similarity(a, b)",https://github.com/xlwings/jsondiff/blob/4aa9a0d9d264a7a701e35d6406836e0fa56cfd0a/jsondiff/__init__.py#L604-L605
tfidf_python_100_1.0,string similarity levenshtein,python,"def similarity(self, other):
        """"""Get similarity as a discrete ratio (1.0 or 0.0).""""""
        ratio = 1.0 if (str(self).lower() == str(other).lower()) else 0.0
        similarity = self.Similarity(ratio)
        return similarity",https://github.com/jacebrowning/comparable/blob/48455e613650e22412d31109681368fcc479298d/comparable/simple.py#L78-L82
tfidf_python_100_1.0,string similarity levenshtein,python,"def _calculate_similarity_matrix(self, tf_scores):
        length = len(tf_scores)

        similarity_matrix = np.zeros([length] * 2)

        for i in range(length):
            for j in range(i, length):
                similarity = self._idf_modified_cosine(tf_scores, i, j)

                if similarity:
                    similarity_matrix[i, j] = similarity
                    similarity_matrix[j, i] = similarity

        return similarity_matrix",https://github.com/crabcamp/lexrank/blob/858dcac163ecd35d1577d5be65ead73349faa499/lexrank/algorithms/summarizer.py#L142-L155
tfidf_python_100_1.0,string similarity levenshtein,python,"def karl_pearson2(datax, datay):
    meanx = mean(datax)
    meany = mean(datay)

    product = 0
    sqmagx = 0
    sqmagy = 0
    mincount = 5

    for elemx, elemy in zip(datax, datay):
        product += (elemx - meanx)*(elemy - meany)
        sqmagx += (elemx - meanx)*(elemx - meanx)
        sqmagy += (elemy - meany)*(elemy - meany)

    similarity = product/sqrt(sqmagx*sqmagy)
    similarity = similarity*100
    if mincount >= len(datax):
        return -999
    return similarity",https://github.com/Utagai/spice/blob/00b2c9e80ef338f4daef7643d99e8c7a0750b57c/spice_api/stats.py#L127-L145
tfidf_python_100_1.0,string similarity levenshtein,python,"def similar(obj1, obj2):
    """"""Calculate similarity between two (Comparable) objects.""""""
    Comparable.log(obj1, obj2, '%')
    similarity = obj1.similarity(obj2)
    Comparable.log(obj1, obj2, '%', result=similarity)
    return similarity",https://github.com/jacebrowning/comparable/blob/48455e613650e22412d31109681368fcc479298d/comparable/base.py#L135-L140
tfidf_python_100_1.0,string similarity levenshtein,python,"def similarity(self, other):
        """"""Get similarity as a ratio of the two texts.""""""
        ratio = SequenceMatcher(a=self.value, b=other.value).ratio()
        similarity = self.Similarity(ratio)
        return similarity",https://github.com/jacebrowning/comparable/blob/48455e613650e22412d31109681368fcc479298d/comparable/simple.py#L65-L69
tfidf_python_100_1.0,string similarity levenshtein,python,"def subject_pair_simj(subject1, subject2, **kwargs):
    """"""
    Jaccard similarity
    """"""
    i, u = subject_pair_overlap(subject1, subject2, **kwargs)
    if i==0:
        return 0.0
    return i / u",https://github.com/biolink/ontobio/blob/4e512a7831cfe6bc1b32f2c3be2ba41bc5cf7345/ontobio/golr/golr_sim.py#L38-L45
tfidf_python_100_1.0,string similarity levenshtein,python,"def sentences_similarity(self, sentence_1, sentence_2):
        tf_1 = Counter(self.tokenize_sentence(sentence_1))
        tf_2 = Counter(self.tokenize_sentence(sentence_2))

        similarity = self._idf_modified_cosine([tf_1, tf_2], 0, 1)

        return similarity",https://github.com/crabcamp/lexrank/blob/858dcac163ecd35d1577d5be65ead73349faa499/lexrank/algorithms/summarizer.py#L91-L97
tfidf_python_100_1.0,string similarity levenshtein,python,"def levenshtein(left, right):
    """"""Computes the Levenshtein distance of the two given strings.

    >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])
    >>> df0.select(levenshtein('l', 'r').alias('d')).collect()
    [Row(d=3)]
    """"""
    sc = SparkContext._active_spark_context
    jc = sc._jvm.functions.levenshtein(_to_java_column(left), _to_java_column(right))
    return Column(jc)",https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/functions.py#L1634-L1643
tfidf_python_100_1.0,string similarity levenshtein,python,"def similarity(word1: str, word2: str) -> float:
    """"""
    Get cosine similarity between two words.
    If a word is not in the vocabulary, KeyError will be raised.

    :param string word1: first word
    :param string word2: second word
    :return: the cosine similarity between the two word vectors
    """"""
    return _MODEL.similarity(word1, word2)",https://github.com/PyThaiNLP/pythainlp/blob/e9a300b8a99dfd1a67a955e7c06f62e4afe0fbca/pythainlp/word_vector/__init__.py#L63-L72
tfidf_python_100_1.0,string similarity levenshtein,python,"def bestfn(self, subentry):
        value = super(SimilaritySorter, self).bestfn(subentry)
        sn = subentry['SubFileName']
        similarity = _similarity(sn[:sn.rindex('.')], self.movie)
        logging.info(""{}: Similarity is {}, lang {}"".format(
            subentry['SubFileName'], similarity, subentry['SubLanguageID']))
        return 1.1 * value + 1 - similarity",https://github.com/luisguilherme/framboise/blob/1774211e7a65d5ef80af2341ae7f6c4ef181e58c/framboise/sorting.py#L32-L38
tfidf_python_100_1.0,string similarity levenshtein,python,"def similar(self, similarity):
        """""" Returns a new Pattern with the specified similarity threshold """"""
        pattern = Pattern(self.path)
        pattern.similarity = similarity
        return pattern",https://github.com/glitchassassin/lackey/blob/7adadfacd7f45d81186710be992f5668b15399fe/lackey/RegionMatching.py#L73-L77
tfidf_python_100_1.0,string similarity levenshtein,python,"def levenshtein(x, y):
    """"""Levenshtein edit distance

    :param x:
    :param y: strings
    :returns: distance
    :complexity: `O(|x|*|y|)`
    """"""
    n = len(x)
    m = len(y)
    #                         initializing row 0 and column 0
    A = [[i + j for j in range(m + 1)] for i in range(n + 1)]
    for i in range(n):
        for j in range(m):
            A[i + 1][j + 1] = min(A[i][j + 1] + 1,              # insert
                                  A[i + 1][j] + 1,              # delete
                                  A[i][j] + int(x[i] != y[j]))  # subst.
    return A[n][m]",https://github.com/jilljenn/tryalgo/blob/89a4dd9655e7b6b0a176f72b4c60d0196420dfe1/tryalgo/levenshtein.py#L8-L25
tfidf_python_100_1.0,string similarity levenshtein,python,"def edit_distance(string1, string2):
    """"""
    Edit distance algorithm. String1 and string2 can be either
    strings or lists of strings

    pip install python-Levenshtein

    Args:
        string1 (str or list):
        string2 (str or list):

    CommandLine:
        python -m utool.util_alg edit_distance --show

    Example:
        >>> # DISABLE_DOCTEST
        >>> from utool.util_alg import *  # NOQA
        >>> import utool as ut
        >>> string1 = 'hello world'
        >>> string2 = ['goodbye world', 'rofl', 'hello', 'world', 'lowo']
        >>> edit_distance(['hello', 'one'], ['goodbye', 'two'])
        >>> edit_distance('hello', ['goodbye', 'two'])
        >>> edit_distance(['hello', 'one'], 'goodbye')
        >>> edit_distance('hello', 'goodbye')
        >>> distmat = edit_distance(string1, string2)
        >>> result = ('distmat = %s' % (ut.repr2(distmat),))
        >>> print(result)
        >>> [7, 9, 6, 6, 7]
    """"""

    import utool as ut
    try:
        import Levenshtein
    except ImportError as ex:
        ut.printex(ex, 'pip install python-Levenshtein')
        raise
    #np.vectorize(Levenshtein.distance, [np.int])
    #vec_lev = np.frompyfunc(Levenshtein.distance, 2, 1)
    #return vec_lev(string1, string2)
    import utool as ut
    isiter1 = ut.isiterable(string1)
    isiter2 = ut.isiterable(string2)
    strs1 = string1 if isiter1 else [string1]
    strs2 = string2 if isiter2 else [string2]
    distmat = [
        [Levenshtein.distance(str1, str2) for str2 in strs2]
        for str1 in strs1
    ]
    # broadcast
    if not isiter2:
        distmat = ut.take_column(distmat, 0)
    if not isiter1:
        distmat = distmat[0]
    return distmat",https://github.com/Erotemic/utool/blob/3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a/utool/util_alg.py#L2446-L2499
tfidf_python_100_1.0,string similarity levenshtein,python,"def fuzzy_get_tuple(dict_obj, approximate_key, dict_keys=None, key_and_value=False, similarity=0.6, default=None):
    """"""Find the closest matching key and/or value in a dictionary (must have all string keys!)""""""
    return fuzzy_get(dict(('|'.join(str(k2) for k2 in k), v) for (k, v) in viewitems(dict_obj)),
                     '|'.join(str(k) for k in approximate_key), dict_keys=dict_keys,
                     key_and_value=key_and_value, similarity=similarity, default=default)",https://github.com/totalgood/pugnlp/blob/c43445b14afddfdeadc5f3076675c9e8fc1ee67c/src/pugnlp/util.py#L773-L777
tfidf_python_100_1.0,how to get html of website,python,"def Parse(self,song_name,website):
		'''
		song_name is a list of strings
		website is a string
		It will return the url from where music file needs to be downloaded
		'''
		url_to_be_parsed=self.google_url(song_name,website)
		file_download=FileDownload()
		html=file_download.get_html_response(url_to_be_parsed)
		website_url=self.parse_google(html)
		return website_url",https://github.com/ankitmathur3193/song-cli/blob/ca8ccfe547e9d702313ff6d14e81ae4355989a67/song/commands/SearchEngineParser/GoogleParser.py#L33-L43
tfidf_python_100_1.0,how to get html of website,python,"def run(self):
        for website in self.distribution.websites:
            self.copy_filetypes(website, '.html', 'HTML')
            self.copy_filetypes(website, '.xml', 'XML')
            self.copy_filetypes(website, '.xsd', 'XSD')
            self.copy_filetypes(website, '.css', 'CSS2')
            self.copy_filetypes(website, '.py', 'Python')
            self.copy_filetypes(website, '.js', 'JavaScript')
            self.copy_filetypes(website, '.jpg', 'JPEG')
            self.copy_filetypes(website, '.png', 'PNG')
            self.copy_filetypes(website, '.gif', 'GIF')
            self.copy_filetypes(website, '.ico', 'Favicon')
            self.copy_filetypes(website, '.yaml', 'YAML')
            self.generate_html(website)",https://github.com/lanhel/ftpysetup/blob/9cdea6b82658fb4394b582d1fe5b05eaf5746fde/ftpysetup/website/build_website.py#L80-L93
tfidf_python_100_1.0,how to get html of website,python,"def update_website(self, website):
        self.connect()
        website = self.server.update_website(
            self.session_id,
            website['name'],
            website['ip'],
            website['https'],
            website['subdomains'],
            website['certificate'],
            *website['website_apps']
        )
        return website",https://github.com/dariosky/wfcli/blob/87a9ed30dbd456f801135a55099f0541b0614ccb/wfcli/wfapi.py#L123-L134
tfidf_python_100_1.0,how to get html of website,python,"def __init__(self, webSite, *a, **k):
        """"""
        Create a L{MantissaLivePage}.

        @param webSite: a L{WebSite} with a usable secure port implementation.
        """"""
        self.webSite = webSite
        athena.LivePage.__init__(self, transportRoot=url.root.child('live'),
                                 *a, **k)
        self._jsDepsMemo = self.hashCache.depsMemo",https://github.com/twisted/mantissa/blob/53e5502aba23ce99be78b27f923a276593033fe8/xmantissa/website.py#L76-L85
tfidf_python_100_1.0,how to get html of website,python,"def website_exists_as_secure(self, website):
        """""""" Return true if the website has an equivalent that is secure
            we will have 2 websites with the same name, one insecure (that will contain just
            the redirect and the identity-verification) and one secured
        """"""
        if website['https']:
            logger.info(""website %s is already secured, skip"" % website['name'])
            return website
        # changes in these fields are ignored
        for other in self._websites:
            if other['id'] == website['id']:
                continue
            if other['name'] == website['name'] and other['https']:
                return other
        return None",https://github.com/dariosky/wfcli/blob/87a9ed30dbd456f801135a55099f0541b0614ccb/wfcli/tossl.py#L184-L198
tfidf_python_100_1.0,how to get html of website,python,"def domain_exists_as_secured(self, subdomain):
        for website in self.websites:
            if website['https'] and website['certificate'] and subdomain in website['subdomains']:
                return True
        return False",https://github.com/dariosky/wfcli/blob/87a9ed30dbd456f801135a55099f0541b0614ccb/wfcli/tossl.py#L200-L204
tfidf_python_100_1.0,how to get html of website,python,"def website_exists(self, website, websites=None):
        """""" Look for websites matching the one passed """"""
        if websites is None:
            websites = self.list_websites()
        if isinstance(website, str):
            website = {""name"": website}
        ignored_fields = ('id',)  # changes in these fields are ignored

        results = []
        for other in websites:
            different = False
            for key in website:
                if key in ignored_fields:
                    continue
                if other.get(key) != website.get(key):
                    different = True
                    break
            if different is False:
                results.append(other)
        return results",https://github.com/dariosky/wfcli/blob/87a9ed30dbd456f801135a55099f0541b0614ccb/wfcli/wfapi.py#L136-L155
tfidf_python_100_1.0,how to get html of website,python,"def get_website(bucket_name, **conn):
    try:
        result = get_bucket_website(Bucket=bucket_name, **conn)
    except ClientError as e:
        if ""NoSuchWebsiteConfiguration"" not in str(e):
            raise e
        return None

    website = {}
    if result.get(""IndexDocument""):
        website[""IndexDocument""] = result[""IndexDocument""]
    if result.get(""RoutingRules""):
        website[""RoutingRules""] = result[""RoutingRules""]
    if result.get(""RedirectAllRequestsTo""):
        website[""RedirectAllRequestsTo""] = result[""RedirectAllRequestsTo""]
    if result.get(""ErrorDocument""):
        website[""ErrorDocument""] = result[""ErrorDocument""]

    return website",https://github.com/Netflix-Skunkworks/cloudaux/blob/c4b0870c3ac68b1c69e71d33cf78b6a8bdf437ea/cloudaux/orchestration/aws/s3.py#L160-L178
tfidf_python_100_1.0,how to get html of website,python,"def verify_certificate(self, website, certificate):
        # is the certificate associated with the website?
        certificate_name = certificate['name']
        if website['certificate'] != certificate_name:
            website['certificate'] = certificate_name
            logger.info(""Adding certificate to the secured website %s"" % website['name'])
            self.api.update_website(website)",https://github.com/dariosky/wfcli/blob/87a9ed30dbd456f801135a55099f0541b0614ccb/wfcli/tossl.py#L231-L237
tfidf_python_100_1.0,how to get html of website,python,"def init():
    """"""Initiates a new website""""""

    print(""Blended: Static Website Generator -\n"")

    checkConfig()

    if (sys.version_info > (3, 0)):
        wname = input(""Website Name: "")
        wdesc = input(""Website Description: "")
        wlan = input(""Website Language: "")
        wlic = input(""Website License: "")
        aname = input(""Author(s) Name(s): "")
    else:
        wname = raw_input(""Website Name: "")
        wdesc = raw_input(""Website Description: "")
        wlan = raw_input(""Website Language: "")
        wlic = raw_input(""Website License: "")
        aname = raw_input(""Author(s) Name(s): "")

    createBlendedFolders()

    # Populate the configuration file
    createConfig(app_version=app_version, wname=wname,
                 wdesc=wdesc, wlic=wlic, wlan=wlan, aname=aname)

    print(""\nThe required files for your website have been generated."")",https://github.com/BlendedSiteGenerator/Blended/blob/e5865a8633e461a22c86ef6ee98cdd7051c412ac/blended/__main__.py#L146-L172
tfidf_python_100_1.0,how to get html of website,python,"def configure_website(self, main_page_suffix=None, not_found_page=None):
        """"""Configure website-related properties.

        See https://cloud.google.com/storage/docs/hosting-static-website

        .. note::
          This (apparently) only works
          if your bucket name is a domain name
          (and to do that, you need to get approved somehow...).

        If you want this bucket to host a website, just provide the name
        of an index page and a page to use when a blob isn't found:

        .. literalinclude:: snippets.py
          :start-after: [START configure_website]
          :end-before: [END configure_website]

        You probably should also make the whole bucket public:

        .. literalinclude:: snippets.py
            :start-after: [START make_public]
            :end-before: [END make_public]

        This says: ""Make the bucket public, and all the stuff already in
        the bucket, and anything else I add to the bucket.  Just make it
        all public.""

        :type main_page_suffix: str
        :param main_page_suffix: The page to use as the main page
                                 of a directory.
                                 Typically something like index.html.

        :type not_found_page: str
        :param not_found_page: The file to use when a page isn't found.
        """"""
        data = {""mainPageSuffix"": main_page_suffix, ""notFoundPage"": not_found_page}
        self._patch_property(""website"", data)",https://github.com/googleapis/google-cloud-python/blob/85e80125a59cb10f8cb105f25ecc099e4b940b50/storage/google/cloud/storage/bucket.py#L1624-L1660
tfidf_python_100_1.0,how to get html of website,python,"def __init__(self, id = """", name = """", website = """", price_btc = """", volume_btc = """"):
        '''
        Simple constructor for a Coin.
        '''
        self.id = id
        self.name = name
        self.website = website
        self.price_btc = price_btc
        self.volume_btc = volume_btc",https://github.com/Dirrot/python-cryptocoincharts-api/blob/8bf7a35c1032847aaea322b304014cd52853c273/CryptoCoinChartsApi/Models/Coin.py#L14-L22
tfidf_python_100_1.0,how to get html of website,python,"def level(self, website=None, store_view=None, parent_category=None):
        """"""
        Retrieve one level of categories by website/store view/parent category

        :param website: Website code or ID
        :param store_view: storeview code or ID
        :param parent_category: Parent Category ID
        :return: Dictionary
        """"""
        return self.call(
            'catalog_category.level', [website, store_view, parent_category]
        )",https://github.com/fulfilio/python-magento/blob/720ec136a6e438a9ee4ee92848a9820b91732750/magento/catalog.py#L41-L52
tfidf_python_100_1.0,how to get html of website,python,"def google_url(self,song_name,website):
		''' It will return the google url to be searched'''
		name='+'.join(song_name)
		prefix='https://www.google.co.in/search?q='	
		website=website.split("" "")
		suffix='+'.join(website)
		url=prefix+name+suffix
		#print url
		return url",https://github.com/ankitmathur3193/song-cli/blob/ca8ccfe547e9d702313ff6d14e81ae4355989a67/song/commands/SearchEngineParser/GoogleParser.py#L10-L18
tfidf_python_100_1.0,how to get html of website,python,"def __eq__(self, other):
        return (self.name == other.name and self.website == other.website)",https://github.com/MonashBI/arcana/blob/d6271a29d13733d00422d11417af8d200be62acc/arcana/environment/requirement/base.py#L376-L377
tfidf_python_100_1.0,how to get html of website,python,"def convert_to_redirect(self, website):
        self.create_redirect_app()
        if website['https']:
            logger.error(""Convert to redirect should be used only for insecure websites"")
            return

        apps = [
            [self.REDIRECT_TO_SECURE_APP_NAME, ""/""],
            [self.LETSENCRYPT_VERIFY_APP_NAME, '/.well-known'],
        ]
        if website['website_apps'] != apps:
            logger.info(""Convert the insecure website %s to a redirect"" % website['name'])

            # TODO: Wait that the https answer with the correct certificate

            # confirm = input(
            #     ""This operation change the insecure site with a redirect, proceed [Y, n] ? "")
            # if confirm.strip().lower() == 'n':
            #     logger.warning(""Redirection cancelled"")
            #     return

            # updating the apps with the new one
            website['website_apps'] = apps
            self.api.update_website(website)",https://github.com/dariosky/wfcli/blob/87a9ed30dbd456f801135a55099f0541b0614ccb/wfcli/tossl.py#L379-L402
tfidf_python_100_1.0,how to get html of website,python,"def _put_bucket_website(self):
        """"""Configure static website on S3 bucket.""""""
        if self.s3props['website']['enabled']:
            website_config = {
                'ErrorDocument': {
                    'Key': self.s3props['website']['error_document']
                },
                'IndexDocument': {
                    'Suffix': self.s3props['website']['index_suffix']
                }
            }
            _response = self.s3client.put_bucket_website(Bucket=self.bucket, WebsiteConfiguration=website_config)
            self._put_bucket_cors()
            self._set_bucket_dns()
        else:
            _response = self.s3client.delete_bucket_website(Bucket=self.bucket)
            self._put_bucket_cors()
        LOG.debug('Response setting up S3 website: %s', _response)
        LOG.info('S3 website settings updated')",https://github.com/foremast/foremast/blob/fb70f29b8ce532f061685a17d120486e47b215ba/src/foremast/s3/s3apps.py#L113-L131
tfidf_python_100_1.0,how to get html of website,python,"def website_verificable(self, website):
        """""" True if the website is LetsEncrypt verificable: it should have the verification app
            on the /.well-known path """"""
        required_app = [self.LETSENCRYPT_VERIFY_APP_NAME, '/.well-known']
        for app in website['website_apps']:
            if app == required_app:
                return True
        return False",https://github.com/dariosky/wfcli/blob/87a9ed30dbd456f801135a55099f0541b0614ccb/wfcli/tossl.py#L314-L321
tfidf_python_100_1.0,how to get html of website,python,"def cloud_cover_to_irradiance(self, cloud_cover, how='clearsky_scaling',
                                  **kwargs):
        """"""
        Convert cloud cover to irradiance. A wrapper method.

        Parameters
        ----------
        cloud_cover : Series
        how : str, default 'clearsky_scaling'
            Selects the method for conversion. Can be one of
            clearsky_scaling or liujordan.
        **kwargs
            Passed to the selected method.

        Returns
        -------
        irradiance : DataFrame
            Columns include ghi, dni, dhi
        """"""

        how = how.lower()
        if how == 'clearsky_scaling':
            irrads = self.cloud_cover_to_irradiance_clearsky_scaling(
                cloud_cover, **kwargs)
        elif how == 'liujordan':
            irrads = self.cloud_cover_to_irradiance_liujordan(
                cloud_cover, **kwargs)
        else:
            raise ValueError('invalid how argument')

        return irrads",https://github.com/pvlib/pvlib-python/blob/2e844a595b820b43d1170269781fa66bd0ccc8a3/pvlib/forecast.py#L539-L569
tfidf_python_100_1.0,how to get html of website,python,"def execute_arbitrary_series_groupby(op, data, _, aggcontext=None, **kwargs):
    how = op.how
    if how is None:
        how = 'first'

    if how not in {'first', 'last'}:
        raise com.OperationNotDefinedError(
            'Arbitrary {!r} is not supported'.format(how)
        )
    return aggcontext.agg(data, how)",https://github.com/ibis-project/ibis/blob/1e39a5fd9ef088b45c155e8a5f541767ee8ef2e7/ibis/pandas/execution/generic.py#L451-L460
tfidf_python_100_1.0,buffered file reader read text,python,"def _mark_good(self, file, buffered):
        self.last_good_position = get_known_read_position(file.fileobj,
                                                          buffered)",https://github.com/TeamHG-Memex/json-lines/blob/b0488ac746c90065f1599e51b710205d9c2392ad/json_lines/_lib.py#L114-L116
tfidf_python_100_1.0,buffered file reader read text,python,"def readline(self):
        buffered = self.flush_buffer()
        nl_idx = buffered.find(b'\n')
        if nl_idx < 0:
            more_data = yield from self._reader.readline()
            return b''.join([buffered, more_data])
        else:
            self.put(buffered[(nl_idx+1):])
            return buffered[0:(nl_idx+1)]",https://github.com/l04m33/pyx/blob/b70efec605832ba3c7079e991584db3f5d1da8cb/pyx/io.py#L316-L324
tfidf_python_100_1.0,buffered file reader read text,python,"def read(self, n=-1):
        buffered, more = self.read_from_buffer(n)
        if more != 0:
            more_data = yield from self._reader.read(more)
            return b''.join([buffered, more_data])
        else:
            return buffered",https://github.com/l04m33/pyx/blob/b70efec605832ba3c7079e991584db3f5d1da8cb/pyx/io.py#L327-L333
tfidf_python_100_1.0,buffered file reader read text,python,"def flush_buffer(self):
        """"""flush all buffered string into code""""""
        self.code_builder.add_line('{0}.extend([{1}])',
                                   self.result_var, ','.join(self.buffered))
        self.buffered = []",https://github.com/mozillazg/bustard/blob/bd7b47f3ba5440cf6ea026c8b633060fedeb80b7/bustard/template.py#L306-L310
tfidf_python_100_1.0,buffered file reader read text,python,"def read(cls, reader, ptg):
        subex_len = reader.read_short()
        subex = reader.read(subex_len)
        return cls(subex, ptg)",https://github.com/wwwiiilll/pyxlsb/blob/f77d99832edd337570f3e0c9c2135d0ce6b966b1/pyxlsb/ptgs.py#L804-L807
tfidf_python_100_1.0,buffered file reader read text,python,"def __init__(self, session_, url, buffered=True, **args):
        Stream.__init__(self, session_)

        self.args = dict(url=url, **args)
        self.buffered = buffered",https://github.com/streamlink/streamlink/blob/c8ed1daff14ac03195870238b9b900c1109dd5c1/src/streamlink/stream/http.py#L35-L39
tfidf_python_100_1.0,buffered file reader read text,python,"def buffered(self):
        """""" Whether write operations should be buffered, i.e. run against a
        local graph before being stored to the main data store. """"""
        if 'buffered' not in self.config:
            return not isinstance(self.store, (Memory, IOMemory))
        return self.config.get('buffered')",https://github.com/pudo/jsongraph/blob/35e4f397dbe69cd5553cf9cb9ab98859c3620f03/jsongraph/graph.py#L82-L87
tfidf_python_100_1.0,buffered file reader read text,python,"def buffered(self):
        return self.stream_type == self.BufferedType or self.stream_type == self.OutputType",https://github.com/iotile/coretools/blob/2d794f5f1346b841b0dcd16c9d284e9bf2f3c6ec/iotilesensorgraph/iotile/sg/stream.py#L69-L70
tfidf_python_100_1.0,buffered file reader read text,python,"def get_tokens_unprocessed(self, text):
        buffered = ''
        insertions = []
        lng_buffer = []
        for i, t, v in self.language_lexer.get_tokens_unprocessed(text):
            if t is self.needle:
                if lng_buffer:
                    insertions.append((len(buffered), lng_buffer))
                    lng_buffer = []
                buffered += v
            else:
                lng_buffer.append((i, t, v))
        if lng_buffer:
            insertions.append((len(buffered), lng_buffer))
        return do_insertions(insertions,
                             self.root_lexer.get_tokens_unprocessed(buffered))",https://github.com/wakatime/wakatime/blob/74519ace04e8472f3a3993269963732b9946a01d/wakatime/packages/pygments/lexer.py#L225-L240
tfidf_python_100_1.0,buffered file reader read text,python,"def socks_address(reader, n):
    return socket.inet_ntoa(reader.read(4)) if n == 1 else \
           reader.read(reader.read(1)[0]).decode() if n == 3 else \
           socket.inet_ntop(socket.AF_INET6, reader.read(16)), \
           int.from_bytes(reader.read(2), 'big')",https://github.com/qwj/python-proxy/blob/32c6a63d639792dfab2430d2cf985f53678b09a9/pproxy/proto.py#L22-L26
tfidf_python_100_1.0,buffered file reader read text,python,"def read(self, json):
        reader = BankReader(self.system_effect)

        return reader.read(json)",https://github.com/PedalPi/PluginsManager/blob/2dcc9f6a79b48e9c9be82efffd855352fa15c5c7/pluginsmanager/util/persistence_decoder.py#L31-L34
tfidf_python_100_1.0,buffered file reader read text,python,"def drain(self):
        if not self.process:
            return

        try:
            data = self.process.stdout.read(8192)
        except IOError:
            return

        data = data.decode('utf-8', 'replace')
        if data == '':
            self.eof = True

        self.buffered += data
        while '\n' in self.buffered:
            line, self.buffered = self.buffered.split('\n', 1)
            if line.strip():
                self.log('%s', line)",https://github.com/doptio/heywood/blob/40c2b1f28d43524a16b390fd3a4f6832f16fec41/src/heywood/manager.py#L91-L108
tfidf_python_100_1.0,buffered file reader read text,python,"def get_known_read_position(fp, buffered=True):
    """""" 
    Return a position in a file which is known to be read & handled.
    It assumes a buffered file and streaming processing. 
    """"""
    buffer_size = io.DEFAULT_BUFFER_SIZE if buffered else 0
    return max(fp.tell() - buffer_size, 0)",https://github.com/TeamHG-Memex/json-lines/blob/b0488ac746c90065f1599e51b710205d9c2392ad/json_lines/_gzip.py#L10-L16
tfidf_python_100_1.0,buffered file reader read text,python,"def udp_client(self, data):
        reader = io.BytesIO(data)
        n = reader.read(1)[0]
        host_name, port = socks_address(reader, n)
        return reader.read()",https://github.com/qwj/python-proxy/blob/32c6a63d639792dfab2430d2cf985f53678b09a9/pproxy/proto.py#L150-L154
tfidf_python_100_1.0,buffered file reader read text,python,"def read(cls, reader, ptg):
        sheet_idx = reader.read_short()
        name_idx = reader.read_short()
        res = reader.read(2)  # Reserved
        return cls(sheet_idx, name_idx, res, ptg)",https://github.com/wwwiiilll/pyxlsb/blob/f77d99832edd337570f3e0c9c2135d0ce6b966b1/pyxlsb/ptgs.py#L584-L588
tfidf_python_100_1.0,buffered file reader read text,python,"def read_exodus(filename,
                animate_mode_shapes=True,
                apply_displacements=True,
                displacement_magnitude=1.0,
                enabled_sidesets=None):
    """"""Read an ExodusII file (``'.e'`` or ``'.exo'``)""""""
    reader = vtk.vtkExodusIIReader()
    reader.SetFileName(filename)
    reader.UpdateInformation()
    reader.SetAnimateModeShapes(animate_mode_shapes)
    reader.SetApplyDisplacements(apply_displacements)
    reader.SetDisplacementMagnitude(displacement_magnitude)

    if enabled_sidesets is None:
        enabled_sidesets = list(range(reader.GetNumberOfSideSetArrays()))

    for sideset in enabled_sidesets:
        if isinstance(sideset, int):
            name = reader.GetSideSetArrayName(sideset)
        elif isinstance(sideset, str):
            name = sideset
        else:
            raise ValueError('Could not parse sideset ID/name: {}'.format(sideset))

        reader.SetSideSetArrayStatus(name, 1)

    reader.Update()
    return vtki.wrap(reader.GetOutput())",https://github.com/vtkiorg/vtki/blob/5ccad7ae6d64a03e9594c9c7474c8aab3eb22dd1/vtki/readers.py#L200-L227
tfidf_python_100_1.0,buffered file reader read text,python,"def deserialize(self, reader: BinaryReader):
        self.peer_pubkey = a2b_hex(reader.read_var_str()).hex()
        self.max_authorize = reader.read_int64()
        self.old_peerCost = reader.read_int64()
        self.new_peer_cost = reader.read_int64()
        self.set_cost_view = reader.read_int32()
        self.field1 = reader.read_var_bytes()
        self.field2 = reader.read_var_bytes()
        self.field3 = reader.read_var_bytes()
        self.field4 = reader.read_var_bytes()",https://github.com/ontio/ontology-python-sdk/blob/ac88bdda941896c5d2ced08422a9c5179d3f9b19/ontology/smart_contract/native_contract/governance.py#L392-L401
tfidf_python_100_1.0,buffered file reader read text,python,"def _read_wkb(reader, dimz, dimm):
        try:
            x = reader.get_double()
            y = reader.get_double()
            if dimz and dimm:
                z = reader.get_double()
                m = reader.get_double()
            elif dimz:
                z = reader.get_double()
                m = None
            elif dimm:
                z = None
                m = reader.get_double()
            else:
                z = None
                m = None
        except TypeError:
            raise WkbError()
        return x, y, z, m",https://github.com/bosth/plpygis/blob/9469cc469df4c8cd407de158903d5465cda804ea/plpygis/geometry.py#L592-L610
tfidf_python_100_1.0,buffered file reader read text,python,"def get_token_sq_equi_class(self):
        if self.reader.p(0) == ""["" and self.reader.p(1) == ""="" and not self.isend(self.reader.p(2)) and self.reader.p(3) == ""="" and self.reader.p(4) == ""]"":
            return self.reader.getn(5)
        return """"",https://github.com/Kuniwak/vint/blob/db29337d859d88239c282c2e9d84c858f23a4a09/vint/_bundles/vimlparser.py#L4139-L4142
tfidf_python_100_1.0,buffered file reader read text,python,"def get_token_sq_coll_element(self):
        if self.reader.p(0) == ""["" and self.reader.p(1) == ""."" and not self.isend(self.reader.p(2)) and self.reader.p(3) == ""."" and self.reader.p(4) == ""]"":
            return self.reader.getn(5)
        return """"",https://github.com/Kuniwak/vint/blob/db29337d859d88239c282c2e9d84c858f23a4a09/vint/_bundles/vimlparser.py#L4133-L4136
tfidf_python_100_1.0,encrypt aes ctr mode,python,"def main():
    runtest(""AES - CTR Mode"", AES.CTREnc, AES.CTRDec)
    runtest(""AES - GCM Mode"", AES.GCMEnc, AES.GCMDec)",https://github.com/ghackebeil/PyORAM/blob/b8832c1b753c0b2148ef7a143c5f5dd3bbbb61e7/examples/aesctr_performance.py#L113-L115
tfidf_python_100_1.0,encrypt aes ctr mode,python,"def _encrypt_xor(a, b, aes):
    """""" Returns encrypt(a ^ b). """"""
    a = unhexlify(""%0.32x"" % (int((a), 16) ^ int(hexlify(b), 16)))
    return aes.encrypt(a)",https://github.com/xeroc/python-graphenelib/blob/8bb5396bc79998ee424cf3813af478304173f3a6/graphenebase/bip38.py#L40-L43
tfidf_python_100_1.0,encrypt aes ctr mode,python,"def aes_encrypt(key, data, mode='ECB', iv=None):
    aes = AES()
    aes.mode = mode
    aes.iv = iv
    aes.key = key
    return aes.encrypt(data)",https://github.com/boldfield/s3-encryption/blob/d88549ba682745dc6b199934c5b5221de7f8d8bc/s3_encryption/crypto.py#L52-L57
tfidf_python_100_1.0,encrypt aes ctr mode,python,"def _get_mode(mode = None):
        """"""
        Return the AES mode, or a list of valid AES modes, if mode == None
        """"""
        from Crypto.Cipher import AES

        AESModeMap = {
            'CCM': AES.MODE_CCM,
            'EAX': AES.MODE_EAX,
            'GCM': AES.MODE_GCM,
            'OCB': AES.MODE_OCB,
        }

        if mode is None:
            return AESModeMap.keys()
        return AESModeMap.get(mode)",https://github.com/frispete/keyrings.cryptfile/blob/cfa80d4848a5c3c0aeee41a954b2b120c80e69b2/keyrings/cryptfile/cryptfile.py#L61-L76
tfidf_python_100_1.0,encrypt aes ctr mode,python,"def encrypt(cls, data, key, iv_data):
        validate_key_size(key, cls.key_size, ""AES"")

        iv, ctr = iv_data
        ciphertext = Crypto_AES.new(key, Crypto_AES.MODE_CTR,
                                    counter=ctr).encrypt(data)
        return iv + ciphertext",https://github.com/keybase/python-triplesec/blob/0a73e18cfe542d0cd5ee57bd823a67412b4b717e/triplesec/crypto.py#L86-L92
tfidf_python_100_1.0,encrypt aes ctr mode,python,"def encrypt(self, msg):
        """"""encrypts a message""""""
        iv = self.random_bytes(AES.block_size)
        ctr = Counter.new(AES.block_size * 8, initial_value=self.bin2long(iv))
        cipher = AES.AESCipher(self._cipherkey, AES.MODE_CTR, counter=ctr)
        cipher_text = cipher.encrypt(msg)
        intermediate = iv + cipher_text
        signature = self.sign(intermediate)
        return signature + intermediate",https://github.com/gilsho/kryptonite/blob/d20a15e4fd28cb880180b827099168dd87eb0291/kryptonite/cipher.py#L54-L62
tfidf_python_100_1.0,encrypt aes ctr mode,python,"def encrypt(self, key):
        """"""This method encrypts and signs the state to make it unreadable by
        the server, since it contains information that would allow faking
        proof of storage.

        :param key: the key to encrypt and sign with
        """"""
        if (self.encrypted):
            return
        # encrypt
        self.iv = Random.new().read(AES.block_size)
        aes = AES.new(key, AES.MODE_CFB, self.iv)
        self.f_key = aes.encrypt(self.f_key)
        self.alpha_key = aes.encrypt(self.alpha_key)
        self.encrypted = True
        # sign
        self.hmac = self.get_hmac(key)",https://github.com/StorjOld/heartbeat/blob/4d54f2011f1e9f688073d4347bc51bb7bd682718/heartbeat/PySwizzle/PySwizzle.py#L162-L178
tfidf_python_100_1.0,encrypt aes ctr mode,python,"def aes_ctr_encrypt(text, key, params):
    iv = big_endian_to_int(decode_hex(params[""iv""]))
    ctr = Counter.new(128, initial_value=iv, allow_wraparound=True)
    mode = AES.MODE_CTR
    encryptor = AES.new(key, mode, counter=ctr)
    return encryptor.encrypt(text)",https://github.com/ethereum/pyethereum/blob/b704a5c6577863edc539a1ec3d2620a443b950fb/ethereum/tools/keys.py#L56-L61
tfidf_python_100_1.0,encrypt aes ctr mode,python,"def __init_section_numbers(self, root):
        if not Formatter.number_headings:
            return {}

        targets = []

        ctr = 0
        while len(targets) <= 1:
            ctr += 1
            if ctr > 5:
                return {}

            targets = root.xpath('.//*[self::h%s]' % ctr)

        section_numbers = {}
        for i in range(ctr, 6):
            section_numbers['h%d' % i] = 0

        section_numbers['first'] = ctr

        return section_numbers",https://github.com/hotdoc/hotdoc/blob/1067cdc8482b585b364a38fb52ca5d904e486280/hotdoc/core/formatter.py#L313-L333
tfidf_python_100_1.0,encrypt aes ctr mode,python,"def EncryptPrivateKey(self, decrypted):
        """"""
        Encrypt the provided plaintext with the initialized private key.

        Args:
            decrypted (byte string): the plaintext to be encrypted.

        Returns:
            bytes: the ciphertext.
        """"""
        aes = AES.new(self._master_key, AES.MODE_CBC, self._iv)
        return aes.encrypt(decrypted)",https://github.com/CityOfZion/neo-python/blob/fe90f62e123d720d4281c79af0598d9df9e776fb/neo/Wallets/Wallet.py#L286-L297
tfidf_python_100_1.0,encrypt aes ctr mode,python,"def _encrypt_session(self, data):
        aes = aes128cbc(settings.BLTI_AES_KEY, settings.BLTI_AES_IV)
        return aes.encrypt(json.dumps(data))",https://github.com/uw-it-aca/django-blti/blob/7f058d119f0006ce1d798456b32e3840198fd939/blti/__init__.py#L41-L43
tfidf_python_100_1.0,encrypt aes ctr mode,python,"def _gen_keystream(cls, length, tfish, ctr):
        req_blocks = length // cls.block_size + 1
        keystream = b''
        for _ in range(req_blocks):
            keystream += tfish.encrypt(check_and_increment_counter(ctr))
        return keystream[:length]",https://github.com/keybase/python-triplesec/blob/0a73e18cfe542d0cd5ee57bd823a67412b4b717e/triplesec/crypto.py#L112-L117
tfidf_python_100_1.0,encrypt aes ctr mode,python,"def __init__(
		self, key_size, block_cipher_mode, init_sequence, padding=None
	):
		"""""" Create new AES-mode.

		:param key_size: secret length
		:param block_cipher_mode: name of block cipher mode of operation
		:param padding: padding object (if required)
		:param init_sequence: AES secret with initialization vector or counter value
		""""""
		self.__key_size = key_size
		self.__mode = block_cipher_mode
		self.__padding = padding
		self.__sequence_chopper = WAESMode.SequenceChopper(key_size, block_cipher_mode, init_sequence)

		if block_cipher_mode == 'AES-CBC':
			mode_code = modes.CBC(self.__sequence_chopper.initialization_vector())
		elif block_cipher_mode == 'AES-CTR':
			mode_code = modes.CTR(self.__sequence_chopper.initialization_counter_value())
		else:
			raise ValueError('Unknown block cipher mode spotted')

		self.__cipher_args = (AES(self.__sequence_chopper.secret()), mode_code)
		self.__cipher_kwargs = {'backend': default_backend()}",https://github.com/a1ezzz/wasp-general/blob/1029839d33eb663f8dec76c1c46754d53c1de4a9/wasp_general/crypto/aes.py#L345-L368
tfidf_python_100_1.0,encrypt aes ctr mode,python,"def encrypt_report(self, device_id, root, data, **kwargs):
        """"""Encrypt a buffer of report data on behalf of a device.

        Args:
            device_id (int): The id of the device that we should encrypt for
            root (int): The root key type that should be used to generate the report
            data (bytearray): The data that we should decrypt
            **kwargs: There are additional specific keyword args that are required
                depending on the root key used.  Typically, you must specify
                - report_id (int): The report id
                - sent_timestamp (int): The sent timestamp of the report

                These two bits of information are used to construct the per report
                signing and encryption key from the specific root key type.

        Returns:
            dict: The encrypted data and any associated metadata about the data.
                The data itself must always be a bytearray stored under the 'data'
                key, however additional keys may be present depending on the encryption method
                used.

        Raises:
            NotFoundError: If the auth provider is not able to decrypt the data.
        """"""

        report_key = self._verify_derive_key(device_id, root, **kwargs)

        try:
            from Crypto.Cipher import AES
            import Crypto.Util.Counter
        except ImportError:
            raise NotFoundError

        # We use AES-128 for encryption
        ctr = Crypto.Util.Counter.new(128)
        encryptor = AES.new(bytes(report_key[:16]), AES.MODE_CTR, counter=ctr)

        encrypted = encryptor.encrypt(bytes(data))
        return {'data': encrypted}",https://github.com/iotile/coretools/blob/2d794f5f1346b841b0dcd16c9d284e9bf2f3c6ec/iotilecore/iotile/core/hw/auth/env_auth_provider.py#L188-L226
tfidf_python_100_1.0,encrypt aes ctr mode,python,"def encrypt_aes_ctr(value, key, iv):
    ctr = Counter.new(128, initial_value=iv, allow_wraparound=True)
    encryptor = AES.new(key, AES.MODE_CTR, counter=ctr)
    ciphertext = encryptor.encrypt(value)
    return ciphertext",https://github.com/ethereum/eth-keyfile/blob/2b46bb5d6e4b4800a95cdca34e390cb8c9bb0198/eth_keyfile/keyfile.py#L253-L257
tfidf_python_100_1.0,encrypt aes ctr mode,python,"def aes_decrypt(key, data, mode='ECB', iv=None):
    aes = AES()
    aes.mode = mode
    aes.iv = iv
    aes.key = key
    return aes.decrypt(data)",https://github.com/boldfield/s3-encryption/blob/d88549ba682745dc6b199934c5b5221de7f8d8bc/s3_encryption/crypto.py#L60-L65
tfidf_python_100_1.0,encrypt aes ctr mode,python,"def _extract_coil_coords(self, aes):
        groups_ix = groupby_ix(aes.secondary_id)
        coils_ix = groups_ix[aes.secondary_type[groups_ix[:, 0]] == 'C']
        
        # We remove id = 0 because they are heteroatoms
        coils_id = aes.secondary_id[coils_ix[:, 0]]
        coils_ix = coils_ix[coils_id != 0, :]
        
        coils_ix[:, 1] += 1
        coils_ix[:, 0] -= 1
        coils_ix[coils_ix > len(aes.secondary_type)] = len(aes.secondary_type)
        coils_ix[coils_ix < 0] = 0
        
        backbone_list = [aes.xyz[aes.types == 'CA'][i:j] for i, j in coils_ix]
        return backbone_list",https://github.com/gabrielelanaro/chemview/blob/2c9768dd23db99e59e27adff2a953bb8ee795fa3/chemview/gg.py#L322-L336
tfidf_python_100_1.0,encrypt aes ctr mode,python,"def encrypt_pycrypto(self, payload):
    aes = AES.new(bytes(self.key), AES.MODE_CBC, bytes(self.iv))
    return aes.encrypt(bytes(payload))",https://github.com/mjg59/python-broadlink/blob/1d6d8d2aee6e221aa3383e4078b19b7b95397f43/broadlink/__init__.py#L175-L177
tfidf_python_100_1.0,encrypt aes ctr mode,python,"def aes(encrypt, key, data):
    """"""
    One-pass AES-256-CBC used in ProcessData. Zero IV (don't panic, IV-like random nonce is included in plaintext in the
    first block in ProcessData).

    Does not use padding (data has to be already padded).

    :param encrypt:
    :param key:
    :param data:
    :return:
    """"""
    cipher = AES.new(key, AES.MODE_CBC, get_zero_vector(16))
    if encrypt:
        return cipher.encrypt(data)
    else:
        return cipher.decrypt(data)",https://github.com/EnigmaBridge/client.py/blob/0fafe3902da394da88e9f960751d695ca65bbabd/ebclient/crypto_util.py#L350-L366
tfidf_python_100_1.0,encrypt aes ctr mode,python,"def aes_cipher(key=None, iv=None, mode=None):
    aes = AES()
    aes.iv = iv if iv else None
    aes.mode = mode if mode else None
    aes.key = key if key else None
    return aes",https://github.com/boldfield/s3-encryption/blob/d88549ba682745dc6b199934c5b5221de7f8d8bc/s3_encryption/crypto.py#L44-L49
tfidf_python_100_1.0,matrix multiply,python,"def multiply(self, matrix):
        """"""
        Multiply this matrix by a local dense matrix on the right.

        :param matrix: a local dense matrix whose number of rows must match the number of columns
                       of this matrix
        :returns: :py:class:`IndexedRowMatrix`

        >>> mat = IndexedRowMatrix(sc.parallelize([(0, (0, 1)), (1, (2, 3))]))
        >>> mat.multiply(DenseMatrix(2, 2, [0, 2, 1, 3])).rows.collect()
        [IndexedRow(0, [2.0,3.0]), IndexedRow(1, [6.0,11.0])]
        """"""
        if not isinstance(matrix, DenseMatrix):
            raise ValueError(""Only multiplication with DenseMatrix ""
                             ""is supported."")
        return IndexedRowMatrix(self._java_matrix_wrapper.call(""multiply"", matrix))",https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/linalg/distributed.py#L705-L720
tfidf_python_100_1.0,matrix multiply,python,"def multiply(self, matrix):
        """"""
        Multiply this matrix by a local dense matrix on the right.

        :param matrix: a local dense matrix whose number of rows must match the number of columns
                       of this matrix
        :returns: :py:class:`RowMatrix`

        >>> rm = RowMatrix(sc.parallelize([[0, 1], [2, 3]]))
        >>> rm.multiply(DenseMatrix(2, 2, [0, 2, 1, 3])).rows.collect()
        [DenseVector([2.0, 3.0]), DenseVector([6.0, 11.0])]
        """"""
        if not isinstance(matrix, DenseMatrix):
            raise ValueError(""Only multiplication with DenseMatrix ""
                             ""is supported."")
        j_model = self._java_matrix_wrapper.call(""multiply"", matrix)
        return RowMatrix(j_model)",https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/linalg/distributed.py#L373-L389
tfidf_python_100_1.0,matrix multiply,python,"def multiply(z, x, y):
  d[x] = y * d[z]
  d[y] = x * d[z]",https://github.com/google/tangent/blob/6533e83af09de7345d1b438512679992f080dcc9/tangent/grads.py#L266-L268
tfidf_python_100_1.0,matrix multiply,python,"def tmultiply(z, x, y):
  d[z] = numpy.multiply(d[x], y) + numpy.multiply(x, d[y])",https://github.com/google/tangent/blob/6533e83af09de7345d1b438512679992f080dcc9/tangent/tangents.py#L238-L239
tfidf_python_100_1.0,matrix multiply,python,"def fuse_stationary_distributions(stationary_distribution_list,
                                  actual_weights):
    number_of_views = len(stationary_distribution_list)

    multiview_implicit_stationary_distribution = np.multiply(stationary_distribution_list[0], actual_weights[0, :])
    # print(calculate_entropy(np.multiply(stationary_distribution_list[0], actual_weights[0, :])))

    for view_counter in range(1, number_of_views):
        multiview_implicit_stationary_distribution += np.multiply(stationary_distribution_list[view_counter], actual_weights[view_counter, :])
        # print(calculate_entropy(np.multiply(stationary_distribution_list[view_counter], actual_weights[view_counter, :])))

    multiview_implicit_stationary_distribution[multiview_implicit_stationary_distribution == 0.0] = np.min(multiview_implicit_stationary_distribution[multiview_implicit_stationary_distribution > 0.0])/2

    return multiview_implicit_stationary_distribution",https://github.com/MKLab-ITI/reveal-graph-embedding/blob/eda862687aa5a64b79c6b12de1b4dca6ce986dc8/reveal_graph_embedding/embedding/implicit.py#L313-L326
tfidf_python_100_1.0,matrix multiply,python,"def __rmul__(self, other):
        """"""Return multiple of Matrix

        :return: Returns a
        :raise: Raises an :py:exc:`ValueError` if the input parameter is not a number
        """"""
        if isinstance(other, Number):
            return self.multiply(other)
        else:
            raise TypeError(""Can't multiply Matrix with type %s"" % type(other).__name__)",https://github.com/T-002/pycast/blob/8a53505c6d8367e0ea572e8af768e80b29e1cc41/pycast/common/matrix.py#L400-L409
tfidf_python_100_1.0,matrix multiply,python,"def setMatrix(self, a, b, c, d, e, f):
        self.transform_dict[""matrix""] = 'matrix(%s %s %s %s %s %s)' % (a, b, c, d, e, f)",https://github.com/alorence/pysvg-py3/blob/ce217a4da3ada44a71d3e2f391d37c67d95c724e/pysvg/builders.py#L316-L317
tfidf_python_100_1.0,matrix multiply,python,"def init_Fx0(self):
        self.Gx0 = spmatrix([], [], [], (self.m, self.n), 'd')
        self.Fy0 = spmatrix([], [], [], (self.n, self.m), 'd')
        self.Fx0 = spmatrix([], [], [], (self.n, self.n), 'd')

        self._temp.update({
            'Fx0': {
                'I': matrix([]),
                'J': matrix([]),
                'V': matrix([])
            },
            'Fy0': {
                'I': matrix([]),
                'J': matrix([]),
                'V': matrix([])
            },
            'Gx0': {
                'I': matrix([]),
                'J': matrix([]),
                'V': matrix([])
            },
        })

        self._set.update({
            'Fx0': {
                'I': matrix([]),
                'J': matrix([]),
                'V': matrix([])
            },
            'Fy0': {
                'I': matrix([]),
                'J': matrix([]),
                'V': matrix([])
            },
            'Gx0': {
                'I': matrix([]),
                'J': matrix([]),
                'V': matrix([])
            },
        })",https://github.com/cuihantao/andes/blob/7067898d4f26ce7534e968b8486c4aa8fe3a511a/andes/variables/dae.py#L307-L346
tfidf_python_100_1.0,matrix multiply,python,"def mult(self, matrix):
        """"""
        Multiply this frame, viewed as a matrix, by another matrix.

        :param matrix: another frame that you want to multiply the current frame by; must be compatible with the
            current frame (i.e. its number of rows must be the same as number of columns in the current frame).
        :returns: new H2OFrame, which is the result of multiplying the current frame by ``matrix``.
        """"""
        if self.ncols != matrix.nrows:
            raise H2OValueError(""Matrix is not compatible for multiplication with the current frame"")
        return H2OFrame._expr(expr=ExprNode(""x"", self, matrix))",https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/frame.py#L697-L707
tfidf_python_100_1.0,matrix multiply,python,"def __init__(self, matrix, explicit_hydrogens, kekulize):
        self.matrix = matrix
        self.explicit_hydrogens = explicit_hydrogens
        self.kekulize = kekulize",https://github.com/mordred-descriptor/mordred/blob/2848b088fd7b6735590242b5e22573babc724f10/mordred/_matrix_attributes.py#L29-L32
tfidf_python_100_1.0,matrix multiply,python,"def vector_from_matrix(matrix):
    N = matrix.shape[0]
    triu_idx = np.triu_indices(n=N, m=N, k=1)
    return matrix[triu_idx]",https://github.com/LCAV/pylocus/blob/c56a38c251d8a435caf4641a8ae6027ecba2c8c6/pylocus/basics.py#L166-L169
tfidf_python_100_1.0,matrix multiply,python,"def __init__(self, *matrixes):
        matrix = self.transformClass()
        if matrixes:
            if isinstance(matrixes[0], (int, float)):
                matrixes = [matrixes]
            for m in matrixes:
                matrix = matrix.transform(m)
        self.matrix = matrix",https://github.com/robotools/fontMath/blob/6abcb9d5a1ca19788fbde4418d7b5630c60990d8/Lib/fontMath/mathTransform.py#L100-L107
tfidf_python_100_1.0,matrix multiply,python,"def __init__(self, m):
        if isinstance(m, ndarray):
            self.matrix = m
        elif isinstance(m, Matrix):
            self.matrix = np_array(m.matrix)
        else:
            self.matrix = np_array(m)
        if len(self.matrix.shape) < 2:
            self.matrix = self.matrix.reshape((self.matrix.shape[0], 1))
        if len(self.matrix.shape) > 2:
            raise ValueError()
        super().__init__(self.matrix)",https://github.com/mabuchilab/QNET/blob/cc20d26dad78691d34c67173e5cd67dcac94208a/src/qnet/algebra/core/matrix_algebra.py#L35-L46
tfidf_python_100_1.0,matrix multiply,python,"def spiralOrder(matrix):
    return matrix and list(matrix.pop(0)) + spiralOrder(list(zip(*matrix))[::-1])",https://github.com/ManiacalLabs/BiblioPixelAnimations/blob/fba81f6b94f5265272a53f462ef013df1ccdb426/BiblioPixelAnimations/cube/wave_spiral.py#L4-L5
tfidf_python_100_1.0,matrix multiply,python,"def data_to_rcv(self, matrix):
        r = [x[0][0] for x in matrix['data']]
        c = [x[0][1] for x in matrix['data']]
        v = [x[1] for x in matrix['data']]

        return r, c, v",https://github.com/pjamesjoyce/lcopt/blob/3f1caca31fece4a3068a384900707e6d21d04597/lcopt/bw2_utils.py#L198-L203
tfidf_python_100_1.0,matrix multiply,python,"def to_sympy_column_matrix(matrix):
    """"""
    Converts a sympy matrix to a column matrix (i.e. transposes it if it was row matrix)
    Raises ValueError if matrix provided is not a vector
    :param matrix: a vector to be converted to column
    :return:
    """"""
    matrix = to_sympy_matrix(matrix)
    if matrix.cols == 1:
        return matrix
    elif matrix.rows == 1:
        return matrix.T
    else:
        raise ValueError('Cannot convert {0!r} to a column matrix'.format(matrix))",https://github.com/theosysbio/means/blob/fe164916a1d84ab2a4fa039871d38ccdf638b1db/src/means/util/sympyhelpers.py#L92-L105
tfidf_python_100_1.0,matrix multiply,python,"def evaluate_mask(matrix, matrix_size):
    """"""\
    Evaluates the provided `matrix` of a QR code.

    ISO/IEC 18004:2015(E) -- 7.8.3 Evaluation of data masking results (page 53)

    :param matrix: The matrix to evaluate
    :param matrix_size: The width (or height) of the matrix.
    :return int: The penalty score of the matrix.
    """"""
    return score_n1(matrix, matrix_size) + score_n2(matrix, matrix_size) \
           + score_n3(matrix, matrix_size) + score_n4(matrix, matrix_size)",https://github.com/heuer/segno/blob/64d912a2bd17d0b5ff3e8b5d37098edfc663c2b3/segno/encoder.py#L736-L747
tfidf_python_100_1.0,matrix multiply,python,"def is_undirected(matrix):
    """"""
    Determine if the matrix reprensents a directed graph

    :param matrix: The matrix to tested
    :returns: boolean
    """"""
    if isspmatrix(matrix):
        return sparse_allclose(matrix, matrix.transpose())
    
    return np.allclose(matrix, matrix.T)",https://github.com/GuyAllard/markov_clustering/blob/28787cf64ef06bf024ff915246008c767ea830cf/markov_clustering/modularity.py#L12-L22
tfidf_python_100_1.0,matrix multiply,python,"def matrix_undirected_unweighted(user):
    """"""
    Returns an undirected, unweighted matrix where an edge exists if the
    relationship is reciprocated.
    """"""
    matrix = matrix_undirected_weighted(user, interaction=None)
    for a, b in combinations(range(len(matrix)), 2):
        if matrix[a][b] is None or matrix[b][a] is None:
            continue

        if matrix[a][b] > 0 and matrix[b][a] > 0:
            matrix[a][b], matrix[b][a] = 1, 1

    return matrix",https://github.com/yvesalexandre/bandicoot/blob/73a658f6f17331541cf0b1547028db9b70e8d58a/bandicoot/network.py#L158-L171
tfidf_python_100_1.0,matrix multiply,python,"def update(self, t, dt):
        matrix = pg.Matrix()
        matrix = self.wasd.get_matrix(matrix)
        matrix = matrix.perspective(65, self.aspect, 0.01, 100)
        self.context1.matrix = matrix
        self.context1.camera_position = self.wasd.position
        self.context1.object_color = (1.0, 0.2, 0.0)
        self.context2.matrix = matrix
        self.context2.camera_position = self.wasd.position",https://github.com/fogleman/pg/blob/124ea3803c788b2c98c4f3a428e5d26842a67b58/examples/context.py#L12-L20
tfidf_python_100_1.0,print model summary,python,"def run(self, *args):
        """""" Print model summary """"""
        if self.source is None:
            self.model.summary()
        else:
            x_data, y_data = next(iter(self.source.train_loader()))
            self.model.summary(input_size=x_data.shape[1:])",https://github.com/MillionIntegrals/vel/blob/e0726e1f63742b728966ccae0c8b825ea0ba491a/vel/commands/summary_command.py#L10-L16
tfidf_python_100_1.0,print model summary,python,"def get_health(self, summary):
        if self.has_xcc:
            return self.immhandler.get_health(summary)
        return super(OEMHandler, self).get_health(summary)",https://github.com/openstack/pyghmi/blob/f710b1d30a8eed19a9e86f01f9351c737666f3e5/pyghmi/ipmi/oem/lenovo/handler.py#L972-L975
tfidf_python_100_1.0,print model summary,python,"def analyze_xml(xml):
    """"""Analyzes `file` against packtools' XMLValidator.
    """"""

    f = StringIO(xml)

    try:
        xml = packtools.XMLValidator.parse(f, sps_version='sps-1.4')
    except packtools.exceptions.PacktoolsError as e:
        logger.exception(e)
        summary = {}
        summary['dtd_is_valid'] = False
        summary['sps_is_valid'] = False
        summary['is_valid'] = False
        summary['parsing_error'] = True
        summary['dtd_errors'] = []
        summary['sps_errors'] = []
        return summary
    except XMLSyntaxError as e:
        logger.exception(e)
        summary = {}
        summary['dtd_is_valid'] = False
        summary['sps_is_valid'] = False
        summary['is_valid'] = False
        summary['parsing_error'] = True
        summary['dtd_errors'] = [e.message]
        summary['sps_errors'] = []
        return summary
    else:
        summary = summarize(xml)

        return summary",https://github.com/scieloorg/processing/blob/629b50b45ba7a176651cd3bfcdb441dab6fddfcc/export/xml_rsps.py#L87-L118
tfidf_python_100_1.0,print model summary,python,"def summary(self, indicator_data):
        """"""Return a summary value for any given indicator type.""""""
        summary = []
        for v in self._value_fields:
            summary.append(indicator_data.get(v, ''))
        return indicator_data.get('summary', ' : '.join(summary))",https://github.com/ThreatConnect-Inc/tcex/blob/dd4d7a1ef723af1561687120191886b9a2fd4b47/tcex/tcex_resources.py#L1406-L1411
tfidf_python_100_1.0,print model summary,python,"def update(self, wrongword):
        if wrongword in self.summary:
            self.summary[wrongword] += 1
        else:
            self.summary[wrongword] = 1",https://github.com/codespell-project/codespell/blob/126e7973ff964f201b7377c45e53a948f89b2c89/codespell_lib/_codespell.py#L101-L105
tfidf_python_100_1.0,print model summary,python,"def __fetch_summary(self):
        """"""Fetch summary""""""

        raw_summary = self.client.summary()
        summary = json.loads(raw_summary)
        summary['fetched_on'] = str(datetime_utcnow())

        yield summary",https://github.com/chaoss/grimoirelab-perceval-mozilla/blob/4514f8d3d609d3cb79d83c72d51fcc4b4a7daeb4/perceval/backends/mozilla/crates.py#L170-L177
tfidf_python_100_1.0,print model summary,python,"def generate_html_report(self, proj_name, proj_module = None):
        html_results = []               
        all_summary = HtmlReporter.get_summary(self.summary, proj_name = proj_name)
        
        for summary in all_summary:
            html_report = os.path.join(self.result_path, u""[{}]{}_{}.html"".format(FileSystemUtils.get_legal_filename(summary[""project_name""]),
                                                                                    FileSystemUtils.get_legal_filename(summary[""module_name""]), 
                                                                                DateTimeUtils.get_stamp_datetime_coherent(),
                                                                                ))        
            if proj_module == None:                
                html_results.append(HtmlReporter.render_html(html_report, summary))
                
            elif summary[""module_name""] == proj_module:
                html_results.append(HtmlReporter.render_html(html_report, summary))
                break
            else:
                summary = {}
        return html_results",https://github.com/RockFeng0/rtsf/blob/fbc0d57edaeca86418af3942472fcc6d3e9ce591/rtsf/p_report.py#L99-L116
tfidf_python_100_1.0,print model summary,python,"def summary(self, indicator_data):
        """"""Return a summary value for any given indicator type.""""""
        summary = None
        for v in self._value_fields:
            if indicator_data.get(v) is not None:
                summary = indicator_data.get(v)
                break
        return indicator_data.get('summary', summary)",https://github.com/ThreatConnect-Inc/tcex/blob/dd4d7a1ef723af1561687120191886b9a2fd4b47/tcex/tcex_resources.py#L1668-L1675
tfidf_python_100_1.0,print model summary,python,"def run(self, input: TraceGraph, summary: Summary) -> Tuple[RunSummary, Summary]:
        self.graph = input
        self.summary = summary

        self._prep_save()
        return self._save(), self.summary",https://github.com/facebook/pyre-check/blob/4a9604d943d28ef20238505a51acfb1f666328d7/sapp/sapp/database_saver.py#L44-L49
tfidf_python_100_1.0,print model summary,python,"def summary(self, summary):
        """"""
        Sets the summary of this DatasetPatchRequest.
        Long-form dataset summary (Markdown supported).

        :param summary: The summary of this DatasetPatchRequest.
        :type: str
        """"""
        if summary is not None and len(summary) > 25000:
            raise ValueError(""Invalid value for `summary`, length must be less than or equal to `25000`"")
        if summary is not None and len(summary) < 0:
            raise ValueError(""Invalid value for `summary`, length must be greater than or equal to `0`"")

        self._summary = summary",https://github.com/datadotworld/data.world-py/blob/ffaeb115f358731ab0b805b0c43b7ff2e3cf0a77/datadotworld/client/_swagger/models/dataset_patch_request.py#L147-L160
tfidf_python_100_1.0,print model summary,python,"def _scalar_summary(self, tf_name, value, step=None):
        summary = summary_pb2.Summary()
        summary.value.add(tag=tf_name, simple_value=value)
        return summary",https://github.com/TeamHG-Memex/tensorboard_logger/blob/93968344a471532530f035622118693845f32649/tensorboard_logger/tensorboard_logger.py#L228-L231
tfidf_python_100_1.0,print model summary,python,"def run(self, input: TraceGraph, summary: Summary) -> Tuple[TraceGraph, Summary]:
        if summary.get(""affected_files"") is None:
            summary[""graph""] = input  # used by ranker
            return input, summary

        log.info(""Trimming graph to affected files."")
        trimmed_graph = TrimmedTraceGraph(
            summary[""affected_files""], summary.get(""affected_issues_only"", False)
        )
        trimmed_graph.populate_from_trace_graph(input)

        summary[""graph""] = trimmed_graph  # used by ranker
        return trimmed_graph, summary",https://github.com/facebook/pyre-check/blob/4a9604d943d28ef20238505a51acfb1f666328d7/sapp/sapp/trim_trace_graph.py#L15-L27
tfidf_python_100_1.0,print model summary,python,"def get_results(self):
        """""" Returns the segmentation summary data.
        
        This data is normalized, to eliminate differences in what is stored
        for different types of segmentation analyses.
        
        The following fields are output:

        * has_template - True if the segmentation found template data.
        * has_complement - True if the segmentation found complement data.
        * first_sample_template - The first sample of the template data in
            the raw data. Only present if has_template is True.
        * duration_template - The duration (in samples) of the template
            data. Only present if has_template is True.
        * first_sample_complement - The first sample of the complement data
            in the raw data. Only present if has_complement is True.
        * duration_complement - The duration (in samples) of the complement
            data. Only present if has_complement is True.
            
        """"""
        summary = self._get_summary_data()
        if summary is None:
            results = {'has_template': False,
                       'has_complement': False}
        else:
            results = {}
            if 'has_template' in summary:
                results['has_template'] = bool(summary['has_template'])
            else:
                results['has_template'] = True if summary['num_temp'] > 0 else False
            if 'has_complement' in summary:
                results['has_complement'] = bool(summary['has_complement'])
            else:
                results['has_complement'] = True if summary['num_comp'] > 0 else False
            need_raw_info = False
            if results['has_template']:
                if 'start_index_temp' in summary:
                    summary['start_event_template'] = summary['start_index_temp']
                    summary['end_event_template'] = summary['end_index_temp']
                if 'first_sample_template' not in summary:
                    need_raw_info = True
            if results['has_complement']:
                if 'start_index_comp' in summary:
                    summary['start_event_complement'] = summary['start_index_comp']
                    summary['end_event_complement'] = summary['end_index_comp']
                if 'first_sample_complement' not in summary:
                    need_raw_info = True
            if need_raw_info:
                self._get_raw_info(summary)
            if results['has_template']:
                results['first_sample_template'] = summary['first_sample_template']
                results['duration_template'] = summary['duration_template']
                if 'start_event_template' in summary:
                    results['start_event_template'] = summary['start_event_template']
                    results['end_event_template'] = summary['end_event_template']
            if results['has_complement']:
                results['first_sample_complement'] = summary['first_sample_complement']
                results['duration_complement'] = summary['duration_complement']
                if 'start_event_complement' in summary:
                    results['start_event_complement'] = summary['start_event_complement']
                    results['end_event_complement'] = summary['end_event_complement']
        return results",https://github.com/nanoporetech/ont_fast5_api/blob/352b3903155fcf4f19234c4f429dcefaa6d6bc4a/ont_fast5_api/analysis_tools/segmentation.py#L16-L77
tfidf_python_100_1.0,print model summary,python,"def __init__(self, Summary):
        self._results = collections.defaultdict(Summary)
        self.Summary = Summary",https://github.com/alphatwirl/alphatwirl/blob/5138eeba6cd8a334ba52d6c2c022b33c61e3ba38/alphatwirl/summary/Summarizer.py#L11-L13
tfidf_python_100_1.0,print model summary,python,"def get_issue_summary(issue):
    summary = issue[u'fields'][u'summary']
    summary = summary.encode('ascii', 'ignore')
    return summary",https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/scripts/recent_changes.py#L24-L27
tfidf_python_100_1.0,print model summary,python,"def summary(self, input_size=None, hashsummary=False):
        """""" Print a model summary """"""

        if input_size is None:
            print(self)
            print(""-"" * 120)
            number = sum(p.numel() for p in self.model.parameters())
            print(""Number of model parameters: {:,}"".format(number))
            print(""-"" * 120)
        else:
            summary(self, input_size)

        if hashsummary:
            for idx, hashvalue in enumerate(self.hashsummary()):
                print(f""{idx}: {hashvalue}"")",https://github.com/MillionIntegrals/vel/blob/e0726e1f63742b728966ccae0c8b825ea0ba491a/vel/api/model.py#L37-L51
tfidf_python_100_1.0,print model summary,python,"def __init__(self, function, summary=None):
        self.function = function
        self.summary = summary if summary else ""(n/a)""",https://github.com/openvax/mhcflurry/blob/deb7c1629111254b484a2711619eb2347db36524/mhcflurry/select_allele_specific_models_command.py#L500-L502
tfidf_python_100_1.0,print model summary,python,"def title(self):
        name = self.name()
        summary  = self.summary()

        title_string = name

        if len(name) > 0 and len(summary) > 0:
            title_string += "": ""

        title_string += summary

        return title_string",https://github.com/oanda/v20-python/blob/f28192f4a31bce038cf6dfa302f5878bec192fe5/src/v20/base_entity.py#L150-L161
tfidf_python_100_1.0,print model summary,python,"def run(self, input: InputFiles, summary: Summary) -> Tuple[DictEntries, Summary]:
        inputfile, previous_inputfile = input

        return (
            self.analysis_output_to_dict_entries(
                inputfile,
                previous_inputfile,
                summary.get(""previous_issue_handles""),
                summary.get(""old_linemap_file""),
            ),
            summary,
        )",https://github.com/facebook/pyre-check/blob/4a9604d943d28ef20238505a51acfb1f666328d7/sapp/sapp/base_parser.py#L203-L214
tfidf_python_100_1.0,print model summary,python,"def head(self):
        """"""Print short description of the object.""""""
        summary = list()
        summary.append(""%s %s"" % (self.geotype, self.name) + ""\n"")
        summary.append("" - Metadata:"" + ""\n"")
        summary.append(
            ""\n"".join(self._get_metadata_as_string().split(""\n"")[:5]) + ""\n"")
        summary.append(""\n"")
        summary.append("" - Columns:"" + ""\n"")
        summary.append(self.columns.to_string() + ""\n"")
        summary.append(""\n"")
        summary.append("" - Table:"" + ""\n"")
        summary.append(
            ""\t"".join([""Index""] + self.table.columns.tolist()) + ""\n"")
        summary.append(self.table.head().to_string(header=None) + ""\n"")
        summary.append("" "" * 40 + ""..."" + "" "" * 40 + ""\n"")
        summary.append("" "" * 40 + ""..."" + "" "" * 40 + ""\n"")
        summary.append("" "" * 40 + ""..."" + "" "" * 40 + ""\n"")
        summary.append(self.table.tail().to_string(header=None) + ""\n"")
        return ""\n"".join([str(s) for s in summary])",https://github.com/guma44/GEOparse/blob/7ee8d5b8678d780382a6bf884afa69d2033f5ca0/GEOparse/GEOTypes.py#L267-L286
tfidf_python_100_1.0,unique elements,python,"def random_choices(self, elements=('a', 'b', 'c'), length=None):
        """"""
        Returns a list of random, non-unique elements from a passed object.

        If `elements` is a dictionary, the value will be used as
        a weighting element. For example::

            random_element({""{{variable_1}}"": 0.5, ""{{variable_2}}"": 0.2, ""{{variable_3}}"": 0.2, ""{{variable_4}}"": 0.1})

        will have the following distribution:
            * `variable_1`: 50% probability
            * `variable_2`: 20% probability
            * `variable_3`: 20% probability
            * `variable_4`: 10% probability

        """"""
        return self.random_elements(elements, length, unique=False)",https://github.com/joke2k/faker/blob/965824b61132e52d92d1a6ce470396dbbe01c96c/faker/providers/__init__.py#L207-L223
tfidf_python_100_1.0,unique elements,python,"def _extractElements(self):
        elements = []
        indexed_face_set = self._getIndexedFaceSet()
        if indexed_face_set is not None and 'coordIndex' in indexed_face_set:
            coordinate_indexes = indexed_face_set['coordIndex']
            elements = _convertToElementList(coordinate_indexes)

        return elements",https://github.com/ABI-Software/MeshParser/blob/08dc0ce7c44d0149b443261ff6d3708e28a928e7/src/meshparser/vrmlparser/parser.py#L64-L71
tfidf_python_100_1.0,unique elements,python,"def tuple2list(elements):

	elements = list(elements)
	for i in xrange(len(elements)):
		if isinstance(elements[i], tuple):
			elements[i] = tuple2list(elements[i])
	return elements",https://github.com/lltk/lltk/blob/d171de55c1b97695fddedf4b02401ae27bf1d634/lltk/utils.py#L43-L49
tfidf_python_100_1.0,unique elements,python,"def add_elements(self, elements):
        if isinstance(elements, list):
            self.elements.extend(elements)
        else:
            self.elements.append(elements)",https://github.com/conlini/bucket_filters/blob/8e69b67a4c2e6616013a348e75cc458adc4b2ffd/bucket_filter/bucket.py#L21-L25
tfidf_python_100_1.0,unique elements,python,"def __init__(self, elements, require_disconnect=False):
        self.elements = elements if isinstance(elements, list) else [elements]
        self.require_disconnect = require_disconnect",https://github.com/internap/fake-switches/blob/ea5f77f0c73493497fb43ce59f3c75b52ce9bac8/fake_switches/netconf/__init__.py#L52-L54
tfidf_python_100_1.0,unique elements,python,"def list2tuple(elements):
	''' '''

	for i in xrange(len(elements)):
		if isinstance(elements[i], list):
			elements[i] = list2tuple(elements[i])
	return tuple(elements)",https://github.com/lltk/lltk/blob/d171de55c1b97695fddedf4b02401ae27bf1d634/lltk/utils.py#L35-L41
tfidf_python_100_1.0,unique elements,python,"def d_cost(self, *elements):
        if not self._ident(*elements) and elements[0] in self.ungrouped:
            return self.group_cost
        return self.r_cost(*elements)",https://github.com/orsinium/textdistance/blob/34d2e40bb0b26efc03da80b63fd58ebbd3f2cdd7/textdistance/algorithms/phonetic.py#L127-L130
tfidf_python_100_1.0,unique elements,python,"def __check_validity_of_elements(self, elements):
        if elements:
            if not isinstance(elements, list):
                elements = [elements]
            for element in elements:
                self.__check_validity(element)",https://github.com/conlini/bucket_filters/blob/8e69b67a4c2e6616013a348e75cc458adc4b2ffd/bucket_filter/bucket.py#L14-L19
tfidf_python_100_1.0,unique elements,python,"def __init__(self, elements=None, top_element_style=None):
        self.elements = elements
        self.top_element_style = top_element_style",https://github.com/BlueHack-Core/blueforge/blob/ac40a888ee9c388638a8f312c51f7500b8891b6c/blueforge/apis/facebook/template.py#L42-L44
tfidf_python_100_1.0,unique elements,python,"def __init__(self, *elements):
        if len(elements) == 1:
            if isinstance(elements[0], Point):
                elements = [elements[0]]
            elif isinstance(elements[0], Iterable):
                elements = list(elements[0])
        else:
            elements = list(elements)
        self._elements = []

        for point in elements:
            self.append(point)",https://github.com/jtauber/sebastian/blob/4e460c3aeab332b45c74fe78e65e76ec87d5cfa8/sebastian/core/elements.py#L38-L49
tfidf_python_100_1.0,unique elements,python,"def __init__(self, elements: Iterable[Operation] = None) -> None:
        if elements is None:
            elements = []
        self.elements = list(elements)",https://github.com/rigetti/quantumflow/blob/13a66cabbe8aabf6e023cc675f4a4ebe6ccda8fb/quantumflow/circuits.py#L44-L47
tfidf_python_100_1.0,unique elements,python,"def compile_pattern(elements):
    if not elements:
        return None
    if isinstance(elements, regexp_type):
        return elements
    if isinstance(elements, str):
        elements = elements.split(',')
    return re.compile(u'|'.join([re.escape(x.lower()) for x in elements]), re.U)",https://github.com/usc-isi-i2/etk/blob/aab077c984ea20f5e8ae33af622fe11d3c4df866/etk/extractors/readability/readability.py#L82-L89
tfidf_python_100_1.0,unique elements,python,"def __init__(self, elements=None):
        elements = elements or []
        self._set = set(elements)
        self._list = elements",https://github.com/michael-lazar/rtv/blob/ccef2af042566ad384977028cf0bde01bc524dda/rtv/config.py#L136-L139
tfidf_python_100_1.0,unique elements,python,"def random_elements(self, elements=('a', 'b', 'c'), length=None, unique=False):
        fn = choices_distribution_unique if unique else choices_distribution

        if length is None:
            length = self.generator.random.randint(1, len(elements))

        if unique and length > len(elements):
            raise ValueError(
                ""Sample length cannot be longer than the number of unique elements to pick from."")

        if isinstance(elements, dict):
            choices = elements.keys()
            probabilities = elements.values()
        else:
            if unique:
                # shortcut
                return self.generator.random.sample(elements, length)
            choices = elements
            probabilities = [1.0 for _ in range(len(choices))]

        return fn(
            list(choices),
            list(probabilities),
            self.generator.random,
            length=length,
        )",https://github.com/joke2k/faker/blob/965824b61132e52d92d1a6ce470396dbbe01c96c/faker/providers/__init__.py#L180-L205
tfidf_python_100_1.0,unique elements,python,"def str2ocrasuite(ocrasuite_description):
    elements = ocrasuite_description.split(':')
    if len(elements) != 3:
        raise ValueError('Bad OcraSuite description %s' % ocrasuite_description)
    if elements[0] != OCRA_1:
        raise ValueError('Unsupported OCRA identifier %s' % elements[0])
    crypto_function = str2cryptofunction(elements[1])
    data_input = str2datainput(elements[2])
    return OcraSuite(ocrasuite_description, crypto_function, data_input)",https://github.com/bdauvergne/python-oath/blob/c37cd63880b39032b9ba69cd1516e6fb06923e46/oath/_ocra.py#L273-L281
tfidf_python_100_1.0,unique elements,python,"def __init__(self, name, elements, ternaries, parameters=None,
                 x=None, y=None, z=None):
        if (len(ternaries) == 3 and
            ternaries[0].elements[0] == ternaries[1].elements[0] and # A
            ternaries[0].elements[0] == ternaries[2].elements[0] and # A
            ternaries[0].elements[1] == ternaries[1].elements[1] and # B
            ternaries[0].elements[2] == ternaries[2].elements[1] and # C
            ternaries[1].elements[2] == ternaries[2].elements[2]):   # D
            # Type 1: AB_{x}C_{y}D_{1-x-y}
            # binaries = (AB, AC, AD)
            # ternaries = (ABC, ABD ,ACD)
            self._type = 1
            self._element_w = ternaries[0].elements[0]
            self._element_x = ternaries[0].elements[1]
            self._element_y = ternaries[0].elements[2]
            self._element_z = ternaries[1].elements[2]
            self.binaries = (ternaries[0].binaries[0],
                             ternaries[0].binaries[1],
                             ternaries[1].binaries[1],)
            calc_elements = (ternaries[0].elements[0],
                             ternaries[0].elements[1],
                             ternaries[0].elements[2],
                             ternaries[1].elements[2])
        elif (len(ternaries) == 3 and
              ternaries[0].elements[0] == ternaries[1].elements[0] and # A
              ternaries[0].elements[1] == ternaries[2].elements[0] and # B
              ternaries[1].elements[1] == ternaries[2].elements[1] and # C
              ternaries[0].elements[2] == ternaries[1].elements[2] and # D
              ternaries[0].elements[2] == ternaries[2].elements[2]):   # D
            # Type 2: A_{x}B_{y}C_{1-x-y}D
            # binaries = (AD, BD, CD)
            # ternaries = (ABD, ACD, BCD)
            self._type = 2
            self._element_x = ternaries[0].elements[0]
            self._element_y = ternaries[2].elements[0]
            self._element_z = ternaries[2].elements[1]
            self._element_w = ternaries[2].elements[2]
            self.binaries = (ternaries[0].binaries[0],
                             ternaries[0].binaries[1],
                             ternaries[1].binaries[1],)
            calc_elements = (ternaries[0].elements[0],
                             ternaries[2].elements[0],
                             ternaries[2].elements[1],
                             ternaries[2].elements[2])
        elif (len(ternaries) == 4 and
              ternaries[0].elements[0] == ternaries[1].elements[0] and # A
              ternaries[0].elements[0] == ternaries[2].elements[0] and # A
              ternaries[0].elements[1] == ternaries[1].elements[1] and # B
              ternaries[0].elements[1] == ternaries[3].elements[0] and # B
              ternaries[0].elements[2] == ternaries[2].elements[1] and # C
              ternaries[0].elements[2] == ternaries[3].elements[1] and # C
              ternaries[1].elements[2] == ternaries[2].elements[2] and # D
              ternaries[1].elements[2] == ternaries[3].elements[2]):   # D
            # Type 3: A_{x}B_{1-x}C_{y}D_{1-y}
            # binaries = (AC, AD, BC, BD)
            # ternaries = (ABC, ABD, ACD, BCD)
            self._type = 3
            self._element_x = ternaries[0].elements[0]
            self._element_1mx = ternaries[0].elements[1]
            self._element_y = ternaries[2].elements[1]
            self._element_1my = ternaries[2].elements[2]
            self.binaries = (ternaries[2].binaries[0],
                             ternaries[2].binaries[1],
                             ternaries[3].binaries[0],
                             ternaries[3].binaries[1],)
            calc_elements = (ternaries[0].elements[0],
                             ternaries[0].elements[1],
                             ternaries[2].elements[1],
                             ternaries[2].elements[2])
        else:
            raise ValueError()
        assert elements == calc_elements
        super(IIIVZincBlendeQuaternary, self).__init__(name, elements,
                                                    parameters=parameters)
        self.ternaries = ternaries
        if x is not None or y is not None or z is not None:
            self._xyz = self._parse_xyz(x, y, z)
        else:
            self._xyz = None",https://github.com/scott-maddox/openbandparams/blob/bc24e59187326bcb8948117434536082c9055777/src/openbandparams/iii_v_zinc_blende_quaternary.py#L29-L107
tfidf_python_100_1.0,unique elements,python,"def map(self, func):
        elements = func(self.elements)
        if not isinstance(elements, list):
            elements = [elements]
        return _ReadResult(
            elements,
            self.extra,
            self.messages)",https://github.com/mwilliamson/python-mammoth/blob/1ed67954b9612e1d764172e6c8abbddc05ebd436/mammoth/docx/body_xml.py#L545-L552
tfidf_python_100_1.0,unique elements,python,"def generate_permutations(elements, n):
    """"""
    Heap's algorithm for generating all n! permutations in a list
    https://en.wikipedia.org/wiki/Heap%27s_algorithm

    """"""
    c = [0] * n
    yield elements
    i = 0
    while i < n:
        if c[i] < i:
            if i % 2 == 0:
                elements[0], elements[i] = elements[i], elements[0]
            else:
                elements[c[i]], elements[i] = elements[i], elements[c[i]]
            yield elements
            c[i] += 1
            i = 0
        else:
            c[i] = 0
            i += 1",https://github.com/charnley/rmsd/blob/cd8af499fb63529a1b5b1f880fdb2dab2731544a/rmsd/calculate_rmsd.py#L383-L403
tfidf_python_100_1.0,unique elements,python,"def __reversed__(self):
        elements = self._queue.values()
        if not isinstance(elements, Sequence):
            elements = list(elements)
        return iter(elements)",https://github.com/ethereum/web3.py/blob/71b8bf03dc6d332dd97d8902a38ffab6f8b5a5ab/web3/datastructures.py#L211-L215
tfidf_python_100_1.0,unique elements,python,"def __init__(self, elements):
        """""" initialize the class with input elements

            elements should be dict converted from json,
            if not convert first by json.loads(elements)
        """"""
        if isinstance(elements, str):
            self.all_elements = json.loads(elements)
        else:  # elements is dict already
            self.all_elements = elements

        self.kws_ele, self.kws_bl = self.getAllKws()",https://github.com/archman/beamline/blob/417bc5dc13e754bc89d246427984590fced64d07/beamline/lattice.py#L427-L438
tfidf_python_100_1.0,extract data from html content,python,"def text_filter(html):
    if isinstance(html, list):
        html = """".join(html)
    ok, content = SoupOps.extract_text(html)
    if ok:
        return content
    else:
        raise RuntimeError(""Extract text failed"")",https://github.com/kibitzr/kibitzr/blob/749da312488f1dda1ed1093cf4c95aaac0a604f7/kibitzr/transformer/jinja_transform.py#L81-L88
tfidf_python_100_1.0,extract data from html content,python,"def extract_email_addresses(content, html=False):
    email_addresses = set()

    if content:
        if html:
            email_addresses = _extract_email_addresses_html(content)
        else:
            email_addresses = _extract_email_addresses_text(content)
    else:
        return email_addresses

    return email_addresses",https://github.com/csirtgadgets/csirtg-mail-py/blob/5fe71a8e4a911619bd8c1ef8e316b963e8813a5a/csirtg_mail/urls.py#L68-L79
tfidf_python_100_1.0,extract data from html content,python,"def extract(z, j):
    q, r, s, t = z
    return (q * j + r) // (s * j + t)",https://github.com/python/performance/blob/2a9524c0a5714e85106671bc61d750e800fe17db/performance/benchmarks/bm_pidigits.py#L35-L37
tfidf_python_100_1.0,extract data from html content,python,"def extract_content(html, encoding=None, as_blocks=False):
    if 'content' not in _LOADED_MODELS:
        _LOADED_MODELS['content'] = load_pickled_model(
            'kohlschuetter_readability_weninger_content_model.pkl.gz')
    return _LOADED_MODELS['content'].extract(html, encoding=encoding, as_blocks=as_blocks)",https://github.com/dragnet-org/dragnet/blob/532c9d9f28e5b1b57f3cabc708218d3863a16322/dragnet/__init__.py#L9-L13
tfidf_python_100_1.0,extract data from html content,python,"def extract(self, url=None, raw_html=None):
        ''' Extract the most likely article content from the html page

            Args:
                url (str): URL to pull and parse
                raw_html (str): String representation of the HTML page
            Returns:
                Article: Representation of the article contents \
                including other parsed and extracted metadata '''
        crawl_candidate = CrawlCandidate(self.config, url, raw_html)
        return self.__crawl(crawl_candidate)",https://github.com/goose3/goose3/blob/e6994b1b1826af2720a091d1bff5ca15594f558d/goose3/__init__.py#L103-L113
tfidf_python_100_1.0,extract data from html content,python,"def extract_btcs(content, html=False):
    btcs = set()

    from .constants import PYVERSION

    if PYVERSION == 3:
        content = str(content)

    if content:
        btcs = _extract_btcs_text(content)
    else:
        return btcs

    return btcs",https://github.com/csirtgadgets/csirtg-mail-py/blob/5fe71a8e4a911619bd8c1ef8e316b963e8813a5a/csirtg_mail/btc.py#L18-L31
tfidf_python_100_1.0,extract data from html content,python,"def set_content(self, data):
        """"""
            handle the content from the data
            :param data: contains the data from the provider
            :type data: dict
            :rtype: string
        """"""
        content = self._get_content(data, 'content')

        if content == '':
            content = self._get_content(data, 'summary_detail')

        if content == '':
            if data.get('description'):
                content = data.get('description')

        return content",https://github.com/push-things/django-th/blob/86c999d16bcf30b6224206e5b40824309834ac8c/django_th/services/services.py#L91-L107
tfidf_python_100_1.0,extract data from html content,python,"def _clean_html(html):
    """"""\
    Removes links (``<a href=""..."">...</a>``) from the provided HTML input.
    Further, it replaces ""&#x000A;"" with ``\n`` and removes ""Â¶"" from the texts.
    """"""
    content = html.replace(u'&#x000A;', u'\n').replace(u'Â¶', '')
    content = _LINK_PATTERN.sub(u'', content)
    content = _HTML_TAG_PATTERN.sub(u'', content)
    content = _BACKSLASH_PATTERN.sub(u'\n', content)
    return content",https://github.com/heuer/cablemap/blob/42066c8fc2972d237a2c35578e14525aaf705f38/cablemap.core/cablemap/core/reader.py#L210-L219
tfidf_python_100_1.0,extract data from html content,python,"def preview_html(filename, kernel=None, style=None):
    with open(filename) as html:
        content = html.read()
    return {'text/html': content, 'text/plain': dehtml(content)}",https://github.com/vatlab/SoS/blob/6b60ed0770916d135e17322e469520d778e9d4e7/src/sos/preview.py#L220-L223
tfidf_python_100_1.0,extract data from html content,python,"def plain(html):
	
	try: html = str(html)
	except:
		pass
	
	if html == ""None"": html = """"
	html = strip_javascript(html)
	html = strip_inline_css(html)
	html = strip_comments(html)
	html = strip_forms(html)
	html = strip_tags(html, columns="""")
	html = replace_entities(html)
	html = collapse_tabs(html)
	html = collapse_spaces(html)
	html = collapse_linebreaks(html)	
	
	return html",https://github.com/shoebot/shoebot/blob/d554c1765c1899fa25727c9fc6805d221585562b/lib/web/html.py#L228-L245
tfidf_python_100_1.0,extract data from html content,python,"def extract_abbreviations(fulltext):
    """"""Extract acronyms from the fulltext.

    :param fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    acronyms = {}
    for k, v in get_acronyms(fulltext).items():
        acronyms[KeywordToken(k, type='acronym')] = v
    return acronyms",https://github.com/inveniosoftware-contrib/invenio-classifier/blob/3c758cf34dca6bf0548e7da5de34e5f72e3b255e/invenio_classifier/engine.py#L72-L84
tfidf_python_100_1.0,extract data from html content,python,"def html(self):
        html = []
        if len(self):
            content = json.loads(self)
            for block in content['data']:
                template_name = 'sirtrevor/blocks/%s.html' % block['type']
                html.append(render_to_string(template_name, block['data']))
        return u''.join(html)",https://github.com/philippbosch/django-sirtrevor/blob/e08d32ed8706a6a18afb6fa6c7a68ab3e8f97e2b/sirtrevor/__init__.py#L8-L15
tfidf_python_100_1.0,extract data from html content,python,"def parse_content(self, content):
        content = ""\n"".join(list(content))
        try:
            inspect_data = unmarshal(content)
            self.data = inspect_data[0]
        except:
            self.data = {}",https://github.com/RedHatInsights/insights-core/blob/b57cbf8ed7c089672426ede0441e0a4f789ef4a1/insights/parsers/docker_inspect.py#L51-L57
tfidf_python_100_1.0,extract data from html content,python,"def fromroot(self, data, byteoffsets, local_entrystart, local_entrystop):
        return self.content.fromroot(data, byteoffsets, local_entrystart, local_entrystop)",https://github.com/scikit-hep/uproot/blob/fc406827e36ed87cfb1062806e118f53fd3a3b0a/uproot/interp/objects.py#L161-L162
tfidf_python_100_1.0,extract data from html content,python,"def clean_attributes(html):
    while htmlstrip.search(html):
        html = htmlstrip.sub('<\\1\\2>', html)
    return html",https://github.com/usc-isi-i2/etk/blob/aab077c984ea20f5e8ae33af622fe11d3c4df866/etk/extractors/readability/cleaners.py#L17-L20
tfidf_python_100_1.0,extract data from html content,python,"def html_to_rgb(html):
  """"""Convert the HTML color to (r, g, b).

  Parameters:
    :html:
      the HTML definition of the color (#RRGGBB or #RGB or a color name).

  Returns:
    The color as an (r, g, b) tuple in the range:
    r[0...1],
    g[0...1],
    b[0...1]

  Throws:
    :ValueError:
      If html is neither a known color name or a hexadecimal RGB
      representation.

  >>> '(%g, %g, %g)' % html_to_rgb('#ff8000')
  '(1, 0.501961, 0)'
  >>> '(%g, %g, %g)' % html_to_rgb('ff8000')
  '(1, 0.501961, 0)'
  >>> '(%g, %g, %g)' % html_to_rgb('#f60')
  '(1, 0.4, 0)'
  >>> '(%g, %g, %g)' % html_to_rgb('f60')
  '(1, 0.4, 0)'
  >>> '(%g, %g, %g)' % html_to_rgb('lemonchiffon')
  '(1, 0.980392, 0.803922)'

  """"""
  html = html.strip().lower()
  if html[0]=='#':
    html = html[1:]
  elif html in NAMED_COLOR:
    html = NAMED_COLOR[html][1:]

  if len(html)==6:
    rgb = html[:2], html[2:4], html[4:]
  elif len(html)==3:
    rgb = ['%c%c' % (v,v) for v in html]
  else:
    raise ValueError(""input #%s is not in #RRGGBB format"" % html)

  return tuple(((int(n, 16) / 255.0) for n in rgb))",https://github.com/xav/Grapefruit/blob/b3d88375be727a3a1ec5839fbc462e0e8e0836e4/grapefruit.py#L869-L912
tfidf_python_100_1.0,extract data from html content,python,"def convert_html_to_plain_text(html):
    if not html:
        return html

    if six.PY2:
        html = html.decode('utf-8')

    html = decode_html_entities(html)
    # Replace HTML break rules with new lines
    html = html.replace('<br>', '\n')
    # Remove multiple spaces
    html = re.sub(' +', ' ', html)

    return html",https://github.com/markfinger/django-node/blob/a2f56bf027fd3c4cbc6a0213881922a50acae1d6/django_node/utils.py#L196-L209
tfidf_python_100_1.0,extract data from html content,python,"def HtmlToRgb(html):
    '''Convert the HTML color to (r, g, b).

    Parameters:
      :html:
        the HTML definition of the color (#RRGGBB or #RGB or a color name).

    Returns:
      The color as an (r, g, b) tuple in the range:
      r[0...1],
      g[0...1],
      b[0...1]

    Throws:
      :ValueError:
        If html is neither a known color name or a hexadecimal RGB
        representation.

    >>> '(%g, %g, %g)' % Color.HtmlToRgb('#ff8000')
    '(1, 0.501961, 0)'
    >>> '(%g, %g, %g)' % Color.HtmlToRgb('ff8000')
    '(1, 0.501961, 0)'
    >>> '(%g, %g, %g)' % Color.HtmlToRgb('#f60')
    '(1, 0.4, 0)'
    >>> '(%g, %g, %g)' % Color.HtmlToRgb('f60')
    '(1, 0.4, 0)'
    >>> '(%g, %g, %g)' % Color.HtmlToRgb('lemonchiffon')
    '(1, 0.980392, 0.803922)'

    '''
    html = html.strip().lower()
    if html[0]=='#':
      html = html[1:]
    elif html in Color.NAMED_COLOR:
      html = Color.NAMED_COLOR[html][1:]

    if len(html)==6:
      rgb = html[:2], html[2:4], html[4:]
    elif len(html)==3:
      rgb = ['%c%c' % (v,v) for v in html]
    else:
      raise ValueError('input #%s is not in #RRGGBB format' % html)

    return tuple(((int(n, 16) / 255.0) for n in rgb))",https://github.com/jsvine/spectra/blob/2269a0ae9b5923154b15bd661fb81179608f7ec2/spectra/grapefruit.py#L955-L998
tfidf_python_100_1.0,extract data from html content,python,"def __init__(self, html):
        if isinstance(html, (str, bytes)):
            self.html = fromstring(html)

        elif isinstance(html, HtmlWrapper):
            self.html = html.html

        elif isinstance(html, HtmlElement):
            self.html = html

        elif isinstance(html, BS4_TYPES):
            self.html = fromstring(str(html))

        else:
            msg = ""Object of type %s not compatible with HtmlWrapper"" % str(type(html))

            raise TypeError(msg)",https://github.com/thismachinechills/html_wrapper/blob/bef8c93f99bdbb4646d96845fed4e2ddf9213947/html_wrapper/wrapper.py#L17-L33
tfidf_python_100_1.0,extract data from html content,python,"def regex_findall_variables(content):
    """""" extract all variable names from content, which is in format $variable

    Args:
        content (str): string content

    Returns:
        list: variables list extracted from string content

    Examples:
        >>> regex_findall_variables(""$variable"")
        [""variable""]

        >>> regex_findall_variables(""/blog/$postid"")
        [""postid""]

        >>> regex_findall_variables(""/$var1/$var2"")
        [""var1"", ""var2""]

        >>> regex_findall_variables(""abc"")
        []

    """"""
    try:
        vars_list = []
        for var_tuple in variable_regex_compile.findall(content):
            vars_list.append(
                var_tuple[0] or var_tuple[1]
            )
        return vars_list
    except TypeError:
        return []",https://github.com/HttpRunner/HttpRunner/blob/f259551bf9c8ba905eae5c1afcf2efea20ae0871/httprunner/parser.py#L62-L93
tfidf_python_100_1.0,heatmap from 3d coordinates,python,"def _format_heatmap(self, filename, heatmap, execution_count):
        """"""Formats heatmap for UI.""""""
        with open(filename) as src_file:
            file_source = src_file.read().split('\n')
            skip_map = self._calc_skips(heatmap, len(file_source))
        run_time = sum(time for time in heatmap.values())
        return {
            'name': filename,
            'heatmap': heatmap,
            'executionCount': execution_count,
            'srcCode': self._skip_lines(file_source, skip_map),
            'runTime': run_time
        }",https://github.com/nvdv/vprof/blob/4c3ff78f8920ab10cb9c00b14143452aa09ff6bb/vprof/code_heatmap.py#L176-L188
tfidf_python_100_1.0,heatmap from 3d coordinates,python,"def colorize(img, heatmap):
    """""" img: bgr, [0,255]
        heatmap: [0,1]
    """"""
    heatmap = viz.intensity_to_rgb(heatmap, cmap='jet')[:, :, ::-1]
    return img * 0.5 + heatmap * 0.5",https://github.com/tensorpack/tensorpack/blob/d7a13cb74c9066bc791d7aafc3b744b60ee79a9f/examples/CaffeModels/load-cpm.py#L27-L32
tfidf_python_100_1.0,heatmap from 3d coordinates,python,"def is_radial(cls, heatmap):
        heatmap = heatmap.last if isinstance(heatmap, HoloMap) else heatmap
        opts = cls.lookup_options(heatmap, 'plot').options
        return ((any(o in opts for o in ('start_angle', 'radius_inner', 'radius_outer'))
                 and not (opts.get('radial') == False)) or opts.get('radial', False))",https://github.com/pyviz/holoviews/blob/ae0dd2f3de448b0ca5e9065aabd6ef8d84c7e655/holoviews/plotting/bokeh/heatmap.py#L60-L64
tfidf_python_100_1.0,heatmap from 3d coordinates,python,"def heatmap(z, x=None, y=None, colorscale='Viridis'):
    """"""Create a heatmap.

    Parameters
    ----------
    z : TODO
    x : TODO, optional
    y : TODO, optional
    colorscale : TODO, optional

    Returns
    -------
    Chart


    """"""
    z = np.atleast_1d(z)
    data = [go.Heatmap(z=z, x=x, y=y, colorscale=colorscale)]
    return Chart(data=data)",https://github.com/jwkvam/plotlywrapper/blob/762b42912e824fecb1212c186900f2ebdd0ab12b/plotlywrapper.py#L832-L850
tfidf_python_100_1.0,heatmap from 3d coordinates,python,"def _profile_package(self):
        """"""Calculates heatmap for package.""""""
        with _CodeHeatmapCalculator() as prof:
            try:
                runpy.run_path(self._run_object, run_name='__main__')
            except SystemExit:
                pass

        heatmaps = []
        for filename, heatmap in prof.heatmap.items():
            if os.path.isfile(filename):
                heatmaps.append(
                    self._format_heatmap(
                        filename, heatmap, prof.execution_count[filename]))

        run_time = sum(heatmap['runTime'] for heatmap in heatmaps)
        return {
            'objectName': self._run_object,
            'runTime': run_time,
            'heatmaps': heatmaps
        }",https://github.com/nvdv/vprof/blob/4c3ff78f8920ab10cb9c00b14143452aa09ff6bb/vprof/code_heatmap.py#L150-L170
tfidf_python_100_1.0,heatmap from 3d coordinates,python,"def _profile_module(self):
        """"""Calculates heatmap for module.""""""
        with open(self._run_object, 'r') as srcfile:
            src_code = srcfile.read()
            code = compile(src_code, self._run_object, 'exec')
        try:
            with _CodeHeatmapCalculator() as prof:
                exec(code, self._globs, None)
        except SystemExit:
            pass

        heatmaps = []
        for filename, heatmap in prof.heatmap.items():
            if os.path.isfile(filename):
                heatmaps.append(
                    self._format_heatmap(
                        filename, heatmap, prof.execution_count[filename]))

        run_time = sum(heatmap['runTime'] for heatmap in heatmaps)
        return {
            'objectName': self._run_object,
            'runTime': run_time,
            'heatmaps': heatmaps
        }",https://github.com/nvdv/vprof/blob/4c3ff78f8920ab10cb9c00b14143452aa09ff6bb/vprof/code_heatmap.py#L190-L213
tfidf_python_100_1.0,heatmap from 3d coordinates,python,"def get_img_heatmap(orig_img, activation_map):
    """"""Draw a heatmap on top of the original image using intensities from activation_map""""""
    heatmap = cv2.applyColorMap(activation_map, cv2.COLORMAP_COOL)
    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)
    img_heatmap = np.float32(heatmap) + np.float32(orig_img)
    img_heatmap = img_heatmap / np.max(img_heatmap)
    img_heatmap *= 255
    return img_heatmap.astype(int)",https://github.com/apache/incubator-mxnet/blob/1af29e9c060a4c7d60eeaacba32afdb9a7775ba7/docs/tutorial_utils/vision/cnn_visualization/gradcam.py#L225-L232
tfidf_python_100_1.0,heatmap from 3d coordinates,python,"def paint(self):
        """"""
        Renders a javascript snippet suitable for use as a mapbox-gl heatmap paint entry

        Returns:
            A dict that can be converted to a mapbox-gl javascript paint snippet
        """"""
        snippet = {
            'heatmap-radius': VectorStyle.get_style_value(self.radius),
            'heatmap-opacity': VectorStyle.get_style_value(self.opacity),
            'heatmap-color': VectorStyle.get_style_value(self.color),
            'heatmap-intensity': VectorStyle.get_style_value(self.intensity),
            'heatmap-weight': VectorStyle.get_style_value(self.weight)
        }

        return snippet",https://github.com/DigitalGlobe/gbdxtools/blob/def62f8f2d77b168aa2bd115290aaa0f9a08a4bb/gbdxtools/vector_styles.py#L278-L293
tfidf_python_100_1.0,heatmap from 3d coordinates,python,"def profile_function(self):
        """"""Calculates heatmap for function.""""""
        with _CodeHeatmapCalculator() as prof:
            result = self._run_object(*self._run_args, **self._run_kwargs)
        code_lines, start_line = inspect.getsourcelines(self._run_object)

        source_lines = []
        for line in code_lines:
            source_lines.append(('line', start_line, line))
            start_line += 1

        filename = os.path.abspath(inspect.getsourcefile(self._run_object))
        heatmap = prof.heatmap[filename]
        run_time = sum(time for time in heatmap.values())
        return {
            'objectName': self._object_name,
            'runTime': run_time,
            'result': result,
            'timestamp': int(time.time()),
            'heatmaps': [{
                'name': self._object_name,
                'heatmap': heatmap,
                'executionCount': prof.execution_count[filename],
                'srcCode': source_lines,
                'runTime': run_time
            }]
        }",https://github.com/nvdv/vprof/blob/4c3ff78f8920ab10cb9c00b14143452aa09ff6bb/vprof/code_heatmap.py#L219-L245
tfidf_python_100_1.0,heatmap from 3d coordinates,python,"def heatmap_base() -> HeatMap:
    value = [[i, j, random.randint(0, 50)] for i in range(24) for j in range(7)]
    c = (
        HeatMap()
        .add_xaxis(Faker.clock)
        .add_yaxis(""series0"", Faker.week, value)
        .set_global_opts(
            title_opts=opts.TitleOpts(title=""HeatMap-åºæ¬ç¤ºä¾""),
            visualmap_opts=opts.VisualMapOpts(),
        )
    )
    return c",https://github.com/pyecharts/pyecharts/blob/02050acb0e94bb9453b88a25028de7a0ce23f125/example/heatmap_example.py#L12-L23
tfidf_python_100_1.0,heatmap from 3d coordinates,python,"def _to_geojson_coordinates(self, dimz):
        coordinates = [self.x, self.y]
        if dimz:
            coordinates.append(self.z)
        return coordinates",https://github.com/bosth/plpygis/blob/9469cc469df4c8cd407de158903d5465cda804ea/plpygis/geometry.py#L579-L583
tfidf_python_100_1.0,heatmap from 3d coordinates,python,"def _get_coordinates_for_dataset_keys(self, dsids):
        """"""Get all coordinates.""""""
        coordinates = {}
        for dsid in dsids:
            cids = self._get_coordinates_for_dataset_key(dsid)
            coordinates.setdefault(dsid, []).extend(cids)
        return coordinates",https://github.com/pytroll/satpy/blob/1f21d20ac686b745fb0da9b4030d139893e066dd/satpy/readers/yaml_reader.py#L651-L657
tfidf_python_100_1.0,heatmap from 3d coordinates,python,"def eta_u(self, u, coordinates):
        return np.sqrt((u * ((coordinates[1] ** 2) + (coordinates[0] ** 2 / (1 - (1 - self.axis_ratio ** 2) * u)))))",https://github.com/Jammy2211/PyAutoLens/blob/91e50369c7a9c048c83d217625578b72423cd5a7/autolens/model/profiles/geometry_profiles.py#L366-L367
tfidf_python_100_1.0,heatmap from 3d coordinates,python,"def match_template(template, image, options=None):
    """"""
    Multi channel template matching using simple correlation distance

    :param template: Template image
    :param image: Search image
    :param options: Other options:
        - distance: Distance measure to use. Default: 'correlation'
        - normalize: Heatmap values will be in the range of 0 to 1. Default: True
        - retain_size: Whether to retain the same size as input image. Default: True
    :return: Heatmap
    """"""
    # If the input has max of 3 channels, use the faster OpenCV matching
    if len(image.shape) <= 3 and image.shape[2] <= 3:
        return match_template_opencv(template, image, options)

    op = _DEF_TM_OPT.copy()
    if options is not None:
        op.update(options)

    template = img_utils.gray3(template)
    image = img_utils.gray3(image)

    h, w, d = template.shape
    im_h, im_w = image.shape[:2]

    template_v = template.flatten()

    heatmap = np.zeros((im_h - h, im_w - w))
    for col in range(0, im_w - w):
        for row in range(0, im_h - h):
            cropped_im = image[row:row + h, col:col + w, :]
            cropped_v = cropped_im.flatten()

            if op['distance'] == 'euclidean':
                heatmap[row, col] = scipy.spatial.distance.euclidean(template_v, cropped_v)
            elif op['distance'] == 'correlation':
                heatmap[row, col] = scipy.spatial.distance.correlation(template_v, cropped_v)

    # normalize
    if op['normalize']:
        heatmap /= heatmap.max()

    # size
    if op['retain_size']:
        hmap = np.ones(image.shape[:2]) * heatmap.max()
        h, w = heatmap.shape
        hmap[:h, :w] = heatmap
        heatmap = hmap

    return heatmap",https://github.com/gmichaeljaison/cv-utils/blob/a8251c870165a7428d8c468a6436aa41d0cf7c09/cv_utils/template_matching.py#L83-L133
tfidf_python_100_1.0,heatmap from 3d coordinates,python,"def surface(self, kdims=None, vdims=None, groupby=None, **kwargs):
        heatmap = self.heatmap(kdims, vdims, **kwargs)
        return Surface(heatmap.data, **dict(self._table.get_param_values(onlychanged=True)))",https://github.com/pyviz/holoviews/blob/ae0dd2f3de448b0ca5e9065aabd6ef8d84c7e655/holoviews/element/__init__.py#L81-L83
tfidf_python_100_1.0,heatmap from 3d coordinates,python,"def match_template_opencv(template, image, options):
    """"""
    Match template using OpenCV template matching implementation.
        Limited by number of channels as maximum of 3.
        Suitable for direct RGB or Gray-scale matching

    :param options: Other options:
        - distance: Distance measure to use. (euclidean | correlation | ccoeff).
            Default: 'correlation'
        - normalize: Heatmap values will be in the range of 0 to 1. Default: True
        - retain_size: Whether to retain the same size as input image. Default: True
    :return: Heatmap
    """"""
    # if image has more than 3 channels, use own implementation
    if len(image.shape) > 3:
        return match_template(template, image, options)

    op = _DEF_TM_OPT.copy()
    if options is not None:
        op.update(options)

    method = cv.TM_CCORR_NORMED
    if op['normalize'] and op['distance'] == 'euclidean':
        method = cv.TM_SQDIFF_NORMED
    elif op['distance'] == 'euclidean':
        method = cv.TM_SQDIFF
    elif op['normalize'] and op['distance'] == 'ccoeff':
        method = cv.TM_CCOEFF_NORMED
    elif op['distance'] == 'ccoeff':
        method = cv.TM_CCOEFF
    elif not op['normalize'] and op['distance'] == 'correlation':
        method = cv.TM_CCORR

    heatmap = cv.matchTemplate(image, template, method)

    # make minimum peak heatmap
    if method not in [cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED]:
        heatmap = heatmap.max() - heatmap

    if op['normalize']:
        heatmap /= heatmap.max()

    # size
    if op['retain_size']:
        hmap = np.ones(image.shape[:2]) * heatmap.max()
        h, w = heatmap.shape
        hmap[:h, :w] = heatmap
        heatmap = hmap

    return heatmap",https://github.com/gmichaeljaison/cv-utils/blob/a8251c870165a7428d8c468a6436aa41d0cf7c09/cv_utils/template_matching.py#L136-L185
tfidf_python_100_1.0,heatmap from 3d coordinates,python,"def flip_coordinates(coordinates, dimensions):
    coordinates = unpack_coordinates(coordinates)
    x, y, z = coordinates
    x_size, y_size, z_size = unpack_coordinates(dimensions)
    return (x, y_size - y, z_size - z)",https://github.com/Opentrons/opentrons/blob/a7c15cc2636ecb64ab56c7edc1d8a57163aaeadf/api/src/opentrons/helpers/helpers.py#L16-L20
tfidf_python_100_1.0,heatmap from 3d coordinates,python,"def coordinates(self):
        source_channels = list(self.source_channels)
        source_channels.sort()
        coordinates = [[v.coordinates.get(ch) for v in self.verts] for ch in source_channels]
        coordinates = zip(*coordinates)
        return source_channels, coordinates",https://github.com/eyurtsev/FlowCytometryTools/blob/4355632508b875273d68c7e2972c17668bcf7b40/FlowCytometryTools/gui/fc_widget.py#L399-L404
tfidf_python_100_1.0,heatmap from 3d coordinates,python,"def unpack_coordinates(coordinates):
    if not isinstance(coordinates, tuple):
        coordinates = tuple([coordinates[axis] for axis in 'xyz'])
    return coordinates",https://github.com/Opentrons/opentrons/blob/a7c15cc2636ecb64ab56c7edc1d8a57163aaeadf/api/src/opentrons/helpers/helpers.py#L10-L13
tfidf_python_100_1.0,heatmap from 3d coordinates,python,"def raster(self, kdims=None, vdims=None, groupby=None, **kwargs):
        heatmap = self.heatmap(kdims, vdims, **kwargs)
        return Raster(heatmap.data, **dict(self._element.get_param_values(onlychanged=True)))",https://github.com/pyviz/holoviews/blob/ae0dd2f3de448b0ca5e9065aabd6ef8d84c7e655/holoviews/element/__init__.py#L65-L67
tfidf_python_100_1.0,get all parents of xml node,python,"def _follow_children(self,node,parents=[]):
      parents = parents[:]
      if node.id in parents:
         while parents[0] != node.id: parents.pop(0)
         return parents
      parents.append(node.id)
      if node.id not in self._p2c: 
         return []
      children = [self._nodes[x] for x in self._p2c[node.id].keys()]
      for c in children:
         v = self._follow_children(c,parents[:])
         if len(v) > 0: return v
      return []",https://github.com/jason-weirather/py-seq-tools/blob/f642c2c73ffef2acc83656a78059a476fc734ca1/seqtools/graph/__init__.py#L184-L196
tfidf_python_100_1.0,get all parents of xml node,python,"def parents( self, node ):
        """"""Retrieve/calculate the set of parents for the given node""""""
        if 'index' in node:
            index = node['index']()
            parents = list(meliaeloader.children( node, index, 'parents' ))
            return parents 
        return []",https://github.com/lrq3000/pyFileFixity/blob/fd5ef23bb13835faf1e3baa773619b86a1cc9bdf/pyFileFixity/lib/profilers/visual/runsnakerun/meliaeadapter.py#L86-L92
tfidf_python_100_1.0,get all parents of xml node,python,"def check_integrity(self, parents=None):
        """"""
        Checks in-memory changeset's integrity. Also, sets parents if not
        already set.

        :raises CommitError: if any error occurs (i.e.
          ``NodeDoesNotExistError``).
        """"""
        if not self.parents:
            parents = parents or []
            if len(parents) == 0:
                try:
                    parents = [self.repository.get_changeset(), None]
                except EmptyRepositoryError:
                    parents = [None, None]
            elif len(parents) == 1:
                parents += [None]
            self.parents = parents

        # Local parents, only if not None
        parents = [p for p in self.parents if p]

        # Check nodes marked as added
        for p in parents:
            for node in self.added:
                try:
                    p.get_node(node.path)
                except NodeDoesNotExistError:
                    pass
                else:
                    raise NodeAlreadyExistsError(""Node at %s already exists ""
                        ""at %s"" % (node.path, p))

        # Check nodes marked as changed
        missing = set(self.changed)
        not_changed = set(self.changed)
        if self.changed and not parents:
            raise NodeDoesNotExistError(str(self.changed[0].path))
        for p in parents:
            for node in self.changed:
                try:
                    old = p.get_node(node.path)
                    missing.remove(node)
                    if old.content != node.content:
                        not_changed.remove(node)
                except NodeDoesNotExistError:
                    pass
        if self.changed and missing:
            raise NodeDoesNotExistError(""Node at %s is missing ""
                ""(parents: %s)"" % (node.path, parents))

        if self.changed and not_changed:
            raise NodeNotChangedError(""Node at %s wasn't actually changed ""
                ""since parents' changesets: %s"" % (not_changed.pop().path,
                    parents)
            )

        # Check nodes marked as removed
        if self.removed and not parents:
            raise NodeDoesNotExistError(""Cannot remove node at %s as there ""
                ""were no parents specified"" % self.removed[0].path)
        really_removed = set()
        for p in parents:
            for node in self.removed:
                try:
                    p.get_node(node.path)
                    really_removed.add(node)
                except ChangesetError:
                    pass
        not_removed = set(self.removed) - really_removed
        if not_removed:
            raise NodeDoesNotExistError(""Cannot remove node at %s from ""
                ""following parents: %s"" % (not_removed[0], parents))",https://github.com/codeinn/vcs/blob/e6cd94188e9c36d273411bf3adc0584ac6ab92a0/vcs/backends/base.py#L845-L917
tfidf_python_100_1.0,get all parents of xml node,python,"def parents(self, node, relations=None):
        """"""
        Return all direct parents of specified node.

        Wraps networkx by default.

        Arguments
        ---------
        node: string
           identifier for node in ontology
        relations: list of strings
           list of relation (object property) IDs used to filter

        """"""
        g = self.get_graph()
        if node in g:
            parents = list(g.predecessors(node))
            if relations is None:
                return parents
            else:
                rset = set(relations)
                return [p for p in parents if len(self.child_parent_relations(node, p, graph=g).intersection(rset)) > 0 ]
        else:
            return []",https://github.com/biolink/ontobio/blob/4e512a7831cfe6bc1b32f2c3be2ba41bc5cf7345/ontobio/ontol.py#L400-L423
tfidf_python_100_1.0,get all parents of xml node,python,"def _find_interesting_parent(self, commit_ref):
        while True:
            if commit_ref not in self.squashed_commits:
                return commit_ref
            parents = self.parents.get(commit_ref)
            if not parents:
                return None
            commit_ref = parents[0]",https://github.com/jelmer/python-fastimport/blob/5cef9e037b7d7b37f58f522ac9ea4e343e6a1dff/fastimport/processors/filter_processor.py#L214-L221
tfidf_python_100_1.0,get all parents of xml node,python,"def __deserialize_from_xml(self, xml):
        self.time_index = xml.get(""timeIndex"")
        for node in xml:
            if (node.tag == namespace + ""MediaReference""):
                self.media_reference = MediaReference(node, self)",https://github.com/varunsrin/one-py/blob/8fcf021bcf776a1802a69f50dfd180daf83536ff/onepy/onepy.py#L476-L480
tfidf_python_100_1.0,get all parents of xml node,python,"def __deserialize_from_xml(self, xml):
        self.recognized_text = xml.get(""recognizedText"")
        self.x = xml.get(""x"")
        self.y = xml.get(""y"")
        self.ink_origin_x = xml.get(""inkOriginX"")
        self.ink_origin_y = xml.get(""inkOriginY"")
        self.width = xml.get(""width"")
        self.height = xml.get(""height"")
            
        for node in xml:
            if (node.tag == namespace + ""CallbackID""):
                self.callback_id = node.get(""callbackID"")
            elif (node.tag == namespace + ""Data""):
                self.data = node.text",https://github.com/varunsrin/one-py/blob/8fcf021bcf776a1802a69f50dfd180daf83536ff/onepy/onepy.py#L532-L545
tfidf_python_100_1.0,get all parents of xml node,python,"def __deserialize_from_xml(self, xml):
        self.format = xml.get(""format"")
        self.original_page_number = xml.get(""originalPageNumber"")
        self.last_modified_time = xml.get(""lastModifiedTime"")
        self.id = xml.get(""objectID"")
        for node in xml:
            if (node.tag == namespace + ""CallbackID""):
                self.callback_id = node.get(""callbackID"")
            elif (node.tag == namespace + ""Data""):
                if (node.text != None):
                    self.data = node.text",https://github.com/varunsrin/one-py/blob/8fcf021bcf776a1802a69f50dfd180daf83536ff/onepy/onepy.py#L567-L577
tfidf_python_100_1.0,get all parents of xml node,python,"def parents(self):
        """"""~TermList: the parents of all the terms in the list.
        """"""
        return TermList(unique_everseen(
            y for x in self for y in x.parents
        ))",https://github.com/althonos/pronto/blob/a768adcba19fb34f26f67cde4a03d317f932c274/pronto/term.py#L406-L411
tfidf_python_100_1.0,get all parents of xml node,python,"def parents(self):
        """"""A list of all the parent nodes of this node, back to the root.
        parents[0] is the root, and parents[-1] is the immediate parent.
        """"""
        try:
            parents = self.parent.parents
            parents.append(self.parent)
        except AttributeError:
            parents = []
        return parents",https://github.com/NJDFan/ctypes-bitfield/blob/ae76b1dcfef7ecc90bd1900735b94ddee41a6376/bitfield/walk.py#L152-L161
tfidf_python_100_1.0,get all parents of xml node,python,"def _import_parents_from_xml(self, xml):
        parents = xml.iterfind('parent')
        for p in parents:
            for o in p:
                # Store a tuple of orgid, identifier
                self._parents.append( o.attrib['frameid'] )",https://github.com/mfitzp/biocyc/blob/2fe81971687e4dcf1fcf869af0e7b3549be535b1/biocyc/biocyc.py#L566-L571
tfidf_python_100_1.0,get all parents of xml node,python,"def get_edges(self, node):
        return [(p, c) for c, p in self.parents.items() if p == node or c == node]",https://github.com/klahnakoski/pyLibrary/blob/fa2dcbc48fda8d26999baef400e9a98149e0b982/mo_graphs/tree_graph.py#L46-L47
tfidf_python_100_1.0,get all parents of xml node,python,"def render(self, xml, context):
        if xml:
            self.xml = xml
            for node in fldChar.find(self.root):
                node.text = context.get(node.name, node.name)

            return self.xml
        else:
            return xml",https://github.com/backbohne/docx-xslt/blob/d4cc76776a75b8213660c3c1717d42afe5189e15/docxxslt/engines.py#L157-L165
tfidf_python_100_1.0,get all parents of xml node,python,"def parents(self) -> List[str]:
        """"""
        Return the list of parents SHAs.

        :return: List[str] parents
        """"""
        parents = []
        for p in self._c_object.parents:
            parents.append(p.hexsha)
        return parents",https://github.com/ishepard/pydriller/blob/71facb32afa085d5ddf0081beba34d00d57b8080/pydriller/domain/commit.py#L347-L356
tfidf_python_100_1.0,get all parents of xml node,python,"def __deserialize_from_xml(self, xml):
        super().__deserialize_from_xml(xml)
        for node in xml:
            if (node.tag == namespace + ""MediaReference""):
                self.media_reference = MediaReference(node, self)",https://github.com/varunsrin/one-py/blob/8fcf021bcf776a1802a69f50dfd180daf83536ff/onepy/onepy.py#L497-L501
tfidf_python_100_1.0,get all parents of xml node,python,"def _find(self, node):
        root = node
        while root in self.parents:
            root = self.parents[root]
        while node in self.parents:
            prev_node = node
            node = self.parents[node]
            self.parents[prev_node] = root
        return root",https://github.com/inveniosoftware-contrib/json-merger/blob/adc6d372da018427e1db7b92424d3471e01a4118/json_merger/contrib/inspirehep/match.py#L199-L207
tfidf_python_100_1.0,get all parents of xml node,python,"def __deserialize_from_xml (self, xml):
        self.name = xml.get(""name"")
        self.id = xml.get(""ID"")
        self.date_time = xml.get(""dateTime"")
        self.last_modified_time = xml.get(""lastModifiedTime"")
        self.page_level = xml.get(""pageLevel"")
        self.is_currently_viewed = xml.get(""isCurrentlyViewed"")
        self._children = [Meta(node) for node in xml]",https://github.com/varunsrin/one-py/blob/8fcf021bcf776a1802a69f50dfd180daf83536ff/onepy/onepy.py#L173-L180
tfidf_python_100_1.0,get all parents of xml node,python,"def best_parent( self, node, tree_type=None ):
        """"""Choose the best parent for a given node""""""
        parents = self.parents(node)
        selected_parent = None
        if node['type'] == 'type':
            module = ""."".join( node['name'].split( '.' )[:-1] )
            if module:
                for mod in parents:
                    if mod['type'] == 'module' and mod['name'] == module:
                        selected_parent = mod 
        if parents and selected_parent is None:
            parents.sort( key = lambda x: self.value(node, x) )
            return parents[-1]
        return selected_parent",https://github.com/lrq3000/pyFileFixity/blob/fd5ef23bb13835faf1e3baa773619b86a1cc9bdf/pyFileFixity/lib/profilers/visual/runsnakerun/meliaeadapter.py#L93-L106
tfidf_python_100_1.0,get all parents of xml node,python,"def __init__(self, warm_start_type, parents):
        """"""Initializes the ``WarmStartConfig`` with the provided ``WarmStartTypes`` and parents.

        Args:
            warm_start_type (sagemaker.tuner.WarmStartTypes): This should be one of the supported warm start types
            in WarmStartType
            parents (set{str}): Set of parent tuning jobs which will be used to warm start the new tuning job.
        """"""

        if warm_start_type not in WarmStartTypes:
            raise ValueError(
                ""Invalid type: {}, valid warm start types are: [{}]"".format(warm_start_type,
                                                                            [t for t in WarmStartTypes]))

        if not parents:
            raise ValueError(""Invalid parents: {}, parents should not be None/empty"".format(parents))

        self.type = warm_start_type
        self.parents = set(parents)",https://github.com/aws/sagemaker-python-sdk/blob/a9e724c7d3f5572b68c3903548c792a59d99799a/src/sagemaker/tuner.py#L70-L88
tfidf_python_100_1.0,get all parents of xml node,python,"def get_immoralities(self):
        """"""
        Finds all the immoralities in the model
        A v-structure X -> Z <- Y is an immorality if there is no direct edge between X and Y .

        Returns
        -------
        set: A set of all the immoralities in the model

        Examples
        ---------
        >>> from pgmpy.base import DAG
        >>> student = DAG()
        >>> student.add_edges_from([('diff', 'grade'), ('intel', 'grade'),
        ...                         ('intel', 'SAT'), ('grade', 'letter')])
        >>> student.get_immoralities()
        {('diff','intel')}
        """"""
        immoralities = set()
        for node in self.nodes():
            for parents in itertools.combinations(self.predecessors(node), 2):
                if not self.has_edge(parents[0], parents[1]) and not self.has_edge(parents[1], parents[0]):
                    immoralities.add(tuple(sorted(parents)))
        return immoralities",https://github.com/pgmpy/pgmpy/blob/9381a66aba3c3871d3ccd00672b148d17d63239e/pgmpy/base/DAG.py#L491-L514
tfidf_python_100_1.0,how to extract zip file recursively,python,"def extract(z, j):
    q, r, s, t = z
    return (q * j + r) // (s * j + t)",https://github.com/python/performance/blob/2a9524c0a5714e85106671bc61d750e800fe17db/performance/benchmarks/bm_pidigits.py#L35-L37
tfidf_python_100_1.0,how to extract zip file recursively,python,"def cloud_cover_to_irradiance(self, cloud_cover, how='clearsky_scaling',
                                  **kwargs):
        """"""
        Convert cloud cover to irradiance. A wrapper method.

        Parameters
        ----------
        cloud_cover : Series
        how : str, default 'clearsky_scaling'
            Selects the method for conversion. Can be one of
            clearsky_scaling or liujordan.
        **kwargs
            Passed to the selected method.

        Returns
        -------
        irradiance : DataFrame
            Columns include ghi, dni, dhi
        """"""

        how = how.lower()
        if how == 'clearsky_scaling':
            irrads = self.cloud_cover_to_irradiance_clearsky_scaling(
                cloud_cover, **kwargs)
        elif how == 'liujordan':
            irrads = self.cloud_cover_to_irradiance_liujordan(
                cloud_cover, **kwargs)
        else:
            raise ValueError('invalid how argument')

        return irrads",https://github.com/pvlib/pvlib-python/blob/2e844a595b820b43d1170269781fa66bd0ccc8a3/pvlib/forecast.py#L539-L569
tfidf_python_100_1.0,how to extract zip file recursively,python,"def execute_arbitrary_series_groupby(op, data, _, aggcontext=None, **kwargs):
    how = op.how
    if how is None:
        how = 'first'

    if how not in {'first', 'last'}:
        raise com.OperationNotDefinedError(
            'Arbitrary {!r} is not supported'.format(how)
        )
    return aggcontext.agg(data, how)",https://github.com/ibis-project/ibis/blob/1e39a5fd9ef088b45c155e8a5f541767ee8ef2e7/ibis/pandas/execution/generic.py#L451-L460
tfidf_python_100_1.0,how to extract zip file recursively,python,"def time_i8merge(self, how):
        merge(self.left, self.right, how=how)",https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/asv_bench/benchmarks/join_merge.py#L213-L214
tfidf_python_100_1.0,how to extract zip file recursively,python,"def _zip_files(self):
        """"""
        Adds the manifest and all support files to the zip file
        :return:
        """"""
        # Create the zip file
        zip = zipfile.ZipFile(self.name + '.zip', 'w', zipfile.ZIP_DEFLATED)

        # Add the manifest file to the zip
        zip.write(MANIFEST_FILE_NAME)

        # Add the support files to the zip
        for file in self.support_files:
            zip.write(file)

        # Close the zip file
        zip.close()",https://github.com/genepattern/genepattern-python/blob/9478ea65362b91c72a94f7300c3de8d710bebb71/gp/modules.py#L188-L204
tfidf_python_100_1.0,how to extract zip file recursively,python,"def Parse(self, how):
        '''Parse the message.
        '''
        if type(how) == types.ClassType: how = how.typecode
        return how.parse(self.body_root, self)",https://github.com/rameshg87/pyremotevbox/blob/123dffff27da57c8faa3ac1dd4c68b1cf4558b1a/pyremotevbox/ZSI/parse.py#L322-L326
tfidf_python_100_1.0,how to extract zip file recursively,python,"def bench_mul(nums1, nums2):
    for num1, num2 in zip(nums1, nums2):
        num1 * num2",https://github.com/n1analytics/python-paillier/blob/955f8c0bfa9623be15b75462b121d28acf70f04b/examples/benchmarks.py#L27-L29
tfidf_python_100_1.0,how to extract zip file recursively,python,"def bench_add(nums1, nums2):
    for num1, num2 in zip(nums1, nums2):
        num1 + num2",https://github.com/n1analytics/python-paillier/blob/955f8c0bfa9623be15b75462b121d28acf70f04b/examples/benchmarks.py#L22-L24
tfidf_python_100_1.0,how to extract zip file recursively,python,"def set_mode(how):
    """""" Sets the behavior of the API

    :param how: if 'remote' all the execution is performed on the remote server; if 'local' all
           it is executed locally. Default = 'local'
    :return: None
    """"""
    global __mode
    if how == ""local"":
        __mode = how
    elif how == ""remote"":
        __mode = how
    else:
        raise ValueError(""how must be 'local' or 'remote'"")",https://github.com/DEIB-GECO/PyGMQL/blob/e58b2f9402a86056dcda484a32e3de0bb06ed991/gmql/settings.py#L79-L92
tfidf_python_100_1.0,how to extract zip file recursively,python,"def _xor_list(self, list_1, list_2):
        return [ i ^ j for i,j in zip(list_1, list_2)]",https://github.com/klahnakoski/pyLibrary/blob/fa2dcbc48fda8d26999baef400e9a98149e0b982/mo_math/vendor/aespython/key_expander.py#L51-L52
tfidf_python_100_1.0,how to extract zip file recursively,python,"def xorstr(self,a,b):
        a = newbytes(a)
        b = newbytes(b)
        return newbytes([x^y for (x,y) in zip(a,b)])",https://github.com/bdcht/crysp/blob/90fd02b23f9503354e18b0b01c8f3dd7e8dd85e7/crysp/mode.py#L35-L38
tfidf_python_100_1.0,how to extract zip file recursively,python,"def dropna(self, how='any'):
        if how not in ('any', 'all'):
            raise ValueError(""invalid how option: {0}"".format(how))

        if self.hasnans:
            return self._shallow_copy(self.values[~self._isnan])
        return self._shallow_copy()",https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/indexes/base.py#L1961-L1967
tfidf_python_100_1.0,how to extract zip file recursively,python,"def apply(self, page_string):
        value = self.extract(page_string)
        
        if self.sub_rules:
            value['sub_rules'] = self.sub_rules.extract(value['extract'])
        
        value['extract'] = self.remove_html(value['extract'])
        return value",https://github.com/usc-isi-i2/etk/blob/aab077c984ea20f5e8ae33af622fe11d3c4df866/etk/dependencies/landmark/landmark_extractor/extraction/Landmark.py#L188-L195
tfidf_python_100_1.0,how to extract zip file recursively,python,"def _complex_store(self, name, value, extract):
        if extract is not None:
            baseval = self._simple_load(name)
            if extract[0] != len(baseval) - 1:
                value = baseval[len(baseval)-1:extract[0]+1].concat(value)
            if extract[1] != 0:
                value = value.concat(baseval[extract[1]-1:0])

        self._simple_store(name, value)",https://github.com/angr/angr/blob/4e2f97d56af5419ee73bdb30482c8dd8ff5f3e40/angr/state_plugins/light_registers.py#L125-L133
tfidf_python_100_1.0,how to extract zip file recursively,python,"def set_meta_profiling(how):
    """""" Enables or disables the profiling of metadata at the loading of a GMQLDataset

    :param how: True if you want to analyze the metadata when a GMQLDataset is created
                by a load_from_*. False otherwise. (Default=True)
    :return: None
    """"""
    global __metadata_profiling
    if isinstance(how, bool):
        __metadata_profiling = how
    else:
        raise TypeError(""how must be boolean. {} was provided"".format(type(how)))",https://github.com/DEIB-GECO/PyGMQL/blob/e58b2f9402a86056dcda484a32e3de0bb06ed991/gmql/settings.py#L136-L147
tfidf_python_100_1.0,how to extract zip file recursively,python,"def __init__(self, item, how):
        self.item = item
        self.how = how",https://github.com/kayak/pypika/blob/bfed26e963b982ecdb9697b61b67d76b493f2115/pypika/queries.py#L1029-L1031
tfidf_python_100_1.0,how to extract zip file recursively,python,"def _quokka_normalize_extract(extract):
    """"""
    Generate a normalized rectangle to be extract from the standard quokka image.

    Parameters
    ----------
    extract : 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage
        Unnormalized representation of the image subarea to be extracted.

            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)``
              will be extracted from the image.
            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``
              and ``y2``.
            * If a BoundingBox, then that bounding box's area will be extracted from the image.
            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box
              and a shape matching the full image dimensions (i.e. (643, 960, *)). Then the
              one bounding box will be used similar to BoundingBox.

    Returns
    -------
    bb : imgaug.BoundingBox
        Normalized representation of the area to extract from the standard quokka image.

    """"""
    # TODO get rid of this deferred import
    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage

    if extract == ""square"":
        bb = BoundingBox(x1=0, y1=0, x2=643, y2=643)
    elif isinstance(extract, tuple) and len(extract) == 4:
        bb = BoundingBox(x1=extract[0], y1=extract[1], x2=extract[2], y2=extract[3])
    elif isinstance(extract, BoundingBox):
        bb = extract
    elif isinstance(extract, BoundingBoxesOnImage):
        do_assert(len(extract.bounding_boxes) == 1)
        do_assert(extract.shape[0:2] == (643, 960))
        bb = extract.bounding_boxes[0]
    else:
        raise Exception(
            ""Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage ""
            + ""for parameter 'extract', got %s."" % (type(extract),)
        )
    return bb",https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/imgaug.py#L464-L506
tfidf_python_100_1.0,how to extract zip file recursively,python,"def set_progress(how):
    """""" Enables or disables the progress bars for the loading, writing and downloading
    of datasets

    :param how: True if you want the progress bar, False otherwise
    :return: None

    Example::

        import gmql as gl

        gl.set_progress(True)   # abilitates progress bars
        # ....do something...
        gl.set_progress(False)  # removes progress bars
        # ....do something...
    """"""
    global __progress_bar
    if isinstance(how, bool):
        __progress_bar = how
    else:
        raise ValueError(
            ""how must be a boolean. {} was found"".format(type(how)))",https://github.com/DEIB-GECO/PyGMQL/blob/e58b2f9402a86056dcda484a32e3de0bb06ed991/gmql/settings.py#L107-L128
tfidf_python_100_1.0,how to extract zip file recursively,python,"def erase_in_display(self, how=0, *args, **kwargs):
        """"""Overloaded to reset history state.""""""
        super(HistoryScreen, self).erase_in_display(how, *args, **kwargs)

        if how == 3:
            self._reset_history()",https://github.com/selectel/pyte/blob/8adad489f86da1788a7995720c344a2fa44f244e/pyte/screens.py#L1204-L1209
tfidf_python_100_1.0,how to extract zip file recursively,python,"def align(self, alignraster, how=np.mean, cxsize=None, cysize=None):
        '''
        geo.align(geo2, how=np.mean)

        Returns both georasters aligned and with the same pixelsize
        '''
        return align_georasters(self, alignraster, how=how, cxsize=cxsize, cysize=cysize)",https://github.com/ozak/georasters/blob/0612bd91bb2a2cb2f1d59ba89c1ff131dae27d70/georasters/georasters.py#L862-L868
tfidf_python_100_1.0,underline text in label widget,python,"def underline(self, text): # Print an underline text
		if self._windows():
			return text
		else:
			return self.UNDERLINE + text + self.NORMAL",https://github.com/dendory/menu3/blob/350414966e8f5c7737cd527369c8087f4f8f600b/menu3/menu3.py#L45-L49
tfidf_python_100_1.0,underline text in label widget,python,"def underline(text: str) -> str:
    return (style(text, underline=True, reset=False) +
            style('', underline=False, reset=False))",https://github.com/lablup/backend.ai-client-py/blob/a063d774fea6f4350b89498c40d3c837ec3029a7/src/ai/backend/client/cli/pretty.py#L36-L38
tfidf_python_100_1.0,underline text in label widget,python,"def set_underline(self, on_off):
        self._set_printing_mode_bit(self.UNDERLINE_BIT, on_off == Underline.on)",https://github.com/lukegb/ticketml/blob/0da6eabead31c43bc8cd7301b5f20abd735e569d/ticketml/ticketml.py#L261-L262
tfidf_python_100_1.0,underline text in label widget,python,"def underline(self):
        self._styled_string = _STYLE_TEMPLATE.format(start=4, end=24, text=self._styled_string)
        return self",https://github.com/skabbass1/escape/blob/030029728978496cba06cfbc40ed8c7f3cc3d660/escape/ansi_styles.py#L181-L183
tfidf_python_100_1.0,underline text in label widget,python,"def val(self):
        """"""
        The underline type corresponding to the ``w:val`` attribute value.
        """"""
        val = self.get(qn('w:val'))
        underline = WD_UNDERLINE.from_xml(val)
        if underline == WD_UNDERLINE.SINGLE:
            return True
        if underline == WD_UNDERLINE.NONE:
            return False
        return underline",https://github.com/python-openxml/python-docx/blob/6756f6cd145511d3eb6d1d188beea391b1ddfd53/docx/oxml/text/font.py#L290-L300
tfidf_python_100_1.0,underline text in label widget,python,"def underline(self, msg):
        """"""Underline the input""""""
        return click.style(msg, underline=True) if self.colorize else msg",https://github.com/awslabs/aws-sam-cli/blob/c05af5e7378c6f05f7d82ad3f0bca17204177db6/samcli/lib/utils/colors.py#L57-L59
tfidf_python_100_1.0,underline text in label widget,python,"def __init__(self, label='', help_text='', widget=None, *args, **kwargs):
        if widget is None:
            widget = widgets.CheckboxWithInlineLabel()
        if isinstance(widget, widgets.CheckboxWithInlineLabel):
            widget.label = label
            widget.help_text = help_text
            label = ''
        super().__init__(label=label, widget=widget, *args, **kwargs)",https://github.com/uktrade/directory-components/blob/305b3cfd590e170255503ae3c41aebcaa658af8e/directory_components/fields.py#L58-L65
tfidf_python_100_1.0,underline text in label widget,python,"def set_underline(self, on_off):
        self._write_immediately(h2b(b'1b2d') + bchr(int(bool(on_off == Underline.on))))",https://github.com/lukegb/ticketml/blob/0da6eabead31c43bc8cd7301b5f20abd735e569d/ticketml/ticketml.py#L182-L183
tfidf_python_100_1.0,underline text in label widget,python,"def underline(self, text, pad_char):
        text = (text or '').rstrip()
        return '%s\n%s\n\n' % (text, pad_char * len(text)) if text else ''",https://github.com/matthewwithanm/python-markdownify/blob/78afcc173e922280ee533f97f8ffa634e6d3edf5/markdownify/__init__.py#L108-L110
tfidf_python_100_1.0,underline text in label widget,python,"def __init__(self, text=None, checkable=False):
        super(MenuAction, self).__init__()

        self.text = text
        self.checkable = checkable

        if checkable:
            self.widget = gtk.CheckMenuItem(label=text)
            self.widget.connect('toggled', self._cb_redirect)
        else:
            self.widget = gtk.MenuItem(label=text)
            self.widget.connect('activate', self._cb_redirect)
        self.widget.show()

        self.enable_callback('activated')",https://github.com/ejeschke/ginga/blob/a78c893ec6f37a837de851947e9bb4625c597915/ginga/gtkw/Widgets.py#L1645-L1659
tfidf_python_100_1.0,underline text in label widget,python,"def underline(text):
    '''Takes a string, and returns it underscored.'''

    text += ""\n""
    for i in range(len(text)-1):
        text += ""=""
    text += ""\n""
    return text",https://github.com/morngrar/ui/blob/93e160b55ff7d486a53dba7a8c0f2d46e6f95ed9/ui/ui.py#L79-L86
tfidf_python_100_1.0,underline text in label widget,python,"def create_attribute_type(name, label, form_field,
                          widget=None, validator_re=None, validator_type=None):
    if not AttributeType.objects.filter(name=name).exists():
        AttributeType.objects.create(
            name=name, label=label, form_field=form_field, widget=widget,
            validator_re=validator_re, validator_type=validator_type
        )",https://github.com/Cadasta/django-jsonattrs/blob/5149e08ec84da00dd73bd3fe548bc52fd361667c/jsonattrs/models.py#L148-L154
tfidf_python_100_1.0,underline text in label widget,python,"def valueFromWidget(self, widget, paramtype):
        try:
            if paramtype == BOOL:
                return widget.checkState(1) == Qt.Checked
            elif paramtype == NUMBER:
                v = float(widget.text())
                return
            elif paramtype == CHOICE:
                return widget.currentText()
            elif paramtype == TEXT:
                return widget.toPlainText()
            elif paramtype == STRING:                
                return widget.text()
            elif paramtype in [CRS, FILES, FOLDER, AUTHCFG]:
                return widget.value
            elif paramtype in [RASTER, VECTOR]:
                return widget.currentLayer()
            else:
                return widget.text()
        except:
            raise",https://github.com/boundlessgeo/lib-qgis-commons/blob/d25d13803db08c18632b55d12036e332f006d9ac/qgiscommons2/gui/paramdialog.py#L180-L200
tfidf_python_100_1.0,underline text in label widget,python,"def _get_widget(self):
        widget = self.widget
        while isinstance(widget, WrapperWidget):
            widget = widget.widget
        return widget",https://github.com/matllubos/django-is-core/blob/3f87ec56a814738683c732dce5f07e0328c2300d/is_core/forms/widgets.py#L160-L164
tfidf_python_100_1.0,underline text in label widget,python,"def _render_title(title, level=1):
    underlines = [""="", ""-"", ""~""]
    underline = underlines[level - 1] * len(title)
    return f""{title}\n{underline}\n\n""",https://github.com/mbarakaja/braulio/blob/70ab6f0dd631ef78c4da1b39d1c6fb6f9a995d2b/braulio/files.py#L77-L80
tfidf_python_100_1.0,underline text in label widget,python,"def _get_subwin(self, widget):
        for subwin in list(self.widget.subWindowList()):
            if subwin.widget() == widget:
                return subwin
        return None",https://github.com/ejeschke/ginga/blob/a78c893ec6f37a837de851947e9bb4625c597915/ginga/qtw/Widgets.py#L1399-L1403
tfidf_python_100_1.0,underline text in label widget,python,"def resetErrors(self):
        for widget in self.reifiedWidgets:
            widget.setErrorString('')
            widget.showErrorString(False)",https://github.com/chriskiehl/Gooey/blob/e598573c6519b953e0ccfc1f3663f827f8cd7e22/gooey/gui/components/config.py#L67-L70
tfidf_python_100_1.0,underline text in label widget,python,"def __init__(self, label, attrs=None, widget=None):
        attrs = attrs or {}
        widget = widget or self.widget
        super().__init__(required=False, label='', initial=label, widget=widget(attrs=attrs))",https://github.com/matllubos/django-is-core/blob/3f87ec56a814738683c732dce5f07e0328c2300d/is_core/forms/fields.py#L47-L50
tfidf_python_100_1.0,underline text in label widget,python,"def ellipsize_labels_recursively(widget, ellipsize=Pango.EllipsizeMode.END, width_chars=1):
    if isinstance(widget, Gtk.Label):
        widget.set_ellipsize(ellipsize)
        widget.set_width_chars(width_chars)
    elif isinstance(widget, Gtk.Container):
        for child_widget in widget.get_children():
            ellipsize_labels_recursively(child_widget, ellipsize, width_chars)",https://github.com/DLR-RM/RAFCON/blob/24942ef1a904531f49ab8830a1dbb604441be498/source/rafcon/gui/helpers/label.py#L348-L354
tfidf_python_100_1.0,underline text in label widget,python,"def append_text(self, text, autoscroll=True):
        if text.endswith('\n'):
            text = text[:-1]
        self.widget.append(text)
        if not autoscroll:
            return

        self.widget.moveCursor(QTextCursor.End)
        self.widget.moveCursor(QTextCursor.StartOfLine)
        self.widget.ensureCursorVisible()",https://github.com/ejeschke/ginga/blob/a78c893ec6f37a837de851947e9bb4625c597915/ginga/qtw/Widgets.py#L242-L251
tfidf_python_100_1.0,unzipping large files,python,"def contains (small, large):
    """""" Returns true iff all elements of 'small' exist in 'large'.
    """"""
    small = to_seq (small)
    large = to_seq (large)

    for s in small:
        if not s in large:
            return False
    return True",https://github.com/apple/turicreate/blob/74514c3f99e25b46f22c6e02977fe3da69221c2e/deps/src/boost_1_68_0/tools/build/src/util/set.py#L31-L40
tfidf_python_100_1.0,unzipping large files,python,"def byte_compile(self, files):
        files = _filter_tests(files)
        install_lib.install_lib.byte_compile(self, files)",https://github.com/PyCQA/pylint/blob/2bf5c61a3ff6ae90613b81679de42c0f19aea600/setup.py#L126-L128
tfidf_python_100_1.0,unzipping large files,python,"def create_files_start_length_list(
        files, func_get_nevents_in_file=None,
        max_events=-1, max_events_per_run=-1,
        max_files=-1, max_files_per_run=1):

    files = _apply_max_files(files, max_files)

    if max_events == 0 or max_events_per_run == 0:
        return [ ]

    if max_events < 0 and max_events_per_run < 0:
        return _fast_path(files, max_files_per_run)

    return _full_path(files, func_get_nevents_in_file, max_events,
                           max_events_per_run, max_files_per_run)",https://github.com/alphatwirl/alphatwirl/blob/5138eeba6cd8a334ba52d6c2c022b33c61e3ba38/alphatwirl/loop/splitfuncs.py#L4-L18
tfidf_python_100_1.0,unzipping large files,python,"def _fast_path(files, max_files_per_run):
    if not files:
        return [ ]
    if max_files_per_run < 0:
        return [(files, 0, -1)]
    if max_files_per_run == 0:
        return [ ]
    return [(files[i:(i + max_files_per_run)], 0, -1) for i in range(0, len(files), max_files_per_run)]",https://github.com/alphatwirl/alphatwirl/blob/5138eeba6cd8a334ba52d6c2c022b33c61e3ba38/alphatwirl/loop/splitfuncs.py#L26-L33
tfidf_python_100_1.0,unzipping large files,python,"def _apply_max_files(files, max_files):
    if max_files < 0:
        return files
    return files[:min(max_files, len(files))]",https://github.com/alphatwirl/alphatwirl/blob/5138eeba6cd8a334ba52d6c2c022b33c61e3ba38/alphatwirl/loop/splitfuncs.py#L21-L24
tfidf_python_100_1.0,unzipping large files,python,"def byte_compile(self, files):
        files = list(filter(_not_async, files))
        install_lib.byte_compile(self, files)",https://github.com/nickoala/telepot/blob/3792fde251d0f1d5a6ca16c8ad1a71f89360c41d/setup.py#L25-L27
tfidf_python_100_1.0,unzipping large files,python,"def _lfs_add(files, git):
    """"""
    Add any oversized files with lfs.
    Throws error if a file is bigger than 2GB or git-lfs is not installed.
    """"""
    # Check for large files > 100 MB (and huge files > 2 GB)
    # https://help.github.com/articles/conditions-for-large-files/
    # https://help.github.com/articles/about-git-large-file-storage/
    larges, huges = [], []
    for file in files:
        size = os.path.getsize(file)
        if size > (100 * 1024 * 1024):
            larges.append(file)
        elif size > (2 * 1024 * 1024 * 1024):
            huges.append(file)

    # Raise Error if a file is >2GB
    if huges:
        raise Error(_(""These files are too large to be submitted:\n{}\n""
                      ""Remove these files from your directory ""
                      ""and then re-run {}!"").format(""\n"".join(huges), org))

    # Add large files (>100MB) with git-lfs
    if larges:
        # Raise Error if git-lfs not installed
        if not shutil.which(""git-lfs""):
            raise Error(_(""These files are too large to be submitted:\n{}\n""
                          ""Install git-lfs (or remove these files from your directory) ""
                          ""and then re-run!"").format(""\n"".join(larges)))

        # Install git-lfs for this repo
        _run(git(""lfs install --local""))

        # For pre-push hook
        _run(git(""config credential.helper cache""))

        # Rm previously added file, have lfs track file, add file again
        for large in larges:
            _run(git(""rm --cached {}"".format(shlex.quote(large))))
            _run(git(""lfs track {}"".format(shlex.quote(large))))
            _run(git(""add {}"".format(shlex.quote(large))))
        _run(git(""add --force .gitattributes""))",https://github.com/cs50/lib50/blob/941767f6c0a3b81af0cdea48c25c8d5a761086eb/lib50/_api.py#L570-L611
tfidf_python_100_1.0,unzipping large files,python,"def prepare_files(self):
        """"""Get files from data dump.""""""
        # Prepare files
        files = {}
        for f in self.data['files']:
            k = f['full_name']
            if k not in files:
                files[k] = []
            files[k].append(f)

        # Sort versions
        for k in files.keys():
            files[k].sort(key=lambda x: x['version'])

        self.files = files",https://github.com/inveniosoftware/invenio-migrator/blob/6902c6968a39b747d15e32363f43b7dffe2622c2/invenio_migrator/records.py#L298-L312
tfidf_python_100_1.0,unzipping large files,python,"def get_unloaded_chunks(files, loaded_chunks):
    filtered_files = [f for f in files if f not in loaded_chunks]
    for f in filtered_files:
        loaded_chunks.add(f)
    return filtered_files",https://github.com/apache/incubator-superset/blob/ca2996c78f679260eb79c6008e276733df5fb653/superset/__init__.py#L85-L89
tfidf_python_100_1.0,unzipping large files,python,"def on_files(self, files, config):
        for f, func in self.files.values():
            files.append(f)
        return files",https://github.com/greenape/mktheapidocs/blob/a45e8b43ddd80ed360fe1e98d4f73dc11c4e7bf7/mktheapidocs/plugin.py#L112-L115
tfidf_python_100_1.0,unzipping large files,python,"def loads_to_post(self, request):
        files = [f for f in request.FILES.values()]
        return files and files[0] or None",https://github.com/django-inplaceedit/django-inplaceedit/blob/7ba18e7906f56c56395ca07e2486755062efce00/inplaceeditform/fields.py#L619-L621
tfidf_python_100_1.0,unzipping large files,python,"def __init__(self, name, files, **kwargs):
        if isinstance(files, tuple):
            files = list(files)
        elif not isinstance(files, list):
            files = [files]
        else:
            files = files[:]
        if not files:
            raise RuntimeError(
                ""unable to initialize TreeChain: no files"")
        self._files = files
        self.curr_file_idx = 0
        super(TreeChain, self).__init__(name, **kwargs)
        self._tchain = QROOT.TChain(name)
        for filename in self._files:
            self._tchain.Add(filename)",https://github.com/rootpy/rootpy/blob/3926935e1f2100d8ba68070c2ab44055d4800f73/rootpy/tree/chain.py#L239-L254
tfidf_python_100_1.0,unzipping large files,python,"def _instance_types(self):
        """"""
        Return a list of all known EC2 instance types

        :returns: list of all valid known EC2 instance types
        :rtype: list
        """"""
        GENERAL_TYPES = [
            'a1.2xlarge',
            'a1.4xlarge',
            'a1.large',
            'a1.medium',
            'a1.xlarge',
            't2.nano',
            't2.micro',
            't2.small',
            't2.medium',
            't2.large',
            't2.xlarge',
            't2.2xlarge',
            't3.nano',
            't3.micro',
            't3.small',
            't3.medium',
            't3.large',
            't3.xlarge',
            't3.2xlarge',
            'm3.medium',
            'm3.large',
            'm3.xlarge',
            'm3.2xlarge',
            'm4.large',
            'm4.xlarge',
            'm4.2xlarge',
            'm4.4xlarge',
            'm4.10xlarge',
            'm4.16xlarge',
            'm5.12xlarge',
            'm5.24xlarge',
            'm5.2xlarge',
            'm5.4xlarge',
            'm5.large',
            'm5.xlarge',
            'm5d.12xlarge',
            'm5d.24xlarge',
            'm5d.2xlarge',
            'm5d.4xlarge',
            'm5d.large',
            'm5d.xlarge',
            'm5a.12xlarge',
            'm5a.24xlarge',
            'm5a.2xlarge',
            'm5a.4xlarge',
            'm5a.large',
            'm5a.xlarge',
        ]

        PREV_GENERAL_TYPES = [
            't1.micro',
            'm1.small',
            'm1.medium',
            'm1.large',
            'm1.xlarge',
        ]

        MEMORY_TYPES = [
            'r3.2xlarge',
            'r3.4xlarge',
            'r3.8xlarge',
            'r3.large',
            'r3.xlarge',
            'r4.2xlarge',
            'r4.4xlarge',
            'r4.8xlarge',
            'r4.16xlarge',
            'r4.large',
            'r4.xlarge',
            'r5.2xlarge',
            'r5.4xlarge',
            'r5.8xlarge',
            'r5.12xlarge',
            'r5.16xlarge',
            'r5.24xlarge',
            'r5.large',
            'r5.metal',
            'r5.xlarge',
            'r5a.12xlarge',
            'r5a.24xlarge',
            'r5a.2xlarge',
            'r5a.4xlarge',
            'r5a.large',
            'r5a.xlarge',
            'r5d.2xlarge',
            'r5d.4xlarge',
            'r5d.8xlarge',
            'r5d.12xlarge',
            'r5d.16xlarge',
            'r5d.24xlarge',
            'r5d.large',
            'r5d.metal',
            'r5d.xlarge',
            'x1.16xlarge',
            'x1.32xlarge',
            'x1e.2xlarge',
            'x1e.4xlarge',
            'x1e.8xlarge',
            'x1e.16xlarge',
            'x1e.32xlarge',
            'x1e.xlarge',
            'z1d.2xlarge',
            'z1d.3xlarge',
            'z1d.6xlarge',
            'z1d.12xlarge',
            'z1d.large',
            'z1d.xlarge',
        ]

        PREV_MEMORY_TYPES = [
            'm2.xlarge',
            'm2.2xlarge',
            'm2.4xlarge',
            'cr1.8xlarge',
        ]

        COMPUTE_TYPES = [
            'c3.large',
            'c3.xlarge',
            'c3.2xlarge',
            'c3.4xlarge',
            'c3.8xlarge',
            'c4.large',
            'c4.xlarge',
            'c4.2xlarge',
            'c4.4xlarge',
            'c4.8xlarge',
            'c5.18xlarge',
            'c5.2xlarge',
            'c5.4xlarge',
            'c5.9xlarge',
            'c5.large',
            'c5.xlarge',
            'c5d.18xlarge',
            'c5d.2xlarge',
            'c5d.4xlarge',
            'c5d.9xlarge',
            'c5d.large',
            'c5d.xlarge',
            'c5n.18xlarge',
            'c5n.2xlarge',
            'c5n.4xlarge',
            'c5n.9xlarge',
            'c5n.large',
            'c5n.xlarge',
        ]

        PREV_COMPUTE_TYPES = [
            'c1.medium',
            'c1.xlarge',
            'cc2.8xlarge',
            'cc1.4xlarge',
        ]

        ACCELERATED_COMPUTE_TYPES = [
            'f1.4xlarge',
            'p2.xlarge',
            'p2.8xlarge',
            'p2.16xlarge',
            'p3.16xlarge',
            'p3.2xlarge',
            'p3.8xlarge',
            'p3dn.24xlarge',
        ]

        STORAGE_TYPES = [
            'i2.xlarge',
            'i2.2xlarge',
            'i2.4xlarge',
            'i2.8xlarge',
            'i3.large',
            'i3.xlarge',
            'i3.2xlarge',
            'i3.4xlarge',
            'i3.8xlarge',
            'i3.16xlarge',
            'i3.metal',
            'h1.16xlarge',
            'h1.2xlarge',
            'h1.4xlarge',
            'h1.8xlarge',
        ]

        PREV_STORAGE_TYPES = [
            # NOTE hi1.4xlarge is no longer in the instance type listings,
            # but some accounts might still have a limit for it
            'hi1.4xlarge',
            'hs1.8xlarge',
        ]

        DENSE_STORAGE_TYPES = [
            'd2.xlarge',
            'd2.2xlarge',
            'd2.4xlarge',
            'd2.8xlarge',
        ]

        GPU_TYPES = [
            'g2.2xlarge',
            'g2.8xlarge',
            'g3.16xlarge',
            'g3.4xlarge',
            'g3.8xlarge',
            'g3s.xlarge',
        ]

        PREV_GPU_TYPES = [
            'cg1.4xlarge',
        ]

        FPGA_TYPES = [
            # note, as of 2016-12-17, these are still in Developer Preview;
            # there isn't a published instance limit yet, so we'll assume
            # it's the default...
            'f1.2xlarge',
            'f1.16xlarge',
        ]

        return (
            GENERAL_TYPES +
            PREV_GENERAL_TYPES +
            MEMORY_TYPES +
            PREV_MEMORY_TYPES +
            COMPUTE_TYPES +
            PREV_COMPUTE_TYPES +
            ACCELERATED_COMPUTE_TYPES +
            STORAGE_TYPES +
            PREV_STORAGE_TYPES +
            DENSE_STORAGE_TYPES +
            GPU_TYPES +
            PREV_GPU_TYPES +
            FPGA_TYPES
        )",https://github.com/jantman/awslimitchecker/blob/e50197f70f3d0abcc5cfc7fde6336f548b790e34/awslimitchecker/services/ec2.py#L585-L825
tfidf_python_100_1.0,unzipping large files,python,"def load_chain(chain, files, nfiles=None):

    if isinstance(nfiles, list) and len(nfiles) == 1:
        files = files[:nfiles[0]]
    elif isinstance(nfiles, list) and len(nfiles) >= 2:
        files = files[nfiles[0]:nfiles[1]]
    elif nfiles is not None:
        files = files[:nfiles]

    print(""Loading %i files..."" % len(files))
    for f in files:
        chain.Add(f)
    return chain",https://github.com/fermiPy/fermipy/blob/9df5e7e3728307fd58c5bba36fd86783c39fbad4/fermipy/validate/utils.py#L55-L67
tfidf_python_100_1.0,unzipping large files,python,"def _normalize_files(item, fc_dir=None):
    """"""Ensure the files argument is a list of absolute file names.
    Handles BAM, single and paired end fastq, as well as split inputs.
    """"""
    files = item.get(""files"")
    if files:
        if isinstance(files, six.string_types):
            files = [files]
        fastq_dir = flowcell.get_fastq_dir(fc_dir) if fc_dir else os.getcwd()
        files = [_file_to_abs(x, [os.getcwd(), fc_dir, fastq_dir]) for x in files]
        files = [x for x in files if x]
        _sanity_check_files(item, files)
        item[""files""] = files
    return item",https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/pipeline/run_info.py#L835-L848
tfidf_python_100_1.0,unzipping large files,python,"def __init__(self, init_dict):
        self.error_message = init_dict['errorMessage']
        files = init_dict['datasetFiles']
        if files:
            self.files = [File(f) for f in files]
        else:
            self.files = {}",https://github.com/Kaggle/kaggle-api/blob/65f14b1386470c5784d4753e491478e7537660d9/kaggle/models/kaggle_models_extended.py#L137-L143
tfidf_python_100_1.0,unzipping large files,python,"def on_files(self, files, config):
        files = Files(
            [
                NotebookFile(f, **config)
                if str(f.abs_src_path).endswith(""ipynb"")
                else f
                for f in files
            ]
        )
        return files",https://github.com/greenape/mknotebooks/blob/408b9c7072fc33af1503fc79aeb31b01292f8b03/mknotebooks/plugin.py#L56-L65
tfidf_python_100_1.0,unzipping large files,python,"def __init__(self, files):
        self.files = files
        self.dirs = {os.path.dirname(f) for f in files}",https://github.com/google/importlab/blob/92090a0b4421137d1369c2ed952eda6bb4c7a155/importlab/fs.py#L49-L51
tfidf_python_100_1.0,unzipping large files,python,"def __init__(self, data=None):

        self.medium = data[""medium""]

        if ""small"" in data:
            self.small = data[""small""]
        else:
            self.small = None

        if ""large"" in data:
            self.large = data[""large""]
        else:
            self.large = None",https://github.com/namaggarwal/splitwise/blob/25dbd1e774969dbd17d1d6933177c13c8f8d61fb/splitwise/picture.py#L4-L16
tfidf_python_100_1.0,unzipping large files,python,"def files_type(fs0, fs1, files):
    """"""Inspects the file type of the given files.""""""
    for file_meta in files['deleted_files']:
        file_meta['type'] = fs0.file(file_meta['path'])
    for file_meta in files['created_files'] + files['modified_files']:
        file_meta['type'] = fs1.file(file_meta['path'])

    return files",https://github.com/noxdafox/vminspect/blob/e685282564877e2d1950f1e09b292f4f4db1dbcd/vminspect/comparator.py#L367-L374
tfidf_python_100_1.0,copying a file to a path,python,"def copyFileToCache(self,path):
    sha1=self.computeFileSha1(path)
    path0=self._get_path(sha1,create=True)
    if not os.path.exists(path0):
      copyfile(path,path0+'.copying')
      os.rename(path0+'.copying',path0)
    return path0",https://github.com/flatironinstitute/kbucket/blob/867915ebb0ea153a399c3e392698f89bf43c7903/kbucket/kbucketclient.py#L522-L528
tfidf_python_100_1.0,copying a file to a path,python,"def place_dockercfg(self):
        dockercfg = self.index_settings.dockercfg
        if dockercfg is not None:
            log.info(""Copying to .dockercfg: %s"" % dockercfg)
            Run()([""cp"", dockercfg, "".dockercfg""])",https://github.com/mesosphere/deimos/blob/b4deead93b6e2ddf4e4a42e33777755942da0b7a/deimos/containerizer/docker.py#L367-L371
tfidf_python_100_1.0,copying a file to a path,python,"def _handle_upload(file_cache, path, body, cache_token=None):
    source = body
    if cache_token:
        cached_file = file_cache.destination(cache_token)
        source = open(cached_file, 'rb')
        log.info(""Copying cached file %s to %s"" % (cached_file, path))
    copy_to_path(source, path)
    return {""path"": path}",https://github.com/galaxyproject/pulsar/blob/9ab6683802884324652da0a9f0808c7eb59d3ab4/pulsar/web/routes.py#L235-L242
tfidf_python_100_1.0,copying a file to a path,python,"def do_send(self, block):
        self.log.debug(""Copying file '%s'"", block.latest_file_info.path)
        shutil.copy(block.latest_file_info.path, self._dir_path)",https://github.com/MatiasSM/fcb/blob/92a6c535287ea1c1ef986954a5d66e7905fb6221/fcb/sending/directory/ToDirectorySender.py#L22-L24
tfidf_python_100_1.0,copying a file to a path,python,"def _async_deepcopy(vs, newlist, oldlist):
    for r in Progress(oldlist, 'copying'):
        newlist.append(deepcopy(r))",https://github.com/saulpw/visidata/blob/32771e0cea6c24fc7902683d14558391395c591f/visidata/vdtui.py#L540-L542
tfidf_python_100_1.0,copying a file to a path,python,"def __eq__(self, file):
        """"""Return True if the absolute path is same""""""
        if isinstance(file, File):
            return self.path == file.path
        else:
            return self.path == os.path.abspath(file)
        return False",https://github.com/weaming/filetree/blob/0e717aaa839b9750d54e2453fb68c357693827f3/filetree/tree.py#L251-L257
tfidf_python_100_1.0,copying a file to a path,python,"def copy_to_inst_dir(self, shared_object):
        dest_name = os.path.join(self._inst_dir, os.path.basename(shared_object))
        if dest_name in self._copied:
            return False
        if not self._mock:
            print(""Copying %s -> %s"" % (shared_object, dest_name))
            shutil.copyfile(shared_object, dest_name)
        self._copied.add(dest_name)
        return dest_name",https://github.com/facebook/watchman/blob/d416c249dd8f463dc69fc2691d0f890598c045a9/winbuild/copy-dyn-deps.py#L137-L145
tfidf_python_100_1.0,copying a file to a path,python,"def copy_file(self, path, prefixed_path, source_storage):
        """"""
        Attempt to copy ``path`` with storage
        """"""
        # Skip this file if it was already copied earlier
        if prefixed_path in self.copied_files:
            return self.log(""Skipping '%s' (already copied earlier)"" % path)
        # Delete the target file if needed or break
        if not self.delete_file(path, prefixed_path, source_storage):
            return
        # The full path of the source file
        source_path = source_storage.path(path)
        # Finally start copying
        if self.dry_run:
            self.log(""Pretending to copy '%s'"" % source_path, level=1)
        else:
            self.log(""Copying '%s'"" % source_path, level=1)
            with source_storage.open(path) as source_file:
                self.storage.save(prefixed_path, source_file)
        self.copied_files.append(prefixed_path)",https://github.com/adrianoveiga/django-media-fixtures/blob/a3f0d9ac84e73d491eeb0c881b23cc47ccca1b54/django_media_fixtures/management/commands/collectmedia.py#L305-L324
tfidf_python_100_1.0,copying a file to a path,python,"def __load__(self, path):
        if os.path.exists(path) and os.path.isfile(path):
            with open(path) as file:
                return file.read()
        raise Exception('Unable to read: %s. File does not exist or is not a file' % path)",https://github.com/troup-system/troup/blob/e3c74c363101243dfbfc3d2f0c7c8fce5d9ad1a8/troup/store.py#L100-L104
tfidf_python_100_1.0,copying a file to a path,python,"def __init__(self, file):
        """"""
        :param str file: Path to config path used
        """"""
        file = os.path.abspath(file)
        super(ConfigFileNotFoundError, self).__init__('The configuration file was not found on ""{}""'.format(file))",https://github.com/Nekmo/amazon-dash/blob/0e2bdc24ff8ea32cecb2f5f54f5cc1c0f99c197b/amazon_dash/exceptions.py#L28-L33
tfidf_python_100_1.0,copying a file to a path,python,"def from_path(cls, path: pathlib.Path) -> 'File':
        """"""
        Create a file entity from a file path.

        :param path: The path of the file.
        :return: A file entity instance representing the file.
        :raises ValueError: If the path does not point to a file.
        """"""
        if not path.is_file():
            raise ValueError('Path does not point to a file')
        return File(path.name, path.stat().st_size, cls._md5(path))",https://github.com/gebn/wood/blob/efc71879890dbd2f2d7a0b1a65ed22a0843139dd/wood/entities.py#L134-L144
tfidf_python_100_1.0,copying a file to a path,python,"def __contains__(self, file):
        if isinstance(file, File):
            return file.path.startswith(self.path)
        elif type(file) in (str, unicode):
            return os.path.basename(file) in os.listdir(self.path)
        return False",https://github.com/weaming/filetree/blob/0e717aaa839b9750d54e2453fb68c357693827f3/filetree/tree.py#L259-L264
tfidf_python_100_1.0,copying a file to a path,python,"def _configure_eslinter(self, bootstrapped_support_path):
    logger.debug('Copying {setupdir} to bootstrapped dir: {support_path}'
                           .format(setupdir=self.eslint_setupdir,
                                   support_path=bootstrapped_support_path))
    safe_rmtree(bootstrapped_support_path)
    shutil.copytree(self.eslint_setupdir, bootstrapped_support_path)
    return True",https://github.com/pantsbuild/pants/blob/b72e650da0df685824ffdcc71988b8c282d0962d/contrib/node/src/python/pants/contrib/node/subsystems/node_distribution.py#L164-L170
tfidf_python_100_1.0,copying a file to a path,python,"def __store__(self, path, data):
        path = self.__to_path__(path)
        with open(path, 'w') as file:
            file.write(data)",https://github.com/troup-system/troup/blob/e3c74c363101243dfbfc3d2f0c7c8fce5d9ad1a8/troup/store.py#L106-L109
tfidf_python_100_1.0,copying a file to a path,python,"def deleteFile(self, path=None, file=None, commitMessage=""Updated via Apitax""):
        if (not file):
            file = self.isFileExists(path)
        if (not file):
            return False
        path = self.getPath(file.path)
        return self.repo.delete_file(path, commitMessage, file.sha)",https://github.com/ShawnClake/Apitax/blob/2eb9c6990d4088b2503c7f13c2a76f8e59606e6d/apitax/integrations/Github.py#L47-L53
tfidf_python_100_1.0,copying a file to a path,python,"def getFileContent(self, path=None, file=None):
        if (not file):
            file = self.isFileExists(path)
        if (not file):
            return False
        if (self.isEncoded(file)):
            return self.decode(file.content)
        return file.content",https://github.com/ShawnClake/Apitax/blob/2eb9c6990d4088b2503c7f13c2a76f8e59606e6d/apitax/integrations/Github.py#L79-L86
tfidf_python_100_1.0,copying a file to a path,python,"def copy_file(src, dst, ignore=None):
   """""" this function will simply copy the file from the source path to the dest
   path given as input
   """"""
   # Sanity checkpoint
   src = re.sub('[^\w/\-\.\*]', '', src)
   dst = re.sub('[^\w/\-\.\*]', '', dst)
   if len(re.sub('[\W]', '', src)) < 5 or len(re.sub('[\W]', '', dst)) < 5:
      debug.log(""Error: Copying file failed. Provided paths are invalid! src='%s' dst='%s'""%(src, dst))
   else:
      # Check destination
      check = False
      if dst[-1] == '/':
         if os.path.exists(dst):
            check = True # Valid Dir
         else:
            debug.log(""Error: Copying file failed. Destination directory does not exist (%s)""%(dst)) #DEBUG
      elif os.path.exists(dst):
         if os.path.isdir(dst):
            check = True # Valid Dir
            dst += '/' # Add missing slash
         else:
            debug.log(""Error: Copying file failed. %s exists!""%dst)
      elif os.path.exists(os.path.dirname(dst)):
         check = True # Valid file path
      else:
         debug.log(""Error: Copying file failed. %s is an invalid distination!""%dst)
      if check:
         # Check source
         files = glob.glob(src)
         if ignore is not None: files = [fil for fil in files if not ignore in fil]
         if len(files) != 0:
            debug.log(""Copying File(s)..."", ""Copy from %s""%src, ""to %s""%dst) #DEBUG
            for file_ in files:
               # Check file exists
               if os.path.isfile(file_):
                  debug.log(""Copying file: %s""%file_) #DEBUG
                  shutil.copy(file_, dst)
               else:
                  debug.log(""Error: Copying file failed. %s is not a regular file!""%file_) #DEBUG
         else: debug.log(""Error: Copying file failed. No files were found! (%s)""%src)",https://github.com/OLC-Bioinformatics/sipprverse/blob/d4f10cdf8e1a39dac0953db61c21c97efc6006de/cgecore/utility.py#L534-L574
tfidf_python_100_1.0,copying a file to a path,python,"def close(self) -> None:
        """"""
        To act as a file.
        """"""
        if self.underlying_stream:
            if self.using_stdout:
                sys.stdout = self.underlying_stream
            else:
                sys.stderr = self.underlying_stream
            self.underlying_stream = None
        if self.file:
            # Do NOT close the file; we don't own it.
            self.file = None
            log.debug(""Finished copying {} to {}"",
                      self.output_description, self.filename)",https://github.com/RudolfCardinal/pythonlib/blob/0b84cb35f38bd7d8723958dae51b480a829b7227/cardinal_pythonlib/tee.py#L272-L286
tfidf_python_100_1.0,copying a file to a path,python,"def write_to_storage(self, file, path):
        self.logger.info(""Writing file to %s"", path)
        self.storage.write_file(file, path)",https://github.com/django-dbbackup/django-dbbackup/blob/77de209e2d5317e51510d0f888e085ee0c400d66/dbbackup/management/commands/_base.py#L86-L88
tfidf_python_100_1.0,copying a file to a path,python,"def files_generator(path, recursive):
    """"""Yield files found in a given path.

    Walk over a given path finding and yielding all
    files found on it. This can be done only on the root
    directory or recursively.

    Args:
        path: Path to the directory.
        recursive: Whether to find files recursively or not.

    Yields:
        A tuple for each file in the given path containing
        the path and the name of the file.
    """"""
    if recursive:
        for (path, _, files) in os.walk(path):
            for file in files:
                if not file.endswith(BATCH_EXTENSION):
                    yield (path, file)
    else:
        for file in os.listdir(path):
            if (os.path.isfile(os.path.join(path, file)) and
                    not file.endswith(BATCH_EXTENSION)):
                yield (path, file)",https://github.com/davidmogar/cucco/blob/e2a0ff3342e4a9f25a65c486206a5a2ae1a4dbd4/cucco/batch.py#L11-L35
tfidf_python_100_1.0,get the description of a http status code,python,"def get(self):
        """"""
        Return the HTTP code status.

        :return: The matched and formatted status code.
        :rtype: str|int|None
        """"""
        if PyFunceble.HTTP_CODE[""active""]:
            # The http status code extraction is activated.

            # We get the http status code.
            http_code = self._access()

            # We initiate a variable which will save the list of allowed
            # http status code.
            list_of_valid_http_code = []

            for codes in [
                PyFunceble.HTTP_CODE[""list""][""up""],
                PyFunceble.HTTP_CODE[""list""][""potentially_down""],
                PyFunceble.HTTP_CODE[""list""][""potentially_up""],
            ]:
                # We loop throught the list of http status code.

                # We extend the list of valid with the currently read
                # codes.
                list_of_valid_http_code.extend(codes)

            if http_code not in list_of_valid_http_code or http_code is None:
                # * The extracted http code is not in the list of valid http code.
                # or
                # * The extracted http code is equal to `None`.

                # We return 3 star in order to mention that we were not eable to extract
                # the http status code.
                return ""*"" * 3

            # * The extracted http code is in the list of valid http code.
            # or
            # * The extracted http code is not equal to `None`.

            # We return the extracted http status code.
            return http_code

        # The http status code extraction is activated.

        # We return None.
        return None",https://github.com/funilrys/PyFunceble/blob/cdf69cbde120199171f7158e1c33635753e6e2f5/PyFunceble/http_code.py#L157-L204
tfidf_python_100_1.0,get the description of a http status code,python,"def __init__(self, code: int, description: str):
        self.code = code
        self.description = description",https://github.com/telminov/sw-python-sms-devino/blob/ce8a8aa67a4585ea6c8af986a280e68374a7e531/sms_devino/client.py#L22-L24
tfidf_python_100_1.0,get the description of a http status code,python,"def check_status(self):
        """"""
        Check if the status of the response is success or not

        :raises: Exception. If the status is not success
        """"""
        status = OneLogin_Saml2_Utils.get_status(self.document)
        code = status.get('code', None)
        if code and code != OneLogin_Saml2_Constants.STATUS_SUCCESS:
            splited_code = code.split(':')
            printable_code = splited_code.pop()
            status_exception_msg = 'The status code of the Response was not Success, was %s' % printable_code
            status_msg = status.get('msg', None)
            if status_msg:
                status_exception_msg += ' -> ' + status_msg
            raise OneLogin_Saml2_ValidationError(
                status_exception_msg,
                OneLogin_Saml2_ValidationError.STATUS_CODE_IS_NOT_SUCCESS
            )",https://github.com/onelogin/python3-saml/blob/064b7275fba1e5f39a9116ba1cdcc5d01fc34daa/src/onelogin/saml2/response.py#L330-L348
tfidf_python_100_1.0,get the description of a http status code,python,"def _set_status(self, status):
        if isinstance(status, int):
            code, status = status, _HTTP_STATUS_LINES.get(status)
        elif ' ' in status:
            status = status.strip()
            code   = int(status.split()[0])
        else:
            raise ValueError('String status line without a reason phrase.')
        if not 100 <= code <= 999: raise ValueError('Status code out of range.')
        self._status_code = code
        self._status_line = str(status or ('%d Unknown' % code))",https://github.com/ManiacalLabs/PixelWeb/blob/9eacbfd40a1d35011c2dcea15c303da9636c6b9e/pixelweb/bottle.py#L1495-L1505
tfidf_python_100_1.0,get the description of a http status code,python,"def __init__(self, code):
        self.code = code
        error_tuple = self._errors.get(code, ('', ''))
        self.description = error_tuple[0]
        self.status = error_tuple[1]",https://github.com/juanifioren/django-oidc-provider/blob/f0daed07b2ac7608565b80d4c80ccf04d8c416a8/oidc_provider/lib/errors.py#L187-L191
tfidf_python_100_1.0,get the description of a http status code,python,"def simple_table_status(description):
    status = ready
    if description.get(""TableStatus"") != ""ACTIVE"":
        status = None
    for index in description.get(""GlobalSecondaryIndexes"", []):
        if index.get(""IndexStatus"") != ""ACTIVE"":
            status = None
    return status",https://github.com/numberoverzero/bloop/blob/4c95f5a0ff0802443a1c258bfaccecd1758363e7/bloop/session.py#L852-L859
tfidf_python_100_1.0,get the description of a http status code,python,"def get_description(status_code):
    """"""
    Get the description for a status code.
    """"""
    description = _descriptions.get(status_code)
    if description is None:
        description = 'code = %s (no description)' % str(status_code)
    return description",https://github.com/vecnet/vecnet.simulation/blob/3a4b3df7b12418c6fa8a7d9cd49656a1c031fc0e/vecnet/simulation/sim_status.py#L47-L54
tfidf_python_100_1.0,get the description of a http status code,python,"def p_moduleComplianceClause(self, p):
        """"""moduleComplianceClause : LOWERCASE_IDENTIFIER MODULE_COMPLIANCE STATUS Status DESCRIPTION Text ReferPart ComplianceModulePart COLON_COLON_EQUAL '{' objectIdentifier '}'""""""
        p[0] = ('moduleComplianceClause',
                p[1],  # id
                #  p[2], # MODULE_COMPLIANCE
                p[4],  # status
                (p[5], p[6]),  # description
                p[7],  # reference
                p[8],  # ComplianceModules
                p[11])",https://github.com/etingof/pysmi/blob/379a0a384c81875731be51a054bdacced6260fd8/pysmi/parser/smi.py#L916-L925
tfidf_python_100_1.0,get the description of a http status code,python,"def p_objectIdentityClause(self, p):
        """"""objectIdentityClause : LOWERCASE_IDENTIFIER OBJECT_IDENTITY STATUS Status DESCRIPTION Text ReferPart COLON_COLON_EQUAL '{' objectIdentifier '}'""""""
        p[0] = ('objectIdentityClause', p[1],  # id
                #  p[2], # OBJECT_IDENTITY
                p[4],  # status
                (p[5], p[6]),  # description
                p[7],  # reference
                p[10])",https://github.com/etingof/pysmi/blob/379a0a384c81875731be51a054bdacced6260fd8/pysmi/parser/smi.py#L363-L370
tfidf_python_100_1.0,get the description of a http status code,python,"def parse_ststus (self, status):
        try:    
            code, status = status.split ("" "", 1)
            code = int (code)
        except:
            raise AssertionError (""Can't understand given status code"")        
        return code, status",https://github.com/hansroh/skitai/blob/d8ed2a02986f8bb0013a593c083a2ca97818f6d2/skitai/http_response.py#L170-L176
tfidf_python_100_1.0,get the description of a http status code,python,"def __init__(self, status, description):
        # status should be a string of form '404 Not Found'
        self.status = status
        self.description = description",https://github.com/sio2project/filetracker/blob/359b474850622e3d0c25ee2596d7242c02f84efb/filetracker/servers/base.py#L19-L22
tfidf_python_100_1.0,get the description of a http status code,python,"def __init__(self, description=None, code=None):
        """""" Class init.

            `description`
                Error description string.
            `code`
                Exit status code. ``Default: 2``
            """"""

        super(FocusError, self).__init__(description)

        if not description is None:
            self.description = description
        else:
            self.description = getattr(self, 'description', None)
        if self.description:
            self.description = common.from_utf8(self.description)

        if not code is None:
            self.code = code
        else:
            self.code = getattr(self, 'code', None) or 2",https://github.com/xtrementl/focus/blob/cbbbc0b49a7409f9e0dc899de5b7e057f50838e4/focus/errors.py#L18-L39
tfidf_python_100_1.0,get the description of a http status code,python,"def __str__(self):
        return six.text_type(
            '<Error: status={status}, code={code}>'.format(
                status=self.status, code=self.code
            )
        )",https://github.com/sbg/sevenbridges-python/blob/f62640d1018d959f0b686f2dbe5e183085336607/sevenbridges/models/compound/error.py#L18-L23
tfidf_python_100_1.0,get the description of a http status code,python,"def to_short_dict(self):
        hsh = {
            ""name"": self.name,
            ""status"": self.status,
        }
        if self.description:
            hsh[""description""] = self.description
        return hsh",https://github.com/spulec/moto/blob/4a286c4bc288933bb023396e2784a6fdbb966bc9/moto/swf/models/domain.py#L34-L41
tfidf_python_100_1.0,get the description of a http status code,python,"def __init__(self, code=None, description=None, avs_response=None, cvv_response=None):
        self.code = code
        self.description = description
        self.avs_response = avs_response
        self.cvv_response = cvv_response",https://github.com/OpenPaymentPlatform/python/blob/6fd5b77f2fade38a9b8dcf57addad59619131b5e/opp/facade.py#L468-L472
tfidf_python_100_1.0,get the description of a http status code,python,"def __init__(self, code=None, description=None,
                 not_before=None, not_after=None):
        self.code = code
        self.description = description
        self.not_before = not_before
        self.not_after = not_after",https://github.com/yyuu/botornado/blob/fffb056f5ff2324d1d5c1304014cfb1d899f602e/boto/ec2/instancestatus.py#L53-L58
tfidf_python_100_1.0,get the description of a http status code,python,"def build_phenotype(phenotype_info):
    phenotype_obj = {}
    phenotype_obj['mim_number'] = phenotype_info['mim_number']
    phenotype_obj['description'] = phenotype_info['description']
    phenotype_obj['inheritance_models'] = list(phenotype_info.get('inheritance', set()))
    phenotype_obj['status'] = phenotype_info['status']
    
    return phenotype_obj",https://github.com/Clinical-Genomics/scout/blob/90a551e2e1653a319e654c2405c2866f93d0ebb9/scout/build/genes/hgnc_gene.py#L7-L14
tfidf_python_100_1.0,get the description of a http status code,python,"def get_exception_description(self):
        """"""
        @rtype:  str
        @return: User-friendly name of the exception.
        """"""
        code = self.get_exception_code()
        description = self.__exceptionDescription.get(code, None)
        if description is None:
            try:
                description = 'Exception code %s (%s)'
                description = description % (HexDump.integer(code),
                                             ctypes.FormatError(code))
            except OverflowError:
                description = 'Exception code %s' % HexDump.integer(code)
        return description",https://github.com/fabioz/PyDev.Debugger/blob/ed9c4307662a5593b8a7f1f3389ecd0e79b8c503/pydevd_attach_to_process/winappdbg/event.py#L440-L454
tfidf_python_100_1.0,get the description of a http status code,python,"def p_objectGroupClause(self, p):
        """"""objectGroupClause : LOWERCASE_IDENTIFIER OBJECT_GROUP ObjectGroupObjectsPart STATUS Status DESCRIPTION Text ReferPart COLON_COLON_EQUAL '{' objectIdentifier '}'""""""
        p[0] = ('objectGroupClause',
                p[1],  # id
                p[3],  # objects
                p[5],  # status
                (p[6], p[7]),  # description
                p[8],  # reference
                p[11])",https://github.com/etingof/pysmi/blob/379a0a384c81875731be51a054bdacced6260fd8/pysmi/parser/smi.py#L896-L904
tfidf_python_100_1.0,get the description of a http status code,python,"def __init__(self, status, description=None, data=None, field=None, field_problem=None):
        if not isinstance(status, ServerResponseStatus):
            status = self.UNKNOWN

        self.status = status
        self.description = self.status.description
        if isinstance(description, str):
            self.description = description

        self.data = data
        self.field = field
        self.field_problem = field_problem",https://github.com/ktsstudio/tornkts/blob/db47e4550426282960a7e4486dcc4399c8d52e02/tornkts/base/server_response.py#L48-L59
tfidf_python_100_1.0,randomly extract x items from a list,python,"def extract_abbreviations(fulltext):
    """"""Extract acronyms from the fulltext.

    :param fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """"""
    acronyms = {}
    for k, v in get_acronyms(fulltext).items():
        acronyms[KeywordToken(k, type='acronym')] = v
    return acronyms",https://github.com/inveniosoftware-contrib/invenio-classifier/blob/3c758cf34dca6bf0548e7da5de34e5f72e3b255e/invenio_classifier/engine.py#L72-L84
tfidf_python_100_1.0,randomly extract x items from a list,python,"def extract(z, j):
    q, r, s, t = z
    return (q * j + r) // (s * j + t)",https://github.com/python/performance/blob/2a9524c0a5714e85106671bc61d750e800fe17db/performance/benchmarks/bm_pidigits.py#L35-L37
tfidf_python_100_1.0,randomly extract x items from a list,python,"def apply(self, page_string):
        value = self.extract(page_string)
        
        if self.sub_rules:
            value['sub_rules'] = self.sub_rules.extract(value['extract'])
        
        value['extract'] = self.remove_html(value['extract'])
        return value",https://github.com/usc-isi-i2/etk/blob/aab077c984ea20f5e8ae33af622fe11d3c4df866/etk/dependencies/landmark/landmark_extractor/extraction/Landmark.py#L188-L195
tfidf_python_100_1.0,randomly extract x items from a list,python,"def clear_dict(self, d):
        return {k: v for k, v in list(d.items()) if v}",https://github.com/CivicSpleen/ambry/blob/d7f2be4bf1f7ffd086f3fadd4fcae60c32473e42/ambry/identity.py#L150-L151
tfidf_python_100_1.0,randomly extract x items from a list,python,"def _quokka_normalize_extract(extract):
    """"""
    Generate a normalized rectangle to be extract from the standard quokka image.

    Parameters
    ----------
    extract : 'square' or tuple of number or imgaug.BoundingBox or imgaug.BoundingBoxesOnImage
        Unnormalized representation of the image subarea to be extracted.

            * If string ``square``, then a squared area ``(x: 0 to max 643, y: 0 to max 643)``
              will be extracted from the image.
            * If a tuple, then expected to contain four numbers denoting ``x1``, ``y1``, ``x2``
              and ``y2``.
            * If a BoundingBox, then that bounding box's area will be extracted from the image.
            * If a BoundingBoxesOnImage, then expected to contain exactly one bounding box
              and a shape matching the full image dimensions (i.e. (643, 960, *)). Then the
              one bounding box will be used similar to BoundingBox.

    Returns
    -------
    bb : imgaug.BoundingBox
        Normalized representation of the area to extract from the standard quokka image.

    """"""
    # TODO get rid of this deferred import
    from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage

    if extract == ""square"":
        bb = BoundingBox(x1=0, y1=0, x2=643, y2=643)
    elif isinstance(extract, tuple) and len(extract) == 4:
        bb = BoundingBox(x1=extract[0], y1=extract[1], x2=extract[2], y2=extract[3])
    elif isinstance(extract, BoundingBox):
        bb = extract
    elif isinstance(extract, BoundingBoxesOnImage):
        do_assert(len(extract.bounding_boxes) == 1)
        do_assert(extract.shape[0:2] == (643, 960))
        bb = extract.bounding_boxes[0]
    else:
        raise Exception(
            ""Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage ""
            + ""for parameter 'extract', got %s."" % (type(extract),)
        )
    return bb",https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/imgaug.py#L464-L506
tfidf_python_100_1.0,randomly extract x items from a list,python,"def _complex_store(self, name, value, extract):
        if extract is not None:
            baseval = self._simple_load(name)
            if extract[0] != len(baseval) - 1:
                value = baseval[len(baseval)-1:extract[0]+1].concat(value)
            if extract[1] != 0:
                value = value.concat(baseval[extract[1]-1:0])

        self._simple_store(name, value)",https://github.com/angr/angr/blob/4e2f97d56af5419ee73bdb30482c8dd8ff5f3e40/angr/state_plugins/light_registers.py#L125-L133
tfidf_python_100_1.0,randomly extract x items from a list,python,"def flattenResult(extraction_object, name = 'root'):
    result = {}
    if isinstance(extraction_object, dict):
        if 'sub_rules' in extraction_object:
            for item in extraction_object['sub_rules']:
                result[item] = flattenResult(extraction_object['sub_rules'][item], item)
        elif 'sequence' in extraction_object:
            result = flattenResult(extraction_object['sequence'], 'sequence')
        elif 'extract' in extraction_object:
            return extraction_object['extract']
        else:
            for extract in extraction_object:
                result[extract] = flattenResult(extraction_object[extract], extract)
    
    if isinstance(extraction_object, list):
        result = []
        for extract in extraction_object:
            result.append(flattenResult(extract, 'sequence'))
    return result",https://github.com/usc-isi-i2/etk/blob/aab077c984ea20f5e8ae33af622fe11d3c4df866/etk/dependencies/landmark/landmark_extractor/extraction/Landmark.py#L36-L54
tfidf_python_100_1.0,randomly extract x items from a list,python,"def items(self):
        ""Returns a list of (key, value) pairs as 2-tuples.""
        return (list(self._pb.IntMap.items()) + list(self._pb.StringMap.items()) +
                list(self._pb.FloatMap.items()) + list(self._pb.BoolMap.items()))",https://github.com/intelsdi-x/snap-plugin-lib-py/blob/8da5d00ac5f9d2b48a7239563ac7788209891ca4/snap_plugin/v1/config_map.py#L190-L193
tfidf_python_100_1.0,randomly extract x items from a list,python,"def _merge_relationships(forward_relations, reverse_relations):
    return OrderedDict(
        list(forward_relations.items()) +
        list(reverse_relations.items())
    )",https://github.com/ronaldguillen/wave/blob/20bb979c917f7634d8257992e6d449dc751256a9/wave/utils/model_meta.py#L180-L184
tfidf_python_100_1.0,randomly extract x items from a list,python,"def not_null(self, field_extraction, param = None):
        #print ""in not_null""
        extract = field_extraction['extract']
        if extract is None or not extract:
            return False
        else:
            return True",https://github.com/usc-isi-i2/etk/blob/aab077c984ea20f5e8ae33af622fe11d3c4df866/etk/dependencies/landmark/landmark_extractor/validation/Validation.py#L48-L54
tfidf_python_100_1.0,randomly extract x items from a list,python,"def _complex_load(self, name, extract):
        val = self._simple_load(name)
        if extract is not None:
            val = val[extract[0]:extract[1]]
        return val",https://github.com/angr/angr/blob/4e2f97d56af5419ee73bdb30482c8dd8ff5f3e40/angr/state_plugins/light_registers.py#L80-L84
tfidf_python_100_1.0,randomly extract x items from a list,python,"def update(self, d=None, **kwargs):
        items = []
        if d is not None:
            items = list(d.items())
        for k, v in items + list(kwargs.items()):
            self[k] = v",https://github.com/juiceinc/recipe/blob/2e60c2242aeaea3029a2274b31bc3a937761e568/recipe/shelf.py#L524-L529
tfidf_python_100_1.0,randomly extract x items from a list,python,"def visit_extract(self, extract, **kwargs):
        field = self.extract_map.get(extract.field, extract.field)
        return {
            'type': 'extract',
            'field': field,
            'expression': extract.expr._compiler_dispatch(self, **kwargs)}",https://github.com/OpenEnergyPlatform/oedialect/blob/40a8d9e9b272ea4674d2c40dd6b3e6cc15f91c1e/oedialect/compiler.py#L765-L770
tfidf_python_100_1.0,randomly extract x items from a list,python,"def __item(self, items):
        if not items:
            items = []
        elif callable_attr(items, 'allitems'):
            items = list(items.allitems())
        elif callable_attr(items, 'iterallitems'):
            items = list(items.iterallitems())
        elif callable_attr(items, 'items'):
            items = list(six.iteritems(items))
        elif isinstance(items, six.string_types):
            items = self.__extract_items_from_string(items)
        else:
            items = list(items)
        return items",https://github.com/by46/simplekit/blob/33f3ce6de33accc185e1057f096af41859db5976/simplekit/url/query.py#L69-L82
tfidf_python_100_1.0,randomly extract x items from a list,python,"def __init__(self, items):
        """"""Initialize the list with one item or a list of items.

        Args:
            items (iterable, ``pyof_class``): Items to be stored.
        """"""
        super().__init__()
        if isinstance(items, list):
            self.extend(items)
        elif items:
            self.append(items)",https://github.com/kytos/python-openflow/blob/4f2d0d08ab28e102ed88fe57a4ee17729f1e1bb7/pyof/foundation/basic_types.py#L522-L532
tfidf_python_100_1.0,randomly extract x items from a list,python,"def gen_pi_digits():
    z = (1, 0, 0, 1)
    x = gen_x()
    while 1:
        y = extract(z, 3)
        while y != extract(z, 4):
            z = compose(z, next(x))
            y = extract(z, 3)
        z = compose((10, -10 * y, 0, 1), z)
        yield y",https://github.com/python/performance/blob/2a9524c0a5714e85106671bc61d750e800fe17db/performance/benchmarks/bm_pidigits.py#L40-L49
tfidf_python_100_1.0,randomly extract x items from a list,python,"def __init__(self, nonce_size: int = SecretBox.NONCE_SIZE, secret_size: int = SecretBox.KEY_SIZE):
        # Amount of bytes the randomly generated nonces
        self.nonce_size = nonce_size

        # Amount of bytes for the randomly generated secret keys
        self.secret_size = secret_size",https://github.com/drakantas/Ryoken/blob/fb88049c62556d929525509f5e9c192ae2fc6f3b/ryoken/tokenizer.py#L12-L17
tfidf_python_100_1.0,randomly extract x items from a list,python,"def extract(msg):
    return napalm_logs.utils.extract(_RGX, msg, _RGX_PARTS, _TIME_FORMAT) or\
           napalm_logs.utils.extract(_ALT_RGX, msg, _ALT_RGX_PARTS, _ALT_TIME_FORMAT)",https://github.com/napalm-automation/napalm-logs/blob/4b89100a6e4f994aa004f3ea42a06dc803a7ccb0/napalm_logs/config/nxos/__init__.py#L39-L41
tfidf_python_100_1.0,randomly extract x items from a list,python,"def extract(self, page_string):
        try:
            begin_match_end = 0
            if self.begin_regex:
                begin_match = self.begin_rule.search(page_string)
                begin_match_end = begin_match.end()
            end_match_start = len(page_string)
            end_match_end = len(page_string)
            
            if self.end_regex:
                end_match = self.end_rule.search(page_string[begin_match_end:])
                end_match_start = end_match.start()
                end_match_end   = end_match.end()
            
            if self.include_end_regex:
                extract = page_string[begin_match_end:begin_match_end+end_match_end]
                begin_index = begin_match_end
                end_index = begin_match_end+end_match_end
            else:
                extract = page_string[begin_match_end:begin_match_end+end_match_start]
                begin_index = begin_match_end
                end_index = begin_match_end+end_match_start
            
            if extract and self.strip_end_regex:
                extract = re.sub(self.strip_end_regex+'$', '', extract)
                end_index = begin_index + len(extract)
        except:
            extract = ''
            begin_index = -1
            end_index = -1
        return {'rule_id': self.id,'extract': extract,'begin_index':begin_index,'end_index':end_index}",https://github.com/usc-isi-i2/etk/blob/aab077c984ea20f5e8ae33af622fe11d3c4df866/etk/dependencies/landmark/landmark_extractor/extraction/Landmark.py#L197-L227
tfidf_python_100_1.0,randomly extract x items from a list,python,"def create_combo_box_widget(self, items=None, item_text_getter=None):
        combo_box_widget = ComboBoxWidget(self.__ui)
        combo_box_widget.item_text_getter = item_text_getter
        combo_box_widget.items = items if items is not None else list()
        return combo_box_widget",https://github.com/nion-software/nionswift/blob/d43693eaf057b8683b9638e575000f055fede452/nion/swift/Facade.py#L588-L592
tfidf_python_100_1.0,convert a date string into yyyymmdd,python,"def convertDate(date):
    """"""Convert DATE string into a decimal year.""""""

    d, t = date.split('T')
    return decimal_date(d, timeobs=t)",https://github.com/spacetelescope/stsci.tools/blob/9a022503ad24ca54ce83331482dfa3ff6de9f403/lib/stsci/tools/fileutil.py#L171-L175
tfidf_python_100_1.0,convert a date string into yyyymmdd,python,"def _convert_date(self, date):
        """"""Convert '106/05/01' to '2017/05/01'""""""
        return '/'.join([str(int(date.split('/')[0]) + 1911)] + date.split('/')[1:])",https://github.com/mlouielu/twstock/blob/cddddcc084d2d00497d591ab3059e3205b755825/twstock/stock.py#L31-L33
tfidf_python_100_1.0,convert a date string into yyyymmdd,python,"def convert_date(date):
    """"""Convert string to datetime object.""""""
    date = convert_month(date, shorten=False)
    clean_string = convert_string(date)
    return datetime.strptime(clean_string, DATE_FMT.replace('-',''))",https://github.com/sharibarboza/py_zap/blob/ce90853efcad66d3e28b8f1ac910f275349d016c/py_zap/utils.py#L91-L95
tfidf_python_100_1.0,convert a date string into yyyymmdd,python,"def _discover_publication_date(opf_xmldoc, date_html=None):
    date = __discover_dc(opf_xmldoc, 'date')

    if not date and date_html is not None:
        date = _find_publish_date_from_dom(date_html)

    return date",https://github.com/paulocheque/epub-meta/blob/3f0efb9f29a286b1a6896ad05422b23f10e10164/epub_meta/collector.py#L165-L171
tfidf_python_100_1.0,convert a date string into yyyymmdd,python,"def get_date(date):
    """"""
    Get the date from a value that could be a date object or a string.

    :param date: The date object or string.

    :returns: The date object.
    """"""
    if type(date) is str:
        return datetime.strptime(date, '%Y-%m-%d').date()
    else:
        return date",https://github.com/Ex-Mente/auxi.0/blob/2dcdae74154f136f8ca58289fe5b20772f215046/auxi/core/helpers.py#L23-L34
tfidf_python_100_1.0,convert a date string into yyyymmdd,python,"def date(self):
        """"""Convert instant to a date.

        >>> instant(2014).date
        datetime.date(2014, 1, 1)
        >>> instant('2014-2').date
        datetime.date(2014, 2, 1)
        >>> instant('2014-2-3').date
        datetime.date(2014, 2, 3)
        """"""
        instant_date = date_by_instant_cache.get(self)
        if instant_date is None:
            date_by_instant_cache[self] = instant_date = datetime.date(*self)
        return instant_date",https://github.com/openfisca/openfisca-core/blob/92ce9396e29ae5d9bac5ea604cfce88517c6b35c/openfisca_core/periods.py#L64-L77
tfidf_python_100_1.0,convert a date string into yyyymmdd,python,"def _get_crime_categories(self, date=None):
        if date not in self.crime_categories:
            self._populate_crime_categories(date=date)
        return self.crime_categories[date]",https://github.com/rkhleics/police-api-client-python/blob/b5c1e493487eb2409e2c04ed9fbd304f73d89fdc/police_api/__init__.py#L166-L169
tfidf_python_100_1.0,convert a date string into yyyymmdd,python,"def to_date_string(date):
    if isinstance(date, numpy.int64) or isinstance(date, int):
        date = str(date)
        date = '%s-%s-%s' % (date[:4], date[4:6], date[6:8])
        return date",https://github.com/CxAalto/gtfspy/blob/bddba4b74faae6c1b91202f19184811e326547e5/gtfspy/util.py#L192-L196
tfidf_python_100_1.0,convert a date string into yyyymmdd,python,"def QA_util_date_int2str(int_date):
    """"""
    ç±»ådatetime.datatime
    :param date: int 8ä½æ´æ°
    :return: ç±»åstr
    """"""
    date = str(int_date)
    if len(date) == 8:
        return str(date[0:4] + '-' + date[4:6] + '-' + date[6:8])
    elif len(date) == 10:
        return date",https://github.com/QUANTAXIS/QUANTAXIS/blob/bb1fe424e4108b62a1f712b81a05cf829297a5c0/QUANTAXIS/QAUtil/QADate.py#L74-L84
tfidf_python_100_1.0,convert a date string into yyyymmdd,python,"def rollforward(self, date):
        if self.onOffset(date):
            return date
        else:
            return date + type(self)()",https://github.com/pydata/xarray/blob/6d93a95d05bdbfc33fff24064f67d29dd891ab58/xarray/coding/cftime_offsets.py#L141-L145
tfidf_python_100_1.0,convert a date string into yyyymmdd,python,"def check_date(date):
    if isinstance(date, six.string_types):
        return Date.parseISO(date)
    else:
        return Date.fromDateTime(date)",https://github.com/iLampard/x-utils/blob/291d92832ee0e0c89bc22e10ecf2f44445e0d300/xutils/date_utils/date.py#L254-L258
tfidf_python_100_1.0,convert a date string into yyyymmdd,python,"def stringDate(date):
    # Convert to isoFormat
    try:
        string = date.isoformat(timespec='microseconds')
    # py2.7 to py3.5 does not have timespec
    except TypeError as e:
        string = date.isoformat()
        if string.find('.') == -1:
            string += '.000'
    string = datefmt_re.sub(r'\1Z', string)
    return string",https://github.com/taskcluster/json-e/blob/ac0c9fba1de3ed619f05a64dae929f6687789cbc/jsone/shared.py#L114-L124
tfidf_python_100_1.0,convert a date string into yyyymmdd,python,"def parse_date(dt, ignoretz=True, as_tz=None):
    """"""
    :param dt: string datetime to convert into datetime object.
    :return: date object if the string can be parsed into a date. Otherwise,
        return None.

    :see: http://labix.org/python-dateutil

    Examples:

    >>> parse_date('2011-12-30')
    datetime.date(2011, 12, 30)
    >>> parse_date('12/30/2011')
    datetime.date(2011, 12, 30)
    """"""
    dttm = parse_datetime(dt, ignoretz=ignoretz)
    return None if dttm is None else dttm.date()",https://github.com/InfoAgeTech/django-core/blob/9664a145473b75120bf71e1644e9c8086e7e8955/django_core/utils/date_parsers.py#L117-L133
tfidf_python_100_1.0,convert a date string into yyyymmdd,python,"def _to_date_str(cls, date):
        if isinstance(date, (datetime.datetime, datetime.date)):
            return date.strftime('%Y-%m-%d')
        elif isinstance(date, six.string_types):
            return date
        else:
            raise ValueError('Can not convert %s type to str', type(date))",https://github.com/jxtech/wechatpy/blob/4df0da795618c0895a10f1c2cde9e9d5c0a93aaa/wechatpy/client/api/datacube.py#L15-L21
tfidf_python_100_1.0,convert a date string into yyyymmdd,python,"def format_date(self, date):
        if isinstance(date, datetime.datetime):
            date = date.date()

        if isinstance(date, datetime.date):
            date = date.strftime('%d%m%y')

        if not patterns.DATE.match(date):
            raise ValueError(""Invalid date: "" + date)

        return date",https://github.com/Turbo87/aerofiles/blob/d8b7b04a1fcea5c98f89500de1164619a4ec7ef4/aerofiles/igc/writer.py#L24-L34
tfidf_python_100_1.0,convert a date string into yyyymmdd,python,"def maybe_convert_to_index_date_type(index, date):
    """"""Convert a datetime-like object to the index's date type.

    Datetime indexing in xarray can be done using either a pandas
    DatetimeIndex or a CFTimeIndex.  Both support partial-datetime string
    indexing regardless of the calendar type of the underlying data;
    therefore if a string is passed as a date, we return it unchanged.  If a
    datetime-like object is provided, it will be converted to the underlying
    date type of the index.  For a DatetimeIndex that is np.datetime64; for a
    CFTimeIndex that is an object of type cftime.datetime specific to the
    calendar used.

    Parameters
    ----------
    index : pd.Index
        Input time index
    date : datetime-like object or str
        Input datetime

    Returns
    -------
    date of the type appropriate for the time index of the Dataset
    """"""
    if isinstance(date, str):
        return date

    if isinstance(index, pd.DatetimeIndex):
        if isinstance(date, np.datetime64):
            return date
        else:
            return np.datetime64(str(date))
    else:
        date_type = index.date_type
        if isinstance(date, date_type):
            return date
        else:
            if isinstance(date, np.datetime64):
                # Convert to datetime.date or datetime.datetime object
                date = date.item()

            if isinstance(date, datetime.date):
                # Convert to a datetime.datetime object
                date = datetime.datetime.combine(
                    date, datetime.datetime.min.time())

            return date_type(date.year, date.month, date.day, date.hour,
                             date.minute, date.second, date.microsecond)",https://github.com/spencerahill/aospy/blob/2f6e775b9b9956c54af117fdcdce2c87196afb6c/aospy/utils/times.py#L568-L614
tfidf_python_100_1.0,convert a date string into yyyymmdd,python,"def isEndOfMonth(date):
        m = date.month()
        y = date.year()
        return date.dayOfMonth() == _month_length(m, Date.isLeap(y))",https://github.com/iLampard/x-utils/blob/291d92832ee0e0c89bc22e10ecf2f44445e0d300/xutils/date_utils/date.py#L179-L182
tfidf_python_100_1.0,convert a date string into yyyymmdd,python,"def endOfMonth(date):
        m = date.month()
        y = date.year()
        return Date(y, m, _month_length(m, Date.isLeap(y)))",https://github.com/iLampard/x-utils/blob/291d92832ee0e0c89bc22e10ecf2f44445e0d300/xutils/date_utils/date.py#L173-L176
tfidf_python_100_1.0,convert a date string into yyyymmdd,python,"def __deserialize_date(self, string):
        """"""
        Deserializes string to date.

        :param string: str.
        :return: date.
        """"""
        try:
            from dateutil.parser import parse
            return parse(string).date()
        except ImportError:
            return string
        except ValueError:
            raise ApiException(
                status=0,
                reason=""Failed to parse `{0}` into a date object"".format(string)
            )",https://github.com/kubernetes-client/python/blob/5e512ff564c244c50cab780d821542ed56aa965a/kubernetes/client/api_client.py#L573-L589
tfidf_python_100_1.0,convert a date string into yyyymmdd,python,"def _containsdate(self, date):
        date = Date(date)
        return ((self.firstdate <= date <= self.lastdate) and
                ((date-self.firstdate) // self.stepsize))",https://github.com/hydpy-dev/hydpy/blob/1bc6a82cf30786521d86b36e27900c6717d3348d/hydpy/core/timetools.py#L1534-L1537
tfidf_python_100_1.0,convert a utc time to epoch,python,"def _datetime_to_utc_int(date):
    """"""Convert the integer UTC time value into a local datetime.""""""
    if date is None:
      return None

    # Convert localized datetime to a UTC integer
    epoch = dsub_util.replace_timezone(datetime.utcfromtimestamp(0), pytz.utc)
    return (date - epoch).total_seconds()",https://github.com/DataBiosphere/dsub/blob/443ce31daa6023dc2fd65ef2051796e19d18d5a7/dsub/providers/google.py#L466-L473
tfidf_python_100_1.0,convert a utc time to epoch,python,"def to_local_time(timestamp):
    """"""Convert a datatime object from UTC time to local time.

    Adopted from:
    http://stackoverflow.com/questions/4770297/python-convert-utc-datetime-string-to-local-datetime

    Parameters
    ----------
    timestamp : string
        Default string representation of timestamps expected to be in
        UTC time zone

    Returns
    -------
    datetime
        Datetime object in local time zone
    """"""
    utc = dt.datetime.strptime(timestamp, '%Y-%m-%dT%H:%M:%S.%f')
    # Get UTC and local time zone
    from_zone = tz.gettz('UTC')
    to_zone = tz.tzlocal()

    # Tell the utc object that it is in UTC time zone
    utc = utc.replace(tzinfo=from_zone)

    # Convert time zone
    return utc.astimezone(to_zone)",https://github.com/heikomuller/sco-client/blob/c4afab71297f73003379bba4c1679be9dcf7cef8/scocli/scoserv.py#L303-L329
tfidf_python_100_1.0,convert a utc time to epoch,python,"def _format_timestamp(self, epoch):
        t = time.strftime(""%Y/%m/%d %H:%M:%S"", time.localtime(epoch))
        t += str(epoch % 1)[1:]
        return t",https://github.com/f3at/feat/blob/15da93fc9d6ec8154f52a9172824e25821195ef8/src/feat/agencies/journaler.py#L1446-L1449
tfidf_python_100_1.0,convert a utc time to epoch,python,"def to_utc(a_datetime, keep_utc_tzinfo=False):
    """"""
    Convert a time awared datetime to utc datetime.

    :param a_datetime: a timezone awared datetime. (If not, then just returns)
    :param keep_utc_tzinfo: whether to retain the utc time zone information.

    **ä¸­æææ¡£**

    å°ä¸ä¸ªå¸¦æ¶åºçæ¶é´è½¬åæUTCæ¶é´ãèå¯¹äºUTCæ¶é´èè¨, ææ²¡ææ¶åºä¿¡æ¯é½æ æè°äºã
    """"""
    if a_datetime.tzinfo:
        utc_datetime = a_datetime.astimezone(utc)  # convert to utc time
        if keep_utc_tzinfo is False:
            utc_datetime = utc_datetime.replace(tzinfo=None)
        return utc_datetime
    else:
        return a_datetime",https://github.com/MacHu-GWU/rolex-project/blob/a1111b410ed04b4b6eddd81df110fa2dacfa6537/rolex/util.py#L74-L91
tfidf_python_100_1.0,convert a utc time to epoch,python,"def cirs_xyz(self, epoch):
        """"""Compute cartesian CIRS coordinates at a given epoch (x, y, z).

        Calculate coordinates in the Celestial Intermediate Reference System
        (CIRS), a dynamical coordinate system referenced to the Celestial
        Intermediate Origin (CIO). As this is a dynamical system it must be
        calculated at a specific epoch.
        """"""
        if isinstance(epoch, Time):
            pass
        elif isinstance(epoch, float):
            epoch = Time(None, tt=epoch)
        elif epoch == 'date':
            epoch = self.t
        else:
            raise ValueError('the epoch= must be a Time object,'
                             ' a floating point Terrestrial Time (TT),'
                             ' or the string ""date"" for epoch-of-date')

        vector = einsum('ij...,j...->i...', epoch.C, self.position.au)
        return Distance(vector)",https://github.com/skyfielders/python-skyfield/blob/51d9e042e06457f6b1f2415296d50a38cb3a300f/skyfield/positionlib.py#L159-L179
tfidf_python_100_1.0,convert a utc time to epoch,python,"def epoch(self, epoch):
        if isinstance(epoch, Time):
            self._epoch = epoch.gps
        elif isinstance(epoch, units.Quantity):
            self._epoch = epoch.value
        else:
            self._epoch = float(epoch)",https://github.com/gwpy/gwpy/blob/7a92b917e7dd2d99b15895293a1fa1d66cdb210a/gwpy/timeseries/statevector.py#L379-L385
tfidf_python_100_1.0,convert a utc time to epoch,python,"def epoch(self, epoch):
        if epoch is None:
            self._epoch = None
        else:
            self._epoch = Decimal(str(to_gps(epoch)))",https://github.com/gwpy/gwpy/blob/7a92b917e7dd2d99b15895293a1fa1d66cdb210a/gwpy/types/array.py#L321-L325
tfidf_python_100_1.0,convert a utc time to epoch,python,"def _parse_epoch_frame_apy(epoch):
    if epoch == 2000.0 or epoch == '2000': epoch= 'J2000'
    elif epoch == 1950.0 or epoch == '1950': epoch= 'B1950'
    if not epoch is None and 'J' in epoch: frame= 'fk5'
    elif not epoch is None and 'B' in epoch: frame= 'fk4'
    else: frame= 'icrs'
    return (epoch,frame)",https://github.com/jobovy/galpy/blob/9c5b9fe65d58835624dffe432be282060918ee08/galpy/util/bovy_coords.py#L2440-L2446
tfidf_python_100_1.0,convert a utc time to epoch,python,"def utc(self):
        utc = self._utc_tuple()
        return array(utc) if self.shape else utc",https://github.com/skyfielders/python-skyfield/blob/51d9e042e06457f6b1f2415296d50a38cb3a300f/skyfield/timelib.py#L652-L654
tfidf_python_100_1.0,convert a utc time to epoch,python,"def __init__(self, msec=False, utc=True):
        self.msec = msec
        self.utc = utc",https://github.com/baverman/covador/blob/1597759f7ba77004efef1b27bf804539663b5488/covador/types.py#L605-L607
tfidf_python_100_1.0,convert a utc time to epoch,python,"def date_to_milliseconds(date_str):
    """"""Convert UTC date to milliseconds

    If using offset strings add ""UTC"" to date string e.g. ""now UTC"", ""11 hours ago UTC""

    See dateparse docs for formats http://dateparser.readthedocs.io/en/latest/

    :param date_str: date in readable format, i.e. ""January 01, 2018"", ""11 hours ago UTC"", ""now UTC""
    :type date_str: str
    """"""
    # get epoch value in UTC
    epoch = datetime.utcfromtimestamp(0).replace(tzinfo=pytz.utc)
    # parse our date string
    d = dateparser.parse(date_str)
    # if the date is not timezone aware apply UTC timezone
    if d.tzinfo is None or d.tzinfo.utcoffset(d) is None:
        d = d.replace(tzinfo=pytz.utc)

    # return the difference in time
    return int((d - epoch).total_seconds() * 1000.0)",https://github.com/sammchardy/python-binance/blob/31c0d0a32f9edd528c6c2c1dd3044d9a34ce43cc/examples/save_historical_data.py#L10-L29
tfidf_python_100_1.0,convert a utc time to epoch,python,"def set_epoch(self, epoch):
        """"""Set the GPS epoch
        """"""
        if epoch is None:
            self._epoch = None
            return
        if isinstance(epoch, (Number, Decimal)):
            self._epoch = float(epoch)
        else:
            self._epoch = float(to_gps(epoch))",https://github.com/gwpy/gwpy/blob/7a92b917e7dd2d99b15895293a1fa1d66cdb210a/gwpy/plot/gps.py#L99-L108
tfidf_python_100_1.0,convert a utc time to epoch,python,"def radec(self, epoch=None):
        r""""""Compute equatorial (RA, declination, distance)

        When called without a parameter, this returns standard ICRF
        right ascension and declination:

        >>> ra, dec, distance = ICRF([1, 2, 3]).radec()
        >>> print(ra, dec, distance, sep='\n')
        04h 13m 44.39s
        +53deg 18' 02.8""
        3.74166 au

        If you instead want the coordinates referenced to the dynamical
        system defined by the Earth's mean equator and equinox, provide
        an epoch time.  To get J2000.0 coordinates, for example:

        >>> ra, dec, distance = ICRF([1, 2, 3]).radec(ts.J2000)
        >>> print(ra, dec, sep='\n')
        04h 13m 43.32s
        +53deg 17' 55.1""

        """"""
        position_au = self.position.au
        if epoch is not None:
            if isinstance(epoch, Time):
                pass
            elif isinstance(epoch, float):
                epoch = Time(None, tt=epoch)
            elif epoch == 'date':
                epoch = self.t
            else:
                raise ValueError('the epoch= must be a Time object,'
                                 ' a floating point Terrestrial Time (TT),'
                                 ' or the string ""date"" for epoch-of-date')
            position_au = einsum('ij...,j...->i...', epoch.M, position_au)
        r_au, dec, ra = to_polar(position_au)
        return (Angle(radians=ra, preference='hours'),
                Angle(radians=dec, signed=True),
                Distance(r_au))",https://github.com/skyfielders/python-skyfield/blob/51d9e042e06457f6b1f2415296d50a38cb3a300f/skyfield/positionlib.py#L94-L132
tfidf_python_100_1.0,convert a utc time to epoch,python,"def ecliptic_xyz(self, epoch=None):
        """"""Compute J2000 ecliptic position vector (x, y, z).

        If you instead want the coordinates referenced to the dynamical
        system defined by the Earth's true equator and equinox, provide
        an epoch time.

        """"""
        if epoch is None:
            vector = _ECLIPJ2000.dot(self.position.au)
            return Distance(vector)

        position_au = self.position.au

        if isinstance(epoch, Time):
            pass
        elif isinstance(epoch, float):
            epoch = Time(None, tt=epoch)
        elif epoch == 'date':
            epoch = self.t
        else:
            raise ValueError('the epoch= must be a Time object,'
                             ' a floating point Terrestrial Time (TT),'
                             ' or the string ""date"" for epoch-of-date')

        oblm, oblt, eqeq, psi, eps = epoch._earth_tilt
        e = oblt * DEG2RAD
        rotation = einsum('ij...,jk...->ik...', rot_x(-e), epoch.M)
        position_au = einsum('ij...,j...->i...', rotation, position_au)
        return Distance(position_au)",https://github.com/skyfielders/python-skyfield/blob/51d9e042e06457f6b1f2415296d50a38cb3a300f/skyfield/positionlib.py#L195-L224
tfidf_python_100_1.0,convert a utc time to epoch,python,"def __init__(self, username=None, password=None):
        UsernameToken.__init__(
            self,
            username,
            password
        )
        utc = UsernameToken.utc()
        utc = datetime(
            utc.year, utc.month, utc.day,
            utc.hour, utc.minute, utc.second,
            tzinfo=utc.tzinfo
        )
        self.setcreated(utc)
        self.setnonce()",https://github.com/OnroerendErfgoed/crabpy/blob/3a6fd8bc5aca37c2a173e3ea94e4e468b8aa79c1/crabpy/wsse.py#L26-L39
tfidf_python_100_1.0,convert a utc time to epoch,python,"def start_new_epoch(self, epoch, load_data_dir=None):
    if not isinstance(epoch, int):
      raise ValueError(""Epoch should be integer, got {}"".format(epoch))
    if epoch in self._rollouts_by_epoch_and_split:
      raise ValueError(""Epoch {} already registered"".format(epoch))
    self.current_epoch = epoch
    self._current_epoch_rollouts = []
    self._rollouts_by_epoch_and_split[epoch] = collections.defaultdict(list)
    self._current_batch_frames = [None for _ in range(self.batch_size)]
    self._current_batch_rollouts = [[] for _ in range(self.batch_size)]
    if load_data_dir is not None:
      self._load_epoch_data(load_data_dir)",https://github.com/tensorflow/tensor2tensor/blob/272500b6efe353aeb638d2745ed56e519462ca31/tensor2tensor/data_generators/gym_env.py#L175-L186
tfidf_python_100_1.0,convert a utc time to epoch,python,"def report(self, epoch, train_error, validation_error=None,
               new_best=None, epoch_t=None):
        self.train_error.append((epoch, train_error))
        if validation_error is not None:
            self.validation_error.append((epoch, validation_error))

        # Print logs
        self.print_error(epoch, train_error, validation_error, new_best)

        if epoch_t is not None and epoch > 0:
            self.avg_epoch_t = ((epoch - 1) * \
                                self.avg_epoch_t + epoch_t) / epoch \
                                if self.avg_epoch_t is not None else epoch_t
        sys.stdout.flush()",https://github.com/hannes-brt/hebel/blob/1e2c3a9309c2646103901b26a55be4e312dd5005/hebel/monitors.py#L184-L197
tfidf_python_100_1.0,convert a utc time to epoch,python,"def parse_epoch(self, epoch_str):
        """"""
        Converts epoch field to a float value (adding 24... prefix), or
        ``None`` if there is no epoch in GCVS record.
        """"""
        epoch = epoch_str.translate(TRANSLATION_MAP)[:10].strip()
        return 2400000.0 + float(epoch) if epoch else None",https://github.com/zsiciarz/pygcvs/blob/ed5522ab9cf9237592a6af7a0bc8cad079afeb67/pygcvs/parser.py#L202-L208
tfidf_python_100_1.0,convert a utc time to epoch,python,"def datetime_to_timestamp(dt):
    """"""Convert timezone-aware `datetime` to POSIX timestamp and
    return seconds since UNIX epoch.

    Note: similar to `datetime.timestamp()` in Python 3.3+.
    """"""

    epoch = datetime.utcfromtimestamp(0).replace(tzinfo=UTC)
    return (dt - epoch).total_seconds()",https://github.com/dwavesystems/dwave-cloud-client/blob/df3221a8385dc0c04d7b4d84f740bf3ad6706230/dwave/cloud/utils.py#L193-L201
tfidf_python_100_1.0,convert a utc time to epoch,python,"def send_optimizer_info(self, epoch):
        self.learning_rate_channel.send(epoch, [self.learning_rate_start, self.get_learning_rate()])",https://github.com/aetros/aetros-cli/blob/a2a1f38d6af1660e1e2680c7d413ec2aef45faab/aetros/KerasCallback.py#L296-L297
tfidf_python_100_1.0,all permutations of a list,python,"def print_permutations(self):
        """"""Print all valid permutations.""""""
        index = 0
        permutations = []
        for p in self._input_permutations:
            permutations.append({'index': index, 'args': p})
            index += 1
        with open('permutations.json', 'w') as fh:
            json.dump(permutations, fh, indent=2)
        print('All permutations written to the ""permutations.json"" file.')",https://github.com/ThreatConnect-Inc/tcex/blob/dd4d7a1ef723af1561687120191886b9a2fd4b47/tcex/tcex_bin_profile.py#L259-L268
tfidf_python_100_1.0,all permutations of a list,python,"def circ_permutation(items):
    """"""Calculate the circular permutation for a given list of items.""""""
    permutations = []
    for i in range(len(items)):
        permutations.append(items[i:] + items[:i])
    return permutations",https://github.com/tamasgal/km3pipe/blob/7a9b59ac899a28775b5bdc5d391d9a5340d08040/km3pipe/math.py#L224-L229
tfidf_python_100_1.0,all permutations of a list,python,"def distinct_permutations(iterable):
    """"""Yield successive distinct permutations of the elements in *iterable*.

        >>> sorted(distinct_permutations([1, 0, 1]))
        [(0, 1, 1), (1, 0, 1), (1, 1, 0)]

    Equivalent to ``set(permutations(iterable))``, except duplicates are not
    generated and thrown away. For larger input sequences this is much more
    efficient.

    Duplicate permutations arise when there are duplicated elements in the
    input iterable. The number of items returned is
    `n! / (x_1! * x_2! * ... * x_n!)`, where `n` is the total number of
    items input, and each `x_i` is the count of a distinct item in the input
    sequence.

    """"""
    def make_new_permutations(permutations, e):
        """"""Internal helper function.
        The output permutations are built up by adding element *e* to the
        current *permutations* at every possible position.
        The key idea is to keep repeated elements (reverse) ordered:
        if e1 == e2 and e1 is before e2 in the iterable, then all permutations
        with e1 before e2 are ignored.

        """"""
        for permutation in permutations:
            for j in range(len(permutation)):
                yield permutation[:j] + [e] + permutation[j:]
                if permutation[j] == e:
                    break
            else:
                yield permutation + [e]

    permutations = [[]]
    for e in iterable:
        permutations = make_new_permutations(permutations, e)

    return (tuple(t) for t in permutations)",https://github.com/erikrose/more-itertools/blob/6a91b4e25c8e12fcf9fc2b53cf8ee0fba293e6f9/more_itertools/more.py#L528-L566
tfidf_python_100_1.0,all permutations of a list,python,"def _prepare(q, p):
    d = len(q) - len(p) + 1
    if d:
        p.extend([({}, {})] * d)
    doubles = []
    for q, p, c in zip(permutations(q), permutations(p[1:]), repeat(p[0])):
        if q in doubles:
            continue
        doubles.append(q)
        yield q, p, c",https://github.com/cimm-kzn/CGRtools/blob/15a19b04f6e4e1d0dab8e0d32a0877c7f7d70f34/CGRtools/algorithms/standardize.py#L63-L72
tfidf_python_100_1.0,all permutations of a list,python,"def __init__(self, e, b, w, adjusted=True, transformation=""r"",
                 permutations=PERMUTATIONS, geoda_quads=False):
        e = np.asarray(e).flatten()
        b = np.asarray(b).flatten()
        if adjusted:
            y = assuncao_rate(e, b)
        else:
            y = e * 1.0 / b
        Moran_Local.__init__(self, y, w,
                             transformation=transformation,
                             permutations=permutations,
                             geoda_quads=geoda_quads)",https://github.com/pysal/esda/blob/2fafc6ec505e153152a86601d3e0fba080610c20/esda/moran.py#L1333-L1344
tfidf_python_100_1.0,all permutations of a list,python,"def __init__(self, e, b, w, adjusted=True, transformation=""r"",
                 permutations=PERMUTATIONS, two_tailed=True):
        e = np.asarray(e).flatten()
        b = np.asarray(b).flatten()
        if adjusted:
            y = assuncao_rate(e, b)
        else:
            y = e * 1.0 / b
        Moran.__init__(self, y, w, transformation=transformation,
                       permutations=permutations, two_tailed=two_tailed)",https://github.com/pysal/esda/blob/2fafc6ec505e153152a86601d3e0fba080610c20/esda/moran.py#L667-L676
tfidf_python_100_1.0,all permutations of a list,python,"def coordination_geometry_symmetry_measures_standard(self,
                                                         coordination_geometry,
                                                         algo,
                                                         points_perfect=None,
                                                         optimization=None):
        """"""
        Returns the symmetry measures for a set of permutations (whose setup depends on the coordination geometry)
        for the coordination geometry ""coordination_geometry"". Standard implementation looking for the symmetry
        measures of each permutation

        :param coordination_geometry: The coordination geometry to be investigated
        :return: The symmetry measures for the given coordination geometry for each permutation investigated
        """"""
        # permutations_symmetry_measures = np.zeros(len(algo.permutations),
        #                                           np.float)
        if optimization == 2:
            permutations_symmetry_measures = [None] * len(algo.permutations)
            permutations = list()
            algos = list()
            local2perfect_maps = list()
            perfect2local_maps = list()
            for iperm, perm in enumerate(algo.permutations):

                local2perfect_map = {}
                perfect2local_map = {}
                permutations.append(perm)
                for iperfect, ii in enumerate(perm):
                    perfect2local_map[iperfect] = ii
                    local2perfect_map[ii] = iperfect
                local2perfect_maps.append(local2perfect_map)
                perfect2local_maps.append(perfect2local_map)

                points_distorted = self.local_geometry.points_wcs_ctwcc(
                    permutation=perm)

                sm_info = symmetry_measure(points_distorted=points_distorted,
                                              points_perfect=points_perfect)
                sm_info['translation_vector'] = self.local_geometry.centroid_with_centre

                permutations_symmetry_measures[iperm] = sm_info
                algos.append(str(algo))
            return permutations_symmetry_measures, permutations, algos, local2perfect_maps, perfect2local_maps
        else:
            permutations_symmetry_measures = [None] * len(algo.permutations)
            permutations = list()
            algos = list()
            local2perfect_maps = list()
            perfect2local_maps = list()
            for iperm, perm in enumerate(algo.permutations):

                local2perfect_map = {}
                perfect2local_map = {}
                permutations.append(perm)
                for iperfect, ii in enumerate(perm):
                    perfect2local_map[iperfect] = ii
                    local2perfect_map[ii] = iperfect
                local2perfect_maps.append(local2perfect_map)
                perfect2local_maps.append(perfect2local_map)

                points_distorted = self.local_geometry.points_wcs_ctwcc(
                    permutation=perm)

                sm_info = symmetry_measure(points_distorted=points_distorted,
                                           points_perfect=points_perfect)
                sm_info['translation_vector'] = self.local_geometry.centroid_with_centre

                permutations_symmetry_measures[iperm] = sm_info
                algos.append(str(algo))
            return permutations_symmetry_measures, permutations, algos, local2perfect_maps, perfect2local_maps",https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/analysis/chemenv/coordination_environments/coordination_geometry_finder.py#L1222-L1290
tfidf_python_100_1.0,all permutations of a list,python,"def __init__(self, y, w, permutations=PERMUTATIONS):
        y = np.asarray(y).flatten()
        self.n = len(y)
        self.y = y
        w.transform = ""B""
        self.w = w
        self.permutations = permutations
        self.__moments()
        self.y2 = y * y
        y = y.reshape(len(y), 1)  # Ensure that y is an n by 1 vector, otherwise y*y.T == y*y
        self.den_sum = (y * y.T).sum() - (y * y).sum()
        self.G = self.__calc(self.y)
        self.z_norm = (self.G - self.EG) / np.sqrt(self.VG)
        self.p_norm = 1.0 - stats.norm.cdf(np.abs(self.z_norm))

        if permutations:
            sim = [self.__calc(np.random.permutation(self.y))
                   for i in range(permutations)]
            self.sim = sim = np.array(sim)
            above = sim >= self.G
            larger = sum(above)
            if (self.permutations - larger) < larger:
                larger = self.permutations - larger
            self.p_sim = (larger + 1.0) / (permutations + 1.)
            self.EG_sim = sum(sim) / permutations
            self.seG_sim = sim.std()
            self.VG_sim = self.seG_sim ** 2
            self.z_sim = (self.G - self.EG_sim) / self.seG_sim
            self.p_z_sim = 1. - stats.norm.cdf(np.abs(self.z_sim))",https://github.com/pysal/esda/blob/2fafc6ec505e153152a86601d3e0fba080610c20/esda/getisord.py#L100-L128
tfidf_python_100_1.0,all permutations of a list,python,"def permutation_from_block_permutations(permutations):
    """"""Reverse operation to :py:func:`permutation_to_block_permutations`
    Compute the concatenation of permutations

        ``(1,2,0) [+] (0,2,1) --> (1,2,0,3,5,4)``

    :param permutations: A list of permutation tuples
                                 ``[t = (t_0,...,t_n1), u = (u_0,...,u_n2),..., z = (z_0,...,z_nm)]``
    :type permutations: list of tuples
    :return: permutation image tuple
                    ``s = t [+] u [+] ... [+] z``
    :rtype: tuple
    """"""
    offset = 0
    new_perm = []
    for p in permutations:
        new_perm[offset: offset +len(p)] = [p_i + offset for p_i in p]
        offset += len(p)
    return tuple(new_perm)",https://github.com/mabuchilab/QNET/blob/cc20d26dad78691d34c67173e5cd67dcac94208a/src/qnet/utils/permutations.py#L166-L184
tfidf_python_100_1.0,all permutations of a list,python,"def __init__(self,
                 permutations,
                 bigdl_type=""float""):
        super(Transpose, self).__init__(None, bigdl_type,
                                        permutations)",https://github.com/intel-analytics/BigDL/blob/e9c19788285986ab789a2e2998f9a85d7524779f/pyspark/bigdl/nn/layer.py#L4891-L4895
tfidf_python_100_1.0,all permutations of a list,python,"def __pseudop(self, sim, g):
        above = sim >= g
        larger = above.sum()
        psim = (larger + 1.) / (self.permutations + 1.)
        if psim > 0.5:
            psim = (self.permutations - larger + 1.) / (self.permutations + 1.)
        return psim",https://github.com/pysal/esda/blob/2fafc6ec505e153152a86601d3e0fba080610c20/esda/gamma.py#L226-L232
tfidf_python_100_1.0,all permutations of a list,python,"def _get_sym_mappings_from_permutations(permutations, atom_list_done):

    """"""This can be thought of as computing 'map_atom_disp' and 'map_sym'
    for all atoms, except done using permutations instead of by
    computing overlaps.

    Input:
        * permutations, shape [num_rot][num_pos]
        * atom_list_done

    Output:
        * map_atoms, shape [num_pos].
        Maps each atom in the full structure to its equivalent atom in
        atom_list_done.  (each entry will be an integer found in
        atom_list_done)

        * map_syms, shape [num_pos].
        For each atom, provides the index of a rotation that maps it
        into atom_list_done.  (there might be more than one such
        rotation, but only one will be returned) (each entry will be
        an integer 0 <= i < num_rot)

    """"""

    assert permutations.ndim == 2
    num_pos = permutations.shape[1]

    # filled with -1
    map_atoms = np.zeros((num_pos,), dtype='intc') - 1
    map_syms = np.zeros((num_pos,), dtype='intc') - 1

    atom_list_done = set(atom_list_done)
    for atom_todo in range(num_pos):
        for (sym_index, permutation) in enumerate(permutations):
            if permutation[atom_todo] in atom_list_done:
                map_atoms[atom_todo] = permutation[atom_todo]
                map_syms[atom_todo] = sym_index
                break
        else:
            text = (""Input forces are not enough to calculate force constants,""
                    ""or something wrong (e.g. crystal structure does not ""
                    ""match)."")
            print(textwrap.fill(text))
            raise ValueError

    assert set(map_atoms) & set(atom_list_done) == set(map_atoms)
    assert -1 not in map_atoms
    assert -1 not in map_syms
    return map_atoms, map_syms",https://github.com/atztogo/phonopy/blob/869cc2ba9e7d495d5f4cf6942415ab3fc9e2a10f/phonopy/harmonic/force_constants.py#L778-L826
tfidf_python_100_1.0,all permutations of a list,python,"def block_perms(self):
        """"""If the circuit is reducible into permutations within subranges of
        the full range of channels, this yields a tuple with the internal
        permutations for each such block.

        :type: tuple
        """"""
        if not self._block_perms:
            self._block_perms = permutation_to_block_permutations(
                self.permutation)
        return self._block_perms",https://github.com/mabuchilab/QNET/blob/cc20d26dad78691d34c67173e5cd67dcac94208a/src/qnet/algebra/core/circuit_algebra.py#L794-L804
tfidf_python_100_1.0,all permutations of a list,python,"def __init__(self, permutations):
        """"""
            Initializes a separation plane for a given perfect coordination geometry
        """"""
        super().__init__(
            algorithm_type=EXPLICIT_PERMUTATIONS)
        self._permutations = permutations",https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/analysis/chemenv/coordination_environments/coordination_geometries.py#L66-L72
tfidf_python_100_1.0,all permutations of a list,python,"def number_of_permutations(self):
        """"""
        Returns the number of permutations of this coordination geometry.
        """"""
        if self.permutations_safe_override:
            return factorial(self.coordination)
        elif self.permutations is None:
            return factorial(self.coordination)
        return len(self.permutations)",https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/analysis/chemenv/coordination_environments/coordination_geometries.py#L659-L667
tfidf_python_100_1.0,all permutations of a list,python,"def __init__(self, y, w, permutations=PERMUTATIONS):
        y = np.asarray(y).flatten()
        w.transformation = 'b'  # ensure we have binary weights
        self.w = w
        self.y = y
        self.permutations = permutations
        self.J = w.s0 / 2.
        self.bb, self.ww, self.bw = self.__calc(self.y)

        if permutations:
            sim = [self.__calc(np.random.permutation(self.y))
                   for i in range(permutations)]
            sim_jc = np.array(sim)
            self.sim_bb = sim_jc[:, 0]
            self.min_bb = np.min(self.sim_bb)
            self.mean_bb = np.mean(self.sim_bb)
            self.max_bb = np.max(self.sim_bb)
            self.sim_bw = sim_jc[:, 2]
            self.min_bw = np.min(self.sim_bw)
            self.mean_bw = np.mean(self.sim_bw)
            self.max_bw = np.max(self.sim_bw)
            p_sim_bb = self.__pseudop(self.sim_bb, self.bb)
            p_sim_bw = self.__pseudop(self.sim_bw, self.bw)
            self.p_sim_bb = p_sim_bb
            self.p_sim_bw = p_sim_bw",https://github.com/pysal/esda/blob/2fafc6ec505e153152a86601d3e0fba080610c20/esda/join_counts.py#L125-L149
tfidf_python_100_1.0,all permutations of a list,python,"def __init__(self, y, w, transform='R', permutations=PERMUTATIONS, star=False):
        y = np.asarray(y).flatten()
        self.n = len(y)
        self.y = y
        self.w = w
        self.w_original = w.transform
        self.w.transform = self.w_transform = transform.lower()
        self.permutations = permutations
        self.star = star
        self.calc()
        self.p_norm = np.array(
            [1 - stats.norm.cdf(np.abs(i)) for i in self.Zs])
        if permutations:
            self.__crand()
            sim = np.transpose(self.rGs)
            above = sim >= self.Gs
            larger = sum(above)
            low_extreme = (self.permutations - larger) < larger
            larger[low_extreme] = self.permutations - larger[low_extreme]
            self.p_sim = (larger + 1.0) / (permutations + 1)
            self.sim = sim
            self.EG_sim = sim.mean()
            self.seG_sim = sim.std()
            self.VG_sim = self.seG_sim * self.seG_sim
            self.z_sim = (self.Gs - self.EG_sim) / self.seG_sim
            self.p_z_sim = 1 - stats.norm.cdf(np.abs(self.z_sim))",https://github.com/pysal/esda/blob/2fafc6ec505e153152a86601d3e0fba080610c20/esda/getisord.py#L355-L380
tfidf_python_100_1.0,all permutations of a list,python,"def minimize_matrix(self):
        """"""
        This method finds and returns the permutations that produce the lowest
        ewald sum calls recursive function to iterate through permutations
        """"""
        if self._algo == EwaldMinimizer.ALGO_FAST or \
                        self._algo == EwaldMinimizer.ALGO_BEST_FIRST:
            return self._recurse(self._matrix, self._m_list,
                                 set(range(len(self._matrix))))",https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/analysis/ewald.py#L473-L481
tfidf_python_100_1.0,all permutations of a list,python,"def __init__(self, x, y, w, transformation=""r"", permutations=PERMUTATIONS):
        x = np.asarray(x).flatten()
        y = np.asarray(y).flatten()
        zy = (y - y.mean()) / y.std(ddof=1)
        zx = (x - x.mean()) / x.std(ddof=1)
        self.y = y
        self.x = x
        self.zx = zx
        self.zy = zy
        n = x.shape[0]
        self.den = n - 1.  # zx'zx = zy'zy = n-1
        w.transform = transformation
        self.w = w
        self.I = self.__calc(zy)
        if permutations:
            nrp = np.random.permutation
            sim = [self.__calc(nrp(zy)) for i in range(permutations)]
            self.sim = sim = np.array(sim)
            above = sim >= self.I
            larger = above.sum()
            if (permutations - larger) < larger:
                larger = permutations - larger
            self.p_sim = (larger + 1.) / (permutations + 1.)
            self.EI_sim = sim.sum() / permutations
            self.seI_sim = np.array(sim).std()
            self.VI_sim = self.seI_sim ** 2
            self.z_sim = (self.I - self.EI_sim) / self.seI_sim
            if self.z_sim > 0:
                self.p_z_sim = 1 - stats.norm.cdf(self.z_sim)
            else:
                self.p_z_sim = stats.norm.cdf(self.z_sim)",https://github.com/pysal/esda/blob/2fafc6ec505e153152a86601d3e0fba080610c20/esda/moran.py#L372-L402
tfidf_python_100_1.0,all permutations of a list,python,"def __init__(self, x, y, w, transformation=""r"", permutations=PERMUTATIONS,
                 geoda_quads=False):
        x = np.asarray(x).flatten()
        y = np.asarray(y).flatten()
        self.y = y
        self.x =x
        n = len(y)
        self.n = n
        self.n_1 = n - 1
        zx = x - x.mean()
        zy = y - y.mean()
        # setting for floating point noise
        orig_settings = np.seterr()
        np.seterr(all=""ignore"")
        sx = x.std()
        zx /= sx
        sy = y.std()
        zy /= sy
        np.seterr(**orig_settings)
        self.zx = zx
        self.zy = zy
        w.transform = transformation
        self.w = w
        self.permutations = permutations
        self.den = (zx * zx).sum()
        self.Is = self.calc(self.w, self.zx, self.zy)
        self.geoda_quads = geoda_quads
        quads = [1, 2, 3, 4]
        if geoda_quads:
            quads = [1, 3, 2, 4]
        self.quads = quads
        self.__quads()
        if permutations:
            self.__crand()
            sim = np.transpose(self.rlisas)
            above = sim >= self.Is
            larger = above.sum(0)
            low_extreme = (self.permutations - larger) < larger
            larger[low_extreme] = self.permutations - larger[low_extreme]
            self.p_sim = (larger + 1.0) / (permutations + 1.0)
            self.sim = sim
            self.EI_sim = sim.mean(axis=0)
            self.seI_sim = sim.std(axis=0)
            self.VI_sim = self.seI_sim * self.seI_sim
            self.z_sim = (self.Is - self.EI_sim) / self.seI_sim
            self.p_z_sim = 1 - stats.norm.cdf(np.abs(self.z_sim))",https://github.com/pysal/esda/blob/2fafc6ec505e153152a86601d3e0fba080610c20/esda/moran.py#L1089-L1134
tfidf_python_100_1.0,extract latitude and longitude from given input,python,"def serialise_lat_long(self, lat_long):
        return GeoPoint(latitude=lat_long.latitude, longitude=lat_long.longitude)",https://github.com/paulharter/fam/blob/5915dd60637d18f74022a2456fbaaddc707d3e60/src/fam/database/firestore_adapter.py#L20-L21
tfidf_python_100_1.0,extract latitude and longitude from given input,python,"def nearest_city(latitude, longitude):
    nearest_city_coordinate = _world_cities_kdtree.search_nn((latitude, longitude, ))
    return WORLD_CITIES_DICT[nearest_city_coordinate[0].data]",https://github.com/wingchen/citipy/blob/e0f6965d5ce8c0accdc3aa9b25de374883fc3fbe/citipy/citipy.py#L36-L38
tfidf_python_100_1.0,extract latitude and longitude from given input,python,"def __init__(self, longitude, latitude):
        self.longitude = longitude
        self.latitude = latitude",https://github.com/eternnoir/pyTelegramBotAPI/blob/47b53b88123097f1b9562a6cd5d4e080b86185d1/telebot/types.py#L674-L676
tfidf_python_100_1.0,extract latitude and longitude from given input,python,"def __init__(self, latitude, longitude):
        self._lat = latitude
        self._long = longitude",https://github.com/boatd/python-boatd/blob/404ff0d0c389f6ed84ddbfea1c41db6569ad2ed4/boatdclient/point.py#L12-L14
tfidf_python_100_1.0,extract latitude and longitude from given input,python,"def generate_detector_strain(template_params, h_plus, h_cross):
    latitude = 0
    longitude = 0
    polarization = 0

    if hasattr(template_params, 'latitude'):
        latitude = template_params.latitude
    if hasattr(template_params, 'longitude'):
        longitude = template_params.longitude
    if hasattr(template_params, 'polarization'):
        polarization = template_params.polarization

    f_plus, f_cross = generate_fplus_fcross(latitude, longitude, polarization)

    return (f_plus*h_plus+f_cross*h_cross)",https://github.com/gwastro/pycbc/blob/7a64cdd104d263f1b6ea0b01e6841837d05a4cb3/tools/timing/banksim/banksim.py#L59-L73
tfidf_python_100_1.0,extract latitude and longitude from given input,python,"def __init__(self, latitude, longitude):
        self.latitude = latitude
        self.longitude = longitude
        self.easting, self.northing, self.zone_number, self.zone_letter = (
            utm.from_latlon(self.latitude, self.longitude)
        )",https://github.com/XiaochenCui/cxc-gis/blob/6186e0cbe61f1b037afa39839b6da2e0519c9cf8/cxc_gis/models.py#L17-L22
tfidf_python_100_1.0,extract latitude and longitude from given input,python,"def __init__(self, latitude=0, longitude=0):
        """"""

        :param latitude: çº¬åº¦
        :type latitude: int or float
        :param longitude: ç»åº¦
        :type longitude: int or float
        :return: GeoPoint
        """"""
        self._validate(latitude, longitude)
        self._latitude = latitude
        self._longitude = longitude",https://github.com/leancloud/python-sdk/blob/fea3240257ce65e6a32c7312a5cee1f94a51a587/leancloud/geo_point.py#L15-L26
tfidf_python_100_1.0,extract latitude and longitude from given input,python,"def to_dic(self):
        json_dic = {'latitude': self.latitude, 'longitude': self.longitude}
        if self.live_period:
            json_dic['live_period'] = self.live_period
        return json_dic",https://github.com/eternnoir/pyTelegramBotAPI/blob/47b53b88123097f1b9562a6cd5d4e080b86185d1/telebot/types.py#L1057-L1061
tfidf_python_100_1.0,extract latitude and longitude from given input,python,"def get_lonlats(self):
        """"""Get longitude and latitude arrays from the file.""""""
        longitudes = self.get_sds_variable('Longitude')
        latitudes = self.get_sds_variable('Latitude')
        return longitudes, latitudes",https://github.com/pytroll/satpy/blob/1f21d20ac686b745fb0da9b4030d139893e066dd/satpy/readers/caliop_l2_cloud.py#L101-L105
tfidf_python_100_1.0,extract latitude and longitude from given input,python,"def _location(self, key, value):
    latitude = maybe_float(value.get('f'))
    longitude = maybe_float(value.get('d'))

    if latitude and longitude:
        return {
            'latitude': latitude,
            'longitude': longitude,
        }",https://github.com/inspirehep/inspire-dojson/blob/17f3789cd3d5ae58efa1190dc0eea9efb9c8ca59/inspire_dojson/institutions/rules.py#L46-L54
tfidf_python_100_1.0,extract latitude and longitude from given input,python,"def __init__(self, latitude, longitude, *args, **kwargs):

        self.latitude = latitude
        self.longitude = longitude
        self.forecasts = None

        self.params = {'latitude' : self.latitude,
                       'longitude' : self.longitude}

        self._get(*args, **kwargs)

        if self.ok:
            self._generate_forecasts_dict()",https://github.com/cjtapper/solcast-py/blob/5bb0c157cb855fb74721fa63bb71194a9e016e20/solcast/radiation_forecasts.py#L13-L25
tfidf_python_100_1.0,extract latitude and longitude from given input,python,"def get_data(latitude=52.091579, longitude=5.119734, usexml=False):
    """"""Get buienradar xml data and return results.""""""
    if usexml:
        log.info(""Getting buienradar XML data for latitude=%s, longitude=%s"",
                 latitude, longitude)
        return get_xml_data(latitude, longitude)
    else:
        log.info(""Getting buienradar JSON data for latitude=%s, longitude=%s"",
                 latitude, longitude)
        return get_json_data(latitude, longitude)",https://github.com/mjj4791/python-buienradar/blob/a70436f54e007ce921d5210cb296cf3e4adf9d09/buienradar/buienradar.py#L18-L27
tfidf_python_100_1.0,extract latitude and longitude from given input,python,"def point(self):
        p = PointMixin()
        p.latitude = self.latitude
        p.longitude = self.longitude
        p.name = self.icao
        return p",https://github.com/xoolive/traffic/blob/d1a8878098f16759f6b6e0e8d8b8f32e34a680a8/traffic/data/basic/airport.py#L94-L99
tfidf_python_100_1.0,extract latitude and longitude from given input,python,"def moveMarker(self, key, latitude, longitude) :
		return self.runScript(
			""gmap_moveMarker({!r}, {}, {});"".format(key,latitude,longitude))",https://github.com/vokimon/python-qgmap/blob/9b01b48c5a8f4726938d38326f89644e7fb95f51/qgmap/__init__.py#L159-L161
tfidf_python_100_1.0,extract latitude and longitude from given input,python,"def unpack(self, tcd, rec):
        latitude = rec.latitude
        longitude = rec.longitude
        if latitude == 0 and longitude == 0:
            latitude = longitude = None
        yield 'latitude', latitude
        yield 'longitude', longitude",https://github.com/dairiki/python-libtcd/blob/c9d1fd3f30e3088f125bf05fb592f30daf9de51d/libtcd/api.py#L231-L237
tfidf_python_100_1.0,extract latitude and longitude from given input,python,"def latlon_to_zone_number(latitude, longitude):
    # If the input is a numpy array, just use the first element
    # User responsibility to make sure that all points are in one zone
    if use_numpy:
        if isinstance(latitude, mathlib.ndarray):
            latitude = latitude.flat[0]
        if isinstance(longitude, mathlib.ndarray):
            longitude = longitude.flat[0]

    if 56 <= latitude < 64 and 3 <= longitude < 12:
        return 32

    if 72 <= latitude <= 84 and longitude >= 0:
        if longitude < 9:
            return 31
        elif longitude < 21:
            return 33
        elif longitude < 33:
            return 35
        elif longitude < 42:
            return 37

    return int((longitude + 180) / 6) + 1",https://github.com/Turbo87/utm/blob/efdd46ab0a341ce2aa45f8144d8b05a4fa0fd592/utm/conversion.py#L261-L283
tfidf_python_100_1.0,extract latitude and longitude from given input,python,"def register_obstory(self, obstory_id, obstory_name, latitude, longitude):
        self.con.execute(""""""
INSERT INTO archive_observatories
(publicId, name, latitude, longitude)
VALUES
(%s, %s, %s, %s);
"""""", (obstory_id, obstory_name, latitude, longitude))
        return obstory_id",https://github.com/camsci/meteor-pi/blob/7b01527650bd1b2b76d6f364e8122e25b8812c8d/src/pythonModules/meteorpi_db/meteorpi_db/__init__.py#L151-L158
tfidf_python_100_1.0,extract latitude and longitude from given input,python,"def create_location(latitude, longitude):
        return Location(_type=Location, latitude=latitude, longitude=longitude)",https://github.com/alvarogzp/telegram-bot-framework/blob/7b597a415c1901901c677976cb13100fc3083107/bot/api/domain.py#L169-L170
tfidf_python_100_1.0,extract latitude and longitude from given input,python,"def pack(self, tcd, station):
        latitude = station.latitude
        longitude = station.longitude
        # XXX: Warning if latitude == longitude == 0
        if latitude is None or longitude is None:
            latitude = longitude = 0
            # XXX: Warning if latitude is not None or longitude is not None?
        yield 'latitude', latitude
        yield 'longitude', longitude",https://github.com/dairiki/python-libtcd/blob/c9d1fd3f30e3088f125bf05fb592f30daf9de51d/libtcd/api.py#L239-L247
tfidf_python_100_1.0,extract latitude and longitude from given input,python,"def __init__(self, place, latitude, longitude):

        if isinstance(latitude, float) or isinstance(latitude, int):
            latitude = str(latitude)
        if isinstance(longitude, float) or isinstance(longitude, int):
            longitude = str(longitude)

        self.place = place
        self.latitude = Decimal(latitude)
        self.longitude = Decimal(longitude)",https://github.com/oscarmcm/django-places/blob/99d61b53945b06eb5a588347904b980d0fa4007a/places/__init__.py#L18-L27
tfidf_python_100_1.0,how to check if a checkbox is checked,python,"def set_checkbox(self, data, uncheck_other_boxes=True):
        """"""Set the *checked*-attribute of input elements of type ""checkbox""
        specified by ``data`` (i.e. check boxes).

        :param data: Dict of ``{name: value, ...}``.
            In the family of checkboxes whose *name*-attribute is ``name``,
            check the box whose *value*-attribute is ``value``. All boxes in
            the family can be checked (unchecked) if ``value`` is True (False).
            To check multiple specific boxes, let ``value`` be a tuple or list.
        :param uncheck_other_boxes: If True (default), before checking any
            boxes specified by ``data``, uncheck the entire checkbox family.
            Consider setting to False if some boxes are checked by default when
            the HTML is served.
        """"""
        for (name, value) in data.items():
            # Case-insensitive search for type=checkbox
            checkboxes = self.find_by_type(""input"", ""checkbox"", {'name': name})
            if not checkboxes:
                raise InvalidFormMethod(""No input checkbox named "" + name)

            # uncheck if requested
            if uncheck_other_boxes:
                self.uncheck_all(name)

            # Wrap individual values (e.g. int, str) in a 1-element tuple.
            if not isinstance(value, list) and not isinstance(value, tuple):
                value = (value,)

            # Check or uncheck one or more boxes
            for choice in value:
                choice_str = str(choice)  # Allow for example literal numbers
                for checkbox in checkboxes:
                    if checkbox.attrs.get(""value"", ""on"") == choice_str:
                        checkbox[""checked""] = """"
                        break
                    # Allow specifying True or False to check/uncheck
                    elif choice is True:
                        checkbox[""checked""] = """"
                        break
                    elif choice is False:
                        if ""checked"" in checkbox.attrs:
                            del checkbox.attrs[""checked""]
                        break
                else:
                    raise LinkNotFoundError(
                        ""No input checkbox named %s with choice %s"" %
                        (name, choice)
                    )",https://github.com/MechanicalSoup/MechanicalSoup/blob/027a270febf5bcda6a75db60ea9838d631370f4b/mechanicalsoup/form.py#L99-L146
tfidf_python_100_1.0,how to check if a checkbox is checked,python,"def checkbox_check(self, force_check=False):
        """"""
        Wrapper to check a  checkbox
        """"""
        if not self.get_attribute('checked'):
            self.click(force_click=force_check)",https://github.com/Shapeways/coyote_framework/blob/cb29899b984a21d56bf65d0b1d907073948fe16c/coyote_framework/webdriver/webdriverwrapper/WebElementWrapper.py#L655-L660
tfidf_python_100_1.0,how to check if a checkbox is checked,python,"def addCheckBox(self, *args, **kwargs):
        checkbox = BeakerxCheckbox(description=self.getDescription(args, kwargs))
        checkbox.value = getValue(kwargs, 'value', False)
        self.children += (checkbox,)
        self.components[checkbox.description] = checkbox
        return checkbox",https://github.com/twosigma/beakerx/blob/404de61ed627d9daaf6b77eb4859e7cb6f37413f/beakerx/beakerx/easyform/easyform.py#L112-L117
tfidf_python_100_1.0,how to check if a checkbox is checked,python,"def assert_checked_checkbox(self, value):
    """"""Assert the checkbox with label (recommended), name or id is checked.""""""
    check_box = find_field(world.browser, 'checkbox', value)
    assert check_box, ""Cannot find checkbox '{}'."".format(value)
    assert check_box.is_selected(), ""Check box should be selected.""",https://github.com/aloetesting/aloe_webdriver/blob/65d847da4bdc63f9c015cb19d4efdee87df8ffad/aloe_webdriver/__init__.py#L564-L568
tfidf_python_100_1.0,how to check if a checkbox is checked,python,"def render(self, obj):
        checked = bool(Accessor(self.field).resolve(obj)) if self.field else False
        if checked:
            return mark_safe('<input checked type=""checkbox"">')
        else:
            return mark_safe('<input type=""checkbox"">')",https://github.com/shymonk/django-datatable/blob/f20a6ed2ce31aa7488ff85b4b0e80fe1ad94ec44/table/columns/checkboxcolumn.py#L16-L21
tfidf_python_100_1.0,how to check if a checkbox is checked,python,"def get_checkbox_state_by_name(self, checkbox_text):
        return [checkbox.get_active() for checkbox in self.checkboxes if checkbox.get_label() == checkbox_text]",https://github.com/DLR-RM/RAFCON/blob/24942ef1a904531f49ab8830a1dbb604441be498/source/rafcon/gui/utils/dialog.py#L233-L234
tfidf_python_100_1.0,how to check if a checkbox is checked,python,"def checkbox_uncheck(self, force_check=False):
        """"""
        Wrapper to uncheck a checkbox
        """"""
        if self.get_attribute('checked'):
            self.click(force_click=force_check)",https://github.com/Shapeways/coyote_framework/blob/cb29899b984a21d56bf65d0b1d907073948fe16c/coyote_framework/webdriver/webdriverwrapper/WebElementWrapper.py#L662-L667
tfidf_python_100_1.0,how to check if a checkbox is checked,python,"def check_checkbox(self, value):
    """"""Check the checkbox with label (recommended), name or id.""""""
    check_box = find_field(world.browser, 'checkbox', value)
    assert check_box, ""Cannot find checkbox '{}'."".format(value)
    if not check_box.is_selected():
        check_box.click()",https://github.com/aloetesting/aloe_webdriver/blob/65d847da4bdc63f9c015cb19d4efdee87df8ffad/aloe_webdriver/__init__.py#L542-L547
tfidf_python_100_1.0,how to check if a checkbox is checked,python,"def create_checkbox(self, parent, label, getter, setter):
        checkbox = QtGui.QCheckBox(label, parent)
        checkbox.setChecked(getter())

        def stateChanged(state):
            value = state == QtCore.Qt.Checked
            setter(value)

        checkbox.stateChanged.connect(stateChanged)
        return checkbox",https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-ui/vaex/ui/layers.py#L2427-L2436
tfidf_python_100_1.0,how to check if a checkbox is checked,python,"def set_checked(self, checked):
        if self._clone_original:
            self._clone_original.set_checked(checked)
        else:
            self._checked = checked",https://github.com/subdownloader/subdownloader/blob/bbccedd11b18d925ad4c062b5eb65981e24d0433/subdownloader/client/gui/searchFileModel.py#L27-L31
tfidf_python_100_1.0,how to check if a checkbox is checked,python,"def handle_positioned_check_box(self, checked):
        if checked:
            self.__scan_hardware_source.validate_probe_position()
        else:
            self.__scan_hardware_source.probe_position = None",https://github.com/nion-software/nionswift-instrumentation-kit/blob/b20c4fff17e840e8cb3d544705faf5bd05f1cbf7/nionswift_plugin/nion_instrumentation_ui/ScanControlPanel.py#L382-L386
tfidf_python_100_1.0,how to check if a checkbox is checked,python,"def input_check(self, name, label, multi_line=False):
        """"""
        {% if multiple_choice_1 %}
            {% set checked = ""checked"" %}
        {% else %}
            {% set checked = """" %}
        {% endif %}
        <input type=""checkbox"" name=""multiple_choice_1"" value=""multiple_choice_1"" {{checked}}> multiple_choice_1
        """"""
        lines = list()
        lines.append('{%% if %s %%}' % name)
        lines.append(self.tab + '{% set checked = ""checked"" %}')
        lines.append('{% else %}')
        lines.append(self.tab  + '{% set checked = """" %}')
        lines.append('{% endif %}')
        if multi_line:
            line_break = ""<br>""
        else:
            line_break = """"
        lines.append('<input type=""checkbox"" name=""%s"" value=""%s"" {{checked}}> %s %s' % (name, name, label, line_break))        
        return ""\n"".join(lines)",https://github.com/MacHu-GWU/angora-project/blob/689a60da51cd88680ddbe26e28dbe81e6b01d275/angora/markup/html.py#L85-L105
tfidf_python_100_1.0,how to check if a checkbox is checked,python,"def satisfied_by_checked(self, req):
        """"""
        Check if requirement is already satisfied by what was previously checked

        :param Requirement req: Requirement to check
        """"""
        req_man = RequirementsManager([req])

        return any(req_man.check(*checked) for checked in self.checked)",https://github.com/maxzheng/bumper-lib/blob/32a9dec5448673825bb2d7d92fa68882b597f794/bumper/cars.py#L193-L201
tfidf_python_100_1.0,how to check if a checkbox is checked,python,"def set_value(self, checked, update_ui=1):
        if checked:
            self.attributes['checked'] = 'checked'
        else:
            if 'checked' in self.attributes:
                del self.attributes['checked']",https://github.com/dddomodossola/remi/blob/85206f62220662bb7ecd471042268def71ccad28/remi/gui.py#L2776-L2781
tfidf_python_100_1.0,how to check if a checkbox is checked,python,"def get_selected_events(self, time_selection=None):
        """"""Returns which events are present in one time window.

        Parameters
        ----------
        time_selection : tuple of float
            start and end of the window of interest

        Returns
        -------
        list of dict
            list of events in the window of interest
        """"""
        events = []
        for checkbox in self.idx_eventtype_list:
            if checkbox.checkState() == Qt.Checked:
                events.extend(self.annot.get_events(name=checkbox.text(),
                                                    time=time_selection))

        return events",https://github.com/wonambi-python/wonambi/blob/1d8e3d7e53df8017c199f703bcab582914676e76/wonambi/widgets/notes.py#L738-L757
tfidf_python_100_1.0,how to check if a checkbox is checked,python,"def _checked_change(self, *args):
        for pony in self._missing_ponies.values():
            pony.checked = self.checked",https://github.com/Arzaroth/CelestiaSunrise/blob/a2dcc5e905114705cd4a93dd37b5fd8a9858c467/celestia/gui/missingponiesframe.py#L50-L52
tfidf_python_100_1.0,how to check if a checkbox is checked,python,"def handle_ac_line_sync_check_box(self, checked):
        frame_parameters = self.__scan_hardware_source.get_frame_parameters(self.__scan_hardware_source.selected_profile_index)
        frame_parameters.ac_line_sync = checked
        self.__scan_hardware_source.set_frame_parameters(self.__scan_hardware_source.selected_profile_index, frame_parameters)",https://github.com/nion-software/nionswift-instrumentation-kit/blob/b20c4fff17e840e8cb3d544705faf5bd05f1cbf7/nionswift_plugin/nion_instrumentation_ui/ScanControlPanel.py#L388-L391
tfidf_python_100_1.0,how to check if a checkbox is checked,python,"def on_event_pre(self, e: Event) -> None:
        """"""Set values set on browser before calling event listeners.""""""
        super().on_event_pre(e)
        ct_msg = e.init.get('currentTarget', dict())
        if e.type in ('input', 'change'):
            # Update user inputs
            if self.type.lower() == 'checkbox':
                self._set_attribute('checked', ct_msg.get('checked'))
            elif self.type.lower() == 'radio':
                self._set_attribute('checked', ct_msg.get('checked'))
                for other in self._find_grouped_nodes():
                    if other is not self:
                        other._remove_attribute('checked')
            else:
                self._set_attribute('value', ct_msg.get('value'))",https://github.com/miyakogi/wdom/blob/a21bcd23e94baceee71161829f6897bee3fd39c1/wdom/element.py#L859-L873
tfidf_python_100_1.0,how to check if a checkbox is checked,python,"def __str__(self):
        checked = """"
        if self.checked:
            checked = ""checked='checked'""
        return (
            ""<input ""
            ""type='radio' ""
            ""name='%(name)s' ""
            ""value='%(value)s' ""
            ""%(checked)s>""
            ""%(description)s""
            ""</input><br />"" % {
                'name': self.name,
                'value': self.value,
                'checked': checked,
                'description': self.description
            }
        )",https://github.com/ic-labs/django-icekit/blob/c507ea5b1864303732c53ad7c5800571fca5fa94/icekit/middleware.py#L119-L136
tfidf_python_100_1.0,how to check if a checkbox is checked,python,"def init_layout(self):
        """""" Set the checked state after all children have
        been populated.
        
        """"""
        super(AndroidRadioGroup, self).init_layout()
        d = self.declaration
        w = self.widget
        if d.checked:
            self.set_checked(d.checked)
        else:
            #: Check if any of the children have ""checked = True""
            for c in d.children:
                if c.checked:
                    d.checked = c

        w.setOnCheckedChangeListener(w.getId())
        w.onCheckedChanged.connect(self.on_checked_changed)",https://github.com/codelv/enaml-native/blob/c33986e9eda468c508806e0a3e73c771401e5718/src/enamlnative/android/android_radio_group.py#L47-L64
tfidf_python_100_1.0,converting uint8 array to image,python,"def update_colors(self, colors_a, colors_b):
        self.colors_a = np.array(colors_a, 'uint8')
        self.colors_b = np.array(colors_b, 'uint8')
        
        self.cr1.update_colors(self.colors_a)
        self.cr2.update_colors(self.colors_b)",https://github.com/chemlab/chemlab/blob/c8730966316d101e24f39ac3b96b51282aba0abe/chemlab/graphics/renderers/bond.py#L99-L104
tfidf_python_100_1.0,converting uint8 array to image,python,"def sendPixelFormat(self, pixelFormat):
        """"""
        @summary:  Send pixel format structure
                    Very important packet that inform the image struct supported by the client
        @param pixelFormat: PixelFormat struct
        """"""
        self.send((UInt8(ClientToServerMessages.PIXEL_FORMAT), UInt16Be(), UInt8(), pixelFormat))",https://github.com/citronneur/rdpy/blob/4109b7a6fe2abf3ddbaed54e29d2f31e63ed97f6/rdpy/protocol/rfb/rfb.py#L453-L459
tfidf_python_100_1.0,converting uint8 array to image,python,"def __init__(self):
        CompositeType.__init__(self)
        self.BitsPerPixel = UInt8(32)
        self.Depth = UInt8(24)
        self.BigEndianFlag = UInt8(False)
        self.TrueColorFlag = UInt8(True)
        self.RedMax = UInt16Be(255)
        self.GreenMax = UInt16Be(255)
        self.BlueMax = UInt16Be(255)
        self.RedShift = UInt8(16)
        self.GreenShift = UInt8(8)
        self.BlueShift = UInt8(0)
        self.padding = (UInt16Be(), UInt8())",https://github.com/citronneur/rdpy/blob/4109b7a6fe2abf3ddbaed54e29d2f31e63ed97f6/rdpy/protocol/rfb/rfb.py#L82-L94
tfidf_python_100_1.0,converting uint8 array to image,python,"def rescale_image(image):
        """""" Normalise and scale image in 0-255 range """"""
        s2_min_value, s2_max_value = 0, 1
        out_min_value, out_max_value = 0, 255
        # Clamp values in 0-1 range
        image[image > s2_max_value] = s2_max_value
        image[image < s2_min_value] = s2_min_value
        # Rescale to uint8 range
        out_image = out_max_value + (image-s2_min_value)*(out_max_value-out_min_value)/(s2_max_value-s2_min_value)
        return out_image.astype(np.uint8)",https://github.com/sentinel-hub/eo-learn/blob/b8c390b9f553c561612fe9eb64e720611633a035/coregistration/eolearn/coregistration/coregistration.py#L368-L377
tfidf_python_100_1.0,converting uint8 array to image,python,"def run(self, image):  # pylint: disable=arguments-differ
    if not isinstance(image, np.ndarray):
      raise ValueError(""'image' must be a numpy array: %r"" % image)
    if image.dtype != np.uint8:
      raise ValueError(""'image' dtype must be uint8, but is %r"" % image.dtype)
    return self._encode_op.eval(feed_dict={self._image_placeholder: image})",https://github.com/tensorflow/tensorboard/blob/8e5f497b48e40f2a774f85416b8a35ac0693c35e/tensorboard/util/encoder.py#L55-L60
tfidf_python_100_1.0,converting uint8 array to image,python,"def __init__(self, size, pduType2 = 0, shareId = 0):
        CompositeType.__init__(self)
        self.shareId = UInt32Le(shareId)
        self.pad1 = UInt8()
        self.streamId = UInt8(StreamId.STREAM_LOW)
        self.uncompressedLength = UInt16Le(lambda:(UInt16Le(size).value - 8))
        self.pduType2 = UInt8(pduType2)
        self.compressedType = UInt8()
        self.compressedLength = UInt16Le()",https://github.com/citronneur/rdpy/blob/4109b7a6fe2abf3ddbaed54e29d2f31e63ed97f6/rdpy/protocol/rdp/pdu/data.py#L457-L465
tfidf_python_100_1.0,converting uint8 array to image,python,"def _image_data(self):
        """"""Returns the data in image format, with scaling and conversion to uint8 types.

        Returns
        -------
        :obj:`numpy.ndarray` of uint8
            A 3D matrix representing the image. The first dimension is rows, the
            second is columns, and the third is simply the IR entry scaled to between 0 and BINARY_IM_MAX_VAL.
        """"""
        return (self._data * (float(BINARY_IM_MAX_VAL) / MAX_IR)).astype(np.uint8)",https://github.com/BerkeleyAutomation/perception/blob/03d9b37dd6b66896cdfe173905c9413c8c3c5df6/perception/image.py#L1897-L1906
tfidf_python_100_1.0,converting uint8 array to image,python,"def image2array(self, image):
        """"""
        returns 1d array of values in image in idex_mask
        :param image:
        :param idex_mask:
        :return:
        """"""
        idex_mask = self._idex_mask
        array = util.image2array(image)
        if self._idex_mask_bool is True:
            return array[idex_mask == 1]
        else:
            return array",https://github.com/sibirrer/lenstronomy/blob/4edb100a4f3f4fdc4fac9b0032d2b0283d0aa1d6/lenstronomy/ImSim/image_numerics.py#L174-L186
tfidf_python_100_1.0,converting uint8 array to image,python,"def _image_processing(self, image):
        assert image.dtype == numpy.uint8
        image = image.copy()
        x, y = self.blur_x, self.blur_y
        if x or y:
            x += (x + 1) % 2  # opencv needs a
            y += (y + 1) % 2  # odd number...
            image = cv2.GaussianBlur(image, (x, y), 0)
        return image",https://github.com/andrewda/frc-livescore/blob/71594cd6d2c8b6c5feb3889bb05552d09b8128b1/livescore/simpleocr_utils/opencv_utils.py#L62-L70
tfidf_python_100_1.0,converting uint8 array to image,python,"def _image_processing(self, image):
        b = self.brightness
        assert image.dtype == numpy.uint8
        assert -1 <= b <= 1
        image = image.copy()
        with OverflowPreventer(image) as img:
            img += int(b * 256)
        return image",https://github.com/andrewda/frc-livescore/blob/71594cd6d2c8b6c5feb3889bb05552d09b8128b1/livescore/simpleocr_utils/opencv_utils.py#L29-L36
tfidf_python_100_1.0,converting uint8 array to image,python,"def full_texture_augmentation(image):
    image = image.copy()
    image = saturation(image).astype(np.float64)
    image = contrast(image)
    image = brightness(image)
    image = jitter(image)
    #image = salt(image)
    #image = pepper(image)
    image = value_crop(image)

    return image.astype(np.uint8)",https://github.com/penguinmenac3/opendatalake/blob/77c888377095e1812a16982c8efbd2f6b1697a33/opendatalake/texture_augmentation.py#L10-L20
tfidf_python_100_1.0,converting uint8 array to image,python,"def sendSetEncoding(self):
        """"""
        @summary:  Send set encoding packet
                    Actually only RAW bitmap encoding are used
        """"""
        self.send((UInt8(ClientToServerMessages.ENCODING), UInt8(), UInt16Be(1), SInt32Be(Encoding.RAW)))",https://github.com/citronneur/rdpy/blob/4109b7a6fe2abf3ddbaed54e29d2f31e63ed97f6/rdpy/protocol/rfb/rfb.py#L461-L466
tfidf_python_100_1.0,converting uint8 array to image,python,"def enforce_epsilon_and_compute_hash(dataset_batch_dir, adv_dir, output_dir,
                                     epsilon):
  """"""Enforces size of perturbation on images, and compute hashes for all images.

  Args:
    dataset_batch_dir: directory with the images of specific dataset batch
    adv_dir: directory with generated adversarial images
    output_dir: directory where to copy result
    epsilon: size of perturbation

  Returns:
    dictionary with mapping form image ID to hash.
  """"""
  dataset_images = [f for f in os.listdir(dataset_batch_dir)
                    if f.endswith('.png')]
  image_hashes = {}
  resize_warning = False
  for img_name in dataset_images:
    if not os.path.exists(os.path.join(adv_dir, img_name)):
      logging.warning('Image %s not found in the output', img_name)
      continue
    image = np.array(
        Image.open(os.path.join(dataset_batch_dir, img_name)).convert('RGB'))
    image = image.astype('int32')
    image_max_clip = np.clip(image + epsilon, 0, 255).astype('uint8')
    image_min_clip = np.clip(image - epsilon, 0, 255).astype('uint8')
    # load and resize adversarial image if needed
    adv_image = Image.open(os.path.join(adv_dir, img_name)).convert('RGB')
    # Image.size is reversed compared to np.array.shape
    if adv_image.size[::-1] != image.shape[:2]:
      resize_warning = True
      adv_image = adv_image.resize((image.shape[1], image.shape[0]),
                                   Image.BICUBIC)
    adv_image = np.array(adv_image)
    clipped_adv_image = np.clip(adv_image,
                                image_min_clip,
                                image_max_clip)
    Image.fromarray(clipped_adv_image).save(os.path.join(output_dir, img_name))
    # compute hash
    image_hashes[img_name[:-4]] = hashlib.sha1(
        clipped_adv_image.view(np.uint8)).hexdigest()
  if resize_warning:
    logging.warning('One or more adversarial images had incorrect size')
  return image_hashes",https://github.com/tensorflow/cleverhans/blob/97488e215760547b81afc53f5e5de8ba7da5bd98/examples/nips17_adversarial_competition/eval_infra/code/eval_lib/dataset_helper.py#L81-L124
tfidf_python_100_1.0,converting uint8 array to image,python,"def binarize(image, threshold):
    image_binarized = (image > threshold).astype(np.uint8)
    return image_binarized",https://github.com/Microsoft/nni/blob/c7cc8db32da8d2ec77a382a55089f4e17247ce41/examples/trials/kaggle-tgs-salt/postprocessing.py#L41-L43
tfidf_python_100_1.0,converting uint8 array to image,python,"def _normalize_8bit(array):
    if array.dtype == numpy.uint8:
        return array
    return numpy.asarray(numpy.clip(array*255, 0.0, 255.0), dtype=numpy.uint8)",https://github.com/chainer/chainerui/blob/87ad25e875bc332bfdad20197fd3d0cb81a078e8/chainerui/report/image_report.py#L98-L101
tfidf_python_100_1.0,converting uint8 array to image,python,"def petsc_vec_to_array(vec):
    logger.debug('Converting petsc vector to array.')
    array = np.array(vec, copy=True)
    return array",https://github.com/jor-/util/blob/0eb0be84430f88885f4d48335596ca8881f85587/util/petsc/with_petsc4py.py#L42-L45
tfidf_python_100_1.0,converting uint8 array to image,python,"def get_image(self, image, output='vector'):
        """""" A flexible method for transforming between different
        representations of image data.
        Args:
            image: The input image. Can be a string (filename of image),
                NiBabel image, N-dimensional array (must have same shape as
                self.volume), or vectorized image data (must have same length
                as current conjunction mask).
            output: The format of the returned image representation. Must be
                one of:
                    'vector': A 1D vectorized array
                    'array': An N-dimensional array, with
                        shape = self.volume.shape
                    'image': A NiBabel image
        Returns: An object containing image data; see output options above.
        """"""
        if isinstance(image, string_types):
            image = nb.load(image)

        if type(image).__module__.startswith('nibabel'):
            if output == 'image':
                return image
            image = image.get_data()

        if not type(image).__module__.startswith('numpy'):
            raise ValueError(""Input image must be a string, a NiBabel image, ""
                             ""or a numpy array."")

        if image.shape[:3] == self.volume.shape:
            if output == 'image':
                return nb.nifti1.Nifti1Image(image, None, self.get_header())
            elif output == 'array':
                return image
            else:
                image = image.ravel()

        if output == 'vector':
            return image.ravel()

        image = np.reshape(image, self.volume.shape)

        if output == 'array':
            return image

        return nb.nifti1.Nifti1Image(image, None, self.get_header())",https://github.com/neurosynth/neurosynth/blob/948ce7edce15d7df693446e76834e0c23bfe8f11/neurosynth/base/mask.py#L92-L136
tfidf_python_100_1.0,converting uint8 array to image,python,"def extract_properties(image):
    ft00 = ft_of_rotation(image, 0)
    ft30 = ft_of_rotation(image, 30)
    ft60 = ft_of_rotation(image, 60)
    ft90 = ft_of_rotation(image, 90)
    s00, k00 = ft_skew_and_kurtosis(ft00)
    s30, k30 = ft_skew_and_kurtosis(ft30)
    s60, k60 = ft_skew_and_kurtosis(ft60)
    s90, k90 = ft_skew_and_kurtosis(ft90)
    return numpy.array([
        ft_mean(ft00) / image.shape[0],
        fast_sum(image, None) / image.max(),
        s00,
        k00,
        s30,
        k30,
        s60,
        k60,
        s90,
        k90,
    ], dtype=numpy.float64)",https://github.com/ten10solutions/Geist/blob/a1ef16d8b4c3777735008b671a50acfde3ce7bf1/geist/ocr.py#L237-L257
tfidf_python_100_1.0,converting uint8 array to image,python,"def from_array(cls, array, name=None, log_in_history=True):
        """"""Return :class:`jicimagelib.image.Image` instance from an array.
        
        :param array: :class:`numpy.ndarray`
        :param name: name of the image
        :param log_in_history: whether or not to log the creation event
                               in the image's history
        :returns: :class:`jicimagelib.image.Image`
        """"""
        image = array.view(cls)
        event = 'Created image from array'
        if name:
            event = '{} as {}'.format(event, name)
        if log_in_history:
            image.history.append(event)
        return image",https://github.com/JIC-CSB/jicimagelib/blob/fbd67accb2e6d55969c6d4ed7e8b4bb4ab65cd44/jicimagelib/image.py#L20-L35
tfidf_python_100_1.0,converting uint8 array to image,python,"def get_tensor_info(self):
    # Image is returned as a 3-d uint8 tf.Tensor.
    return feature.TensorInfo(shape=self._shape, dtype=tf.uint8)",https://github.com/tensorflow/datasets/blob/46ceb0cf7b4690f38ecbbc689e4d659a903d08dc/tensorflow_datasets/core/features/image_feature.py#L113-L115
tfidf_python_100_1.0,memoize to disk  - persistent memoization,python,"def __init__(self, gen_code=True, memoize=True, debug=False):
        self.gen_code = gen_code
        self.memoize = memoize
        self.debug = debug",https://github.com/lisael/fastidious/blob/2542db9de779ddabc3a64e9eb19a4e2de99741dc/fastidious/fastidious_compiler.py#L470-L473
tfidf_python_100_1.0,memoize to disk  - persistent memoization,python,"def random_insert_small_native_pvector_evolver():
    e = small_native_vector.evolver()
    for x in (9, 1, 4, 5, 7, 7, 3, 2):
        e[x] = x
    v = e.persistent()",https://github.com/tobgu/pyrsistent/blob/c84dab0daaa44973cbe83830d14888827b307632/performance_suites/pvector.py#L157-L161
tfidf_python_100_1.0,memoize to disk  - persistent memoization,python,"def random_replace_large_pmap_evolver():
    e = large_pmap.evolver()
    for x in (999, 111, 74, 1233, 6, 1997, 400, 1000):
        e[x] = x
    m = e.persistent()",https://github.com/tobgu/pyrsistent/blob/c84dab0daaa44973cbe83830d14888827b307632/performance_suites/pmap.py#L98-L102
tfidf_python_100_1.0,memoize to disk  - persistent memoization,python,"def random_replace_small_pmap_evolver():
    e = small_pmap.evolver()
    for x in (9, 1, 4, 5, 7, 7, 3, 2):
        e[x] = x
    m = e.persistent()",https://github.com/tobgu/pyrsistent/blob/c84dab0daaa44973cbe83830d14888827b307632/performance_suites/pmap.py#L90-L94
tfidf_python_100_1.0,memoize to disk  - persistent memoization,python,"def random_insert_large_native_pvector_evolver():
    e = large_python_vector.evolver()
    for x in (999, 111, 74, 1233, 6, 1997, 400, 1000):
        e[x] = x
    v = e.persistent()",https://github.com/tobgu/pyrsistent/blob/c84dab0daaa44973cbe83830d14888827b307632/performance_suites/pvector.py#L181-L185
tfidf_python_100_1.0,memoize to disk  - persistent memoization,python,"def random_insert_large_native_pvector_evolver():
    e = large_native_vector.evolver()
    for x in (999, 111, 74, 1233, 6, 1997, 400, 1000):
        e[x] = x
    v = e.persistent()",https://github.com/tobgu/pyrsistent/blob/c84dab0daaa44973cbe83830d14888827b307632/performance_suites/pvector.py#L173-L177
tfidf_python_100_1.0,memoize to disk  - persistent memoization,python,"def random_insert_small_python_pvector_evolver():
    e = small_python_vector.evolver()
    for x in (9, 1, 4, 5, 7, 7, 3, 2):
        e[x] = x
    v = e.persistent()",https://github.com/tobgu/pyrsistent/blob/c84dab0daaa44973cbe83830d14888827b307632/performance_suites/pvector.py#L165-L169
tfidf_python_100_1.0,memoize to disk  - persistent memoization,python,"def __init__(self, solr_cores=SolrCores(admin_path='/admin/cores',
                                            cores=[SolrCore('core0', 'core0')]), persistent=None):
        super(SolrMulticoreXML, self).__init__(solr_cores=solr_cores,
                                               persistent=persistent if persistent is not None else self.persistent)",https://github.com/albarrentine/bebop/blob/0bc71460112872cb761e692436d5ecb25ca2a263/bebop/config.py#L206-L209
tfidf_python_100_1.0,memoize to disk  - persistent memoization,python,"def __init__(self, dfk, memoize=True, checkpoint={}):
        """"""Initialize the memoizer.

        Args:
            - dfk (DFK obj): The DFK object

        KWargs:
            - memoize (Bool): enable memoization or not.
            - checkpoint (Dict): A checkpoint loaded as a dict.
        """"""
        self.dfk = dfk
        self.memoize = memoize

        if self.memoize:
            logger.info(""App caching initialized"")
            self.memo_lookup_table = checkpoint
        else:
            logger.info(""App caching disabled for all apps"")
            self.memo_lookup_table = {}",https://github.com/Parsl/parsl/blob/d7afb3bc37f50dcf224ae78637944172edb35dac/parsl/dataflow/memoization.py#L38-L56
tfidf_python_100_1.0,memoize to disk  - persistent memoization,python,"def _make_jbod(self, disk, realcfg):
        currstatus = self._get_status(disk, realcfg)
        if currstatus.lower() == 'jbod':
            return
        self._make_available(disk, realcfg)
        self._set_drive_state(disk, 16)",https://github.com/openstack/pyghmi/blob/f710b1d30a8eed19a9e86f01f9351c737666f3e5/pyghmi/ipmi/oem/lenovo/imm.py#L995-L1000
tfidf_python_100_1.0,memoize to disk  - persistent memoization,python,"def spindown(self, disk, spindown=1):
        """"""
        Spindown a disk
        :param disk str: Full path to a disk like /dev/sda
        :param spindown int: spindown value should be in [1, 240]
        """"""
        args = {
            ""disk"": disk,
            ""spindown"": spindown
        }
        self._spindown_chk.check(args)
        response = self._client.raw('disk.spindown', args)

        result = response.get()
        if result.state != 'SUCCESS':
            raise RuntimeError(""Failed to spindown disk {} to {}."".format(disk, spindown))",https://github.com/zero-os/0-core/blob/69f6ce845ab8b8ad805a79a415227e7ac566c218/client/py-client/zeroos/core0/client/client.py#L1828-L1843
tfidf_python_100_1.0,memoize to disk  - persistent memoization,python,"def memoize(f):
    """"""Cache value returned by the function.""""""
    @wraps(f)
    def w(*args, **kw):
        memoize.mem[f] = v = f(*args, **kw)
        return v
    return w",https://github.com/Arkq/flake8-requirements/blob/d7cb84af2429a63635528b531111a5da527bf2d1/src/flake8_requirements/checker.py#L34-L40
tfidf_python_100_1.0,memoize to disk  - persistent memoization,python,"def __str__(self):
        return (
            'SolrConnection{host=%s, solrBase=%s, persistent=%s, postHeaders=%s, reconnects=%s}'
            % (
                self.host,
                self.solrBase,
                self.persistent,
                self.xmlheaders,
                self.reconnects,
            )
        )",https://github.com/DataONEorg/d1_python/blob/3ac4d4f3ca052d3e8641a6a329cab526c8ddcb0d/client_onedrive/src/d1_onedrive/impl/drivers/dokan/solrclient.py#L140-L150
tfidf_python_100_1.0,memoize to disk  - persistent memoization,python,"def delete_export(self, path, cloud_host=None):
        disk = self.host_class.objects.get(nfsaas_path_host=path)

        try:
            self.create_access(disk.nfsaas_path_host, disk.host.address)
        except AccessAPIError:
            pass

        delete_all_disk_files(
            disk.nfsaas_path, disk.nfsaas_path_host,
            disk.nfsaas_export_id, disk.host, cloud_host
        )

        request = self.client.export_delete(disk.nfsaas_path_host)
        if request[0] != 200:
            raise DeleteExportAPIError(request)

        disk.delete()

        if disk.group:
            group = disk.group
            if not group.hosts.all():
                group.delete()

        return True",https://github.com/globocom/dbaas-nfsaas/blob/c05dad5493454289ce4cd91f96cd44bfcc66b184/dbaas_nfsaas/faas_provider.py#L61-L85
tfidf_python_100_1.0,memoize to disk  - persistent memoization,python,"def set_ipcsem_params(self, ftok=None, persistent=None):
        """"""Sets ipcsem lock engine params.

        :param str|unicode ftok: Set the ipcsem key via ftok() for avoiding duplicates.

        :param bool persistent: Do not remove ipcsem's on shutdown.

        """"""
        self._set('ftok', ftok)
        self._set('persistent-ipcsem', persistent, cast=bool)

        return self._section",https://github.com/idlesign/uwsgiconf/blob/475407acb44199edbf7e0a66261bfeb51de1afae/uwsgiconf/options/locks.py#L32-L43
tfidf_python_100_1.0,memoize to disk  - persistent memoization,python,"def Get(self,key):
		""""""Get disk by providing mount point or ID

		If key is not unique and finds multiple matches only the first
		will be returned
		""""""

		for disk in self.disks:
			if disk.id == key:  return(disk)
			elif key in disk.partition_paths:  return(disk)",https://github.com/CenturyLinkCloud/clc-python-sdk/blob/f4dba40c627cb08dd4b7d0d277e8d67578010b05/src/clc/APIv2/disk.py#L31-L40
tfidf_python_100_1.0,memoize to disk  - persistent memoization,python,"def resize_disk(self, disk):
        value = min(disk, self.sz_max['disk'])
        self.guestshell('guestshell resize rootfs {}'.format(value))",https://github.com/Apstra/aeon-venos/blob/4d4f73d5904831ddc78c30922a8a226c90cf7d90/pylib/aeon/nxos/autoload/guestshell.py#L138-L140
tfidf_python_100_1.0,memoize to disk  - persistent memoization,python,"def get_diskinfo(opts, show_all=False, debug=False, local_only=False):
    ''' Returns a list holding the current disk info,
        stats divided by the ouptut unit.
    '''
    outunit = opts.outunit
    disks = []
    try:
        label_map = get_label_map(opts)
        lines = run(diskcmd).splitlines()[1:]   # dump header
        for line in lines:
            tokens  = line.split()
            mntp = b' '.join(tokens[8:])
            dev = basename(tokens[0])
            disk = DiskInfo()
            if (dev in devfilter) or (mntp in mntfilter):
                if show_all:
                    if dev == b'map':           # fix alignment :-/
                        dev = tokens[0] = b'%b %b' % (dev, tokens[1])
                        del tokens[1]
                    disk.isram = True
                else:
                    continue

            # convert to bytes as integer, then output units
            disk.dev    = dev = dev.decode('ascii')
            disk.ocap   = float(tokens[1]) * 1024
            disk.cap    = disk.ocap / outunit
            disk.free   = float(tokens[3]) * 1024 / outunit
            disk.pcnt   = int(tokens[4][:-1])
            disk.used   = float(tokens[2]) * 1024 / outunit

            disk.mntp   = mntp.decode('utf8')
            disk.label  = label_map.get(disk.mntp)
            disk.ismntd = bool(disk.mntp)
            disk.isnet  = ':' in dev  # cheesy but may work? (macos)
            if local_only and disk.isnet:
                continue
            if disk.ismntd:
                if disk.mntp == '/':
                    disk.rw = True
                else:
                    disk.rw = os.access(disk.mntp, os.W_OK)

            # ~ disk.isopt  = None  # TODO: not sure how to get these
            # ~ disk.isrem  = None
            disks.append(disk)
    except IOError as err:
        print(err)
        return None

    if opts.debug:
        print()
        for disk in disks:
            print(disk.dev, disk)
            print()
    disks.sort()
    return disks",https://github.com/mixmastamyk/fr/blob/f96df8ed7210a033b9e711bbed768d4116213bfb/fr/darwin.py#L57-L113
tfidf_python_100_1.0,memoize to disk  - persistent memoization,python,"def addattr(self, attrname, value=None, persistent=True):
        """"""Adds an attribute to self. If persistent is True, the attribute will
        be made a persistent attribute. Persistent attributes are copied
        whenever a view or copy of this array is created. Otherwise, new views
        or copies of this will not have the attribute.
        """"""
        setattr(self, attrname, value)
        # add as persistent
        if persistent and attrname not in self.__persistent_attributes__:
            self.__persistent_attributes__.append(attrname)",https://github.com/gwastro/pycbc/blob/7a64cdd104d263f1b6ea0b01e6841837d05a4cb3/pycbc/io/record.py#L922-L931
tfidf_python_100_1.0,memoize to disk  - persistent memoization,python,"def __init__(self, reactor, persistent=True, debug_requests=False, **pool_kwz):
		super(QuietHTTPConnectionPool, self).__init__(reactor, persistent=persistent)
		for k, v in pool_kwz.viewitems():
			getattr(self, k) # to somewhat protect against typos
			setattr(self, k, v)",https://github.com/mk-fg/txu1/blob/0326e9105f3cf9efa17a3d2ed1dd5606e0ad57d6/txu1/api_v1.py#L268-L272
tfidf_python_100_1.0,parse command line argument,python,"def command(data, line):
    if parse.empty_line(line):
        return command
    current_command = utils.get_current_command(data)
    if parse.comment_delimiter(line):
        return command_comment
    if parse.command_divisor(line):
        return post
    l = parse.indented_line(line)
    if l:
        current_command['pre'] = [l]
        return pre
    else:
        raise SyntaxError(error.COMMAND_HEADER_UNEXPECTED_UNINDENTED_ERROR)",https://github.com/tiborsimon/projects/blob/44d1caf2bab001a2b0bf33c40d7669ae1206f534/projects/projectfile/parser/state.py#L76-L89
tfidf_python_100_1.0,parse command line argument,python,"def command(self, command):
        self.write(command + '\n')
        line = self.read_line()
        return line",https://github.com/gunyarakun/python-shogi/blob/137fe5f5e72251e8a97a1dba4a9b44b7c3c79914/shogi/CSA.py#L334-L337
tfidf_python_100_1.0,parse command line argument,python,"def match_option(argument, short_option, long_option):
    """"""
    Match a command line argument against a short and long option.

    :param argument: The command line argument (a string).
    :param short_option: The short option (a string).
    :param long_option: The long option (a string).
    :returns: :data:`True` if the argument matches, :data:`False` otherwise.
    """"""
    return short_option[1] in argument[1:] if is_short_option(argument) else argument == long_option",https://github.com/paylogic/pip-accel/blob/ccad1b784927a322d996db593403b1d2d2e22666/pip_accel/utils.py#L305-L314
tfidf_python_100_1.0,parse command line argument,python,"def before_commands(data, line):
    if parse.empty_line(line):
        return before_commands
    if parse.comment_delimiter(line):
        return main_comment
    v = parse.variable(line)
    if v:
        data.update({'variables': v})
        return variables
    c = parse.command_header(line)
    if c:
        data['commands'] = c
        return command
    else:
        raise SyntaxError(error.COMMAND_HEADER_SYNTAX_ERROR)",https://github.com/tiborsimon/projects/blob/44d1caf2bab001a2b0bf33c40d7669ae1206f534/projects/projectfile/parser/state.py#L17-L31
tfidf_python_100_1.0,parse command line argument,python,"def _name_for_command(command):
  r""""""Craft a simple command name from the command.

  The best command strings for this are going to be those where a simple
  command was given; we will use the command to derive the name.

  We won't always be able to figure something out and the caller should just
  specify a ""--name"" on the command-line.

  For example, commands like ""export VAR=val\necho ${VAR}"", this function would
  return ""export"".

  If the command starts space or a comment, then we'll skip to the first code
  we can find.

  If we find nothing, just return ""command"".

  >>> _name_for_command('samtools index ""${BAM}""')
  'samtools'
  >>> _name_for_command('/usr/bin/sort ""${INFILE}"" > ""${OUTFILE}""')
  'sort'
  >>> _name_for_command('# This should be ignored')
  'command'
  >>> _name_for_command('\\\n\\\n# Bad continuations, but ignore.\necho hello.')
  'echo'

  Arguments:
    command: the user-provided command
  Returns:
    a proposed name for the task.
  """"""

  lines = command.splitlines()
  for line in lines:
    line = line.strip()
    if line and not line.startswith('#') and line != '\\':
      return os.path.basename(re.split(r'\s', line)[0])

  return 'command'",https://github.com/DataBiosphere/dsub/blob/443ce31daa6023dc2fd65ef2051796e19d18d5a7/dsub/commands/dsub.py#L1155-L1193
tfidf_python_100_1.0,parse command line argument,python,"def dereference_run(self, arg_r):
		""""""
		.. _dereference_run:

		Converts the commands to opcodes and inserts the (relative or static) references.
		
		""""""
		wc = 0
		der_run = []
		for line in arg_r:
			args = []
			for argument in line[3]:
				logging.debug(""dereference run: handling argument "" + str(argument))
				if(isinstance(argument, int)):
					logging.debug(""Argument interpreted as integer"")
					args.append(argument)
					continue
				if((not argument in self.refs) and 
						(not argument in self.static_refs)):
					raise ArgumentError(""[line {}]: Argument '{}' is neither an int nor a reference."".format(line[0], argument))
				if(argument in self.static_refs):
					logging.debug(""Argument interpreted as static reference"")
					args.append(self.static_refs[argument][0])
					continue
				my_word = wc
				ref_word = self.refs[argument][0]
				args.append(ref_word - my_word)
				logging.debug(""Argument interpreted as reference"")
			data = []
			if(line[1] == ""command""):
				data = [line[2].opcode()]
			data.extend(args)
			wc += len(data)
			der_run.append((line[0], line[1], data))
		return der_run",https://github.com/daknuett/py_register_machine2/blob/599c53cd7576297d0d7a53344ed5d9aa98acc751/tools/assembler/assembler.py#L212-L246
tfidf_python_100_1.0,parse command line argument,python,"def execute(self, command, objRef=None, *arguments):
        flatten_arguments = []
        for argument in arguments:
            flatten_arguments.append(' '.join(argument) if type(argument) in [list, tuple] else argument)
        return self.ixnCommand('exec ' + command, *flatten_arguments)",https://github.com/shmir/PyIxNetwork/blob/e7d7a89c08a5d3a1382b4dcfd915bbfc7eedd33f/ixnetwork/api/ixn_tcl.py#L52-L56
tfidf_python_100_1.0,parse command line argument,python,"def _recv(self, *expected_commands):
        line = SocketError.wrap(self._readline)

        command = self._get_command(line)
        if command not in expected_commands:
            raise UnexpectedResponse(line)

        result = command.match(line.encode('utf-8'))
        if result is None:
            raise UnknownResponse(command.pattern, line)

        return command, result",https://github.com/mcuadros/pynats/blob/afbf0766c5546f9b8e7b54ddc89abd2602883b6c/pynats/connection.py#L253-L264
tfidf_python_100_1.0,parse command line argument,python,"def _line_received(self, line: str) -> None:
        LOGGER.debug(""%s: >> %s"", self, line)

        self.line_received(line)

        if self.command:
            self.command._line_received(self, line)",https://github.com/niklasf/python-chess/blob/d91f986ca3e046b300a0d7d9ee2a13b07610fe1a/chess/engine.py#L641-L647
tfidf_python_100_1.0,parse command line argument,python,"def _apply_variables_to_command_lines(command, data):
    temp_lines = []
    for line in command['script']:
        line = substitute_variables(line, data['variables'])
        temp_lines.append(line)
    command['script'] = temp_lines",https://github.com/tiborsimon/projects/blob/44d1caf2bab001a2b0bf33c40d7669ae1206f534/projects/projectfile/command_processor.py#L182-L187
tfidf_python_100_1.0,parse command line argument,python,"def parse(self, argument):
    """"""Parses argument as whitespace-separated list of strings.

    It also parses argument as comma-separated list of strings if requested.

    Args:
      argument: string argument passed in the commandline.

    Returns:
      [str], the parsed flag value.
    """"""
    if isinstance(argument, list):
      return argument
    elif not argument:
      return []
    else:
      if self._comma_compat:
        argument = argument.replace(',', ' ')
      return argument.split()",https://github.com/abseil/abseil-py/blob/9d73fdaa23a6b6726aa5731390f388c0c6250ee5/absl/flags/_argument_parser.py#L534-L552
tfidf_python_100_1.0,parse command line argument,python,"def parse(self, argument):
    """"""See base class.""""""
    if isinstance(argument, list):
      return argument
    elif not argument:
      return []
    else:
      return [s.strip() for s in argument.split(self._token)]",https://github.com/abseil/abseil-py/blob/9d73fdaa23a6b6726aa5731390f388c0c6250ee5/absl/flags/_argument_parser.py#L474-L481
tfidf_python_100_1.0,parse command line argument,python,"def get_parsed_commands(self):
        opts = self.parse_command_line_options()
        number_of_output_rows = 10
        output_file_name = self.input_csv[:-4] + '_small.csv'

        for command, argument in opts:
            if command in ['-o', '--output']:
                output_file_name = argument
            elif command in ['-l', '--lines']:
                number_of_output_rows = argument
        return {'rows': int(number_of_output_rows), 'output': output_file_name}",https://github.com/lappis-unb/salic-ml/blob/1b3ebc4f8067740999897ccffd9892dc94482a93/research/data/csv_sample_factory.py#L46-L56
tfidf_python_100_1.0,parse command line argument,python,"def parse_exception(line):
    '''Parse the first line of a Cartouche exception description.

    Args:
        line (str): A single line Cartouche exception description.

    Returns:
        A 2-tuple containing the exception type and the first line of the description.
    '''
    m = RAISES_REGEX.match(line)
    if m is None:
        raise CartoucheSyntaxError('Cartouche: Invalid argument syntax ""{line}"" for Raises block'.format(line=line))
    return m.group(2), m.group(1)",https://github.com/rob-smallshire/cartouche/blob/d45a8fc1fb4820bf8e741a70a8d5dafe5d26b43a/cartouche/parser.py#L250-L262
tfidf_python_100_1.0,parse command line argument,python,"def argument_run(self, sp_r):
		""""""
		.. _argument_run:

		Converts Arguments according to ``to_int``
		""""""
		arg_run = []

		for line in sp_r:
			logging.debug(""argument run: handling: "" + str(line))
			if(line[1] == ""data""):
				arg_run.append( (line[0], line[1], line[2], line[2].get_words(line[3])))
				continue
			if(line[1] == ""command""):
				self.checkargs(line[0], line[2], line[3])
				arg_run.append( (line[0], line[1], line[2], [a for a in self.convert_args(line[2], line[3])]))
		return arg_run",https://github.com/daknuett/py_register_machine2/blob/599c53cd7576297d0d7a53344ed5d9aa98acc751/tools/assembler/assembler.py#L162-L178
tfidf_python_100_1.0,parse command line argument,python,"def delete_char(self, argument=1):
        if argument < 0:
            self.backward_delete_char(-argument)
        if self.delete_selection():
            argument -= 1
        for x in range(argument):
            del self[Point]",https://github.com/SeattleTestbed/seash/blob/40f9d2285662ff8b61e0468b4196acee089b273b/pyreadline/lineeditor/lineobj.py#L551-L557
tfidf_python_100_1.0,parse command line argument,python,"def backward_delete_word(self, argument=1):
        if argument < 0:
            self.forward_delete_word(-argument)
        if self.delete_selection():
            argument -= 1
        for x in range(argument):
            del self[PrevWordStart:Point]",https://github.com/SeattleTestbed/seash/blob/40f9d2285662ff8b61e0468b4196acee089b273b/pyreadline/lineeditor/lineobj.py#L577-L583
tfidf_python_100_1.0,parse command line argument,python,"def __init__(self, t, argument=None):
        self.type = t
        self.argument = argument",https://github.com/Kitware/tangelo/blob/470034ee9b3d7a01becc1ce5fddc7adc1d5263ef/tangelo/tangelo/server.py#L20-L22
tfidf_python_100_1.0,parse command line argument,python,"def forward_word_end(self, argument=1):
        if argument < 0:
            self.backward_word_end(-argument)
        self.selection_mark = -1
        for x in range(argument):
            self.point = NextWordEnd",https://github.com/SeattleTestbed/seash/blob/40f9d2285662ff8b61e0468b4196acee089b273b/pyreadline/lineeditor/lineobj.py#L461-L466
tfidf_python_100_1.0,parse command line argument,python,"def forward_word(self,argument=1):
        if argument<0:
            self.backward_word(-argument)
        self.selection_mark=-1
        for x in range(argument):
            self.point = NextWordStart",https://github.com/SeattleTestbed/seash/blob/40f9d2285662ff8b61e0468b4196acee089b273b/pyreadline/lineeditor/lineobj.py#L447-L452
tfidf_python_100_1.0,how to read the contents of a .gz compressed file?,python,"def read(self):
        """"""
        Read and return the contents of the file.
        """"""
        with open(self.path) as f:
            d = f.read()
        return d",https://github.com/snare/scruffy/blob/0fedc08cfdb6db927ff93c09f25f24ce5a04c541/scruffy/file.py#L121-L127
tfidf_python_100_1.0,how to read the contents of a .gz compressed file?,python,"def is_file(file, compressed=None):
    if compressed is None:
        return is_file(file, compressed=False) or is_file(file, compressed=True)
    else:
        ext = get_ext(compressed=compressed)
        return util.io.fs.has_file_ext(file, ext)",https://github.com/jor-/util/blob/0eb0be84430f88885f4d48335596ca8881f85587/util/io/np.py#L19-L24
tfidf_python_100_1.0,how to read the contents of a .gz compressed file?,python,"def encode(term, compressed=False):
  body = encode_term(term)
  if compressed:
    if 0 > compressed or compressed > 9:
      raise ValueError(""Invalid compression level: {0}"".format(compressed))
    else:
      compressed = compress(body, compressed)
      compressed_length = len(compressed)
      if compressed_length + 5 <= len(body):
        return ERL_MAGIC + ERL_COMPRESSED + _int4_pack(compressed_length) + compressed
  return ERL_MAGIC + body",https://github.com/dveselov/termformat/blob/cb281fb25942758e0b10b5470f9ce5f3bb5e1413/termformat/__init__.py#L144-L154
tfidf_python_100_1.0,how to read the contents of a .gz compressed file?,python,"def term_to_binary(term, compressed=False):
    """"""
    Encode Python types into Erlang terms in binary data
    """"""
    data_uncompressed = _term_to_binary(term)
    if compressed is False:
        return b_chr(_TAG_VERSION) + data_uncompressed
    else:
        if compressed is True:
            compressed = 6
        if compressed < 0 or compressed > 9:
            raise InputException('compressed in [0..9]')
        data_compressed = zlib.compress(data_uncompressed, compressed)
        size_uncompressed = len(data_uncompressed)
        if size_uncompressed > 4294967295:
            raise OutputException('uint32 overflow')
        return (
            b_chr(_TAG_VERSION) + b_chr(_TAG_COMPRESSED_ZLIB) +
            struct.pack(b'>I', size_uncompressed) + data_compressed
        )",https://github.com/okeuday/erlang_py/blob/81b7c2ace66b6bdee23602a6802efff541223fa3/erlang.py#L461-L480
tfidf_python_100_1.0,how to read the contents of a .gz compressed file?,python,"def compress_dummy(input):
    input_length = len(input)
    compressed = bytearray()

    extra_bytes = input_length % 8

    for i in range(0, input_length-extra_bytes, 8):
        compressed.append(0xFF)
        compressed.extend(input[i:i+8])

    if extra_bytes > 0:
        compressed.append(0xFF >> (8 - extra_bytes))
        compressed.extend(input[-extra_bytes:])

    compressed.append(0)
    compressed.append(0)
    compressed.append(0)

    return bytes(compressed)",https://github.com/mon/ifstools/blob/ccd9c1c3632aa22cdcc4e064f17e07803b1d27ba/ifstools/handlers/lz77.py#L101-L119
tfidf_python_100_1.0,how to read the contents of a .gz compressed file?,python,"def add_file_ext(file, compressed=False):
    ext = get_ext(compressed=compressed)
    return util.io.fs.add_file_ext_if_needed(file, ext)",https://github.com/jor-/util/blob/0eb0be84430f88885f4d48335596ca8881f85587/util/io/np.py#L27-L29
tfidf_python_100_1.0,how to read the contents of a .gz compressed file?,python,"def get_file(self, file_path):
    path = self.get_path_to_file(file_path)

    compressed = os.path.exists(path + '.gz')
      
    if compressed:
      path += '.gz'

    encoding = 'gzip' if compressed else None
    
    try:
      with open(path, 'rb') as f:
        data = f.read()
      return data, encoding
    except IOError:
      return None, encoding",https://github.com/seung-lab/cloud-volume/blob/d2fd4500333f1bc3cd3e3919a8b649cec5d8e214/cloudvolume/storage.py#L480-L495
tfidf_python_100_1.0,how to read the contents of a .gz compressed file?,python,"def stream(self, **kwargs):
        contents = self.read()
        while contents:
            yield contents
            contents = self.read()",https://github.com/spulec/moto/blob/4a286c4bc288933bb023396e2784a6fdbb966bc9/moto/core/models.py#L256-L260
tfidf_python_100_1.0,how to read the contents of a .gz compressed file?,python,"def save(file, values, compressed=None, make_read_only=False, overwrite=False, create_path_if_not_exists=True):
    ## check input values
    is_values_dict = isinstance(values, dict)
    is_values_tuple = (isinstance(values, tuple) or isinstance(values, list)) and all(map(lambda a: isinstance(a, np.ndarray), values))
    
    if not is_values_dict and not is_values_tuple:
        values = np.asanyarray(values)
    
    if is_file(file, compressed=False):
        if compressed:
            raise ValueError('Compressed values can only be stored in ""npz"" file format, but the file {} has ending ""npy"".'.format(file))
        if is_values_dict or is_values_tuple:
            raise ValueError('Multiple values {} can only be stored in ""npz"" file format, but the file {} has ending ""npy"".'.format(file))
    
    ## set file ext
    use_npz = is_values_dict or is_values_tuple or (compressed is not None and compressed) or is_file(file, compressed=True)    
    file = add_file_ext(file, compressed=use_npz)
    
    ## set compressed if not passed
    if compressed is None:
        compressed = use_npz
    
    ## create dir
    if create_path_if_not_exists:
        (dir, filename) = os.path.split(file)
        os.makedirs(dir, exist_ok=True)
    
    ## remove if overwrite
    if overwrite:
        util.io.fs.remove_file(file, force=True, not_exist_okay=True)
    
    ## save
    if use_npz:
        if compressed:
            np.savez_compressed(file, values)
        else:
            np.savez(file, values)
    else:
        np.save(file, values)
    
    ## make read only
    if make_read_only:
        util.io.fs.make_read_only(file)",https://github.com/jor-/util/blob/0eb0be84430f88885f4d48335596ca8881f85587/util/io/np.py#L32-L74
tfidf_python_100_1.0,how to read the contents of a .gz compressed file?,python,"def uncompress(compressed):
  fileobj = BytesIO(compressed)
  gz = gzip.GzipFile(fileobj=fileobj, mode='r')
  uncompressed = gz.read()
  gz.close()

  return uncompressed",https://github.com/kamicut/tilepie/blob/103ae2be1c3c4e6f7ec4a3bdd265ffcddee92b96/tilepie/__init__.py#L8-L14
tfidf_python_100_1.0,how to read the contents of a .gz compressed file?,python,"def compress(self, stream):
        compressed = BytesIO()

        with zipfile.ZipFile(file=compressed, mode='w', allowZip64=True) as f:
            f.writestr('data', stream.read())

        compressed.seek(0)
        return compressed",https://github.com/svenkreiss/pysparkling/blob/596d0ef2793100f7115efe228ff9bfc17beaa08d/pysparkling/fileio/codec/zip.py#L15-L22
tfidf_python_100_1.0,how to read the contents of a .gz compressed file?,python,"def ecdsa_private_key(privkey_str=None, compressed=None):
    """"""
    Make a private key, but enforce the following rule:
    * unless the key's hex encoding specifically ends in '01', treat it as uncompressed.
    """"""
    if compressed is None:
        compressed = False
        if privkey_str is not None:
            if len(privkey_str) == 66 and privkey_str[-2:] == '01':
                compressed = True

    return _ECPrivateKey(privkey_str, compressed=compressed)",https://github.com/blockstack/virtualchain/blob/fcfc970064ca7dfcab26ebd3ab955870a763ea39/virtualchain/lib/ecdsalib.py#L221-L232
tfidf_python_100_1.0,how to read the contents of a .gz compressed file?,python,"def _process(self, data):
        compressed = self._compressor.compress(data)
        if compressed:
            yield compressed",https://github.com/JohnVinyard/featureflow/blob/7731487b00e38fa4f58c88b7881870fda2d69fdb/featureflow/encoder.py#L71-L74
tfidf_python_100_1.0,how to read the contents of a .gz compressed file?,python,"def compress(self, stream):
        compressed = BytesIO()

        with gzip.GzipFile(fileobj=compressed, mode='wb') as f:
            f.write(stream.read())

        compressed.seek(0)
        return compressed",https://github.com/svenkreiss/pysparkling/blob/596d0ef2793100f7115efe228ff9bfc17beaa08d/pysparkling/fileio/codec/gz.py#L15-L22
tfidf_python_100_1.0,how to read the contents of a .gz compressed file?,python,"def _process_file(input_file, output_file, apikey):
    """"""Shrinks input_file to output_file.

    This function should be used only inside process_directory.
    It takes input_file, tries to shrink it and if shrink was successful
    save compressed image to output_file. Otherwise raise exception.

    @return compressed: PNGResponse
    """"""
    bytes_ = read_binary(input_file)
    compressed = shrink(bytes_, apikey)

    if compressed.success and compressed.bytes:
        write_binary(output_file, compressed.bytes)
    else:
        if compressed.errno in FATAL_ERRORS:
            raise StopProcessing(compressed)
        elif compressed.errno == TinyPNGError.InternalServerError:
            raise RetryProcessing(compressed)

    return compressed",https://github.com/vasilcovsky/pytinypng/blob/ac633e4aa41122c49a806f411e43a76d8f73058e/pytinypng/pytinypng.py#L29-L49
tfidf_python_100_1.0,how to read the contents of a .gz compressed file?,python,"def load_dawg(f, t=dawg.IntDAWG):
    if not f.endswith('.gz'):
        if not os.path.exists(f):
            f += '.gz'
    T = t()
    T.read(open_(f, 'rb'))
    return T",https://github.com/rchatterjee/pwmodels/blob/e277411f8ebaf4ad1c208d2b035b4b68f7471517/src/pwmodel/helper.py#L257-L263
tfidf_python_100_1.0,how to read the contents of a .gz compressed file?,python,"def ecdsa_public_key(pubkey_str, compressed=None):
    """"""
    Make a public key object, but enforce the following rule:
    * if compressed is True or False, make the key compressed/uncompressed.
    * otherwise, return whatever the hex encoding is
    """"""
    if compressed == True:
        pubkey_str = keylib.key_formatting.compress(pubkey_str)
    elif compressed == False:
        pubkey_str = keylib.key_formatting.decompress(pubkey_str)

    return _ECPublicKey(pubkey_str)",https://github.com/blockstack/virtualchain/blob/fcfc970064ca7dfcab26ebd3ab955870a763ea39/virtualchain/lib/ecdsalib.py#L235-L246
tfidf_python_100_1.0,how to read the contents of a .gz compressed file?,python,"def encode(term, compressed=False):
    """"""Encode Erlang external term.""""""
    encoded_term = encode_term(term)
    # False and 0 do not attempt compression.
    if compressed:
        if compressed is True:
            # default compression level of 6
            compressed = 6
        elif compressed < 0 or compressed > 9:
            raise ValueError(""invalid compression level: %r"" % (compressed,))
        zlib_term = compress(encoded_term, compressed)
        ln = len(encoded_term)
        if len(zlib_term) + 5 <= ln:
            # Compressed term should be smaller
            return b""\x83P"" + _int4_pack(ln) + zlib_term
    return b""\x83"" + encoded_term",https://github.com/hdima/erlport/blob/246b7722d62b87b48be66d9a871509a537728962/priv/python3/erlport/erlterms.py#L317-L332
tfidf_python_100_1.0,how to read the contents of a .gz compressed file?,python,"def read_file(file_path, mode = 'rt'):
    """"""
    Read the contents of a file

    :param file_path:                   Path of the file to be read

    :return:                            Contents of the file
    """"""
    contents = ''
    with open(file_path, mode) as f:
        contents = f.read()
    return contents",https://github.com/nccgroup/opinel/blob/2d4f5b96e0a1f9cb0356629f4f87e4ed99ce2606/opinel/utils/fs.py#L99-L110
tfidf_python_100_1.0,how to read the contents of a .gz compressed file?,python,"def safe_gz_unzip(contents):
    ''' Takes a file's contents passed as a string (contents) and either gz-unzips the contents and returns the uncompressed data or else returns the original contents.
        This function raises an exception if passed what appears to be gz-zipped data (from the magic number) but if gzip fails to decompress the contents.
        A cleaner method would use zlib directly rather than writing a temporary file but zlib.decompress(contents, 16+zlib.MAX_WBITS) fix did not work for me immediately and I had things to get done!'''
    if len(contents) > 1 and ord(contents[0]) == 31 and ord(contents[1]) == 139:
        #contents = zlib.decompress(contents, 16+zlib.MAX_WBITS)
        fname = write_temp_file('/tmp', contents)
        try:
            f = gzip.open(fname, 'rb')
            contents = f.read()
            f.close()
        except:
            os.remove(fname)
            raise
        return contents
    else:
        return contents",https://github.com/Kortemme-Lab/klab/blob/6d410ad08f1bd9f7cbbb28d7d946e94fbaaa2b6b/klab/fs/io.py#L48-L64
tfidf_python_100_1.0,sending binary data over a serial connection,python,"def __init__(self, serial=""""):
        self.serial = serial
        if serial:
            self.fastboot_str = ""fastboot -s {}"".format(serial)
        else:
            self.fastboot_str = ""fastboot""",https://github.com/google/mobly/blob/38ba2cf7d29a20e6a2fca1718eecb337df38db26/mobly/controllers/android_device_lib/fastboot.py#L50-L55
tfidf_python_100_1.0,sending binary data over a serial connection,python,"def serial(self):
        """"""Returns true if the CI server should run in serial mode.
        """"""
        serial = self.property_get(""SERIAL"", False)
        if isinstance(serial, str):
            return serial.lower() == ""true""
        else:
            return serial",https://github.com/rosenbrockc/ci/blob/4d5a60291424a83124d1d962d17fb4c7718cde2b/pyci/config.py#L429-L436
tfidf_python_100_1.0,sending binary data over a serial connection,python,"def setup(self):
        try:
            self.serial = serial.Serial('/dev/ttyUSB0')
        except serial.serialutil.SerialException:
            self.serial = serial.Serial('/dev/ttyUSB1')",https://github.com/oksome/Intercom/blob/234f181ff812420516273093ec6dfedd7fe9c074/intercom/minions/arduino.py#L39-L43
tfidf_python_100_1.0,sending binary data over a serial connection,python,"def deserialize(self, serial):
        if not isinstance(serial, Atomic):
            m = 'Objects of type {} cannot be deserialized'
            raise DeserializationError(m.format(type(serial).__name__), serial)

        if self.is_valid_length(len(serial)):
            return serial
        else:
            raise DeserializationError('{} has invalid length'.format(type(serial)), serial)",https://github.com/ethereum/pyrlp/blob/bb898f8056da3973204c699621350bf9565e43df/rlp/sedes/binary.py#L44-L52
tfidf_python_100_1.0,sending binary data over a serial connection,python,"def serials(self):
        if hasattr(self, '_serials'):
            return self._serials
        serial = environ.get('ANDROID_SERIAL')
        if serial:
            return serial.split(',')
        l = self.buildozer.cmd('{} devices'.format(self.adb_cmd),
                               get_stdout=True)[0].splitlines()
        serials = []
        for serial in l:
            if not serial:
                continue
            if serial.startswith('*') or serial.startswith('List '):
                continue
            serials.append(serial.split()[0])
        self._serials = serials
        return serials",https://github.com/kivy/buildozer/blob/586152c6ce2b6cde4d5a081d9711f9cb037a901c/buildozer/targets/android.py#L1141-L1157
tfidf_python_100_1.0,sending binary data over a serial connection,python,"def deserialize(self, serial):
        if self.l is not None and len(serial) != self.l:
            raise DeserializationError('Invalid serialization (wrong size)',
                                       serial)
        if self.l is None and len(serial) > 0 and serial[0:1] == b'\x00':
            raise DeserializationError('Invalid serialization (not minimal '
                                       'length)', serial)

        serial = serial or b'\x00'
        return big_endian_to_int(serial)",https://github.com/ethereum/pyrlp/blob/bb898f8056da3973204c699621350bf9565e43df/rlp/sedes/big_endian_int.py#L38-L47
tfidf_python_100_1.0,sending binary data over a serial connection,python,"def get_serial_no(self):
        cmd = ""host-serial:{serial}:get-serialno"".format(serial=self.serial)
        return self._execute_cmd(cmd)",https://github.com/Swind/pure-python-adb/blob/8e076bc2b25ad33b6c5eb14293329a017d83455f/adb/command/serial/__init__.py#L63-L65
tfidf_python_100_1.0,sending binary data over a serial connection,python,"def get_device_path(self):
        cmd = ""host-serial:{serial}:get-devpath"".format(serial=self.serial)
        return self._execute_cmd(cmd)",https://github.com/Swind/pure-python-adb/blob/8e076bc2b25ad33b6c5eb14293329a017d83455f/adb/command/serial/__init__.py#L59-L61
tfidf_python_100_1.0,sending binary data over a serial connection,python,"def deserialize(self, serial):
        if not isinstance(serial, Atomic):
            m = 'Objects of type {} cannot be deserialized'
            raise DeserializationError(m.format(type(serial).__name__), serial)

        try:
            text_value = serial.decode(self.encoding)
        except UnicodeDecodeError as err:
            raise DeserializationError(str(err), serial)

        if self.is_valid_length(len(text_value)):
            return text_value
        else:
            raise DeserializationError('{} has invalid length'.format(type(serial)), serial)",https://github.com/ethereum/pyrlp/blob/bb898f8056da3973204c699621350bf9565e43df/rlp/sedes/text.py#L47-L60
tfidf_python_100_1.0,sending binary data over a serial connection,python,"def write(self, data):
        r = self.serial.write(data)
        self.serial.flush()
        return r",https://github.com/decryptus/sonicprobe/blob/72f73f3a40d2982d79ad68686e36aa31d94b76f8/sonicprobe/libs/sp_serial.py#L70-L73
tfidf_python_100_1.0,sending binary data over a serial connection,python,"def json_serial(o):
    if isinstance(o, datetime):
        serial = o.strftime('%Y-%m-%dT%H:%M:%S.%f')
    elif isinstance(o, Decimal):
        if o % 1 > 0:
            serial = float(o)
        elif six.PY3:
            serial = int(o)
        elif o < sys.maxsize:
            serial = int(o)
        else:
            serial = long(o)
    elif isinstance(o, uuid.UUID):
        serial = str(o.hex)
    elif isinstance(o, set):
        serial = list(o)
    else:
        serial = o
    return serial",https://github.com/Alonreznik/dynamodb-json/blob/f6a5c472fc349f51281fb2ecd4679479f01ee2f3/dynamodb_json/json_util.py#L11-L29
tfidf_python_100_1.0,sending binary data over a serial connection,python,"def __init__(self, *, serial: int=1, **kwargs) -> None:
        super(SpatialAnchorsAccountKeyRegenerateRequest, self).__init__(**kwargs)
        self.serial = serial",https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-mgmt-mixedreality/azure/mgmt/mixedreality/models/spatial_anchors_account_key_regenerate_request_py3.py#L26-L28
tfidf_python_100_1.0,sending binary data over a serial connection,python,"def deserialize(self, serial):
    cheat, cheat_card_ids, deck_ids = serial
    return self(deck_ids=deck_ids, cheat=cheat, cheat_card_ids=cheat_card_ids)",https://github.com/ishikota/PyPokerEngine/blob/a52a048a15da276005eca4acae96fb6eeb4dc034/pypokerengine/engine/deck.py#L34-L36
tfidf_python_100_1.0,sending binary data over a serial connection,python,"def __init__(self, message, serial):
        super(DeserializationError, self).__init__(message)
        self.serial = serial",https://github.com/ethereum/pyrlp/blob/bb898f8056da3973204c699621350bf9565e43df/rlp/exceptions.py#L94-L96
tfidf_python_100_1.0,sending binary data over a serial connection,python,"def cmd(self, *args, **kwargs):
        '''adb command, add -s serial by default. return the subprocess.Popen object.'''
        serial = self.device_serial()
        if serial:
            if "" "" in serial:  # TODO how to include special chars on command line
                serial = ""'%s'"" % serial
            return self.raw_cmd(*[""-s"", serial] + list(args))
        else:
            return self.raw_cmd(*args)",https://github.com/xiaocong/uiautomator/blob/9a0c892ffd056713f91aa2153d1533c5b0553a1c/uiautomator/__init__.py#L277-L285
tfidf_python_100_1.0,sending binary data over a serial connection,python,"def __init__(self, port):

        # Open serial port
        serial_opts = {""port"": port,
                       ""baudrate"": 9600,
                       ""parity"": serial.PARITY_NONE,
                       ""bytesize"": serial.EIGHTBITS,
                       ""stopbits"": serial.STOPBITS_ONE,
                       ""xonxoff"": False,
                       ""timeout"": 1}
        self.serial = serial.Serial(**serial_opts)

        self.get_iss_info()
        self.get_iss_serial_no()",https://github.com/DancingQuanta/pyusbiss/blob/fc64e123f1c97f53ad153c474d230ad38044c3cb/usbiss/usbiss.py#L51-L64
tfidf_python_100_1.0,sending binary data over a serial connection,python,"def deserialize(self, serial):
        if not is_sequence(serial):
            raise ListDeserializationError('Can only deserialize sequences', serial)

        if self.strict and len(serial) != len(self):
            raise ListDeserializationError(
                'Deserializing list length (%d) does not match sedes (%d)' % (
                    len(serial), len(self)),
                serial)

        for idx, (sedes, element) in enumerate(zip(self, serial)):
            try:
                yield sedes.deserialize(element)
            except DeserializationError as e:
                raise ListDeserializationError(serial=serial, element_exception=e, index=idx)",https://github.com/ethereum/pyrlp/blob/bb898f8056da3973204c699621350bf9565e43df/rlp/sedes/lists.py#L81-L95
tfidf_python_100_1.0,sending binary data over a serial connection,python,"def execute(self, connection):
        connection.serial.write(b""@r"")
        length = int(connection.serial.read(), 16)
        if length == 0:
            return
        data = connection.serial.read(length * 2)
        message = parse(data)
        # Filter duplicated state messages. The filtering feature
        # of SCSGate is buggy and causes @r to always return 0 available
        # messages
        if isinstance(message, StateMessage):
            if self._last_raw_state_message == data:
                return
            else:
                self._last_raw_state_message = data
        self._notification_endpoint(message)",https://github.com/flavio/scsgate/blob/aad1d181eef4714ab475f4ff7fcfac4a6425fbb4/scsgate/tasks.py#L28-L43
tfidf_python_100_1.0,sending binary data over a serial connection,python,"def _open(self):
        self.serial = serial.Serial(self.device, 115200, timeout=10, writeTimeout=10)",https://github.com/keepkey/python-keepkey/blob/8318e3a8c4025d499342130ce4305881a325c013/keepkeylib/transport_serial.py#L17-L18
tfidf_python_100_1.0,sending binary data over a serial connection,python,"def get_state(self):
        cmd = ""host-serial:{serial}:get-state"".format(serial=self.serial)
        return self._execute_cmd(cmd)",https://github.com/Swind/pure-python-adb/blob/8e076bc2b25ad33b6c5eb14293329a017d83455f/adb/command/serial/__init__.py#L67-L69
tfidf_python_100_1.0,extracting data from a text file,python,"def extract(fname, header, out_fname=None):
    with sys.stdout if out_fname is None else open(out_fname, 'w') as output, \
            open(fname, encoding='utf-8') as input:
        extracting = False
        for line in input:
            if line[0] == '>':
                if extracting:
                    break
                elif line[1:].startswith(header):
                    extracting = True
            if extracting:
                output.write(line)",https://github.com/childsish/lhc-python/blob/0a669f46a40a39f24d28665e8b5b606dc7e86beb/lhc/io/fasta/__main__.py#L55-L66
tfidf_python_100_1.0,extracting data from a text file,python,"def report_line(file, text):
    print ""%s %s -- %s"" % (mod_astro.time_print(file.file_time), file.id, text)",https://github.com/camsci/meteor-pi/blob/7b01527650bd1b2b76d6f364e8122e25b8812c8d/src/cmdLineAdmin/recalculateSkyClarity.py#L60-L61
tfidf_python_100_1.0,extracting data from a text file,python,"def write_message_to_pipe(pipe_handle, text):
    data = text.encode('utf-8')
    yield From(write_message_bytes_to_pipe(pipe_handle, data))",https://github.com/prompt-toolkit/pymux/blob/3f66e62b9de4b2251c7f9afad6c516dc5a30ec67/pymux/pipes/win32.py#L151-L153
tfidf_python_100_1.0,extracting data from a text file,python,"def extract_2bit(self,filename_full_archive,filename_full_file):
		self.status_message(""Extracting a 2bit file: {}"".format(filename_full_archive))
		self.shell('""{}"" ""{}"" ""{}""'.format(
				smbl.prog.TWOBITTOFA,
				filename_full_archive,
				filename_full_file
			))",https://github.com/karel-brinda/smbl/blob/5922fa2fc4060d86172e991361a1cceb0af51af8/smbl/fasta/fastafile.py#L109-L115
tfidf_python_100_1.0,extracting data from a text file,python,"def __init__(self, data):
        self.author = data[0].text 
        self.datePosted = data[1].text 
        self.overallRating = int(data[2].text )
        self.parts = int(data[3].text )
        self.buildingExperience = int(data[4].text )
        self.playability = int(data[5].text )
        self.valueForMoney = int(data[6].text )
        self.title = data[7].text
        self.review = data[8].text 
        self.HTML = {
            'true': True,
            'false': False,
            'True': True,
            'False': False,
            '1': True,
            '0': False
        }[data[9].text]",https://github.com/4Kaylum/Brickfront/blob/9545f2183249862b077677d48fcfb9b4bfe1f87d/brickfront/review.py#L17-L34
tfidf_python_100_1.0,extracting data from a text file,python,"def validate_text(ctx, param, text):
    """"""Validation callback for the <text> argument.
    Ensures <text> (arg) and <file> (opt) are mutually exclusive
    """"""
    if not text and 'file' not in ctx.params:
        # No <text> and no <file>
        raise click.BadParameter(
            ""<text> or -f/--file <file> required"")
    if text and 'file' in ctx.params:
        # Both <text> and <file>
        raise click.BadParameter(
            ""<text> and -f/--file <file> can't be used together"")
    return text",https://github.com/pndurette/gTTS/blob/b01ac4eb22d40c6241202e202d0418ccf4f98460/gtts/cli.py#L45-L57
tfidf_python_100_1.0,extracting data from a text file,python,"def _ExtractContentFromDataStream(
      self, mediator, file_entry, data_stream_name):
    """"""Extracts content from a data stream.

    Args:
      mediator (ParserMediator): mediates the interactions between
          parsers and other components, such as storage and abort signals.
      file_entry (dfvfs.FileEntry): file entry to extract its content.
      data_stream_name (str): name of the data stream whose content is to be
          extracted.
    """"""
    self.processing_status = definitions.STATUS_INDICATOR_EXTRACTING

    if self._processing_profiler:
      self._processing_profiler.StartTiming('extracting')

    self._event_extractor.ParseDataStream(
        mediator, file_entry, data_stream_name)

    if self._processing_profiler:
      self._processing_profiler.StopTiming('extracting')

    self.processing_status = definitions.STATUS_INDICATOR_RUNNING

    self.last_activity_timestamp = time.time()",https://github.com/log2timeline/plaso/blob/9c564698d2da3ffbe23607a3c54c0582ea18a6cc/plaso/engine/worker.py#L312-L336
tfidf_python_100_1.0,extracting data from a text file,python,"def data_find_text_or_all(data, path, dyn_cls=False):
    text = data_find_text(data, path)
    if text:
        return text
    return data_find_all(data, path, dyn_cls=dyn_cls)",https://github.com/artefactual-labs/mets-reader-writer/blob/d95939cabdfdc25cb1bf67df0c84bd0d6e6a73ff/metsrw/plugins/premisrw/premis.py#L832-L836
tfidf_python_100_1.0,extracting data from a text file,python,"def echo(text, fg=None, bg=None, style=None, file=None, err=False, color=None):
    """"""Write the given text to the provided stream or **sys.stdout** by default.

    Provides optional foreground and background colors from the ansi defaults:
    **grey**, **red**, **green**, **yellow**, **blue**, **magenta**, **cyan**
    or **white**.

    Available styles include **bold**, **dark**, **underline**, **blink**, **reverse**,
    **concealed**

    :param str text: Text to write
    :param str fg: Foreground color to use (default: None)
    :param str bg: Foreground color to use (default: None)
    :param str style: Style to use (default: None)
    :param stream file: File to write to (default: None)
    :param bool color: Whether to force color (i.e. ANSI codes are in the text)
    """"""

    if file and not hasattr(file, ""write""):
        raise TypeError(""Expected a writable stream, received {0!r}"".format(file))
    if not file:
        if err:
            file = _text_stderr()
        else:
            file = _text_stdout()
    if text and not isinstance(text, (six.string_types, bytes, bytearray)):
        text = six.text_type(text)
    text = """" if not text else text
    if isinstance(text, six.text_type):
        text += ""\n""
    else:
        text += b""\n""
    if text and six.PY3 and is_bytes(text):
        buffer = _get_binary_buffer(file)
        if buffer is not None:
            file.flush()
            buffer.write(text)
            buffer.flush()
            return
    if text and not is_bytes(text):
        can_use_color = _can_use_color(file, color=color)
        if any([fg, bg, style]):
            text = colorize(text, fg=fg, bg=bg, attrs=style)
        if not can_use_color or (os.name == ""nt"" and not _wrap_for_color):
            text = ANSI_REMOVAL_RE.sub("""", text)
        elif os.name == ""nt"" and _wrap_for_color:
            file = _wrap_for_color(file, color=color)
    if text:
        file.write(text)
    file.flush()",https://github.com/sarugaku/vistir/blob/96c008ee62a43608d1e70797f74634cb66a004c1/src/vistir/misc.py#L938-L987
tfidf_python_100_1.0,extracting data from a text file,python,"def __init__(self, data: BeautifulSoup):
        super().__init__(data)
        self.request_delivered_receipt = data.request['d'] == 'true'
        self.requets_read_receipt = data.request['r'] == 'true'
        self.video_url = data.find('file-url').text
        self.file_content_type = data.find('file-content-type').text if data.find('file-content-type') else None
        self.duration_milliseconds = data.find('duration').text if data.find('duration') else None
        self.file_size = data.find('file-size').text
        self.from_jid = data['from']
        self.to_jid = data['to']
        self.group_jid = data.g['jid']",https://github.com/tomer8007/kik-bot-api-unofficial/blob/2ae5216bc05e7099a41895382fc8e428a7a5c3ac/kik_unofficial/datatypes/xmpp/chatting.py#L341-L351
tfidf_python_100_1.0,extracting data from a text file,python,"def _prepare_data(self, data: dict) -> dict:
        text = data.pop(""message"")
        data[""text""] = text
        if data.get(""icon_emoji""):
            icon_emoji = data[""icon_emoji""]
            if not icon_emoji.startswith("":""):
                icon_emoji = f"":{icon_emoji}""
            if not icon_emoji.endswith("":""):
                icon_emoji += "":""
            data[""icon_emoji""] = icon_emoji
        return data",https://github.com/notifiers/notifiers/blob/6dd8aafff86935dbb4763db9c56f9cdd7fc08b65/notifiers/providers/slack.py#L132-L142
tfidf_python_100_1.0,extracting data from a text file,python,"def data(self, text):
        if debug:
            print(""data %s"" % text)
        self._data.append(text)",https://github.com/apple/turicreate/blob/74514c3f99e25b46f22c6e02977fe3da69221c2e/deps/src/libxml2-2.9.1/python/generator.py#L55-L58
tfidf_python_100_1.0,extracting data from a text file,python,"def add_to_total_stats(txtobj): #Text
    total_stats.line_toll += txtobj.line_toll
    total_stats.word_toll += txtobj.word_toll
    total_stats.letter_toll += txtobj.letter_toll",https://github.com/Ezhil-Language-Foundation/open-tamil/blob/b7556e88878d29bbc6c944ee17cdd3f75b8ea9f0/examples/tamilwordcount.py#L95-L98
tfidf_python_100_1.0,extracting data from a text file,python,"def handle_data(self, data):
        text = data.strip()
        if len(text) > 0:
            text = re.sub('[ \t\r\n]+', ' ', text)
            self.__text.append(text + ' ')",https://github.com/vatlab/SoS/blob/6b60ed0770916d135e17322e469520d778e9d4e7/src/sos/utils.py#L568-L572
tfidf_python_100_1.0,extracting data from a text file,python,"def __repr__(self):
        text = []
        for i in self.data:
            i = _i_to_bro(i, self.cols)
            text.append(i)

        text = ""\n"".join(text)

        text = ""{0}\n{1}"".format(HEADER, text)
        return text",https://github.com/csirtgadgets/csirtg-indicator-py/blob/7bd5e4e2a8bc085e1025f917f64fc2d7e1c687fe/csirtg_indicator/format/zbro.py#L91-L100
tfidf_python_100_1.0,extracting data from a text file,python,"def unstripForHTML(self, text):
		text = self.unstrip(text)
		text = self.unstripNoWiki(text)
		return text",https://github.com/zikzakmedia/python-mediawiki/blob/7c26732efa520e16c35350815ce98cd7610a0bcb/mediawiki/wikimarkup/__init__.py#L1163-L1166
tfidf_python_100_1.0,extracting data from a text file,python,"def parse(self, text):
		utf8 = isinstance(text, str)
		text = to_unicode(text)
		if text[-1:] != u'\n':
			text = text + u'\n'
			taggedNewline = True
		else:
			taggedNewline = False

		text = self.strip(text)
		text = self.removeHtmlTags(text)
		text = self.doTableStuff(text)
		text = self.parseHorizontalRule(text)
		text = self.checkTOC(text)
		text = self.parseHeaders(text)
		text = self.parseAllQuotes(text)
		text = self.replaceExternalLinks(text)
		if not self.show_toc and text.find(u""<!--MWTOC-->"") == -1:
			self.show_toc = False
		text = self.formatHeadings(text, True)
		text = self.unstrip(text)
		text = self.fixtags(text)
		text = self.doBlockLevels(text, True)
		text = self.unstripNoWiki(text)
		text = text.split(u'\n')
		text = u'\n'.join(text)
		if taggedNewline and text[-1:] == u'\n':
			text = text[:-1]
		if utf8:
			return text.encode(""utf-8"")
		return text",https://github.com/zikzakmedia/python-mediawiki/blob/7c26732efa520e16c35350815ce98cd7610a0bcb/mediawiki/wikimarkup/__init__.py#L1607-L1637
tfidf_python_100_1.0,extracting data from a text file,python,"def parse(self, text):
		utf8 = isinstance(text, str)
		text = to_unicode(text)
		if text[-1:] != u'\n':
			text = text + u'\n'
			taggedNewline = True
		else:
			taggedNewline = False

		text = self.strip(text)
		text = self.removeHtmlTags(text)
		text = self.parseHorizontalRule(text)
		text = self.parseAllQuotes(text)
		text = self.replaceExternalLinks(text)
		text = self.unstrip(text)
		text = self.fixtags(text)
		text = self.doBlockLevels(text, True)
		text = self.unstripNoWiki(text)
		text = text.split(u'\n')
		text = u'\n'.join(text)
		if taggedNewline and text[-1:] == u'\n':
			text = text[:-1]
		if utf8:
			return text.encode(""utf-8"")
		return text",https://github.com/zikzakmedia/python-mediawiki/blob/7c26732efa520e16c35350815ce98cd7610a0bcb/mediawiki/wikimarkup/__init__.py#L466-L490
tfidf_python_100_1.0,extracting data from a text file,python,"def complete_text(self, total_spaces, text, white_space):
        text = text[-total_spaces:]
        if len(text) < total_spaces:
            text = white_space * (total_spaces - len(text)) + text

        return text",https://github.com/PedalPi/Raspberry-Physical/blob/3dc71b6997ef36d0de256c5db7a1b38178937fd5/physical/sevensegments/text_write_strategy.py#L28-L33
tfidf_python_100_1.0,extracting data from a text file,python,"def log(self, text=""\n""):
        if text != ""\n"": text = text + ""\n""
        logdraft = LogDraft(self.daemon_drafter, text=text)
        return logdraft",https://github.com/cooperhammond/python-draftlog/blob/3d4a2b3ce3725f52a4a1a15d95f4de35297cae70/draftlog/drafter.py#L107-L110
tfidf_python_100_1.0,positions of substrings in string,python,"def __init__(self, config, *args, **kwargs):
        super(filter_domains_substrings, self).__init__(config, *args, **kwargs)

        ## cleanse the input substrings lists
        self.substrings = set()

        include_substrings = self.config.get('include_substrings', [])
        map(self.substrings.add, include_substrings)

        include_substrings_path = self.config.get('include_substrings_path', [])
        if not isinstance(include_substrings_path, list):
            include_substrings_path = [include_substrings_path]
        map(self.substrings.add, 
            imap(lambda s: s.strip(), 
                 chain(*imap(open, include_substrings_path))))

        logger.info('filter_substrings configured with %d domain names',
                    len(self.substrings))
        logger.info('filter_domans_substrings.substrings = %r', self.substrings)",https://github.com/trec-kba/streamcorpus-pipeline/blob/8bb82ea1beb83c6b40ed03fa1659df2897c2292a/streamcorpus_pipeline/_filters.py#L348-L366
tfidf_python_100_1.0,positions of substrings in string,python,"def word_wrap(self, text, width=1023):
        """"""
        A simple word wrapping greedy algorithm that puts
        as many words into a single string as possible.
        """"""
        substrings = []

        string = text
        while len(string) > width:
            index = width - 1
            while not string[index].isspace():
                index = index - 1

            line = string[0:index]
            substrings.append(line)

            string = string[index + 1:]

        substrings.append(string)

        return substrings",https://github.com/zorg/zorg-emic/blob/34d49897131cf7773b2b0f46e1e0a796911144e3/zorg_emic/emic2.py#L66-L86
tfidf_python_100_1.0,positions of substrings in string,python,"def remove_invalid_ipa_characters(unicode_string, return_invalid=False, single_char_parsing=False):
    """"""
    Remove all Unicode characters that are not IPA valid
    from the given string,
    and return a list of substrings of the given string,
    each mapping to a (known) valid IPA character.

    Return ``None`` if ``unicode_string`` is ``None``.

    :param str unicode_string: the Unicode string to be parsed
    :param bool return_invalid: if ``True``, return a pair ``(valid, invalid)``,
                                where ``invalid`` is a list of Unicode characters
                                that are not IPA valid.
    :param bool single_char_parsing: if ``True``, parse one Unicode character at a time
    :rtype: list of str
    """"""
    if unicode_string is None:
        return None
    substrings = ipa_substrings(unicode_string, single_char_parsing=single_char_parsing)
    valid = [s for s in substrings if s in UNICODE_TO_IPA]
    if return_invalid:
        return (valid, [s for s in substrings if s not in UNICODE_TO_IPA])
    return valid",https://github.com/pettarin/ipapy/blob/ede4b3c40636f6eb90068369d31a2e75c7115324/ipapy/__init__.py#L140-L162
tfidf_python_100_1.0,positions of substrings in string,python,"def _generate_substrings(self, ngram, size):
        """"""Returns a list of all substrings of `ngram`.

        :param ngram: n-gram to generate substrings of
        :type ngram: `str`
        :param size: size of `ngram`
        :type size: `int`
        :rtype: `list`

        """"""
        text = Text(ngram, self._tokenizer)
        substrings = []
        for sub_size, ngrams in text.get_ngrams(1, size-1):
            for sub_ngram, count in ngrams.items():
                substrings.extend([sub_ngram] * count)
        return substrings",https://github.com/ajenhl/tacl/blob/b8a343248e77f1c07a5a4ac133a9ad6e0b4781c2/tacl/results.py#L523-L538
tfidf_python_100_1.0,positions of substrings in string,python,"def _find_first_of(line, substrings):
    """"""Find earliest occurrence of one of substrings in line.

    Returns pair of index and found substring, or (-1, None)
    if no occurrences of any of substrings were found in line.
    """"""
    starts = ((line.find(i), i) for i in substrings)
    found = [(i, sub) for i, sub in starts if i != -1]
    if found:
        return min(found)
    else:
        return -1, None",https://github.com/openstack/hacking/blob/10e58f907181cac91d3b2af422c2458b04a1ec79/hacking/checks/docstrings.py#L150-L161
tfidf_python_100_1.0,positions of substrings in string,python,"def deselect_nodenames(self, *substrings: str) -> 'Selection':
        """"""Restrict the current selection to all nodes with a name
        not containing at least one of the given substrings (does not
        affect any elements).

        See the documentation on method |Selection.search_nodenames| for
        additional information.
        """"""
        self.nodes -= self.search_nodenames(*substrings).nodes
        return self",https://github.com/hydpy-dev/hydpy/blob/1bc6a82cf30786521d86b36e27900c6717d3348d/hydpy/core/selectiontools.py#L734-L743
tfidf_python_100_1.0,positions of substrings in string,python,"def select_nodenames(self, *substrings: str) -> 'Selection':
        """"""Restrict the current selection to all nodes with a name
        containing at least one of the given substrings  (does not
        affect any elements).

        See the documentation on method |Selection.search_nodenames| for
        additional information.
        """"""
        self.nodes = self.search_nodenames(*substrings).nodes
        return self",https://github.com/hydpy-dev/hydpy/blob/1bc6a82cf30786521d86b36e27900c6717d3348d/hydpy/core/selectiontools.py#L723-L732
tfidf_python_100_1.0,positions of substrings in string,python,"def deselect_elementnames(self, *substrings: str) -> 'Selection':
        """"""Restrict the current selection to all elements with a name
        not containing at least one of the given substrings.   (does
        not affect any nodes).

        See the documentation on method |Selection.search_elementnames| for
        additional information.
        """"""
        self.elements -= self.search_elementnames(*substrings).elements
        return self",https://github.com/hydpy-dev/hydpy/blob/1bc6a82cf30786521d86b36e27900c6717d3348d/hydpy/core/selectiontools.py#L819-L828
tfidf_python_100_1.0,positions of substrings in string,python,"def select_elementnames(self, *substrings: str) -> 'Selection':
        """"""Restrict the current selection to all elements with a name
        containing at least one of the given substrings (does not
        affect any nodes).

        See the documentation on method |Selection.search_elementnames| for
        additional information.
        """"""
        self.elements = self.search_elementnames(*substrings).elements
        return self",https://github.com/hydpy-dev/hydpy/blob/1bc6a82cf30786521d86b36e27900c6717d3348d/hydpy/core/selectiontools.py#L808-L817
tfidf_python_100_1.0,positions of substrings in string,python,"def chars2str(chars) -> List[str]:
    """"""Inversion function of function |str2chars|.

    >>> from hydpy.core.netcdftools import chars2str

    >>> chars2str([[b'z', b'e', b'r', b'o', b's'],
    ...            [b'o', b'n', b'e', b's', b'']])
    ['zeros', 'ones']

    >>> chars2str([])
    []
    """"""
    strings = collections.deque()
    for subchars in chars:
        substrings = collections.deque()
        for char in subchars:
            if char:
                substrings.append(char.decode('utf-8'))
            else:
                substrings.append('')
        strings.append(''.join(substrings))
    return list(strings)",https://github.com/hydpy-dev/hydpy/blob/1bc6a82cf30786521d86b36e27900c6717d3348d/hydpy/core/netcdftools.py#L287-L308
tfidf_python_100_1.0,positions of substrings in string,python,"def substrings(self, w):
        for i in range(1, len(w)):
            yield w[:i]
        yield w",https://github.com/coleifer/walrus/blob/82bf15a6613487b5b5fefeb488f186d7e0106547/walrus/autocomplete.py#L82-L85
tfidf_python_100_1.0,positions of substrings in string,python,"def process_properties_args(args):
    # Properties
    properties = None
    if args.properties is not None:
        properties = {}
        for keyeqval in args.properties:
            substrings = split_unescaped('=', keyeqval, include_empty_strings=True)
            if len(substrings) != 2:
                raise DXParserError('Property key-value pair must be given using syntax ""property_key=property_value""')
            elif substrings[0] == '':
                raise DXParserError('Property keys must be nonempty strings')
            else:
                properties[substrings[0]] = substrings[1]
    args.properties = properties",https://github.com/dnanexus/dx-toolkit/blob/74befb53ad90fcf902d8983ae6d74580f402d619/src/python/dxpy/cli/parsers.py#L111-L124
tfidf_python_100_1.0,positions of substrings in string,python,"def isIambic(self):
		if len(self.positions) < 2:
			return None
		else:
			return self.positions[0].meterVal == 'w' and self.positions[1].meterVal == 's'",https://github.com/quadrismegistus/prosodic/blob/8af66ed9be40c922d03a0b09bc11c87d2061b618/prosodic/Parse.py#L357-L361
tfidf_python_100_1.0,positions of substrings in string,python,"def _sanitize_positions(positions):
        if isinstance(positions, u.Quantity):
            if positions.unit == u.pixel:
                positions = np.atleast_2d(positions.value)
            else:
                raise u.UnitsError('positions should be in pixel units')
        elif isinstance(positions, (list, tuple, np.ndarray)):
            positions = np.atleast_2d(positions)
            if positions.shape[1] != 2:
                if positions.shape[0] == 2:
                    positions = np.transpose(positions)
                else:
                    raise TypeError('List or array of (x, y) pixel '
                                    'coordinates is expected, got ""{0}"".'
                                    .format(positions))
        elif isinstance(positions, zip):
            # This is needed for zip to work seamlessly in Python 3
            # (e.g. positions = zip(xpos, ypos))
            positions = np.atleast_2d(list(positions))
        else:
            raise TypeError('List or array of (x, y) pixel coordinates '
                            'is expected, got ""{0}"".'.format(positions))

        if positions.ndim > 2:
            raise ValueError('{0}D position array is not supported. Only 2D '
                             'arrays are supported.'.format(positions.ndim))

        return positions",https://github.com/astropy/photutils/blob/cc9bb4534ab76bac98cb5f374a348a2573d10401/photutils/aperture/core.py#L75-L102
tfidf_python_100_1.0,positions of substrings in string,python,"def _check_positions(self, positions):
        if positions is None:
            return
        positions = _as_array(positions)
        if positions.shape[0] != self.n_channels:
            raise ValueError(""'positions' ""
                             ""(shape {0:s})"".format(str(positions.shape)) +
                             "" and 'n_channels' ""
                             ""({0:d})"".format(self.n_channels) +
                             "" do not match."")",https://github.com/kwikteam/phy/blob/7e9313dc364304b7d2bd03b92938347343703003/phy/electrode/mea.py#L140-L149
tfidf_python_100_1.0,positions of substrings in string,python,"def maximum_separations(self):
        return list(map(lambda positions: self.max_separation_of_grid(positions), self.positions))",https://github.com/Jammy2211/PyAutoLens/blob/91e50369c7a9c048c83d217625578b72423cd5a7/autolens/lens/lens_fit.py#L313-L314
tfidf_python_100_1.0,positions of substrings in string,python,"def get_positions(self):
        """"""
        Returns a list of positions.

        http://dev.wheniwork.com/#listing-positions
        """"""
        url = ""/2/positions""

        data = self._get_resource(url)
        positions = []
        for entry in data['positions']:
            positions.append(self.position_from_json(entry))

        return positions",https://github.com/uw-it-cte/uw-restclients-wheniwork/blob/0d3ca09d5bbe808fec12e5f943596570d33a1731/uw_wheniwork/positions.py#L16-L29
tfidf_python_100_1.0,positions of substrings in string,python,"def __init__(self, positions=None, best_channels=None):
        super(ProbeView, self).__init__()
        self.positions = positions
        self.best_channels = best_channels",https://github.com/kwikteam/phy/blob/7e9313dc364304b7d2bd03b92938347343703003/phy/cluster/views/probe.py#L23-L26
tfidf_python_100_1.0,positions of substrings in string,python,"def get_positions(self, id_image, id_user=None, showDetails=None, afterthan=None,
                      beforethan=None, maxperpage=None):
        from .models.social import PositionCollection
        positions = PositionCollection(filters={""imageinstance"": id_image}, max=maxperpage)
        positions.user = id_user
        positions.afterThan = afterthan
        positions.beforeThan = beforethan
        positions.showDetails = showDetails
        return positions.fetch()",https://github.com/cytomine/Cytomine-python-client/blob/bac19722b900dd32c6cfd6bdb9354fc784d33bc4/cytomine/cytomine.py#L1071-L1079
tfidf_python_100_1.0,positions of substrings in string,python,"def _stack_positions(positions, pos_in_dollars=True):
    """"""
    Convert positions to percentages if necessary, and change them
    to long format.

    Parameters
    ----------
    positions: pd.DataFrame
        Daily holdings (in dollars or percentages), indexed by date.
        Will be converted to percentages if positions are in dollars.
        Short positions show up as cash in the 'cash' column.

    pos_in_dollars : bool
        Flag indicating whether `positions` are in dollars or percentages
        If True, positions are in dollars.
    """"""
    if pos_in_dollars:
        # convert holdings to percentages
        positions = get_percent_alloc(positions)

    # remove cash after normalizing positions
    positions = positions.drop('cash', axis='columns')

    # convert positions to long format
    positions = positions.stack()
    positions.index = positions.index.set_names(['dt', 'ticker'])

    return positions",https://github.com/quantopian/pyfolio/blob/712716ab0cdebbec9fabb25eea3bf40e4354749d/pyfolio/perf_attrib.py#L620-L647
tfidf_python_100_1.0,reading element from html - <td>,python,"def _telescope_pointing_widget(cluster_name):
    html = '<table><thead><tr>'
    html += '<td><b>Telescope pointing</b></td>'
    html += '<td><b>Cluster Name</b></td>'
    html += '<td><b>Image number</b></td>'
    html += '<td><b>Right ascension</b></td>'
    html += '<td><b>Declination</b></td>'
    html += '</tr></thead><tbody><tr>'
    html += '<td><img src=""http://assets.lsst.rocks/data/sphere.png""></td>'
    html += '<td>%s</td>' % cluster_name
    html += '<td>20221274993</td>'
    html += '<td>05h 32m 37s</td>'
    html += '<td>+00h 11m 18s</td>'
    html += '</tr></tbody></table>'
    return Div(text=html, width=600, height=175)",https://github.com/lsst-epo/vela/blob/8e17ebec509be5c3cc2063f4645dfe9e26b49c18/astropixie-widgets/astropixie_widgets/visual.py#L39-L53
tfidf_python_100_1.0,reading element from html - <td>,python,"def save_html_table(self, filename=""results_table.html""):
        html = """"""
            <table>
            <th>
                <td>Rank</td>
                <td>Name</td>
                <td>Mean Squared Error</td>
                <td>Mean Absolute Error</td>
            </th>
        """"""
        for (rank, name, mse, mae) in self.sorted_errors():
            html += """"""
            <tr>
                <td>%d</td>
                <td>%s</td>
                <td>%0.4f</td>
                <td>%0.4f</td>
            </tr>
            """""" % (rank, name, mse, mae)
        html += ""</table>""
        self.ensure_dir(self.dirname)
        path = join(self.dirname, filename)
        with open(path, ""w"") as f:
            f.write(html)
        return html",https://github.com/iskandr/fancyimpute/blob/9f0837d387c7303d5c8c925a9989ca77a1a96e3e/experiments/complete_faces.py#L206-L230
tfidf_python_100_1.0,reading element from html - <td>,python,"def htmlListToTR(l,trClass=None,tdClass=None,td1Class=None):
    """"""
    turns a list into a <tr><td>something</td></tr>
    call this when generating HTML tables dynamically.
    """"""
    html=""<tr>""
    for item in l:
        if 'array' in str(type(item)):
            item=item[0] #TODO: why is this needed
        html+=""<td>%s</td>""%item
    html+=""</tr>""
    if trClass:
        html=html.replace(""<tr>"",'<tr class=""%s"">'%trClass)
    if td1Class:
        html=html.replace(""<td>"",'<td class=""%s"">'%td1Class,1)
    if tdClass:
        html=html.replace(""<td>"",'<td class=""%s"">'%tdClass)


    return html",https://github.com/swharden/SWHLab/blob/a86c3c65323cec809a4bd4f81919644927094bf5/doc/oldcode/swhlab/core/common.py#L192-L211
tfidf_python_100_1.0,reading element from html - <td>,python,"def to_ms(td: Optional[SecondsTimedelta]) -> Optional[int]:
    if td is None:
        return td
    elif isinstance(td, timedelta):
        td = td.total_seconds()
    return as_int(td * 1000)",https://github.com/samuelcolvin/arq/blob/1434646b48c45bd27e392f0162976404e4d8021d/arq/utils.py#L36-L41
tfidf_python_100_1.0,reading element from html - <td>,python,"def future_dt_str(dt, td):
    """""".""""""
    if isinstance(td, str):
        td = float(td)
    td = timedelta(seconds=td)
    future_dt = dt + td
    return future_dt.strftime(DT_PRINT_FORMAT)",https://github.com/juga0/dhcpcanon/blob/9f51a29e57fe93dc93fb22bb0ed12fcfe9557e59/dhcpcanon/timers.py#L18-L24
tfidf_python_100_1.0,reading element from html - <td>,python,"def get_content(self):
        fileLinks = [
            ""<a href='%s'>%s</a>\n"" % (os.path.basename(f), f)
            for f in self.data[""resPathList""]
        ]
        dict = self.data.copy()
        dict[""fileLinks""] = "", "".join(fileLinks)
        if self.name == "".Info.html"":
            html = (
                """"""\
            <html><head>
            <title>%(title)s</title>
            </head><body>
            <h1>%(title)s</h1>
            <table>
            <tr>
                <td>Description</td>
                <td>%(description)s</td>
            </tr><tr>
                <td>Status</td>
                <td>%(status)s</td>
            </tr><tr>
                <td>Tags</td>
                <td>%(tags)s</td>
            </tr><tr>
                <td>Orga unit</td>
                <td>%(orga)s</td>
            </tr><tr>
                <td>Files</td>
                <td>%(fileLinks)s</td>
            </tr><tr>
                <td>Key</td>
                <td>%(key)s</td>
            </tr>
            </table>
            <p>This is a virtual WsgiDAV resource called '%(title)s'.</p>
            </body></html>""""""
                % dict
            )
        elif self.name == "".Info.txt"":
            lines = [
                self.data[""title""],
                ""="" * len(self.data[""title""]),
                self.data[""description""],
                """",
                ""Status: %s"" % self.data[""status""],
                ""Orga:   %8s"" % self.data[""orga""],
                ""Tags:   '%s'"" % ""', '"".join(self.data[""tags""]),
                ""Key:    %s"" % self.data[""key""],
            ]
            html = ""\n"".join(lines)
        elif self.name == "".Description.txt"":
            html = self.data[""description""]
        else:
            raise DAVError(HTTP_INTERNAL_ERROR, ""Invalid artifact '%s'"" % self.name)
        return compat.BytesIO(compat.to_bytes(html))",https://github.com/mar10/wsgidav/blob/cec0d84222fc24bea01be1cea91729001963f172/wsgidav/samples/virtual_dav_provider.py#L485-L540
tfidf_python_100_1.0,reading element from html - <td>,python,"def to_seconds(td: Optional[SecondsTimedelta]) -> Optional[float]:
    if td is None:
        return td
    elif isinstance(td, timedelta):
        return td.total_seconds()
    return td",https://github.com/samuelcolvin/arq/blob/1434646b48c45bd27e392f0162976404e4d8021d/arq/utils.py#L44-L49
tfidf_python_100_1.0,reading element from html - <td>,python,"def set_expiry(self, td):
        if type(td) == timedelta:
            self.expire_records_after = td
        else:
            raise BioCycInvalidExpiry",https://github.com/mfitzp/biocyc/blob/2fe81971687e4dcf1fcf869af0e7b3549be535b1/biocyc/biocyc.py#L308-L312
tfidf_python_100_1.0,reading element from html - <td>,python,"def _get_thermal_displacements(self, proj_dir):
        td = ThermalDisplacements(self._mesh_phonon,
                                  projection_direction=proj_dir,
                                  freq_min=self._fmin,
                                  freq_max=self._fmax)
        td.set_temperatures([self._T])
        td.run()
        return td.get_thermal_displacements()",https://github.com/atztogo/phonopy/blob/869cc2ba9e7d495d5f4cf6942415ab3fc9e2a10f/phonopy/spectrum/dynamic_structure_factor.py#L222-L229
tfidf_python_100_1.0,reading element from html - <td>,python,"def _repr_html_(self):
        Score_htmlContent = '<table><tr><td></td><td></td>'
        BackTrace_htmlContent = '<table><tr><td></td><td></td>'

        for c in self.w2:
            Score_htmlContent += '<td>' + str(c) + '</td>'
            BackTrace_htmlContent += '<td>' + str(c) + '</td>'

       # BackTrace_htmlContent = '<table><tr><td></td>' + str(['<td>' + str(c) + '</td>' for c in self.w2]) + '</tr>'
        for i, row in enumerate(self.score_mat):
            pre = '<td></td>' if i ==0 else '<td>' +  str(self.w1[i-1]) +'</td>'
            Score_htmlContent += '<tr>' + pre
            BackTrace_htmlContent += '<tr>' + pre
            for j, cell in enumerate(row):
                Score_htmlContent += '<td>' + str(cell) + '</td>'
                BackTrace_htmlContent += '<td>' + str(self.path_mat[i][j]) + '</td>'
            Score_htmlContent += '</tr>'
            BackTrace_htmlContent += '</tr>'
        Score_htmlContent += '</table>'
        BackTrace_htmlContent += '</table>'
        return Score_htmlContent + BackTrace_htmlContent",https://github.com/glaunay/pyproteins/blob/d19a8e84fc95d53eea3106a611f0131a6ac224a3/src/pyproteins/alignment/nw_custom.py#L168-L188
tfidf_python_100_1.0,reading element from html - <td>,python,"def to_html(ds: Any) -> str:
	""""""
	Return an HTML representation of the loom file or view, showing the upper-left 10x10 corner.
	""""""
	rm = min(10, ds.shape[0])
	cm = min(10, ds.shape[1])
	html = ""<p>""
	if ds.attrs.__contains__(""title""):
		html += ""<strong>"" + ds.attrs[""title""] + ""</strong> ""
	html += f""{ds.shape[0]} rows, {ds.shape[1]} columns, {len(ds.layers)} layer{'s' if len(ds.layers) > 1 else ''}<br/>(showing up to 10x10)<br/>""
	html += ds.filename + ""<br/>""
	for (name, val) in ds.attrs.items():
		html += f""name: <em>{val}</em><br/>""
	html += ""<table>""
	# Emit column attributes
	for ca in ds.col_attrs.keys():
		html += ""<tr>""
		for ra in ds.row_attrs.keys():
			html += ""<td>&nbsp;</td>""  # Space for row attrs
		html += ""<td><strong>"" + ca + ""</strong></td>""  # Col attr name
		for v in ds.col_attrs[ca][:cm]:
			html += ""<td>"" + str(v) + ""</td>""
		if ds.shape[1] > cm:
			html += ""<td>...</td>""
		html += ""</tr>""

	# Emit row attribute names
	html += ""<tr>""
	for ra in ds.row_attrs.keys():
		html += ""<td><strong>"" + ra + ""</strong></td>""  # Row attr name
	html += ""<td>&nbsp;</td>""  # Space for col attrs
	for v in range(cm):
		html += ""<td>&nbsp;</td>""
	if ds.shape[1] > cm:
		html += ""<td>...</td>""
	html += ""</tr>""

	# Emit row attr values and matrix values
	for row in range(rm):
		html += ""<tr>""
		for ra in ds.row_attrs.keys():
			html += ""<td>"" + str(ds.row_attrs[ra][row]) + ""</td>""
		html += ""<td>&nbsp;</td>""  # Space for col attrs

		for v in ds[row, :cm]:
			html += ""<td>"" + str(v) + ""</td>""
		if ds.shape[1] > cm:
			html += ""<td>...</td>""
		html += ""</tr>""
	# Emit ellipses
	if ds.shape[0] > rm:
		html += ""<tr>""
		for v in range(rm + 1 + len(ds.row_attrs.keys())):
			html += ""<td>...</td>""
		if ds.shape[1] > cm:
			html += ""<td>...</td>""
		html += ""</tr>""
	html += ""</table>""
	return html",https://github.com/linnarsson-lab/loompy/blob/62c8373a92b058753baa3a95331fb541f560f599/loompy/to_html.py#L4-L62
tfidf_python_100_1.0,reading element from html - <td>,python,"def field_timedelta_to_json(self, td):
        """"""Convert timedelta to value containing total number of seconds.

        If there are fractions of a second the return value will be a
        string, otherwise it will be an int.
        """"""
        if isinstance(td, six.string_types):
            td = parse_duration(td)
        if not td:
            return None
        if td.microseconds > 0:
            return str(td.total_seconds())
        else:
            return int(td.total_seconds())",https://github.com/jwhitlock/drf-cached-instances/blob/ec4e8a6e1e83eeea6ec0b924b2eaa40a38d5963a/drf_cached_instances/cache.py#L285-L298
tfidf_python_100_1.0,reading element from html - <td>,python,"def __get_wiki_review(email_cnt, idx):
    '''
    Review for wikis.
    '''
    recent_posts = MWiki.query_recent_edited(tools.timestamp() - TIME_LIMIT, kind='2')
    for recent_post in recent_posts:
        hist_rec = MWikiHist.get_last(recent_post.uid)
        if hist_rec:
            foo_str = '''
                    <tr><td>{0}</td><td>{1}</td><td class=""diff_chg"">Edit</td><td>{2}</td>
                    <td><a href=""{3}"">{3}</a></td></tr>
                    '''.format(idx, recent_post.user_name, recent_post.title,
                               os.path.join(SITE_CFG['site_url'], 'page', recent_post.uid))
            email_cnt = email_cnt + foo_str
        else:
            foo_str = '''
                    <tr><td>{0}</td><td>{1}</td><td class=""diff_add"">New </td><td>{2}</td>
                    <td><a href=""{3}"">{3}</a></td></tr>
                    '''.format(idx, recent_post.user_name, recent_post.title,
                               os.path.join(SITE_CFG['site_url'], 'page', recent_post.uid))
            email_cnt = email_cnt + foo_str
        idx = idx + 1
    email_cnt = email_cnt + '</table>'
    return email_cnt, idx",https://github.com/bukun/TorCMS/blob/6567c7fe2604a1d646d4570c017840958630ed2b/torcms/script/script_review.py#L53-L76
tfidf_python_100_1.0,reading element from html - <td>,python,"def __get_post_review(email_cnt, idx):
    '''
    Review for posts.
    '''
    for key in router_post:
        recent_posts = MPost.query_recent_edited(tools.timestamp() - TIME_LIMIT, kind=key)
        for recent_post in recent_posts:
            hist_rec = MPostHist.get_last(recent_post.uid)
            if hist_rec:
                foo_str = '''
                    <tr><td>{0}</td><td>{1}</td><td class=""diff_chg"">Edit</td><td>{2}</td>
                    <td><a href=""{3}"">{3}</a></td></tr>
                    '''.format(idx, recent_post.user_name, recent_post.title,
                               os.path.join(SITE_CFG['site_url'], router_post[key],
                                            recent_post.uid))
                email_cnt = email_cnt + foo_str
            else:
                foo_str = '''
                    <tr><td>{0}</td><td>{1}</td><td class=""diff_add"">New </td><td>{2}</td>
                    <td><a href=""{3}"">{3}</a></td></tr>
                    '''.format(idx, recent_post.user_name, recent_post.title,
                               os.path.join(SITE_CFG['site_url'], router_post[key],
                                            recent_post.uid))
                email_cnt = email_cnt + foo_str
            idx = idx + 1

    return email_cnt, idx",https://github.com/bukun/TorCMS/blob/6567c7fe2604a1d646d4570c017840958630ed2b/torcms/script/script_review.py#L105-L131
tfidf_python_100_1.0,reading element from html - <td>,python,"def textDocument(self, title, htmltext):
		td = QTextDocument()
		td.setMetaInformation(QTextDocument.DocumentTitle, title)
		if self.ss:
			td.setDefaultStyleSheet(self.ss)
		td.setHtml(htmltext)
		td.setDefaultFont(globalSettings.font)
		return td",https://github.com/retext-project/retext/blob/ad70435341dd89c7a74742df9d1f9af70859a969/ReText/window.py#L941-L948
tfidf_python_100_1.0,reading element from html - <td>,python,"def get_registered_credits (self):
        """""" ç»é²åä½æ°åè¨ã®åå¾ """"""

        self.req(""RSW0001000-flow"")
        r = self.get({
                       ""_eventId"":     ""search"",
                       ""moduleCode"":   1,
                       ""gakkiKbnCode"": ""A""
                   })
        html = r.text

        # XXX
        for l in html.split(""\n""):
            if ""åä½</td>"" in l:
                return float(l.strip().replace('<td align=""center"">', """").
                                       replace(""åä½</td>"", """"))

        return 0.",https://github.com/coins13/twins/blob/d66cc850007a25f01812a9d8c7e3efe64a631ca2/twins/twins.py#L255-L272
tfidf_python_100_1.0,reading element from html - <td>,python,"def generateSummaryHTMLTable(self, extraLapse = TYPICAL_LAPSE):
		'''Generates a summary in HTML of the status of the expected scripts broken based on the log.
			This summary is returned as a list of strings. 
		'''
		scriptsRun = self.scriptsRun
		
		html = []
		html.append(""<table style='text-align:center;border:1px solid black;margin-left: auto;margin-right: auto;'>\n"") # Start summary table
		html.append('	<tr><td colspan=""4"" style=""text-align:center""></td></tr>\n')
		html.append('	<tr style=""font-weight:bold;background-color:#cccccc;text-align:center""><td>Script</td><td>Last status</td><td>Last run</td><td>Last success</td></tr>\n')
		
		# Alternate shades between rows
		tablestyle = ['background-color:#33dd33;', 'background-color:#33ff33;']
		warningstyle = ['background-color:#EA8737;', 'background-color:#f5b767;']
		failstyle = ['background-color:#dd3333;', 'background-color:#ff3333;']
		
		count = 0
		for name, details in sorted(scriptsRun.iteritems()):
			status = None
			
			rowstyle = tablestyle[count % 2]
			if details[""lastSuccess""] and expectedScripts.get(name):
				if not expectedScripts.check(name, details[""lastSuccess""], extraLapse): 
					status = ""STOPPED""
			else:
				rowstyle = failstyle[count % 2]
				status = ""FAIL""
				
			laststatusstyle = tablestyle[count % 2]
			if details[""status""] & RETROSPECT_FAIL:
				laststatusstyle = failstyle[count % 2]
				status = ""FAIL""
			elif status != ""STOPPED"" and details[""status""] & RETROSPECT_WARNING:
				laststatusstyle = warningstyle[count % 2]
				status = ""WARNINGS""
			elif status != ""FAIL"" and status != ""STOPPED"":
				status = ""OK""
				
			# Start a row	
			html.append('<tr style=""text-align:left;%s"">\n' % rowstyle)
			
			# Script name field
			if status == ""STOPPED"":
				html.append('\t<td style=""%s"">%s</td>\n' % (failstyle[count % 2], name))
			else:
				html.append('\t<td style=""%s"">%s</td>' % (tablestyle[count % 2], name))
			
				
			# Last status field
			if details[""lastRun""]:
				if status == ""STOPPED"":
					html.append('\t<td style=""%s""><a href=""#%s"">%s</a></td>\n' % (failstyle[count % 2], self.createAnchorID(name, details[""lastRun""]), status))
				else:
					html.append('\t<td style=""%s""><a href=""#%s"">%s</a></td>\n' % (laststatusstyle, self.createAnchorID(name, details[""lastRun""]), status))
			else:
				html.append('\t<td style=""%s"">%s</td>\n' % (laststatusstyle, status))
			
			# Last run field
			if details[""lastRun""]:
				html.append('\t<td style=""%s""><a href=""#%s"">%s</a></td>\n' % (laststatusstyle, self.createAnchorID(name, details[""lastRun""]), details[""lastRun""]))
			else:
				html.append('\t<td style=""%s"">none found</td>\n' % laststatusstyle)
			
			# Last success field
			if details[""lastSuccess""]:
				html.append('\t<td><a href=""#%s"">%s</a></td>\n' % (self.createAnchorID(name, details[""lastSuccess""]), details[""lastSuccess""]))
			else:
				html.append('\t<td>none found</td>\n')
			html.append('</tr>\n')
			count += 1
		html.append(""</table>"")
		return html",https://github.com/Kortemme-Lab/klab/blob/6d410ad08f1bd9f7cbbb28d7d946e94fbaaa2b6b/klab/retrospect.py#L411-L482
tfidf_python_100_1.0,reading element from html - <td>,python,"def timedelta_total_seconds(td):
    if sys.version_info >= (2, 7):
        return td.total_seconds()
    return td.days * 24 * 60 * 60 + td.seconds",https://github.com/mapmyfitness/jtime/blob/402fb6b40ac7a78c23fd02fac50c6dbe49e5ebfd/jtime/utils.py#L60-L63
tfidf_python_100_1.0,reading element from html - <td>,python,"def _repr_html_(self):
        html = '<table>'
        html += '<thead><tr><th>Field</th><th>Type</th><th>Roles</th><th>Continuity</th></tr></thead>'
        html += '<tbody>'
        for f in sorted(self):
            html += '<tr>'
            html += '<td>%s</td>' % f.name
            html += '<td>%s</td>' % f._repr_type_()
            html += '<td>%s</td>' % f._repr_role_()
            html += '<td>%s</td>' % ('Continuous' if f.continuity == FieldContinuity.CONTINUOUS else 'Discrete')
            html += '</tr>'
        html += '</tbody></table>'
        return html",https://github.com/aliyun/aliyun-odps-python-sdk/blob/4b0de18f5864386df6068f26f026e62f932c41e4/odps/ml/utils.py#L63-L75
tfidf_python_100_1.0,reading element from html - <td>,python,"def emit_tree(modules, fd, ctx, path):
    global levelcnt
    for module in modules:
        bstr = """"
        b = module.search_one('belongs-to')
        if b is not None:
            bstr = "" (belongs-to %s)"" % b.arg
        ns = module.search_one('namespace')
        if ns is not None:
            nsstr = ns.arg
        pr = module.search_one('prefix')
        if pr is not None:
            prstr = pr.arg
        else:
            prstr = """"

        temp_mod_arg = module.arg
        # html plugin specific changes
        if hasattr(ctx, 'html_plugin_user'):
           from pyang.plugins.html import force_link
           temp_mod_arg = force_link(ctx,module,module)

        levelcnt[1] += 1
        chs = [ch for ch in module.i_children
               if ch.keyword in statements.data_definition_keywords]
        if path is not None and len(path) > 0:
            chs = [ch for ch in chs if ch.arg == path[0]]
            path = path[1:]

        if len(chs) > 0:
            fd.write(""""""<tr id=""%s"" class=""a"">
                         <td id=""p1"">
                            <div id=""p2"" class=""tier1"">
                               <a href=""#"" id=""p3""
                                  onclick=""toggleRows(this);return false;""
                                  class=""folder"">&nbsp;
                               </a>
                               <font color=blue>%s</font>
                            </div>
                         </td> \n"""""" %(levelcnt[1], temp_mod_arg))
            fd.write(""""""<td>%s</td><td></td><td></td><td></td><td>
                        </td></tr>\n"""""" %module.keyword)
            #fd.write(""<td>module</td><td></td><td></td><td></td><td></td></tr>\n"")

            # print_children(chs, module, fd, '  ', path, 'data', depth, llen)
            print_children(chs, module, fd, ' ', path, ctx, 2)

        rpcs = module.search('rpc')
        if path is not None:
            if len(path) > 0:
                rpcs = [rpc for rpc in rpcs if rpc.arg == path[0]]
                path = path[1:]
            else:
                rpcs = []

        levelcnt[1] += 1
        if len(rpcs) > 0:
            fd.write(""""""<tr id=""%s"" class=""a"">
                         <td nowrap id=""p1000"">
                            <div id=""p2000"" class=""tier1"">
                               <a href=""#"" id=""p3000""
                                  onclick=""toggleRows(this);
                                  return false;"" class=""folder"">&nbsp;
                               </a>
                               %s:rpcs
                            </div>
                         </td> \n"""""" %(levelcnt[1],prstr))
            fd.write(""<td></td><td></td><td></td><td></td><td></td></tr>\n"")
            print_children(rpcs, module, fd, ' ', path, ctx, 2)

        notifs = module.search('notification')
        if path is not None:
            if len(path) > 0:
                notifs = [n for n in notifs if n.arg == path[0]]
                path = path[1:]
            else:
                notifs = []
        levelcnt[1] += 1
        if len(notifs) > 0:
            fd.write(""""""<tr id=""%s"" class=""a"">
                        <td nowrapid=""p4000"">
                           <div id=""p5000"" class=""tier1"">
                              <a href=""#"" id=""p6000""
                                 onclick=""toggleRows(this);return false;""
                                 class=""folder"">&nbsp;
                              </a>%s:notifs
                           </div>
                        </td> \n"""""" %(levelcnt[1],prstr))
            fd.write(""<td></td><td></td><td></td><td></td><td></td></tr>\n"")
            print_children(notifs, module, fd, ' ', path, ctx, 2)",https://github.com/mbj4668/pyang/blob/f2a5cc3142162e5b9ee4e18d154568d939ff63dd/pyang/plugins/jstree.py#L266-L355
tfidf_python_100_1.0,deducting the median from each column,python,"def calculate_median(given_list):
        """"""
        Returns the median of values in the given list.
        """"""
        median = None

        if not given_list:
            return median

        given_list = sorted(given_list)
        list_length = len(given_list)

        if list_length % 2:
            median = given_list[int(list_length / 2)]
        else:
            median = (given_list[int(list_length / 2)] + given_list[int(list_length / 2) - 1]) / 2.0

        return median",https://github.com/RIPE-NCC/ripe.atlas.sagan/blob/f0e57221cf0ba3504baddd3ea460fc955bc41cc6/ripe/atlas/sagan/base.py#L264-L281
tfidf_python_100_1.0,deducting the median from each column,python,"def median(data, freq_col=1):
    """"""Compute the median of the <freq_col>'th column of the values is <data>.

>>> chart_data.median([(10,20), (20,4), (30,5)], 0)
20
>>> chart_data.median([(10,20), (20,4), (30,5)], 1)
5.
    """"""

    nr_data = _nr_data(data, freq_col)
    median_idx = nr_data / 2
    i = 0
    for d in data:
        i += d[freq_col]
        if i >= median_idx:
            return d
    raise Exception(""??? median ???"")",https://github.com/ska-sa/purr/blob/4c848768d0485d0f88b30850d0d5372221b21b66/Purr/Plugins/local_pychart/chart_data.py#L319-L335
tfidf_python_100_1.0,deducting the median from each column,python,"def compute_lower_upper_errors(sample, num_sigma=1):
    """"""
    computes the upper and lower sigma from the median value.
    This functions gives good error estimates for skewed pdf's
    :param sample: 1-D sample
    :return: median, lower_sigma, upper_sigma
    """"""
    if num_sigma > 3:
        raise ValueError(""Number of sigma-constraints restircted to three. %s not valid"" % num_sigma)
    num = len(sample)
    num_threshold1 = int(round((num-1)*0.833))
    num_threshold2 = int(round((num-1)*0.977249868))
    num_threshold3 = int(round((num-1)*0.998650102))

    median = np.median(sample)
    sorted_sample = np.sort(sample)
    if num_sigma > 0:
        upper_sigma1 = sorted_sample[num_threshold1-1]
        lower_sigma1 = sorted_sample[num-num_threshold1-1]
    else:
        return median, [[]]
    if num_sigma > 1:
        upper_sigma2 = sorted_sample[num_threshold2-1]
        lower_sigma2 = sorted_sample[num-num_threshold2-1]
    else:
        return median, [[median-lower_sigma1, upper_sigma1-median]]
    if num_sigma > 2:
        upper_sigma3 = sorted_sample[num_threshold3-1]
        lower_sigma3 = sorted_sample[num-num_threshold3-1]
        return median, [[median-lower_sigma1, upper_sigma1-median], [median-lower_sigma2, upper_sigma2-median],
                      [median-lower_sigma3, upper_sigma3-median]]
    else:
        return median, [[median-lower_sigma1, upper_sigma1-median], [median-lower_sigma2, upper_sigma2-median]]",https://github.com/sibirrer/lenstronomy/blob/4edb100a4f3f4fdc4fac9b0032d2b0283d0aa1d6/lenstronomy/Util/prob_density.py#L137-L169
tfidf_python_100_1.0,deducting the median from each column,python,"def median(x):
    """"""
    Return a numpy array of column median.
    It does not affect if the array is one dimension

    Parameters
    ----------
    x : ndarray
        A numpy array instance

    Returns
    -------
    ndarray
        A 1 x n numpy array instance of column median

    Examples
    --------
    >>> a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
    >>> np.array_equal(median(a), [2, 5, 8])
    True
    >>> a = np.array([1, 2, 3])
    >>> np.array_equal(median(a), [1, 2, 3])
    True

    """"""
    if x.ndim > 1 and len(x[0]) > 1:
        return np.median(x, axis=1)
    return x",https://github.com/lambdalisue/maidenhair/blob/d5095c1087d1f4d71cc57410492151d2803a9f0d/src/maidenhair/statistics/__init__.py#L66-L93
tfidf_python_100_1.0,deducting the median from each column,python,"def median_abs_dev(values):
    # Median Absolute Deviation
    median = float(statistics.median(values))
    return statistics.median([abs(median - sample) for sample in values])",https://github.com/vstinner/perf/blob/cf096c0c0c955d0aa1c893847fa6393ba4922ada/perf/_utils.py#L481-L484
tfidf_python_100_1.0,deducting the median from each column,python,"def median(self, *args, **kwargs):
        return np.median(self.rri, *args, **kwargs)",https://github.com/rhenanbartels/hrv/blob/cd4c7e6e508299d943930886d20413f63845f60f/hrv/rri.py#L123-L124
tfidf_python_100_1.0,deducting the median from each column,python,"def mad(a):
    '''Calculate the median absolute deviation of a sample
    
    a - a numpy array-like collection of values
    
    returns the median of the deviation of a from its median.
    '''
    a = np.asfarray(a).flatten()
    return np.median(np.abs(a - np.median(a)))",https://github.com/CellProfiler/centrosome/blob/7bd9350a2d4ae1b215b81eabcecfe560bbb1f32a/centrosome/threshold.py#L520-L528
tfidf_python_100_1.0,deducting the median from each column,python,"def adjust_to_level(self, level, x, op, median):
        if x > median:
            if level > 0.5:
                result = median + (x - median) * ((level - 0.5) / 0.5)
            else:
                result = op + (median - op) * (level / 0.5)
        else:
            if level > 0.5:
                result = x + (median - x) * ((level - 0.5) / 0.5)
            else:
                result = median + (op - median) * (level / 0.5)
        return result",https://github.com/adaptive-learning/proso-apps/blob/8278c72e498d6ef8d392cc47b48473f4ec037142/proso/models/option_selection.py#L195-L206
tfidf_python_100_1.0,deducting the median from each column,python,"def complex_median(complex_list):
    """""" Get the median value of a list of complex numbers.

    Parameters
    ----------
    complex_list: list
        List of complex numbers to calculate the median.

    Returns
    -------
    a + 1.j*b: complex number
        The median of the real and imaginary parts.
    """"""
    median_real = numpy.median([complex_number.real
                     for complex_number in complex_list])
    median_imag = numpy.median([complex_number.imag
                     for complex_number in complex_list])
    return median_real + 1.j*median_imag",https://github.com/gwastro/pycbc/blob/7a64cdd104d263f1b6ea0b01e6841837d05a4cb3/pycbc/strain/lines.py#L22-L39
tfidf_python_100_1.0,deducting the median from each column,python,"def median(self):
        """"""Compute median of groups, excluding missing values.

        For multiple groupings, the result index will be a MultiIndex.
        """"""
        self._prep_pandas_groupby()
        return DataFrame.fromDataFrameRDD(
            self._regroup_mergedRDD().values().map(
                lambda x: x.median()), self.sql_ctx)",https://github.com/sparklingpandas/sparklingpandas/blob/7d549df4348c979042b683c355aa778fc6d3a768/sparklingpandas/groupby.py#L146-L154
tfidf_python_100_1.0,deducting the median from each column,python,"def fitness_vs(self):
        ""Median Fitness in the validation set""
        l = [x.fitness_vs for x in self.models]
        return np.median(l)",https://github.com/mgraffg/EvoDAG/blob/e11fa1fd1ca9e69cca92696c86661a3dc7b3a1d5/EvoDAG/model.py#L335-L338
tfidf_python_100_1.0,deducting the median from each column,python,"def stats(self):
        if len(self.bins) == 0:
            return None

        total = sum([self.bins[x] for x in self.bins])

        pc5 = None
        pc95 = None
        median = None
        sspace_sd = None
        i = 0

        for x in sorted(self.bins):
            i += self.bins[x]
            if i >= 0.05 * total and pc5 is None:
                pc5 = x + 0.5 * self.bin_width
            if i >= 0.5 * total and median is None:
                median = x + 0.5 * self.bin_width
            if i >= 0.95 * total and pc95 is None:
                pc95 = x + 0.5 + self.bin_width

        if None not in [pc5, pc95, median]:
            sspace_sd = round(max(pc95 - median, median - pc5) / median, 2)

        return (pc5, median, pc95, sspace_sd)",https://github.com/sanger-pathogens/ariba/blob/16a0b1916ce0e886bd22550ba2d648542977001b/ariba/histogram.py#L29-L53
tfidf_python_100_1.0,deducting the median from each column,python,"def median(self, *args, **kwargs):
        '''
        geo.median(axis=None, out=None, overwrite_input=False)

        axis : int, optional
            Axis along which the medians are computed. The default (axis=None)
            is to compute the median along a flattened version of the array.
        out : ndarray, optional
            Alternative output array in which to place the result. It must have
            the same shape and buffer length as the expected output, but the
            type (of the output) will be cast if necessary.
        overwrite_input : bool, optional
           If True, then allow use of memory of input array (a) for
           calculations. The input array will be modified by the call to
           median. This will save memory when you do not need to preserve the
           contents of the input array. Treat the input as undefined, but it
           will probably be fully or partially sorted. Default is False. Note
           that, if `overwrite_input` is True and the input is not already an
           ndarray, an error will be raised.
        '''
        return np.ma.median(self.raster, *args, **kwargs)",https://github.com/ozak/georasters/blob/0612bd91bb2a2cb2f1d59ba89c1ff131dae27d70/georasters/georasters.py#L593-L613
tfidf_python_100_1.0,deducting the median from each column,python,"def median(self, **kwargs):
        """"""
        Compute median of groups, excluding missing values.

        For multiple groupings, the result index will be a MultiIndex
        """"""
        try:
            return self._cython_agg_general('median', **kwargs)
        except GroupByError:
            raise
        except Exception:  # pragma: no cover

            def f(x):
                if isinstance(x, np.ndarray):
                    x = Series(x)
                return x.median(axis=self.axis, **kwargs)
            with _group_selection_context(self):
                return self._python_agg_general(f)",https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/groupby/groupby.py#L1159-L1176
tfidf_python_100_1.0,deducting the median from each column,python,"def median(self, default=None):
        """"""
        Calculate the median value over the time series.

        :param default: Value to return as a default should the calculation not be possible.
        :return: Float representing the median value or `None`.
        """"""
        return numpy.asscalar(numpy.median(self.values)) if self.values else default",https://github.com/linkedin/luminol/blob/42e4ab969b774ff98f902d064cb041556017f635/src/luminol/modules/time_series.py#L321-L328
tfidf_python_100_1.0,deducting the median from each column,python,"def _write_median_gradient(self)->None:
        ""Writes the median of the gradients to Tensorboard.""
        median_gradient = statistics.median(x.data.median() for x in self.gradients)
        self._add_gradient_scalar('median_gradient', scalar_value=median_gradient)",https://github.com/fastai/fastai/blob/9fb84a5cdefe5a766cdb792b8f5d8971737b7e67/fastai/callbacks/tensorboard.py#L330-L333
tfidf_python_100_1.0,deducting the median from each column,python,"def medianOnSortedList(N, key=lambda x:x):
    median = percentileOnSortedlist(N, percent=0.5, key=key)
    return median",https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/py2/h2o_summ.py#L78-L80
tfidf_python_100_1.0,deducting the median from each column,python,"def median(self, axis=None, **kwargs):
        return self._wrap_function(numpy.median, axis, **kwargs)",https://github.com/gwpy/gwpy/blob/7a92b917e7dd2d99b15895293a1fa1d66cdb210a/gwpy/types/array.py#L401-L402
tfidf_python_100_1.0,deducting the median from each column,python,"def median(self):
        d = torch.tensor(list(self.deque))
        return d.median().item()",https://github.com/pytorch/vision/blob/3afcf3cd49661c466c75ea536b0b2a7ff57f9a05/references/classification/utils.py#L44-L46
tfidf_python_100_1.0,deducting the median from each column,python,"def median(self):
        """""" -> #float :func:numpy.median of the timing intervals """"""
        return round(float(np.median(self.array)), self.precision)\
            if len(self.array) else None",https://github.com/jaredLunde/vital-tools/blob/ea924c9bbb6ec22aa66f8095f018b1ee0099ac04/vital/debug/__init__.py#L2174-L2177
tfidf_python_100_1.0,concatenate several file remove header lines,python,"def header_without_lines(header, remove):
    """"""Return :py:class:`Header` without lines given in ``remove``

    ``remove`` is an iterable of pairs ``key``/``ID`` with the VCF header key
    and ``ID`` of entry to remove.  In the case that a line does not have
    a ``mapping`` entry, you can give the full value to remove.

    .. code-block:: python

        # header is a vcfpy.Header, e.g., as read earlier from file
        new_header = vcfpy.without_header_lines(
            header, [('assembly', None), ('FILTER', 'PASS')])
        # now, the header lines starting with ""##assembly="" and the ""PASS""
        # filter line will be missing from new_header
    """"""
    remove = set(remove)
    # Copy over lines that are not removed
    lines = []
    for line in header.lines:
        if hasattr(line, ""mapping""):
            if (line.key, line.mapping.get(""ID"", None)) in remove:
                continue  # filter out
        else:
            if (line.key, line.value) in remove:
                continue  # filter out
        lines.append(line)
    return Header(lines, header.samples)",https://github.com/bihealth/vcfpy/blob/99e2165df30f11e0c95f3170f31bc5191d9e9e15/vcfpy/header.py#L227-L253
tfidf_python_100_1.0,concatenate several file remove header lines,python,"def paw_header(filename, ppdesc):
        """"""
        Parse the PAW abinit header. Examples:

        Paw atomic data for element Ni - Generated by AtomPAW (N. Holzwarth) + AtomPAW2Abinit v3.0.5
          28.000  18.000 20061204               : zatom,zion,pspdat
          7  7  2 0   350 0.                    : pspcod,pspxc,lmax,lloc,mmax,r2well
         paw3 1305                              : pspfmt,creatorID
          5 13                                  : basis_size,lmn_size
         0 0 1 1 2                              : orbitals
         3                                      : number_of_meshes
         1 3  350 1.1803778368E-05 3.5000000000E-02 : mesh 1, type,size,rad_step[,log_step]
         2 1  921 2.500000000000E-03                : mesh 2, type,size,rad_step[,log_step]
         3 3  391 1.1803778368E-05 3.5000000000E-02 : mesh 3, type,size,rad_step[,log_step]
          2.3000000000                          : r_cut(SPH)
         2 0.

        Another format:

        C  (US d-loc) - PAW data extracted from US-psp (D.Vanderbilt) - generated by USpp2Abinit v2.3.0
           6.000   4.000 20090106               : zatom,zion,pspdat
          7 11  1 0   560 0.                    : pspcod,pspxc,lmax,lloc,mmax,r2well
         paw4 2230                              : pspfmt,creatorID
          4  8                                  : basis_size,lmn_size
         0 0 1 1                                : orbitals
         5                                      : number_of_meshes
         1 2  560 1.5198032759E-04 1.6666666667E-02 : mesh 1, type,size,rad_step[,log_step]
         2 2  556 1.5198032759E-04 1.6666666667E-02 : mesh 2, type,size,rad_step[,log_step]
         3 2  576 1.5198032759E-04 1.6666666667E-02 : mesh 3, type,size,rad_step[,log_step]
         4 2  666 1.5198032759E-04 1.6666666667E-02 : mesh 4, type,size,rad_step[,log_step]
         5 2  673 1.5198032759E-04 1.6666666667E-02 : mesh 5, type,size,rad_step[,log_step]
          1.5550009124                          : r_cut(PAW)
         3 0.                                   : shape_type,rshape

        Yet nnother one:

        Paw atomic data for element Si - Generated by atompaw v3.0.1.3 & AtomPAW2Abinit v3.3.1
          14.000   4.000 20120814               : zatom,zion,pspdat
          7      11  1 0   663 0.               : pspcod,pspxc,lmax,lloc,mmax,r2well
         paw5 1331                              : pspfmt,creatorID
          4  8                                  : basis_size,lmn_size
         0 0 1 1                                : orbitals
         5                                      : number_of_meshes
         1 2  663 8.2129718540404674E-04 1.1498160595656655E-02 : mesh 1, type,size,rad_step[,log_step]
         2 2  658 8.2129718540404674E-04 1.1498160595656655E-02 : mesh 2, type,size,rad_step[,log_step]
         3 2  740 8.2129718540404674E-04 1.1498160595656655E-02 : mesh 3, type,size,rad_step[,log_step]
         4 2  819 8.2129718540404674E-04 1.1498160595656655E-02 : mesh 4, type,size,rad_step[,log_step]
         5 2  870 8.2129718540404674E-04 1.1498160595656655E-02 : mesh 5, type,size,rad_step[,log_step]
          1.5669671236                          : r_cut(PAW)
         2 0.                                   : shape_type,rshape
        """"""
        supported_formats = [""paw3"", ""paw4"", ""paw5""]
        if ppdesc.format not in supported_formats:
            raise NotImplementedError(""format %s not in %s"" % (ppdesc.format, supported_formats))

        lines = _read_nlines(filename, -1)

        summary = lines[0]
        header = _dict_from_lines(lines[:5], [0, 3, 6, 2, 2], sep="":"")

        lines = lines[5:]
        # TODO
        # Parse orbitals and number of meshes.
        header[""orbitals""] = [int(t) for t in lines[0].split("":"")[0].split()]
        header[""number_of_meshes""] = num_meshes = int(lines[1].split("":"")[0])
        #print filename, header

        # Skip meshes =
        lines = lines[2+num_meshes:]
        #for midx in range(num_meshes):
        #    l = midx + 1

        #print lines[0]
        header[""r_cut""] = float(lines[0].split("":"")[0])
        #print lines[1]
        header.update(_dict_from_lines(lines[1], [2], sep="":""))

        #print(""PAW header\n"", header)
        return PawAbinitHeader(summary, **header)",https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/io/abinit/pseudos.py#L896-L974
tfidf_python_100_1.0,concatenate several file remove header lines,python,"def rst_block(self, header, content_lines):
    self.lines(['.. ' + header, ''])
    self.lines(['   %s' % line for line in content_lines])
    self.lines([''])",https://github.com/rix0rrr/gcl/blob/4e3bccc978a9c60aaaffd20f6f291c4d23775cdf/gcl/doc.py#L23-L26
tfidf_python_100_1.0,concatenate several file remove header lines,python,"def oncvpsp_header(filename, ppdesc):
        """"""
        Parse the ONCVPSP abinit header. Example:

        Li    ONCVPSP  r_core=  2.01  3.02
              3.0000      3.0000      140504    zatom,zion,pspd
             8     2     1     4   600     0    pspcod,pspxc,lmax,lloc,mmax,r2well
          5.99000000  0.00000000  0.00000000    rchrg fchrg qchrg
             2     2     0     0     0    nproj
             0                 extension_switch
           0                        -2.5000025868368D+00 -1.2006906995331D+00
             1  0.0000000000000D+00  0.0000000000000D+00  0.0000000000000D+00
             2  1.0000000000000D-02  4.4140499497377D-02  1.9909081701712D-02
        """"""
        lines = _read_nlines(filename, 6)

        header = _dict_from_lines(lines[:3], [0, 3, 6])
        summary = lines[0]

        # Replace pspd with pspdata
        header.update({'pspdat': header['pspd']})
        header.pop('pspd')

        # Read extension switch
        header[""extension_switch""] = int(lines[5].split()[0])

        return NcAbinitHeader(summary, **header)",https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/io/abinit/pseudos.py#L760-L786
tfidf_python_100_1.0,concatenate several file remove header lines,python,"def remove_programmer(programmer_id):
    """"""remove programmer.

    :param programmer_id: programmer id (e.g. 'avrisp')
    :rtype: None

    """"""

    log.debug('remove %s', programmer_id)
    lines = programmers_txt().lines()
    lines = filter(
        lambda x: not x.strip().startswith(programmer_id + '.'), lines)
    programmers_txt().write_lines(lines)",https://github.com/ponty/confduino/blob/f4c261e5e84997f145a8bdd001f471db74c9054b/confduino/progremove.py#L9-L21
tfidf_python_100_1.0,concatenate several file remove header lines,python,"def samplesheet(self):
        """"""
        Create a custom sample sheet based on the original sample sheet for the run, but only including the samples
        that did not pass the quality threshold on the previous iteration
        """"""
        if self.demultiplex:
            make_path(self.samplesheetpath)
            self.customsamplesheet = os.path.join(self.samplesheetpath, 'SampleSheet.csv')
            header = ['Sample_ID', 'Sample_Name', 'Sample_Plate', 'Sample_Well', 'I7_Index_ID', 'index', 'I5_Index_ID',
                      'index2', 'Sample_Project', 'Description']
            with open(self.customsamplesheet, 'w') as samplesheet:
                lines = str()
                lines += '[Header]\n'
                lines += 'IEMFileVersion,{}\n'.format(self.header.IEMFileVersion)
                lines += 'Investigator Name,{}\n'.format(self.header.InvestigatorName)
                lines += 'Experiment Name,{}\n'.format(self.header.ExperimentName)
                lines += 'Date,{}\n'.format(self.header.Date)
                lines += 'Workflow,{}\n'.format(self.header.Workflow)
                lines += 'Application,{}\n'.format(self.header.Application)
                lines += 'Assay,{}\n'.format(self.header.Assay)
                lines += 'Description,{}\n'.format(self.header.Description)
                lines += 'Chemistry,{}\n'.format(self.header.Chemistry)
                lines += '\n'
                lines += '[Reads]\n'
                lines += str(self.forward) + '\n'
                lines += str(self.reverse) + '\n'
                lines += '\n'
                lines += '[Settings]\n'
                lines += 'ReverseComplement,{}\n'.format(self.header.ReverseComplement)
                lines += 'Adapter,{}\n'.format(self.header.Adapter)
                lines += '\n'
                lines += '[Data]\n'
                lines += ','.join(header)
                lines += '\n'
                # Correlate all the samples added to the list of incomplete samples with their metadata
                for incomplete in self.incomplete:
                    for sample in self.rundata:
                        if incomplete == sample['SampleID']:
                            # Use each entry in the header list as a key for the rundata dictionary
                            for data in header:
                                # Modify the key to be consistent with how the dictionary was populated
                                result = sample[data.replace('_', '')]
                                # Description is the final entry in the list, and shouldn't have a , following the value
                                if data != 'Description':
                                    lines += '{},'.format(result.replace('NA', ''))
                                # This entry should have a newline instead of a ,
                                else:
                                    lines += '{}\n'.format(result.replace('NA', ''))
                # Write the string to the sample sheet
                samplesheet.write(lines)",https://github.com/lowandrew/OLCTools/blob/88aa90ac85f84d0bbeb03e43c29b0a9d36e4ce2a/sipprCommon/create_sample_sheet.py#L10-L59
tfidf_python_100_1.0,concatenate several file remove header lines,python,"def remove_board(board_id):
    """"""remove board.

    :param board_id: board id (e.g. 'diecimila')
    :rtype: None

    """"""

    log.debug('remove %s', board_id)
    lines = boards_txt().lines()
    lines = filter(lambda x: not x.strip().startswith(board_id + '.'), lines)
    boards_txt().write_lines(lines)",https://github.com/ponty/confduino/blob/f4c261e5e84997f145a8bdd001f471db74c9054b/confduino/boardremove.py#L9-L20
tfidf_python_100_1.0,concatenate several file remove header lines,python,"def parse_block(lines, header=False):  # type: (List[str], bool) -> List[str]
    """"""Parse and return a single block, popping off the start of `lines`.

    If parsing a header block, we stop after we reach a line that is not a
    comment. Otherwise, we stop after reaching an empty line.

    :param lines: list of lines
    :param header: whether we are parsing a header block
    :return: list of lines that form the single block
    """"""
    block_lines = []
    while lines and lines[0] and (not header or lines[0].startswith('#')):
        block_lines.append(lines.pop(0))
    return block_lines",https://github.com/pre-commit/pre-commit-hooks/blob/6d7906e131976a1ce14ab583badb734727c5da3e/pre_commit_hooks/sort_simple_yaml.py#L50-L63
tfidf_python_100_1.0,concatenate several file remove header lines,python,"def fhi_header(filename, ppdesc):
        """"""
        Parse the FHI abinit header. Example:

        Troullier-Martins psp for element  Sc        Thu Oct 27 17:33:22 EDT 1994
         21.00000   3.00000    940714                zatom, zion, pspdat
           1    1    2    0      2001    .00000      pspcod,pspxc,lmax,lloc,mmax,r2well
        1.80626423934776     .22824404341771    1.17378968127746   rchrg,fchrg,qchrg
        """"""
        lines = _read_nlines(filename, 4)

        try:
            header = _dict_from_lines(lines[:4], [0, 3, 6, 3])
        except ValueError:
            # The last record with rchrg ... seems to be optional.
            header = _dict_from_lines(lines[:3], [0, 3, 6])

        summary = lines[0]

        return NcAbinitHeader(summary, **header)",https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/io/abinit/pseudos.py#L702-L721
tfidf_python_100_1.0,concatenate several file remove header lines,python,"def hgh_header(filename, ppdesc):
        """"""
        Parse the HGH abinit header. Example:

        Hartwigsen-Goedecker-Hutter psp for Ne,  from PRB58, 3641 (1998)
           10   8  010605 zatom,zion,pspdat
         3 1   1 0 2001 0  pspcod,pspxc,lmax,lloc,mmax,r2well
        """"""
        lines = _read_nlines(filename, 3)

        header = _dict_from_lines(lines[:3], [0, 3, 6])
        summary = lines[0]

        return NcAbinitHeader(summary, **header)",https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/io/abinit/pseudos.py#L724-L737
tfidf_python_100_1.0,concatenate several file remove header lines,python,"def parse_partlist(str):
    '''parse partlist text delivered by eagle.

    header is converted to lowercase

    :param str: input string
    :rtype: tuple of header list and dict list: (['part','value',..], [{'part':'C1', 'value':'1n'}, ..])
    '''
    lines = str.strip().splitlines()
    lines = filter(len, lines)
    hind = header_index(lines)
    if hind is None:
        log.debug('empty partlist found')
        return ([], [])

    header_line = lines[hind]
    header = header_line.split('  ')
    header = filter(len, header)
    positions = [header_line.index(x) for x in header]
    header = [x.strip().split()[0].lower() for x in header]

    data_lines = lines[hind + 1:]

    def parse_data_line(line):
        y = [(h, line[pos1:pos2].strip()) for h, pos1, pos2 in zip(
            header, positions, positions[1:] + [1000])]
        return dict(y)

    data = [parse_data_line(x) for x in data_lines]
    return (header, data)",https://github.com/ponty/eagexp/blob/1dd5108c1d8112cc87d1bda64fa6c2784ccf0ff2/eagexp/partlist.py#L40-L69
tfidf_python_100_1.0,concatenate several file remove header lines,python,"def __init__(self, concat_operator, concatenate=True):
        self.__concatenate = concatenate
        self.children = []
        self.concat_operator = concat_operator",https://github.com/quantmind/dynts/blob/21ac57c648bfec402fa6b1fe569496cf098fb5e8/dynts/dsl/ast/base.py#L165-L168
tfidf_python_100_1.0,concatenate several file remove header lines,python,"def __str__(self):
        """"""Return complete SSDP request.""""""
        lines = list()
        lines.append(' '.join(
            [self.method, self.uri, self.version]
        ))
        for header in self.headers:
            lines.append('%s: %s' % header)
        return '\n'.join(lines)",https://github.com/codingjoe/ssdp/blob/84ff667c792608b221aa726cfd106b554884063d/ssdp/__init__.py#L120-L128
tfidf_python_100_1.0,concatenate several file remove header lines,python,"def format(self, report):
        lines = []
        self._append_heading(lines, 1, report.header.title)
        self._append_info_section(lines, report.header)
        for section in report:
            self._append_heading(lines, 2, section.header.title)
            self._append_info_section(lines, section.header)
            for cond in section:
                self._append_heading(lines, 3, cond.header.title)
                self._append_info_section(lines, cond.header)
                self._append_violations(lines, cond.body)
        return '\n'.join(lines)",https://github.com/materialsproject/pymatgen-db/blob/02e4351c2cea431407644f49193e8bf43ed39b9a/matgendb/vv/report.py#L315-L326
tfidf_python_100_1.0,concatenate several file remove header lines,python,"def __str__(self):
        """"""Return complete SSDP response.""""""
        lines = list()
        lines.append(' '.join(
            [self.version, str(self.status_code), self.reason]
        ))
        for header in self.headers:
            lines.append('%s: %s' % header)
        return '\n'.join(lines)",https://github.com/codingjoe/ssdp/blob/84ff667c792608b221aa726cfd106b554884063d/ssdp/__init__.py#L78-L86
tfidf_python_100_1.0,concatenate several file remove header lines,python,"def __init__(self, snv_file, snv_enum_type):
        self._header_to_column_mapping = {}
        self._path_to_snv_file = snv_file
        self._snv_list = []
        self._snv_enum = snv_enum_type

        with open(snv_file, ""r"", encoding=""utf-8"") as fh:
            lines = fh.readlines()
        if not lines:
            raise MTBParserException(""Parsing failed: File was empty!"")
        header = lines[0]
        self._parse_header(header)
        self._parse_content(lines)",https://github.com/qbicsoftware/mtb-parser-lib/blob/e8b96e34b27e457ea7def2927fe44017fa173ba7/mtbparser/snv_parser.py#L12-L24
tfidf_python_100_1.0,concatenate several file remove header lines,python,"def split_header(fp):
        """"""
        Read file pointer and return pair of lines lists:
        first - header, second - the rest.
        """"""
        body_start, header_ended = 0, False
        lines = []
        for line in fp:
            if line.startswith('#') and not header_ended:
                # Header text
                body_start += 1
            else:
                header_ended = True
            lines.append(line)
        return lines[:body_start], lines[body_start:]",https://github.com/peterdemin/pip-compile-multi/blob/7bd1968c424dd7ce3236885b4b3e4e28523e6915/pipcompilemulti/environment.py#L181-L195
tfidf_python_100_1.0,concatenate several file remove header lines,python,"def __str__(self):
        header = 'â' * 78
        lines = [header]
        lines.append('{0:<35s} {1}'.format(self._key, self._value))
        lines.append(header)
        for item in list(self.keys()):
            if type(self[item]) == _sp.ndarray:
                lines.append('{0:<35s} {1}'.format(item, _sp.shape(self[item])))
            else:
                lines.append('{0:<35s} {1}'.format(item, self[item]))
        lines.append(header)
        return '\n'.join(lines)",https://github.com/PMEAL/OpenPNM/blob/0547b5724ffedc0a593aae48639d36fe10e0baed/openpnm/utils/misc.py#L72-L83
tfidf_python_100_1.0,concatenate several file remove header lines,python,"def remove(self, line):
        old_len = len(self.lines)
        self.lines = [l for l in self.lines if not l.match(line)]
        return old_len - len(self.lines)",https://github.com/rbarrois/confutils/blob/26bbb3f31c09a99ee2104263a9e97d6d3fc8e4f4/confutils/configfile.py#L116-L119
tfidf_python_100_1.0,concatenate several file remove header lines,python,"def _append_basic_row(lines, padded_cells, colwidths, colaligns, rowfmt):
    lines.append(_build_row(padded_cells, colwidths, colaligns, rowfmt))
    return lines",https://github.com/raphaelvallat/pingouin/blob/58b19fa4fffbfe09d58b456e3926a148249e4d9b/pingouin/external/tabulate.py#L1080-L1082
tfidf_python_100_1.0,parse query string in url,python,"def parse(args, data):
    url = urlparse(data)
    query = url.query
    if not args.no_query_params:
        query = parse_qsl(url.query)
    return url, query",https://github.com/jdp/urp/blob/778c16d9a5eae75316ce20aad742af7122be558c/urp.py#L17-L22
tfidf_python_100_1.0,parse query string in url,python,"def parse_query_param(url, param):
    """"""Parses the query string of a URL and returns the value of a parameter.

    Args:
        url: A URL.
        param: A string representing the name of the parameter.

    Returns:
        The value of the parameter.
    """"""

    try:
        return parse.parse_qs(parse.urlparse(url).query)[param][0]
    except:
        return None",https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L285-L299
tfidf_python_100_1.0,parse query string in url,python,"def strip_url_params3(url, strip=None):
    if not strip: strip = []
    
    parse = urllib.parse.urlparse(url)
    query = urllib.parse.parse_qs(parse.query)
    
    query = {k: v[0] for k, v in query.items() if k not in strip}
    query = urllib.parse.urlencode(query)
    new = parse._replace(query=query)
    
    return new.geturl()",https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/strings/strip_url_params.py#L85-L95
tfidf_python_100_1.0,parse query string in url,python,"def fb_user_search(self, query):
        url = 'fbsearch/topsearch/?context=blended&query={query}&rank_token={rank_token}'
        url = url.format(query=query, rank_token=self.rank_token)
        return self.send_request(url)",https://github.com/instagrambot/instabot/blob/d734f892ac4cc35d22746a4f2680425ffaff0927/instabot/api/api.py#L817-L820
tfidf_python_100_1.0,parse query string in url,python,"def search_users(self, query):
        url = 'users/search/?ig_sig_key_version={sig_key}&is_typeahead=true&query={query}&rank_token={rank_token}'
        url = url.format(
            sig_key=config.SIG_KEY_VERSION,
            query=query,
            rank_token=self.rank_token
        )
        return self.send_request(url)",https://github.com/instagrambot/instabot/blob/d734f892ac4cc35d22746a4f2680425ffaff0927/instabot/api/api.py#L822-L829
tfidf_python_100_1.0,parse query string in url,python,"def search_tags(self, query):
        url = 'tags/search/?is_typeahead=true&q={query}&rank_token={rank_token}'
        url = url.format(query=query, rank_token=self.rank_token)
        return self.send_request(url)",https://github.com/instagrambot/instabot/blob/d734f892ac4cc35d22746a4f2680425ffaff0927/instabot/api/api.py#L835-L838
tfidf_python_100_1.0,parse query string in url,python,"def query(self, query=None):
        """"""
        If query is given, modify the URL correspondingly, return the current
        query otherwise.
        """"""
        if query is None:
            return self.url.query
        self.url.query = query",https://github.com/karan/TPB/blob/f424a73a10d4bcf4e363d7e7e8cb915a3a057671/tpb/tpb.py#L213-L220
tfidf_python_100_1.0,parse query string in url,python,"def search_users(self, query):

        if not isinstance(query, (str, unicode)):
            raise ValueError('""query"" must be a string')
        url = self.base_url + ""/search?query="" + query

        return self._get_request(url)",https://github.com/onenameio/onename-python/blob/74c583282f18ad9582c6b57b826126d045321494/onename/client.py#L61-L67
tfidf_python_100_1.0,parse query string in url,python,"def get_blackouts(self, query=None):
        r = self.http.get('/blackouts', query)
        return [Blackout.parse(b) for b in r['blackouts']]",https://github.com/alerta/python-alerta-client/blob/7eb367b5fe87d5fc20b54dea8cddd7f09e251afa/alertaclient/api.py#L146-L148
tfidf_python_100_1.0,parse query string in url,python,"def match_query(string, query):
    '''Test if a string matches a query.

    Parameters
    ----------
    string : str
        The string to test

    query : string, callable, or object
        Either a regular expression, callable function, or object.

    Returns
    -------
    match : bool
        `True` if:
        - `query` is a callable and `query(string) == True`
        - `query` is a regular expression and `re.match(query, string)`
        - or `string == query` for any other query

        `False` otherwise

    '''

    if six.callable(query):
        return query(string)

    elif (isinstance(query, six.string_types) and
          isinstance(string, six.string_types)):
        return re.match(query, string) is not None

    else:
        return query == string",https://github.com/marl/jams/blob/b16778399b9528efbd71434842a079f7691a7a66/jams/core.py#L2047-L2078
tfidf_python_100_1.0,parse query string in url,python,"def parse_code(url):
    """"""
    Parse the code parameter from the a URL

    :param str url: URL to parse
    :return: code query parameter
    :rtype: str
    """"""
    result = urlparse(url)
    query = parse_qs(result.query)
    return query['code']",https://github.com/jingming/spotify/blob/d92c71073b2515f3c850604114133a7d2022d1a4/spotify/auth/util.py#L6-L16
tfidf_python_100_1.0,parse query string in url,python,"def append_query(self, url):
        if not self.query:
            return url

        if '?' not in url:
            url += '?'
        else:
            url += '&'

        url += self.query
        return url",https://github.com/webrecorder/pywb/blob/77f8bb647639dd66f6b92b7a9174c28810e4b1d9/pywb/warcserver/inputrequest.py#L276-L286
tfidf_python_100_1.0,parse query string in url,python,"def extract_url_parameters(url):
        url_parameters = {}
        query = None
        parsedurl = urlparse(url)

        # Parsing the querystrings if available
        if parsedurl.query:
            query = normalize_query_string(parsedurl.query)

        url = parsedurl.path
        if URL_PARAMETER_PATTERN.search(url):
            for k, v in URL_PARAMETER_PATTERN.findall(url):
                url_parameters[k] = v
                url = re.sub(
                    rf'{k}:\s?{URL_PARAMETER_VALUE_PATTERN}', rf':{k}',
                    url
                )

        return url, url_parameters if url_parameters else None, query",https://github.com/Carrene/bddrest/blob/53dfb76ea90998bbb124f7470607c81cfcdd6579/bddrest/specification/call.py#L86-L104
tfidf_python_100_1.0,parse query string in url,python,"def urlQueryParser(url, querydict):
    """"""
    parse a url query
    """"""
    address_parse = urlparse(url)
    return urlunparse(address_parse._replace(query=urlencode(querydict)))",https://github.com/johntruckenbrodt/spatialist/blob/007f49296a156de8d7168ad235b5a5b8e8d3633d/spatialist/ancillary.py#L585-L590
tfidf_python_100_1.0,parse query string in url,python,"def query(self, value=None):
        """"""
        Return or set the query string

        :param string value: the new query string to use
        :returns: string or new :class:`URL` instance
        """"""
        if value is not None:
            return URL._mutate(self, query=value)
        return self._tuple.query",https://github.com/codeinthehole/purl/blob/e70ed132f1fdc17d00c78199cedb1e3adcb2bc55/purl/url.py#L320-L329
tfidf_python_100_1.0,parse query string in url,python,"def _build_k8s_url(self, url, _prepend_namespace=True, **query):
        if _prepend_namespace:
            url = ""namespaces/%s/%s"" % (self.namespace, url)
        if query:
            url += (""?"" + urlencode(query))
        return urljoin(self.k8s_api_url, url)",https://github.com/projectatomic/osbs-client/blob/571fe035dab3a7c02e1dccd5d65ffd75be750458/osbs/core.py#L130-L135
tfidf_python_100_1.0,parse query string in url,python,"def replace_url_query_values(url, replace_vals):
    """"""Replace querystring values in a url string.

    >>> url = 'http://helloworld.com/some/path?test=5'
    >>> replace_vals = {'test': 10}
    >>> replace_url_query_values(url=url, replace_vals=replace_vals)
    'http://helloworld.com/some/path?test=10'
    """"""
    if '?' not in url:
        return url

    parsed_url = urlparse(url)
    query = dict(parse_qsl(parsed_url.query))
    query.update(replace_vals)
    return '{0}?{1}'.format(url.split('?')[0], urlencode(query))",https://github.com/InfoAgeTech/django-core/blob/9664a145473b75120bf71e1644e9c8086e7e8955/django_core/utils/urls.py#L82-L96
tfidf_python_100_1.0,parse query string in url,python,"def url_concat(url, args, keep_existing=True):
    """"""Concatenate url and argument dictionary

    >>> url_concat(""http://example.com/foo?a=b"", dict(c=""d""))
    'http://example.com/foo?a=b&c=d'

    :arg string url: URL being concat to.
    :arg dict args: Args being concat.
    :arg bool keep_existing: (Optional) Whether to keep the args which are
                            alreay in url, default is ``True``.
    """"""
    if not args:
        return url

    if keep_existing:
        if url[-1] not in ('?', '&'):
            url += '&' if ('?' in url) else '?'
        return url + urlencode(args, 1)
    else:
        url, seq, query = url.partition('?')
        query = urlparse.parse_qs(query, True)
        query.update(args)
        return url + '?' + urlencode(query, 1)",https://github.com/ifduyue/urlfetch/blob/e0ea4673367c157eb832ba4ba2635306c81a61be/urlfetch.py#L932-L954
tfidf_python_100_1.0,parse query string in url,python,"def getQueryParams(url):
    """"""Get URL query parameters.""""""
    query = urlsplit(url)[3]
    out.debug(u'Extracting query parameters from %r (%r)...' % (url, query))
    return cgi.parse_qs(query)",https://github.com/wummel/dosage/blob/a0109c3a46219f280e6e5e77183674e40da0f304/dosagelib/util.py#L371-L375
tfidf_python_100_1.0,parse query string in url,python,"def _build_url(self, url, _prepend_namespace=True, **query):
        if _prepend_namespace:
            url = ""namespaces/%s/%s"" % (self.namespace, url)
        if query:
            url += (""?"" + urlencode(query))
        return urljoin(self.os_api_url, url)",https://github.com/projectatomic/osbs-client/blob/571fe035dab3a7c02e1dccd5d65ffd75be750458/osbs/core.py#L137-L142
tfidf_python_100_1.0,fuzzy match ranking,python,"def __rank(ranking, n: int):
        return nlargest(n, ranking, key=ranking.get)",https://github.com/PyThaiNLP/pythainlp/blob/e9a300b8a99dfd1a67a955e7c06f62e4afe0fbca/pythainlp/summarize/freq.py#L23-L24
tfidf_python_100_1.0,fuzzy match ranking,python,"def _rank(self, ranking, n):
    """""" return the first n sentences with highest ranking """"""
    return nlargest(n, ranking, key=ranking.get)",https://github.com/TheSighing/climber/blob/39e4e70c9a768c82a995d8704679d1c046910666/climber/summary.py#L56-L58
tfidf_python_100_1.0,fuzzy match ranking,python,"async def get_final_ranking(self) -> OrderedDict:
        """""" Get the ordered players ranking

        Returns:
            collections.OrderedDict[rank, List[Participant]]:

        Raises:
            APIException

        """"""
        if self._state != TournamentState.complete.value:
            return None

        ranking = {}
        for p in self.participants:
            if p.final_rank in ranking:
                ranking[p.final_rank].append(p)
            else:
                ranking[p.final_rank] = [p]

        return OrderedDict(sorted(ranking.items(), key=lambda t: t[0]))",https://github.com/fp12/achallonge/blob/25780b3c48b66400a50ff9f884e4287afd4c89e4/challonge/tournament.py#L763-L783
tfidf_python_100_1.0,fuzzy match ranking,python,"def Dictionarize(rankings, m):
    rankcnt = {}
    #print(""1"",rankings[1])
    for ranking in rankings:
        #print(""2"",ranking)
        flag = 0
        l = len(ranking)
        if len(set(ranking)) < l:
            print(""Orders with duplicate alternatives are ignored!"")
            continue
        for i in range(l):
            if ranking[i] >= m or ranking[i] < 0:
                flag = 1
        if flag == 1:
            print(""Alternative index out of range! Ranking ignored!"")
            continue
        key = rank2str(ranking)
        if key in rankcnt:
            rankcnt[key] += 1
        else:
            rankcnt[key] = 1
    return rankcnt",https://github.com/PrefPy/prefpy/blob/f395ba3782f05684fa5de0cece387a6da9391d02/prefpy/egmm_mixpl.py#L69-L90
tfidf_python_100_1.0,fuzzy match ranking,python,"def rank2str(ranking):
    s = str(ranking[0])
    for alt in ranking[1:]:
        s += '-'+str(alt)
    return s",https://github.com/PrefPy/prefpy/blob/f395ba3782f05684fa5de0cece387a6da9391d02/prefpy/egmm_mixpl.py#L63-L67
tfidf_python_100_1.0,fuzzy match ranking,python,"def find_peaks(series_data, sliding_window_size=20, percentage_of_sliding_window_votes=0.7):

    ranking = np.zeros(len(series_data))

    for i in range(len(series_data)):
        window_data = series_data[i:i+sliding_window_size]

        ranking[i + np.argmax(window_data)] += 1

    ranking = ranking > percentage_of_sliding_window_votes * sliding_window_size

    peaks = np.where(ranking >= 1)[0].tolist()
    return peaks",https://github.com/nicktgr15/sac/blob/4b1d5d8e6ca2c437972db34ddc72990860865159/sac/methods/peak_detection.py#L4-L16
tfidf_python_100_1.0,fuzzy match ranking,python,"def __init__(self, method: InteractiveMethod, ranking) -> None:
        super().__init__(method)
        self.pref_input = ranking",https://github.com/industrial-optimization-group/DESDEO/blob/c7aebe8adb20942d200b9a411d4cdec21f5f4bff/desdeo/preference/direct.py#L28-L30
tfidf_python_100_1.0,fuzzy match ranking,python,"def get_table_format_matches(self, _, word_before_cursor):
        return self.find_matches(
            word_before_cursor,
            self.table_formats,
            start_only=True,
            fuzzy=False
        )",https://github.com/dbcli/athenacli/blob/bcab59e4953145866430083e902ed4d042d4ebba/athenacli/completer.py#L308-L314
tfidf_python_100_1.0,fuzzy match ranking,python,"def parse_timestamp(self, field):

        return parse(field,
                     fuzzy=self.fuzzy,
                     dayfirst=self.dayfirst,
                     yearfirst=self.yearfirst)",https://github.com/dedupeio/datetime-distance/blob/ad1baf2fa58634871b3a2b8b2eb40b6981ddcb20/datetime_distance/__init__.py#L40-L45
tfidf_python_100_1.0,fuzzy match ranking,python,"def combine_ranked_stations(rankings):
    """""" Combine :any:`pandas.DataFrame` s of candidate weather stations to form
    a hybrid ranking dataframe.

    Parameters
    ----------
    rankings : list of :any:`pandas.DataFrame`
        Dataframes of ranked weather station candidates and metadata.
        All ranking dataframes should have the same columns and must be
        sorted by rank.

    Returns
    -------
    ranked_filtered_candidates : :any:`pandas.DataFrame`
        Dataframe has a rank column and the same columns given in the source
        dataframes.
    """"""

    if len(rankings) == 0:
        raise ValueError(""Requires at least one ranking."")

    combined_ranking = rankings[0]
    for ranking in rankings[1:]:
        filtered_ranking = ranking[~ranking.index.isin(combined_ranking.index)]
        combined_ranking = pd.concat([combined_ranking, filtered_ranking])

    combined_ranking[""rank""] = range(1, 1 + len(combined_ranking))
    return combined_ranking",https://github.com/openeemeter/eeweather/blob/d32b7369b26edfa3ee431c60457afeb0593123a7/eeweather/ranking.py#L323-L350
tfidf_python_100_1.0,fuzzy match ranking,python,"def _remove_last(votes, fpl, cl, ranking):
    """"""Remove last candidate in IRV voting.
    """"""
    for v in votes:
        for r in v:
            if r == fpl[-1]:
                v.remove(r)
    for c in cl:
        if c == fpl[-1]:
            if c not in ranking:
                ranking.append((c, len(ranking) + 1))",https://github.com/assamite/creamas/blob/54dc3e31c97a3f938e58272f8ab80b6bcafeff58/creamas/vote.py#L512-L522
tfidf_python_100_1.0,fuzzy match ranking,python,"def get_special_matches(self, _, word_before_cursor):
        return self.find_matches(
            word_before_cursor,
            self.special_commands,
            start_only=True,
            fuzzy=True
        )",https://github.com/dbcli/athenacli/blob/bcab59e4953145866430083e902ed4d042d4ebba/athenacli/completer.py#L300-L306
tfidf_python_100_1.0,fuzzy match ranking,python,"def prob_pl(gamma, ranking):
    m = len(gamma)
    ranking = ranking.split('-')
    l = len(ranking)
    s = 0
    for i in range(l):
        s += gamma[int(ranking[i])]
    p = 1
    for i in range(l-1):
        alt = int(ranking[i])
        p *= gamma[alt]/s
        s -= gamma[alt]
    return p",https://github.com/PrefPy/prefpy/blob/f395ba3782f05684fa5de0cece387a6da9391d02/prefpy/egmm_mixpl.py#L102-L114
tfidf_python_100_1.0,fuzzy match ranking,python,"def get_alternatives(self, rank):
        """"""
        Description:
            Returns the alternative(s) with the given ranking in the
            computed aggregate ranking.  An error is thrown if the
            ranking does not exist.  
        """"""
        if self.ranks_to_alts is None:
            raise ValueError(""Aggregate ranking must be created first"")
        try:
            alts = self.ranks_to_alts[rank]
            return alts
        except KeyError:
            raise KeyError(""No ranking \""{}\"" found in "".format(str(rank)) +
                           ""the aggregate ranking"")",https://github.com/PrefPy/prefpy/blob/f395ba3782f05684fa5de0cece387a6da9391d02/prefpy/aggregate.py#L55-L69
tfidf_python_100_1.0,fuzzy match ranking,python,"def from_dict(cls, data, ranking=None):
        pontos = data['pontos'][ranking] if ranking and ranking in data['pontos'] else None
        return cls(data['time_id'], data['nome'], data['nome_cartola'], data['slug'], data['assinante'], pontos)",https://github.com/vicenteneto/python-cartolafc/blob/15b2a192d7745f454d69a55ac9b7ef7c7abb53b9/cartolafc/models.py#L211-L213
tfidf_python_100_1.0,fuzzy match ranking,python,"def from_rankings(cls, data, penalty):
        """"""Alternative constructor for ranking data.""""""
        top1 = list()
        for ranking in data:
            for i, winner in enumerate(ranking[:-1]):
                top1.append((winner, ranking[i+1:]))
        return cls(top1, penalty)",https://github.com/lucasmaystre/choix/blob/05a57a10bb707338113a9d91601ca528ead7a881/choix/opt.py#L69-L75
tfidf_python_100_1.0,fuzzy match ranking,python,"def _sort_by_ranking(hypotheses: Dict[str, Any], ranking: List[int]) -> Dict[str, Any]:
        def ranksort(l):
            return [l[i] for i in ranking]

        return {key: ranksort(value) for key, value in hypotheses.items()}",https://github.com/awslabs/sockeye/blob/5d64a1ee1ef3cbba17c6d1d94bc061020c43f6ab/sockeye/rerank.py#L70-L74
tfidf_python_100_1.0,fuzzy match ranking,python,"def perform_near_match(graph, ranking):
    # Walk ranking table in reverse order and add near-match edges to graph
    reverse_topological_sorted_vertices = reversed(list(topological_sort(graph.graph)))
    for v in reverse_topological_sorted_vertices:
        ##### Doesn't work:
        #         target_rank = ranking.byVertex[v] # get the rank of a vertex
        #
        # in_edges = graph.in_edges(v) # if it has more than one in_edge, perhaps something before it can be moved
        # if len(in_edges) > 1:
        #     # candidates for movement are the sources of in edges more than one rank earlir
        #     move_candidates = [in_edge[0] for in_edge in in_edges \
        #                        if target_rank > ranking.byVertex[in_edge[0]] + 1]
        #     for move_candidate in move_candidates:
        #         move_candidate_witnesses = set(move_candidate.tokens) # prepare to get intersection later
        #         min_rank = ranking.byVertex[move_candidate] # lowest possible rank is current position
        #         max_rank = target_rank - 1 # highest possible rank is one more before the target
        #         vertices_to_compare = flatten([ranking.byRank[r] for r in range(min_rank, max_rank + 1)])
        #         vertices_to_compare.remove(move_candidate) # don't compare it to itself
        #         print('comparing ', move_candidate, ' to ', vertices_to_compare)
        #         ratio_dict = {} # ratio:vertex_to_compare
        #         for vertex_to_compare in vertices_to_compare:
        #             # don't move if there's already a vertex there with any of the same witnesses
        #             if not move_candidate_witnesses.intersection(vertex_to_compare.tokens):
        #                 print('now comparing move candidate ', move_candidate, \
        #                       ' (witnesses ', move_candidate_witnesses,\
        #                       ') with ', vertex_to_compare, ' (witnesses ', vertex_to_compare.tokens, ')')
        #                 ratio = Levenshtein.ratio(str(move_candidate), str(vertex_to_compare))
        #                 ratio_dict[ratio] = vertex_to_compare
        #         # Create only winning edge; losing edges can create later cycles
        #         graph.connect_near(ratio_dict[max(ratio_dict)], move_candidate, ratio)
        #         print('connected ', move_candidate, ' to ', ratio_dict[max(ratio_dict)], \
        #               ' with ratio ', max(ratio_dict))
        ######
        in_edges = graph.in_edges(v, data=True)
        for source, target, edgedata in in_edges:
            # can only move if two conditions are both true:
            # 1) rank of source differs from v by more than 1; max target rank will be rank of v - 1
            # 2) out_edges from source must have no target at exactly one rank higher than source
            if ranking.byVertex[v] - ranking.byVertex[source] > 1 and \
                    1 not in [ranking.byVertex[v] - ranking.byVertex[u] for (u,v) in graph.out_edges(source)]:
                min_rank = ranking.byVertex[source]
                max_rank = ranking.byVertex[v]
                match_candidates = [item for item in flatten([ranking.byRank[rank] \
                                            for rank in range(min_rank, max_rank)]) if item is not source]
                # print(match_candidates)
                levenshtein_dict = defaultdict(list)
                for match_candidate in match_candidates:
                    ratio = Levenshtein.ratio(str(source), str(match_candidate))
                    # print(source, match_candidate, ratio)
                    levenshtein_dict[ratio].append(match_candidate)
                weight = max(levenshtein_dict)
                winner = levenshtein_dict[max(levenshtein_dict)][0]
                # print('weight:',weight,'source:',winner)
                graph.connect_near(winner,source,weight)
                # print('before: byRank',str(ranking.byRank))
                # print('before: byVertex',str(ranking.byVertex))
                # update ranking table for next pass through loop and verify
                ranking = VariantGraphRanking.of(graph)
                # print('after: byRank',str(ranking.byRank))
                # print('after: byVertex',str(ranking.byVertex))
    # Create new ranking table (passed along to creation of alignment table)
    return VariantGraphRanking.of(graph)",https://github.com/interedition/collatex/blob/76dd1fcc36047bc66a87d31142e72e98b5347821/collatex-pythonport/collatex/near_matching.py#L29-L90
tfidf_python_100_1.0,fuzzy match ranking,python,"def best_item_from_list(item,options,fuzzy=90,fname_match=True,fuzzy_fragment=None,guess=False):
    '''Returns just the best item, or ``None``'''
    match = best_match_from_list(item,options,fuzzy,fname_match,fuzzy_fragment,guess)
    if match:
        return match[0]
    return None",https://github.com/azraq27/gini/blob/3c2b5265d096d606b303bfe25ac9adb74b8cee14/gini/matching.py#L63-L68
tfidf_python_100_1.0,fuzzy match ranking,python,"def _remove_zeros(votes, fpl, cl, ranking):
    """"""Remove zeros in IRV voting.
    """"""
    for v in votes:
        for r in v:
            if r not in fpl:
                v.remove(r)
    for c in cl:
        if c not in fpl:
            if c not in ranking:
                ranking.append((c, 0))",https://github.com/assamite/creamas/blob/54dc3e31c97a3f938e58272f8ab80b6bcafeff58/creamas/vote.py#L499-L509
tfidf_python_100_1.0,output to html file,python,"def store_output(self, contentitem, output):
        # Strip all output from HTML tags while collecting
        # (the output is already cached at this point)
        output.html = get_cleaned_string(output.html)
        output.cacheable = False
        super(SearchResultTracker, self).store_output(contentitem, output)",https://github.com/django-fluent/django-fluent-contents/blob/896f14add58471b98d7aa295b2c9e6abedec9003/fluent_contents/rendering/search.py#L10-L15
tfidf_python_100_1.0,output to html file,python,"def plain(html):
	
	try: html = str(html)
	except:
		pass
	
	if html == ""None"": html = """"
	html = strip_javascript(html)
	html = strip_inline_css(html)
	html = strip_comments(html)
	html = strip_forms(html)
	html = strip_tags(html, columns="""")
	html = replace_entities(html)
	html = collapse_tabs(html)
	html = collapse_spaces(html)
	html = collapse_linebreaks(html)	
	
	return html",https://github.com/shoebot/shoebot/blob/d554c1765c1899fa25727c9fc6805d221585562b/lib/web/html.py#L228-L245
tfidf_python_100_1.0,output to html file,python,"def html_to_rgb(html):
  """"""Convert the HTML color to (r, g, b).

  Parameters:
    :html:
      the HTML definition of the color (#RRGGBB or #RGB or a color name).

  Returns:
    The color as an (r, g, b) tuple in the range:
    r[0...1],
    g[0...1],
    b[0...1]

  Throws:
    :ValueError:
      If html is neither a known color name or a hexadecimal RGB
      representation.

  >>> '(%g, %g, %g)' % html_to_rgb('#ff8000')
  '(1, 0.501961, 0)'
  >>> '(%g, %g, %g)' % html_to_rgb('ff8000')
  '(1, 0.501961, 0)'
  >>> '(%g, %g, %g)' % html_to_rgb('#f60')
  '(1, 0.4, 0)'
  >>> '(%g, %g, %g)' % html_to_rgb('f60')
  '(1, 0.4, 0)'
  >>> '(%g, %g, %g)' % html_to_rgb('lemonchiffon')
  '(1, 0.980392, 0.803922)'

  """"""
  html = html.strip().lower()
  if html[0]=='#':
    html = html[1:]
  elif html in NAMED_COLOR:
    html = NAMED_COLOR[html][1:]

  if len(html)==6:
    rgb = html[:2], html[2:4], html[4:]
  elif len(html)==3:
    rgb = ['%c%c' % (v,v) for v in html]
  else:
    raise ValueError(""input #%s is not in #RRGGBB format"" % html)

  return tuple(((int(n, 16) / 255.0) for n in rgb))",https://github.com/xav/Grapefruit/blob/b3d88375be727a3a1ec5839fbc462e0e8e0836e4/grapefruit.py#L869-L912
tfidf_python_100_1.0,output to html file,python,"def clean_attributes(html):
    while htmlstrip.search(html):
        html = htmlstrip.sub('<\\1\\2>', html)
    return html",https://github.com/usc-isi-i2/etk/blob/aab077c984ea20f5e8ae33af622fe11d3c4df866/etk/extractors/readability/cleaners.py#L17-L20
tfidf_python_100_1.0,output to html file,python,"def HtmlToRgb(html):
    '''Convert the HTML color to (r, g, b).

    Parameters:
      :html:
        the HTML definition of the color (#RRGGBB or #RGB or a color name).

    Returns:
      The color as an (r, g, b) tuple in the range:
      r[0...1],
      g[0...1],
      b[0...1]

    Throws:
      :ValueError:
        If html is neither a known color name or a hexadecimal RGB
        representation.

    >>> '(%g, %g, %g)' % Color.HtmlToRgb('#ff8000')
    '(1, 0.501961, 0)'
    >>> '(%g, %g, %g)' % Color.HtmlToRgb('ff8000')
    '(1, 0.501961, 0)'
    >>> '(%g, %g, %g)' % Color.HtmlToRgb('#f60')
    '(1, 0.4, 0)'
    >>> '(%g, %g, %g)' % Color.HtmlToRgb('f60')
    '(1, 0.4, 0)'
    >>> '(%g, %g, %g)' % Color.HtmlToRgb('lemonchiffon')
    '(1, 0.980392, 0.803922)'

    '''
    html = html.strip().lower()
    if html[0]=='#':
      html = html[1:]
    elif html in Color.NAMED_COLOR:
      html = Color.NAMED_COLOR[html][1:]

    if len(html)==6:
      rgb = html[:2], html[2:4], html[4:]
    elif len(html)==3:
      rgb = ['%c%c' % (v,v) for v in html]
    else:
      raise ValueError('input #%s is not in #RRGGBB format' % html)

    return tuple(((int(n, 16) / 255.0) for n in rgb))",https://github.com/jsvine/spectra/blob/2269a0ae9b5923154b15bd661fb81179608f7ec2/spectra/grapefruit.py#L955-L998
tfidf_python_100_1.0,output to html file,python,"def convert_html_to_plain_text(html):
    if not html:
        return html

    if six.PY2:
        html = html.decode('utf-8')

    html = decode_html_entities(html)
    # Replace HTML break rules with new lines
    html = html.replace('<br>', '\n')
    # Remove multiple spaces
    html = re.sub(' +', ' ', html)

    return html",https://github.com/markfinger/django-node/blob/a2f56bf027fd3c4cbc6a0213881922a50acae1d6/django_node/utils.py#L196-L209
tfidf_python_100_1.0,output to html file,python,"def __init__(self, html):
        if isinstance(html, (str, bytes)):
            self.html = fromstring(html)

        elif isinstance(html, HtmlWrapper):
            self.html = html.html

        elif isinstance(html, HtmlElement):
            self.html = html

        elif isinstance(html, BS4_TYPES):
            self.html = fromstring(str(html))

        else:
            msg = ""Object of type %s not compatible with HtmlWrapper"" % str(type(html))

            raise TypeError(msg)",https://github.com/thismachinechills/html_wrapper/blob/bef8c93f99bdbb4646d96845fed4e2ddf9213947/html_wrapper/wrapper.py#L17-L33
tfidf_python_100_1.0,output to html file,python,"def html(self):
        """"""Gives an html representation of the assessment.""""""
        output = self.html_preamble
        output += self._repr_html_()
        output += self.html_post
        return output",https://github.com/sods/ods/blob/3995c659f25a0a640f6009ed7fcc2559ce659b1d/pods/assesser.py#L152-L157
tfidf_python_100_1.0,output to html file,python,"def get_start_widget(appbase, jupbase, notebase):
    html = template.format(appbase=appbase, jupbase=jupbase, notebase=notebase)
    return ipw.HTML(html)",https://github.com/aiidalab/aiidalab-widgets-base/blob/291a9b159eac902aee655862322670ec1b0cd5b1/start.py#L26-L28
tfidf_python_100_1.0,output to html file,python,"def report(self):
        """""" Present all information that was gathered in an html file that
        allows browsing the results.
        """"""
        # make this prettier
        html = u'<hr/><a name=""%s""></a>\n' % self.name()

        # Intro
        html = html + ""<h2> Plugin <em>"" + self.name() + ""</em></h2>\n""

        # Files
        if len(self.copied_files):
            html = html + ""<p>Files copied:<br><ul>\n""
            for afile in self.copied_files:
                html = html + '<li><a href=""%s"">%s</a>' % \
                    (u'..' + _to_u(afile['dstpath']), _to_u(afile['srcpath']))
                if afile['symlink'] == ""yes"":
                    html = html + "" (symlink to %s)"" % _to_u(afile['pointsto'])
                html = html + '</li>\n'
            html = html + ""</ul></p>\n""

        # Command Output
        if len(self.executed_commands):
            html = html + ""<p>Commands Executed:<br><ul>\n""
            # convert file name to relative path from our root
            # don't use relpath - these are HTML paths not OS paths.
            for cmd in self.executed_commands:
                if cmd[""file""] and len(cmd[""file""]):
                    cmd_rel_path = u""../"" + _to_u(self.commons['cmddir']) \
                        + ""/"" + _to_u(cmd['file'])
                    html = html + '<li><a href=""%s"">%s</a></li>\n' % \
                        (cmd_rel_path, _to_u(cmd['exe']))
                else:
                    html = html + '<li>%s</li>\n' % (_to_u(cmd['exe']))
            html = html + ""</ul></p>\n""

        # Alerts
        if len(self.alerts):
            html = html + ""<p>Alerts:<br><ul>\n""
            for alert in self.alerts:
                html = html + '<li>%s</li>\n' % _to_u(alert)
            html = html + ""</ul></p>\n""

        # Custom Text
        if self.custom_text != """":
            html = html + ""<p>Additional Information:<br>\n""
            html = html + _to_u(self.custom_text) + ""</p>\n""

        if six.PY2:
            return html.encode('utf8')
        else:
            return html",https://github.com/sosreport/sos/blob/2ebc04da53dc871c8dd5243567afa4f8592dca29/sos/plugins/__init__.py#L1323-L1374
tfidf_python_100_1.0,output to html file,python,"def html_to_text(html):
    html = html.replace('\n', ' ')
    html = ' '.join(html.split())
    s = HTMLTextExtractor()
    s.feed(html)
    return s.get_text()",https://github.com/asciimoo/searx/blob/a84caa22cf947e973c10aa968d35fb2bdda6d048/searx/utils.py#L136-L141
tfidf_python_100_1.0,output to html file,python,"def _render(self):
        # See if we're not excluded
        if self.field.name in self.exclude.replace("" "", """").split("",""):
            return """"
        # Hidden input requires no special treatment
        if self.field.is_hidden:
            return text_value(self.field)
        # Render the widget
        self.add_widget_attrs()
        html = self.field.as_widget(attrs=self.widget.attrs)
        self.restore_widget_attrs()
        # Start post render
        html = self.post_widget_render(html)
        html = self.wrap_widget(html)
        html = self.make_input_group(html)
        html = self.append_to_field(html)
        html = self.wrap_field(html)
        html = self.add_label(html)
        html = self.wrap_label_and_field(html)
        return html",https://github.com/dyve/django-bootstrap3/blob/1d4095ba113a1faff228f9592bdad4f0b3aed653/bootstrap3/renderers.py#L559-L578
tfidf_python_100_1.0,output to html file,python,"def _attach_body(self):
        if self.body and self.html:
            self.attach_alternative(self.html, 'text/html')
        elif self.html:
            self.body = self.html
            self.content_subtype = 'html'",https://github.com/sunscrapers/django-templated-mail/blob/65e5a34f69d50d4d1b6acdd337f9efcc44b32fae/templated_mail/mail.py#L98-L103
tfidf_python_100_1.0,output to html file,python,"def html_etree(self):
        html_etree = None
        if self.html:
            html_etree = etree.HTML(self.html)
        return html_etree",https://github.com/howie6879/ruia/blob/2dc5262fc9c3e902a8faa7d5fa2f046f9d9ee1fa/ruia/response.py#L117-L121
tfidf_python_100_1.0,output to html file,python,"def render(self, context):
        html = TRACKING_CODE % {'tracker_id': self.tracker_id}
        if is_internal_ip(context, 'CLICKMAP'):
            html = disable_html(html, 'Clickmap')
        return html",https://github.com/jazzband/django-analytical/blob/5487fd677bd47bc63fc2cf39597a0adc5d6c9ab3/analytical/templatetags/clickmap.py#L52-L56
tfidf_python_100_1.0,output to html file,python,"def render(self, context):
        html = TRACKING_CODE % {'portal_id': self.portal_id}
        if is_internal_ip(context, 'HUBSPOT'):
            html = disable_html(html, 'HubSpot')
        return html",https://github.com/jazzband/django-analytical/blob/5487fd677bd47bc63fc2cf39597a0adc5d6c9ab3/analytical/templatetags/hubspot.py#L50-L54
tfidf_python_100_1.0,output to html file,python,"def html(self, html: str) -> None:
        self._html = html.encode(self.encoding)",https://github.com/kennethreitz/requests-html/blob/b59a9f2fb9333d7d467154a0fd82978efdb9d23b/requests_html.py#L110-L111
tfidf_python_100_1.0,output to html file,python,"def prepare(self, html):
        # Clean up faulty HTML before parsing.
        html = html.replace(""<br/>"", ""<br />"")
        html = html.replace(""<hr/>"", ""<hr />"")
        # Display list items with an asterisk.
        #html = html.replace(""li>"", ""li>*"")
        html = re.sub(r""<li.*?>"", ""\n<li>* "", html)
        #html = html.replace(""li>\n"", ""li>"")
        # Make sure there is a space between elements.
        html = html.replace(""><"", ""> <"")
        # Linebreaks in the source should not end up in the output.
        if not self.linebreaks:
        	html = html.replace(""\r"", ""\n"")
        	html = html.replace(""\n"", "" "")
        return html",https://github.com/shoebot/shoebot/blob/d554c1765c1899fa25727c9fc6805d221585562b/lib/web/html.py#L109-L123
tfidf_python_100_1.0,output to html file,python,"def normalize_entities(html):
    # turn &nbsp; and aliases into normal spaces
    html = html.replace(u'&nbsp;', u' ')
    html = html.replace(u'&#160;', u' ')
    html = html.replace(u'&#xA0;', u' ')
    html = html.replace(u'\xa0', u' ')
    return html",https://github.com/christian-oudard/htmltreediff/blob/0e28f56492ae7e69bb0f74f9a79a8909a5ad588d/htmltreediff/util.py#L208-L214
tfidf_python_100_1.0,output to html file,python,"def __init__(self, disabled_template=None, error_template=None):
        self.disabled_template = (
            disabled_template or 'elasticutils/501.html')
        self.error_template = (
            error_template or 'elasticutils/503.html')",https://github.com/mozilla/elasticutils/blob/b880cc5d51fb1079b0581255ec664c1ec934656e/elasticutils/contrib/django/__init__.py#L98-L102
tfidf_python_100_1.0,how to read .csv file in an efficient way?,python,"def CreateAllStaticRAPIDFiles(in_drainage_line,
                              river_id,
                              length_id,
                              slope_id,
                              next_down_id,
                              rapid_output_folder,
                              kfac_celerity=1000.0/3600.0,
                              kfac_formula_type=3,
                              kfac_length_units=""km"",
                              lambda_k=0.35,
                              x_value=0.3,
                              nhdplus=False,
                              taudem_network_connectivity_tree_file=None,
                              file_geodatabase=None):
    """"""
    To generate the static RAPID files (rapid_connect.csv, riv_bas_id.csv,
    kfac.csv, k.csv, x.csv, comid_lat_lon_z.csv) with default values.

    Parameters
    ----------
    in_drainage_line: str
        Path to the stream network (i.e. Drainage Line) shapefile.
    river_id: str
        The name of the field with the river ID
        (Ex. 'HydroID', 'COMID', or 'LINKNO').
    length_id: str
        The field name containging the length of the river segment
        (Ex. 'LENGTHKM' or 'Length').
    slope_id: str
        The field name containging the slope of the river segment
        (Ex. 'Avg_Slope' or 'Slope').
    next_down_id: str
        The name of the field with the river ID of the next downstream river
        segment (Ex. 'NextDownID' or 'DSLINKNO').
    rapid_output_folder: str
        The path to the folder where all of the RAPID output will be generated.
    kfac_celerity: float, optional
        The flow wave celerity for the watershed in meters per second.
        1 km/hr or 1000.0/3600.0 m/s is a reasonable value if unknown.
    kfac_formula_type: int, optional
        An integer representing the formula type to use when calculating kfac.
        Default is 3.
    kfac_length_units: str, optional
        The units for the length_id field. Supported types are ""m"" for meters
        and ""km"" for kilometers. Default is ""km"".
    lambda_k: float, optional
        The value for lambda given from RAPID after the calibration process.
        Default is 0.35.
    x_value: float, optional
        Value for the muskingum X parameter [0-0.5]. Default is 0.3.
    nhdplus: bool, optional
        If True, the drainage line is from the NHDPlus dataset with the VAA
        fields COMID, FROMNODE, TONODE, and DIVERGENCE. Default is False.
    taudem_network_connectivity_tree_file: str, optional
        If set, the connectivity file will be generated from the TauDEM
        connectivity tree file.
    file_geodatabase: str, optional
        Path to the file geodatabase. If you use this option,
        in_drainage_line is the name of the stream network feature class.
        (WARNING: Not always stable with GDAL.)


    Example::

        from RAPIDpy.gis.workflow import CreateAllStaticRAPIDFiles

        CreateAllStaticRAPIDFiles(
            in_drainage_line=""/path/to/drainage_line.shp"",
            river_id=""HydroID"",
            length_id=""LENGTHKM"",
            slope_id=""SLOPE"",
            next_down_river_id=""NextDownID"",
            rapid_output_folder=""/path/to/rapid/output"",
        )
    """"""
    # RAPID connect file
    rapid_connect_file = os.path.join(rapid_output_folder, 'rapid_connect.csv')
    if nhdplus:
        CreateNetworkConnectivityNHDPlus(in_drainage_line,
                                         rapid_connect_file,
                                         file_geodatabase)
    elif taudem_network_connectivity_tree_file:
        CreateNetworkConnectivityTauDEMTree(
            taudem_network_connectivity_tree_file,
            rapid_connect_file)
    else:
        CreateNetworkConnectivity(in_drainage_line,
                                  river_id,
                                  next_down_id,
                                  rapid_connect_file,
                                  file_geodatabase)

    # river basin id file
    riv_bas_id_file = os.path.join(rapid_output_folder, 'riv_bas_id.csv')
    CreateSubsetFile(in_drainage_line,
                     river_id,
                     riv_bas_id_file,
                     file_geodatabase)
    # kfac file
    kfac_file = os.path.join(rapid_output_folder, 'kfac.csv')
    CreateMuskingumKfacFile(in_drainage_line,
                            river_id,
                            length_id,
                            slope_id,
                            kfac_celerity,
                            kfac_formula_type,
                            rapid_connect_file,
                            kfac_file,
                            length_units=kfac_length_units,
                            file_geodatabase=file_geodatabase)
    # k file
    k_file = os.path.join(rapid_output_folder, 'k.csv')
    CreateMuskingumKFile(lambda_k,
                         kfac_file,
                         k_file)
    # x file
    x_file = os.path.join(rapid_output_folder, 'x.csv')
    CreateConstMuskingumXFile(x_value,
                              rapid_connect_file,
                              x_file)
    # comid lat lon z file
    comid_lat_lon_z_file = \
        os.path.join(rapid_output_folder, 'comid_lat_lon_z.csv')
    FlowlineToPoint(in_drainage_line,
                    river_id,
                    comid_lat_lon_z_file,
                    file_geodatabase)",https://github.com/erdc/RAPIDpy/blob/50e14e130554b254a00ff23b226cd7e4c6cfe91a/RAPIDpy/gis/workflow.py#L22-L148
tfidf_python_100_1.0,how to read .csv file in an efficient way?,python,"def append_csv(self, csv):
        csv = CSV(csv, self)
#        if self.get_csv(csv.id):
#            raise Exception('csv already exists: {0}'.format(csv.id))
#        csv.get_import_table()
        self['csvs'].append(csv)
        return self",https://github.com/adamjaso/pyauto/blob/b11da69fb21a49241f5ad75dac48d9d369c6279b/csvdb/pyauto/csvdb/config.py#L31-L37
tfidf_python_100_1.0,how to read .csv file in an efficient way?,python,"def get_csv(self, csv):
        for c in self.csvs:
            if csv == c.get_id():
                return c
        raise Exception('unknown csv: {0}'.format(csv))",https://github.com/adamjaso/pyauto/blob/b11da69fb21a49241f5ad75dac48d9d369c6279b/csvdb/pyauto/csvdb/config.py#L51-L55
tfidf_python_100_1.0,how to read .csv file in an efficient way?,python,"def get_important_sub_metrics_csv(self):
    csv = os.path.join(self.resource_directory, self.label + '.important_sub_metrics.csv')
    return csv",https://github.com/linkedin/naarad/blob/261e2c0760fd6a6b0ee59064180bd8e3674311fe/src/naarad/metrics/metric.py#L162-L164
tfidf_python_100_1.0,how to read .csv file in an efficient way?,python,"def get_sla_csv(self):
    csv = os.path.join(self.resource_directory, self.label + '.sla.csv')
    return csv",https://github.com/linkedin/naarad/blob/261e2c0760fd6a6b0ee59064180bd8e3674311fe/src/naarad/metrics/metric.py#L174-L176
tfidf_python_100_1.0,how to read .csv file in an efficient way?,python,"def __openAndInitCSVFile(self, modelInfo):
    """"""
    - Backs up old report csv file;
    - opens the report csv file in append or overwrite mode (per
      self.__replaceReport);
    - emits column fields;
    - sets up self.__sortedVariableNames, self.__csvFileObj,
      self.__backupCSVPath, and self.__reportCSVPath

    Parameters:
    ----------------------------------------------------------------------
    modelInfo:      First _NupicModelInfo instance passed to emit()
    retval:         nothing
    """"""
    # Get the base path and figure out the path of the report file.
    basePath = self.__outputDirAbsPath

    # Form the name of the output csv file that will contain all the results
    reportCSVName = ""%s_Report.csv"" % (self.__outputLabel,)
    reportCSVPath = self.__reportCSVPath = os.path.join(basePath, reportCSVName)

    # If a report CSV file already exists, back it up
    backupCSVPath = None
    if os.path.exists(reportCSVPath):
      backupCSVPath = self.__backupCSVPath = _backupFile(reportCSVPath)


    # Open report file
    if self.__replaceReport:
      mode = ""w""
    else:
      mode = ""a""
    csv = self.__csvFileObj = open(reportCSVPath, mode)

    # If we are appending, add some blank line separators
    if not self.__replaceReport and backupCSVPath:
      print >> csv
      print >> csv

    # Print the column names
    print >> csv, ""jobID, "",
    print >> csv, ""modelID, "",
    print >> csv, ""status, "" ,
    print >> csv, ""completionReason, "",
    print >> csv, ""startTime, "",
    print >> csv, ""endTime, "",
    print >> csv, ""runtime(s), "" ,
    print >> csv, ""expDesc, "",
    print >> csv, ""numRecords, "",

    for key in self.__sortedVariableNames:
      print >> csv, ""%s, "" % key,
    for key in self.__sortedMetricsKeys:
      print >> csv, ""%s, "" % key,
    print >> csv",https://github.com/numenta/nupic/blob/5922fafffdccc8812e72b3324965ad2f7d4bbdad/src/nupic/swarming/permutations_runner.py#L1293-L1347
tfidf_python_100_1.0,how to read .csv file in an efficient way?,python,"def cloud_cover_to_irradiance(self, cloud_cover, how='clearsky_scaling',
                                  **kwargs):
        """"""
        Convert cloud cover to irradiance. A wrapper method.

        Parameters
        ----------
        cloud_cover : Series
        how : str, default 'clearsky_scaling'
            Selects the method for conversion. Can be one of
            clearsky_scaling or liujordan.
        **kwargs
            Passed to the selected method.

        Returns
        -------
        irradiance : DataFrame
            Columns include ghi, dni, dhi
        """"""

        how = how.lower()
        if how == 'clearsky_scaling':
            irrads = self.cloud_cover_to_irradiance_clearsky_scaling(
                cloud_cover, **kwargs)
        elif how == 'liujordan':
            irrads = self.cloud_cover_to_irradiance_liujordan(
                cloud_cover, **kwargs)
        else:
            raise ValueError('invalid how argument')

        return irrads",https://github.com/pvlib/pvlib-python/blob/2e844a595b820b43d1170269781fa66bd0ccc8a3/pvlib/forecast.py#L539-L569
tfidf_python_100_1.0,how to read .csv file in an efficient way?,python,"def __init__(self, fldr):
        """"""
        loads all the ref_*.csv files relating 
        to character traits
        """"""
        self.ref_folder = fldr
        #print('loading data files')
        self.races = RefFile(fldr, 'ref_races.csv')
        self.classes = RefFile(fldr, 'ref_classes.csv')
        self.stats = RefFile(fldr, 'ref_stats.csv')
        self.skills = RefFile(fldr, 'ref_skills.csv')
        self.stories = RefFile(fldr, 'ref_stories.csv')
        self.inventory = RefFile(fldr, 'ref_objects.csv')",https://github.com/acutesoftware/virtual-AI-simulator/blob/57de679a5b1a58c38fefe6aea58af1f3a7e79c58/vais/character.py#L70-L82
tfidf_python_100_1.0,how to read .csv file in an efficient way?,python,"def way_is_polygon(way):
    return (way.nds[-1] == next(iter(way.nds))) and any([t.key in polygon_way_tags and t.value in polygon_way_tags[t.key] for t in way.tags])",https://github.com/iandees/pyosm/blob/532dffceae91e2bce89c530ceff627bc8210f8aa/pyosm/shapeify.py#L11-L12
tfidf_python_100_1.0,how to read .csv file in an efficient way?,python,"def get_stats_csv(self):
    csv = os.path.join(self.resource_directory, self.label + '.stats.csv')
    return csv",https://github.com/linkedin/naarad/blob/261e2c0760fd6a6b0ee59064180bd8e3674311fe/src/naarad/metrics/metric.py#L166-L168
tfidf_python_100_1.0,how to read .csv file in an efficient way?,python,"def execute_arbitrary_series_groupby(op, data, _, aggcontext=None, **kwargs):
    how = op.how
    if how is None:
        how = 'first'

    if how not in {'first', 'last'}:
        raise com.OperationNotDefinedError(
            'Arbitrary {!r} is not supported'.format(how)
        )
    return aggcontext.agg(data, how)",https://github.com/ibis-project/ibis/blob/1e39a5fd9ef088b45c155e8a5f541767ee8ef2e7/ibis/pandas/execution/generic.py#L451-L460
tfidf_python_100_1.0,how to read .csv file in an efficient way?,python,"def validate_dataset_file(value):
    if not value.name.endswith('.csv'):
        raise ValidationError(u'Please upload a CSV (comma separated values)\
            file.')
    # idk make this better
    try:
        # file_sample = value.file.open(1024)
        csvreader = csv.reader(value.file)
        for row in csvreader:
            if csvreader.line_num >= 25:
                break
            row
        # do sth lol
    except csv.Error:
        raise ValidationError('Failed to parse CSV file.')",https://github.com/DallasMorningNews/django-datafreezer/blob/982dcf2015c80a280f1a093e32977cb71d4ea7aa/datafreezer/validators.py#L11-L25
tfidf_python_100_1.0,how to read .csv file in an efficient way?,python,"def test_csv():
    csv_filename = get_abs_filename_with_sub_path('csv', 'test_csv.csv')[1]
    print(csv_filename)
    csv_list = csv_file_to_list(csv_filename)
    print(csv_list)",https://github.com/chinapnr/fishbase/blob/23c5147a6bc0d8ed36409e55352ffb2c5b0edc82/demo/demo_csv.py#L7-L11
tfidf_python_100_1.0,how to read .csv file in an efficient way?,python,"def time_i8merge(self, how):
        merge(self.left, self.right, how=how)",https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/asv_bench/benchmarks/join_merge.py#L213-L214
tfidf_python_100_1.0,how to read .csv file in an efficient way?,python,"def emit(self, modelInfo):
    """"""Emit model info to csv file

    Parameters:
    ----------------------------------------------------------------------
    modelInfo:      _NupicModelInfo instance
    retval:         nothing
    """"""
    # Open/init csv file, if needed
    if self.__csvFileObj is None:
      # sets up self.__sortedVariableNames and self.__csvFileObj
      self.__openAndInitCSVFile(modelInfo)

    csv = self.__csvFileObj

    # Emit model info row to report.csv
    print >> csv, ""%s, "" % (self.__searchJobID),
    print >> csv, ""%s, "" % (modelInfo.getModelID()),
    print >> csv, ""%s, "" % (modelInfo.statusAsString()),
    if modelInfo.isFinished():
      print >> csv, ""%s, "" % (modelInfo.getCompletionReason()),
    else:
      print >> csv, ""NA, "",
    if not modelInfo.isWaitingToStart():
      print >> csv, ""%s, "" % (modelInfo.getStartTime()),
    else:
      print >> csv, ""NA, "",
    if modelInfo.isFinished():
      dateFormat = ""%Y-%m-%d %H:%M:%S""
      startTime = modelInfo.getStartTime()
      endTime = modelInfo.getEndTime()
      print >> csv, ""%s, "" % endTime,
      st = datetime.strptime(startTime, dateFormat)
      et = datetime.strptime(endTime, dateFormat)
      print >> csv, ""%s, "" % (str((et - st).seconds)),
    else:
      print >> csv, ""NA, "",
      print >> csv, ""NA, "",
    print >> csv, ""%s, "" % str(modelInfo.getModelDescription()),
    print >> csv, ""%s, "" % str(modelInfo.getNumRecords()),
    paramLabelsDict = modelInfo.getParamLabels()
    for key in self.__sortedVariableNames:
      # Some values are complex structures,.. which need to be represented as
      # strings
      if key in paramLabelsDict:
        print >> csv, ""%s, "" % (paramLabelsDict[key]),
      else:
        print >> csv, ""None, "",
    metrics = modelInfo.getReportMetrics()
    for key in self.__sortedMetricsKeys:
      value = metrics.get(key, ""NA"")
      value = str(value)
      value = value.replace(""\n"", "" "")
      print >> csv, ""%s, "" % (value),

    print >> csv",https://github.com/numenta/nupic/blob/5922fafffdccc8812e72b3324965ad2f7d4bbdad/src/nupic/swarming/permutations_runner.py#L1212-L1267
tfidf_python_100_1.0,how to read .csv file in an efficient way?,python,"def Parse(self, how):
        '''Parse the message.
        '''
        if type(how) == types.ClassType: how = how.typecode
        return how.parse(self.body_root, self)",https://github.com/rameshg87/pyremotevbox/blob/123dffff27da57c8faa3ac1dd4c68b1cf4558b1a/pyremotevbox/ZSI/parse.py#L322-L326
tfidf_python_100_1.0,how to read .csv file in an efficient way?,python,"def csv(
            self,
            dirPath=None):
        """"""*Render the results in csv format*

        **Key Arguments:**
            - ``dirPath`` -- the path to the directory to save the rendered results to. Default *None*

        **Return:**
            - `csvSources` -- the top-level transient data
            - `csvPhot` -- all photometry associated with the transients
            - `csvSpec` -- all spectral data associated with the transients
            - `csvFiles`  -- all files associated with the matched transients found on the tns

        **Usage:**

            To render the results in csv format:

            .. code-block:: python

                csvSources, csvPhot, csvSpec, csvFiles  = tns.csv()
                print csvSources

            .. code-block:: text

                TNSId,TNSName,discoveryName,discSurvey,raSex,decSex,raDeg,decDeg,transRedshift,specType,discMag,discMagFilter,discDate,objectUrl,hostName,hostRedshift,separationArcsec,separationNorthArcsec,separationEastArcsec
                2016asf,SN2016asf,ASASSN-16cs,ASAS-SN,06:50:36.73,+31:06:45.36,102.6530,31.1126,0.021,SN Ia,17.1,V-Johnson,2016-03-06 08:09:36,http://wis-tns.weizmann.ac.il/object/2016asf,KUG 0647+311,,0.66,0.65,-0.13

            You can save the results to file by passing in a directory path within which to save the files to. The four flavours of data (sources, photometry, spectra and files) are saved to separate files but all data can be assoicated with its transient source using the transient's unique `TNSId`.

            .. code-block:: python

                tns.csv(""~/tns"")

            .. image:: https://i.imgur.com/BwwqMBg.png
                :width: 800px
                :alt: csv output
        """"""

        if dirPath:
            p = self._file_prefix()
            csvSources = self.sourceResults.csv(
                filepath=dirPath + ""/"" + p + ""sources.csv"")
            csvPhot = self.photResults.csv(
                filepath=dirPath + ""/"" + p + ""phot.csv"")
            csvSpec = self.specResults.csv(
                filepath=dirPath + ""/"" + p + ""spec.csv"")
            csvFiles = self.relatedFilesResults.csv(
                filepath=dirPath + ""/"" + p + ""relatedFiles.csv"")
        else:
            csvSources = self.sourceResults.csv()
            csvPhot = self.photResults.csv()
            csvSpec = self.specResults.csv()
            csvFiles = self.relatedFilesResults.csv()
        return csvSources, csvPhot, csvSpec, csvFiles",https://github.com/thespacedoctor/transientNamer/blob/39be410c84275ed4669632f5df67e728d66a318f/transientNamer/search.py#L226-L280
tfidf_python_100_1.0,how to read .csv file in an efficient way?,python,"def csv_to_ods(trans_csv, meta_csv, local_ods):
    """"""
    Converts csv files to one ods file
    :param trans_csv: path to csv file with translations
    :param meta_csv: path to csv file with metadata
    :param local_ods: path to new ods file
    """"""
    trans_reader = UnicodeReader(trans_csv)
    meta_reader = UnicodeReader(meta_csv)

    ods = ODS()

    trans_title = trans_reader.next()
    meta_reader.next()

    _prepare_ods_columns(ods, trans_title)

    for i, (trans_row, meta_row) in enumerate(izip(trans_reader, meta_reader)):
        _write_row_into_ods(ods, 0, i, trans_row)
        _write_row_into_ods(ods, 1, i, meta_row)

    trans_reader.close()
    meta_reader.close()

    ods.save(local_ods)",https://github.com/VorskiImagineering/C3PO/blob/e3e35835e5ac24158848afed4f905ca44ac3ae00/c3po/converters/po_ods.py#L142-L166
tfidf_python_100_1.0,how to read .csv file in an efficient way?,python,"def __init__(self, csv):
        InstrumentCSVResultsFileParser.__init__(self, csv)
        self._currentresultsheader = []
        self._currentanalysiskw = ''
        self._numline = 0",https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/exportimport/instruments/shimadzu/gcms/tq8030.py#L93-L97
tfidf_python_100_1.0,how to read .csv file in an efficient way?,python,"def import_csv(config, csv_id):
    csv = config.csvdb.get_csv(csv_id)
    with csv.get_import_table() as it:
        it.import_table()
    return csv.database.filename",https://github.com/adamjaso/pyauto/blob/b11da69fb21a49241f5ad75dac48d9d369c6279b/csvdb/pyauto/csvdb/commands.py#L5-L9
